{"text": "\n\\section{Introduction}\n\nWe present \\texttt{scikit-dimension}, an open-source Python package for global and local intrinsic dimension (ID) estimation. The package has two main objectives: (i) foster research in ID estimation by providing code to benchmark algorithms and a platform to share algorithms; (ii) democratize the use of ID estimation by provigin user-friendly implementations of  algorithms using Scikit-Learn application programming interface (API) \\citep{pedregosa2011scikit}.\n\nID intuitively refers to the minimum number of parameters required to represent a dataset with satisfactory accuracy. The meaning of ``accuracy\" can be different among various approaches. ID can be more precisely defined to be $n$ if the data lies closely to a $n$-dimensional manifold embedded in $R^d$ with little information loss, which corresponds to the so called ``manifold hypothesis\" \\citep{bishop1995neural,Fuku1982}. ID can be, however, defined without assuming the existence of a data manifold. In this case, data point cloud characteristics (e.g., linear separability or pattern of covariance) are compared to a model $n$-dimensional distribution (e.g., uniformly sampled $n$-sphere or $n$-dimensional isotropic Gaussian distribution) and the term ``effective dimensionality\" is sometimes used as such $n$ which gives most similar characteristics to the one measured in the studied point cloud \\citep{albergante2019estimating,del2020effective}. In \\texttt{scikit-dimension}, these two notions are not distinguished. \n\nThe knowledge of ID is important to determine the choice of machine learning algorithm and anticipate the uncertainty of its predictions. The well-known \\textit{curse of dimensionality}, which states that many problems become exponentially difficult in high  dimensions, does not depend on the number of features but on the dataset\u2019s ID \\citep{jiang2018trust}. More precisely, the effects of dimensionality curse are expected to be manifested when $ID \\gg ln(M)$, where $M$ is the number of data points \\citep{Bac2020,hino2017ider}.\n\nCurrent ID estimators have diverse operating principles (we refer the reader to \\citep{Campadelli2015} for an overview). Each ID estimator is developed based on a selected feature (such as number of data points in a sphere of fixed radius, linear separability or expected normalized distance to the closest neighbour) which scales with $n$: therefore, various ID estimation methods provide different ID values. Each dataset can be characterized by a unique \\textit{dimensionality profile} of ID estimations according to different existing methods, which can serve as an important signature for choosing the most appropriate data analysis method.\n\nDimensionality estimators that provide a single ID value for the whole dataset belong to the category of global estimators. However, datasets can have complex organizations and contain regions with varying dimensionality \\citep{Bac2020}. In such a case, they can be explored using local estimators, which estimate ID in local neighborhoods around each point. The neighbourhoods are typically defined by considering the $k$ closest neighbours. Such approaches also allow repurposing global estimators as local estimators. \n\nThe idea behind local ID estimation is to operate at a scale where the data manifold can be approximated by its tangent space \\citep{Camastra2016}. In practice, ID is sensitive to scale and choosing neighbourhood size is a trade-off between opposite requirements \\citep{Little,Campadelli2015}: ideally, the neighbourhood should be big relative to the scale of the noise, and contain enough points. At the same time, it should be small enough to be well approximated by a flat and uniform tangent space.\n\nWe perform benchmarking of 19 ID estimators on a large collection of real-life and synthetic datasets. Previously, estimators were benchmarked based mainly on artificial datasets representing uniformly sampled manifolds with known ID \\cite{albergante2019estimating,Hein,Campadelli2015}, comparing them for the ability to estimate the ID value correctly. Several ID estimators were used on real-life datasets to evaluate the degree of dimensionality curse in a study of various metrics in data space\\cite{mirkes2020entropy}. Here we benchmark ID estimation methods focusing on their applicability to a wide range of datasets of different origin, configuration and size. We also look at how different ID estimations are correlated, and show how \\texttt{scikit-dimension} can be used to derive a consensus measure of data dimensionality, by averaging multiple individual measures. The latter can be a robust measure of data dimensionality in various applications.\n\n\\texttt{scikit-dimension} was applied in several recent studies for estimating intrinsic dimensionality of real-life datasets \\citep{Golovenkin2020,Zinovyev2021CellCycleModeling}.\n \n\\section{Materials and Methods}\n\n\\subsection{Software features}\n\n\\texttt{scikit-dimension} consists of two modules. The \\textit{id} module provides ID estimators and the \\textit{datasets} module provides synthetic benchmark datasets.\n\n\\subsubsection{\\textit{id} module}\nThe \\textit{id} module contains estimators based on:\n\\begin{itemize}\n\\item correlation (fractal) dimension (id.CorrInt) \\citep{Grassberger1983}\n\\item manifold-adaptive fractal dimension (id.MADA) \\citep{farahmand2007manifold}\n\\item method of moments (id.MOM) \\citep{amsaleg2018extreme}\n\\item principal component analysis (id.lPCA) \\citep{jackson1993stopping,Fukunaga1971,Fuku1982,Fan}\n\\item maximum likelihood (id.MLE)\n\\citep{hill1975simple,Levina2004,haro2008translated}\n\\item minimum spanning trees (id.KNN) \\citep{Carter2010a}\n\\item estimators based on concentration of measure (id.MiND\\_ML, id.DANCo, id.ESS, id.TwoNN, id.FisherS, id.TLE) \\citep{rozza2012novel,DANCo,Kerstin,Facco2017a,albergante2019estimating,GORBAN2018303,amsaleg2019intrinsic}.\n\\end{itemize}\n\nThe description of the method principles is provided together with the package documentation at \\url{https://scikit-dimension.readthedocs.io/} and in reviews \\citep{Hein,Bac2020,del2020effective}. \n\n\\subsubsection{\\textit{datasets} module}\nThe \\textit{datasets} module allows user to test estimators on synthetic datasets. It can generate several low-dimensional toy datasets to play with different estimators as well as a set of synthetic manifolds commonly used to benchmark ID estimators, introduced by \\citep{Hein} and further extended in \\citep{rozza2012novel,Campadelli2015}.\n\n\\end{paracol}\n\n\\begin{figure}[H]\n\\widefigure\n\\includegraphics[width=16cm]{plotly_dimension_code.png}\n\\caption{Example usage: generating the Line-Disk-Ball dataset \\citep{hino2017ider}) which has clusters of varying local ID, and coloring points by estimates of local ID obtained by id.lPCA. }\n\\end{figure}\n\\begin{paracol}{2}\n\\switchcolumn\n\n\n\n\\subsection{Development}\n\n\\texttt{scikit-dimension} is built according to the scikit-learn API \\citep{pedregosa2011scikit} with support for Linux, MacOS, and Windows and Python $>=$ 3.6. Code style and API design is based on the guidelines of scikit-learn, with NumPy \\citep{harris2020array} documentation format, and continuous integration on all three platforms. The online documentation is built using  \\hyperlink{https://www.sphinx-doc.org/}{Sphinx} and hosted with \\hyperlink{https://readthedocs.org/}{ReadTheDocs}.\n\n\n\\subsection{Dependencies}\n\n\\texttt{scikit-dimension} depends on a limited number of external dependencies on the user side for ease of installation and maintenance:\n\\begin{itemize}\n\\item Matplotlib \\citep{Hunter:2007} \\item Pandas \\citep{reback2020pandas}\n\\item Scikit-learn \\citep{pedregosa2011scikit}\n\\item Numba \\citep{lam2015numba}\n\\item SciPy \\citep{2020SciPy-NMeth} \\item NumPy \\citep{harris2020array} \n\\end{itemize}\n\n\\subsection{Related software}\n\nRelated open source software for ID estimation have previously been developed in different languages such as R, MATLAB or C++ and contributed to the development of \\texttt{scikit-dimension}.\n    \nIn particular, \\citep{intrinsicDimension,hino2017ider,you2020rdimtools}, provide extensive collections of ID estimators and datasets for R users, with \\citep{you2020rdimtools} additionally focusing on dimension reduction algorithms. Similar resources can be found for MATLAB users \\citep{IntDim,IDLombardi,drtoolbox,TLE}. Benchmarking many of the methods for ID estimation included in this package was performed in \\cite{mirkes2020entropy}. Finally there exist several packages implementing standalone algorithms; in particular for Python, we refer the reader to complementary implementations of the GeoMLE, full correlation dimension, GraphDistancesID algorithms \\citep{geomle, pyFCI, graphID}.\n\nTo our knowledge, \\texttt{scikit-dimension} is the first Python implementation of an extensive collection of ID methods. Compared to similar efforts in other languages, the package puts emphasis on estimators quantifying various properties of high-dimensional data geometry such as concentration of measure. It is the only package to include ID estimation based on linear separability of data, using Fisher discriminants \\citep{albergante2019estimating, bac2020local, gorban2019unreasonable,GORBAN2018303}. \n\n\n\\section{Results}\n\n\\subsection{Benchmarking \\texttt{scikit-dimension} on a large collection of datasets}\n\nIn order to demonstrate applicability of \\texttt{scikit-dimension} to a wide range of real-life datasets of various configurations and sizes, we performed a large-scale benchmarking of \\texttt{scikit-dimension} using the collection of datasets from \\texttt{OpenML} repository \\cite{OpenML2013}. We selected those datasets having at least 1000 observations and 10 features, without missing values. We excluded those datasets which were difficult to fetch either because of their size or an error in the \\texttt{OpenML} API. After filtering out repetitive entries, 499 datasets were collected. The number of observations in them varied from 1002 to 9199930 and the number of features varied from 10 to 13196. We focused only on numerical variables, and we subsampled the number of rows in the matrix to maximum 100000. All dataset features were scaled to unit interval using Min/Max scaler in Python. In addition, we filtered out approximate non-unique columns and rows in the data matrices since some of the ID methods could be affected by the presence of identical (or approximately identical) rows or columns.\n\nWe added to the collection 18 datasets, containing single-cell transcriptomic measurements, from the CytoTRACE study\\citep{CytoTRACE2000} and 4 largest datasets from The Cancer Genome Atlas (TCGA), containing bulk transcriptomic measurements. Therefore, our final collection contained 521 datasets altogether.\n\n\\subsubsection{\\texttt{scikit-dimension} ID estimator method features}\n\nWe systematically applied 19 ID estimation methods from \\texttt{scikit-dimension}, with default parameter values, including 7 methods based on application of principal component analysis (\"linear\" or PCA-based ID methods), and 12 based on application of various other principles, including correlation dimension and concentration of measure-based methods (\"nonlinear\" ID methods). \n\nFor KNN and MADA methods we had to further subsample the data matrix to maximum 20000 rows, otherwise they were too greedy in terms of memory consumption. Moreover, DANCo and ESS methods appeared to be too slow, especially in the case of a large number of variables: therefore, we made ID estimations in these cases on small fragments of data matrices. Thus, for DANCo the maximum matrix size was set to 10000x100, and for ESS to 2000x20. The number of features was reduced for these methods when needed, by using PCA-derived coordinates, and the number of observations were reduced by random subsampling.\n\nIn the Table~\\ref{tab1}, we provide the summary of characteristics of the tested methods. In more detail, the following method features have been evaluated (see Figure~\\ref{MethodPerformance}). \n\nFirstly, we simply looked at the ranges of ID values produced by the methods across all the datasets. These ranges varied significantly between the methods, especially for the linear ones (Figure~\\ref{MethodPerformance},A).\n\nSecondly, we tested the methods with respect to their ability to successfully compute ID as a positive finite value. It appeared that certain methods (such as MADA and TLE), in a certain number of cases produced a significant fraction of uninterpretable estimates (such as ``nan\" or negative value), Figure~\\ref{MethodPerformance},B. We assume that in most of such cases, the problem with ID estimation is caused by the method implementation, not anticipating certain relatively rare data point configurations, rather than the methodology itself, and a reasonable ID estimate always exists. Therefore, in case of a method implementation returning uninterpretable value, for further analysis, we considered it possible to impute the ID value from the results of application of other methods, see below.\n\n\\end{paracol}\n\\begin{table}[H] \n\\small\n\\caption{Summary table of ID methods characteristics. The qualitative score changes from \"- - -\" (worst) to \"+++\" (best).\\label{tab1}}\n\\begin{tabularx}{\\textwidth}{XXXXXXXX}\n\\toprule\n\\textbf{Method name}& \\textbf{Short \\newline name(s)}\t&\\textbf{Ref(s)}& \\textbf{Valid \\newline result} & \\textbf{Insensitivity \\newline to redundancy} & \\textbf{Uniform \\newline ID estimate \\newline in similar \\newline datasets} & \\textbf{Performance \\newline with many \\newline observations} & \\textbf{Performance \\newline with many \\newline features} \\\\\n\\midrule\nPCA Fukunaga-Olsen&PCA FO, PFO&\\citep{Fukunaga1971,mirkes2020entropy}&+++&+++&+++&+++&+++\\\\\nPCA Fan&PFN&\\citep{Fan}&+++&+++&+++&+++&+++\\\\\nPCA maxgap&PMG&\\citep{Johnsson2015}&+++&- - -&+&+++&+++\\\\\nPCA ratio&PRT&\\citep{jolliffe2002principal}&+++&+++&+&+++&+++\\\\\nPCA participation ratio&PPR&\\citep{jolliffe2002principal}&+++&+++&++&+++&+++\\\\\nPCA Kaiser&PKS&\\citep{Kaiser1960TheAO,Giuliani2017}&+++&-&+++&+++&+++\\\\\nPCA broken stick&PBS&\\citep{FRONTIER197667,Cangelosi2007}&+++&- -&+++&+++&+++\\\\\nCorrelation (fractal) dimensionality&CorrInt, CID&\\citep{Grassberger1983}&+&+++&++&+&+\\\\\nFisher separability&FisherS, FSH&\\citep{GORBAN2018303,albergante2019estimating}&++&+++&+++&++&+++\\\\\nK-nearest neighbours&KNN&\\citep{Carter2010a}&++&- -&- -&-&++\\\\\nManifold-adaptive fractal dimension&MADA, MDA&\\citep{farahmand2007manifold}&-&+++&+++&-&+\\\\\nMinimum neighbor distance\u2014ML&MIND\\_ML,\\newline MMk, \\newline  MMi&\\citep{rozza2012novel}&+++&+++&++&++&+\\\\\nMaximum likelihood&MLE&\\citep{Levina2004}&++&+++&++&++&+\\\\\nMethods of moments&MOM&\\citep{amsaleg2018extreme}&+++&+++&+++&++&+\\\\\nEstimation within tight localities&TLE&\\citep{amsaleg2019intrinsic}&- -&+++&+++&++&+\\\\\nMinimal neighborhood information&TwoNN,\\newline TNN&\\citep{Facco2017a}&++&+++&+++&++&+++\\\\\nAngle and norm concentration&DANCo,\\newline DNC&\\citep{DANCo}&+&+++&+++&- - -&- - -\\\\\nExpected simplex skewness&ESS&\\citep{Johnsson2015}&+++&+++&+++&- - -&- - -\\\\\n\\bottomrule\n\\end{tabularx}\n\\end{table}\n\\begin{paracol}{2}\n\\switchcolumn\n\nThirdly, for a small number of datasets we performed a test of their sensitivity to the presence of strongly redundant features. For this purpose, we duplicated all features in a matrix and recomputed ID. The resulting sensitivity is the ratio between the ID computed for the larger matrix and the ID computed for the initial matrix, having no duplicated columns. It appears that despite most of the methods being robust with respect to such matrix duplication, some (such as PCA-based broken stick or the famous Kaiser methods popular in various fields such as biology \\citep{Giuliani2017,Cangelosi2007}) tend to be very sensitive, Figure~\\ref{MethodPerformance},C, which is compliant with some previous reports \\citep{mirkes2020entropy}.\n\nSome of the datasets in our collection could be combined in homogeneous groups according to their origin, such as the data coming from Quantitative structure\u2013activity relationship (QSAR)-based quantification of a set of chemicals. The size of the QSAR fingerprint for the molecules is the same in all such datasets (1024 features): therefore, we could assume that the estimate of ID should not vary too much across the datasets from the same group. We computed the coefficient of variation of ID estimate across three such dataset groups, which revealed that certain methods tend to provide less stable estimations than the others, Figure~\\ref{MethodPerformance},D.\n\nFinally, we recorded the computational time needed for each method. We found that the computational time could be estimated with good precision ($R^2>0.93$ for all ID estimators) using multiplicative model: $Time = c\\times N_{obj}^\\alpha \\times N_{var}^\\beta$, where $N_{obj}$ and $N_{var}$ are number of objects and features in a dataset, correspondingly. Using this model fit for each method, we estimated the time needed to estimate ID for data matrices of four characteristic sizes, Figure~\\ref{MethodPerformance},E. \n\n\\end{paracol}\n\\begin{figure}[H]\t\n\\widefigure\n\\includegraphics[width=16cm]{images/Figure_MethodsPerformance_small.png}\n\\caption{Illustrating different ID method general characteristics: A) range of estimated ID values; B) ability to produce interpretable (positive finite value) result; C) sensitivity to feature redundancy (after duplicating matrix columns); D) uniform ID estimation across datasets of similar nature; E) computational time needed to compute ID for matrices of four characteristic sizes.\\label{MethodPerformance}}\n\\end{figure}  \n\\begin{paracol}{2}\n\\switchcolumn\n\n\\end{paracol}\n\\begin{figure}[H]\n\\widefigure\n\\includegraphics[width=16cm]{images/Figure_MethodsMetaanalysis_small.png}\n\\caption{Characterizing \\texttt{OpenML} dataset collection in terms of ID estimates. A) PCA visualizations of datasets characterized by vectors of 19 ID measures. Size of the point corresponds to the logarithm of the number of matrix entries ($N_{obj}\\times N_{var}$). The color corresponds to the mean ID estimate taken as the mean of all ID measure z-scores. B) Loadings of various methods into the first and the second principal component from A). C) Visualization of the mean ID score as a function of data matrix shape. The color is the same as in A). D) Correlation matrix between different ID estimates computed over all analyzed datasets. \\label{MethodMetaanalysis}}\n\\end{figure}\n\\begin{paracol}{2}\n\\switchcolumn\n\n\\subsubsection{\\texttt{scikit-dimension} ID estimates metanalysis}\n\nAfter application of \\texttt{scikit-dimension}, each dataset was characterized by a vector of 19 measurements of intrinsic dimensionality. The resulting matrix of ID values contained 2.5\\% missing values which were imputed using the standard IterativeImputer from \\texttt{sklearn} Python package. \n\n\\newpage\n\n\\end{paracol}\n\\begin{figure}[H]\n\\widefigure\n\\includegraphics[width=15cm]{images/Figure_UMAP_small.png}\n\\caption{A gallery of UMAP plots computed for a selection of datasets from \\texttt{OpenML} collection, with indication of ID estimates, ranked by the ID value estimated using Fisher separability-based method (indicated in the left top corner). The ambient dimension of the data (number of features $N_{var}$) is indicated in the bottom left corner, and the color reflects the $ID/N_{var}$ ratio, from red (close to 0.0 value) to green (close to 1.0). On the right from the UMAP plot, all 19 ID measures are indicated, with color mapped to the value range, from green (small dimension) to red (high dimension). \\label{UMAP}}\n\\end{figure}\n\\begin{paracol}{2}\n\\switchcolumn\n\nUsing the imputed matrix and scaling it to z-scores, we performed principal component analysis (Figure~\\ref{MethodMetaanalysis},A,B). The first principal component explained 42.6\\% percent of the total variance in ID estimations, with all of the methods having positive and comparable loadings to the first principal component. This justifies the computation of the ``consensus\" intrinsic dimension measure, which we define here as the mean value of individual ID estimate z-scores. Therefore, the mean ID can take negative or positive values, roughly dividing the datasets into ``lower-dimensional\" and ``higher-dimensional\" (Figure~\\ref{MethodMetaanalysis},A,C). The consensus ID estimate weakly negatively correlated with  with the number of observations (Pearson $\\rho=-0.25$, p-value=$10^{-9}$) and positively correlated with the number of features in the dataset (r=0.44, p-value=$10^{-25}$). Nevertheless, even for the datasets with similar matrix shapes, the mean ID estimate could be quite different (Figure~\\ref{MethodMetaanalysis},C). \n\nThe second principal component explained 21.3\\% of the total variance in ID estimates. The loadings of this component roughly differentiated between PCA-based ID estimates and ``non-linear\" ID estimation methods, with one exception in the case of the KNN method. \n\nWe computed the correlation matrix between the results of application of different ID methods (Figure~\\ref{MethodMetaanalysis},D), which also distinguished two large groups of PCA-based and ``non-linear\" methods. Furthemore, non-linear methods were split into the group of methods producing results similar to correlation (fractal) dimension (CorrInt, MADA, MOM, TwoNN, MLE, TLE) and methods based on concentration of measure phenomena (FisherS, ESS, DANCo, MiND\\_ML). \n\nIn order to illustrate the relation between the dataset geometry and the intrinsic dimension, we produced a gallery of Uniform Manifold Approximation and Projection (UMAP) dataset visualizations, with indication of the ambient dataset dimension (number of features) and the estimated ID using all methods, Figure~\\ref{UMAP}. One of the conclusions that can be made from this analysis is that UMAP visualization is not insightfull for truely high-dimensional datasets (starting from ID=10 estimated by FisherS method). Also, some datasets having large ambient dimensions, were characterized by low ID, by most of the methods (e.g., 'hill-valley' dataset). \n\n\n\\section{Conclusions}\n\n\\texttt{scikit-dimension} is the first to our knowledge package implemented in Python, containing implementations of most used estimators of data intrinsic dimensionality. \n\nBenchmarking \\texttt{scikit-dimension} on a large collection of real-life and synthetic datasets revealed that different estimators of intrinsic dimensionality possess internal consistency and that the ensemble of ID estimators allows us to achieve more robust classification of datasets into low- or high-dimensional.\n\nFuture releases of \\texttt{scikit-dimension} will continuously seek to incorporate new estimators and benchmark datasets introduced in the literature, or new features such as alternative nearest neighbor search for local ID estimates. The package will also include new ID estimators which can be derived using most recent achievements in understanding the properties of high-dimensional data geometry\\citep{Gorban2021HighDimSepar,GRECHUK202133}.\n\n\n\\vspace{6pt} \n\n\n\n\\authorcontributions{Conceptualization, J.B., A.Z., I.T., A.N.G.; methodology, J.B., E.M., A.N.G.; software, J.B., E.M. and A.Z.; formal analysis, J.B., E.M., A.Z.; data curation, J.B. and A.Z.; writing---original draft preparation, J.B. and A.Z.; writing---review and editing, all authors; supervision, A.Z. and A.N.G. All authors have read and agreed to the published version of the manuscript.}\n\n\\funding{The work was supported by the Ministry of Science and Higher Education of the Russian Federation (Project No. 075-15-2021-634), by the French government under management of Agence Nationale de la Recherche as part of the ``Investissements d\u2019Avenir\" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), by the Association Science et Technologie, the Institut de Recherches Internationales Servier and the doctoral school Fronti\u00e8res de l\u2019Innovation en Recherche et Education Programme Bettencourt. I.T. was supported by the UKRI Turing AI Acceleration Fellowship (EP/V025295/1).}\n\n\n\n\\dataavailability{The datasets used in this study were retrieved from public sources, namely \\texttt{OpenML} repository, CytoTRACE web-site \\url{https://cytotrace.stanford.edu/} (section \"Downloads\"), from the Data Portal of National Cancer Institute \\url{https://portal.gdc.cancer.gov/}.} \n\n\n\\conflictsofinterest{The authors declare no conflict of interest.} \n\n\n\n\n\n\\end{paracol}\n\n\n\\reftitle{References}\n\n\n\\externalbibliography{yes}\n", "meta": {"timestamp": "2021-09-07T02:38:33", "yymm": "2109", "arxiv_id": "2109.02596", "language": "en", "url": "https://arxiv.org/abs/2109.02596"}}
{"text": "\\section{Introduction}\r\n\r\nLet $\\mathbb A$ be a Noetherian commutative ring with identity. All\r\nthe (co)homology groups  occurring in this paper will be with\r\n$\\mathbb A$-coefficients.\r\n\r\n\r\nConsider a resolution of singularities $f:X\\to Y$ of a complex\r\nquasi-projective variety $Y$ of dimension $n$. When $\\mathbb A$ is a field,\r\nthe Decomposition Theorem \\cite[p. 161]{Dimca2} implies there exists a certain\r\ndecomposition\r\n\\begin{equation}\\label{splittingintro}\r\nRf_*\\mathbb A_X[n] \\cong  IC_Y^{\\bullet} \\oplus \\mathcal H\r\n\\end{equation}\r\nin $D_{c}^{b}(Y)$, the derived category of bounded constructible\r\ncomplexes of $\\mathbb A$-sheaves in $Y$. If $Y$ is an $\\mathbb\r\nA$-homology manifold \\cite{Brasselet}, then $IC_Y^{\\bullet}\\cong \\mathbb A_Y[n]$.\r\nHence, we get\r\n\\begin{equation}\\label{splittingintro2}\r\nRf_*\\mathbb A_X[n] \\cong \\mathbb A_Y[n] \\oplus \\mathcal H.\r\n\\end{equation}\r\n\r\nOne of our purposes is to extend the splitting\r\n(\\ref{splittingintro2}) to every ring $\\mathbb A$, for which the\r\nDecomposition Theorem providing (\\ref{splittingintro}) is not\r\navailable. Specifically, we will see that the splitting\r\n(\\ref{splittingintro2}) is equivalent to the existence of a\r\n\\textit{bivariant class of degree one} for $f$, that we are about to\r\ndefine.\r\n\r\nBivariant Theory was introduced in early 1980 by W. Fulton and R.\r\nMacPherson \\cite{FultonCF}, for the purpose of unifying covariant\r\nand contravariant theories. The sheaf-theoretic bivariant homology\r\ntheory associates to a continuous map of topological spaces\r\n$X\\stackrel{f}{\\to} Y$,  the graded group with homogeneous\r\ncomponents\r\n\\begin{equation*}\r\nH^i (X\\stackrel{f}{\\to}  Y)={\\rm{Hom}}_{D^b_c(Y)}(f_!\\mathbb A_X,\r\n\\mathbb A_Y[i]),\r\n\\end{equation*}\r\nwhose elements are called bivariant classes. Bivariant Theory allows\r\na systematic study of generalized wrong-way Gysin morphisms. These\r\nmorphisms find a great use especially in the study of morphisms of\r\nsmooth varieties or, more generally, of locally complete\r\nintersection morphisms.\r\n\r\nIn some cases, a bivariant class determines a very interesting\r\nsplitting in the derived category \\cite[p. 327]{Jouanolou}. Another\r\npurpose of this paper is to show that a similar splitting can be\r\nproved in a more general context, and that the natural definition\r\ninvolved is that of bivariant class of degree one. It is worthy to\r\nstress that such a splitting turns out to be compatible with\r\nPoincar\\'e  Duality (Corollary 5.1). Consider a bivariant class\r\nbelonging to the $0$-th homogeneous component (now we assume $f:X\\to\r\nY$ is proper):\r\n$$\\theta \\in H^0(X\\stackrel{f}\\to Y)={\\rm{Hom}}_{D^b_c(Y)}(f_*\\mathbb A_X, \\mathbb A_Y).$$ Let\r\n$\\theta_0:H^0(X)\\to H^0(Y)$  be the induced Gysin morphism. We say\r\nthat $\\theta$ \\textit{has degree one} (for $f$) if\r\n$\\theta_0(1_X)=1_Y$. This is equivalent to say that $\\theta$ is a\r\nsection of the pull-back $f^*: \\mathbb A_Y\\to Rf_*\\mathbb A_X$, i.e.\r\n$\\theta\\circ f^*={\\text{id}}_{\\mathbb A_Y}$. We will see that the\r\nexistence of a bivariant class of degree one leads to a suitable\r\nsplitting in the derived category. Consequently, we deduce a series\r\nof isomorphisms for (co)homology groups, which extend classic\r\nformulas of the blowing-up, and that we have extensively used in\r\nNoether-Lefschetz Theory \\cite{RCMP}.\r\n\r\nExamples of morphisms admitting a bivariant class of degree one are\r\nblowing-ups at locally complete intersection subvarieties. Indeed,\r\nthe {\\it orientation class} of the blowing-up \\cite[p.\r\n114]{FultonIT}, \\cite[p. 131]{FultonCF} is a bivariant class of\r\ndegree one (Remark \\ref{remintro}, $(iii)$). Other examples are\r\n\\textit{strong orientation classes} (of codimension $0$) $\\theta\\in\r\nH^0(X\\stackrel{f}\\to Y)$ \\cite[p. 27]{FultonCF}, \\cite[p.\r\n803]{Brasselet}, for maps $f$ between varieties of the same\r\ndimension (Corollary 6.3). We will see that the class of birational\r\nmorphisms admitting a bivariant class of degree one is considerably\r\nbroader than the class admitting strong orientation classes. For\r\ninstance, any blowing-up at a locally complete intersection\r\nsubvariety admits a bivariant class $\\theta$ of degree one, but it\r\nis rather unlikely that $\\theta$ is a strong orientation when the\r\ncenter is not smooth (Remark 6.5).\r\n\r\n\r\nWe study this circle of questions for a morphism $f: X\\to Y$\r\nwhich is a resolution of singularities of a complex quasi-projective\r\nvariety $Y$ or, more generally, for a morphism from an $\\mathbb\r\nA$-homology manifold $X$, which is an isomorphism on a non-empty\r\nopen subset $U\\cong f^{-1}(U) \\subset X$. Our main results are\r\ncollected in the following two theorems. The first one, together\r\nwith its consequences for the (co)homology (see Section $4$), should\r\nbe compared  with \\cite[p. 114-118]{FultonIT}, \\cite[p.\r\n327]{Jouanolou}, \\cite[p. 263]{RCMP}, where similar results appear\r\nin the study of the behavior of the (co)homology and of the Chow\r\ngroups, under blowing-up at a locally complete intersection\r\nsubvariety of a quasi-projective variety. The second theorem gives,\r\nas far as we know, a new characterization of homology manifolds, in\r\nterms of their resolution of singularities.\r\n\r\n\\medskip\r\n\\begin{theorem}\r\n\\label{Crossintro} Let $f:X\\to Y$ be a  continuous and proper map,\r\nwith $Y$ path-connected. Let $U\\subseteq Y$ be a non-empty open\r\nsubset such that $f$ induces an homeomorphism $f^{-1}(U)\\cong U$.\r\nSet $W= Y\\backslash U$, and $\\widetilde W=f^{-1}(W)$. The following\r\nproperties are equivalent.\r\n\r\n\\smallskip\r\n$(i)$ There exists a bivariant class $\\theta\\in\r\nHom_{D^{b}_{c}(Y)}(Rf_*\\mathbb A_X, \\mathbb A_Y)$ of degree one.\r\n\r\n\\smallskip $(ii)$ In $D^{b}_{c}(Y)$ there exists a cross isomorphism\r\n$Rf_*\\mathbb A_X\\oplus \\mathbb A_W\\cong Rf_*\\mathbb\r\nA_{\\widetilde W}\\oplus \\mathbb A_Y$.\r\n\r\n\r\n\r\n\\smallskip\r\n$(iii)$ In $D^{b}_{c}(Y)$ there exists a decomposition\r\n$Rf_*\\mathbb A_X \\cong \\mathbb A_Y \\oplus \\mathcal K$.\r\n\\end{theorem}\r\n\r\n\\medskip\r\n\\begin{theorem}\\label{homology}\r\n\r\nLet $f:X\\to Y$ be a projective birational morphism between\r\ncomplex, irreducible, and quasi-projective varieties\r\nof the same dimension $n$. Let $U$ be a non-empty Zariski open subset of $Y$\r\nsuch that $f$ induces an isomorphism $f^{-1}(U)\\cong U$. Set $W=Y\\backslash U$.\r\n\r\n$\\bullet$ If  $Y$ is an  $\\mathbb\r\nA$-homology manifold, then there exists a bivariant class $\\theta$\r\nin $Hom(Rf _* \\mathbb A _X , \\mathbb A_Y)$\r\nof degree one. In this\r\ncase, $\\theta$ is unique, and there exists a decomposition\r\n$Rf _* \\mathbb A _X\\cong \\mathbb A _Y\\oplus \\mathcal K$,\r\nwith $\\mathcal K$ supported on $W$. Moreover, if also $X$ is an\r\n$\\mathbb A$-homology manifold, then $\\mathcal K[n]$ is self-dual.\r\n\r\n$\\bullet$\r\nConversely, if $X$ is an $\\mathbb A$-homology manifold and there\r\nexists a bivariant class $\\theta \\in Hom(Rf _* \\mathbb A _X ,\r\n\\mathbb A_Y)$ of degree one, then  also $Y$ is an $\\mathbb\r\nA$-homology manifold.\r\n\\end{theorem}\r\n\r\n\r\n\r\n\r\nTheorem \\ref{Crossintro} follows from more general results that hold\r\ntrue in any triangulated categories (Lemma 3.2, Lemma 3.5). The\r\ndecompositions $(ii)$ and $(iii)$ in Theorem \\ref{Crossintro} induce\r\nexplicit isomorphisms in (co)homology (Section 4), that are\r\ncompatible with the cap-product with the fundamental class (Section\r\n5). Using which, one may easily prove Theorem \\ref{homology}. Since\r\n$\\mathcal K[n]$ is self-dual, it follows that the Betti numbers of\r\nthe singular locus ${\\text{Sing}}(Y)$ of $Y$, and of\r\n$f^{-1}({\\text{Sing}}(Y))$, are related (Remark \\ref{remfin},\r\n$(ii)$).\r\n\r\nOther results are obtained along the way. Two of them seem  to us\r\nworthy to note.\r\n\\begin{enumerate}\r\n\\item Suppose that the birational morphism $f: X\\to Y$ admits a strong orientation class  $\\theta\\in\r\nH^0(X\\stackrel{f}\\to Y)$. If one between X and Y is an\r\n$\\As$-homology manifold, then the other is too (Theorem\r\n\\ref{homology}, Corollary 6.3, Proposition 6.4). In this case, every\r\nbirational morphism between $X$ and $Y$ admits a strong orientation\r\nclass.\r\n\\item There are examples of  projective\r\nbirational maps $f:X\\to Y$ such that $H^0(X\\stackrel{f}\\to Y)\\neq\r\n0$, without bivariant classes of degree one (Remark 6.2, $(iii)$).\r\n\\end{enumerate}\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\\bigskip\r\n\\section{Notations.}\r\n\r\n\\bigskip\r\n$(i)$ Let $\\mathbb A$ be a Noetherian commutative ring with identity\r\n(e.g. $\\mathbb A=\\mathbb Z$ or $\\mathbb A=\\mathbb Q$). Every\r\ntopological space $V$ occurring in this paper will be assumed to be\r\nimbeddable as a closed subspace of some $\\mathbb R^N$ \\cite[p.\r\n32]{FultonCF} (e.g. a complex quasi-projective\r\nvariety, with the natural topology, and its open subsets).\r\nMaps between topological spaces are assumed continuous of finite\r\ncohomological dimension \\cite[p. 83]{FultonCF} (e.g.\r\nalgebraic maps between  complex quasi-projective\r\nvarieties, and their restrictions on open subsets). We denote by\r\n$H^{i}(V)$ and $H_{i}(V)$ the cohomology and the Borel-Moore\r\nhomology groups, with $\\mathbb A$-coefficients, of $V$\r\n\\cite{FultonYT}. We denote by ${\\text{Sh}}(V)$ the category of\r\nsheaves of $\\mathbb A$-modules on $V$. Let $D_{c}^{b}(V)$ denote the\r\nderived category of bounded constructible complexes of $\\mathbb\r\nA$-sheaves $\\mathcal{F^{\\bullet}}$ on $V$ \\cite{Dimca2},\r\n\\cite{DeCM3}. The symbol $IC_{V}^{\\bullet}$ represents the\r\nintersection cohomology complex of $V$. If $V$ is a smooth,\r\nirreducible, quasi-projective complex variety of dimension $n$, then\r\n$IC_{V}^{\\bullet}\\cong \\mathbb A_V[n]$, where $\\mathbb A_V$ is the\r\nconstant sheaf.\r\n\r\n\r\n\r\n\\medskip $(ii)$ Let $f:X\\to Y$ be a continuous and proper map.\r\nFix a bivariant class \\cite{FultonCF}\r\n$$\\theta \\in H^0(X\\stackrel{f}\\to Y)\\cong\r\nHom_{D^{b}_{c}(Y)}(Rf_*\\mathbb A_X, \\mathbb A_Y).$$ Let\r\n$\\theta_0:H^0(X)\\to H^0(Y)$ be the induced Gysin homomorphism. We\r\nsay that {\\it $\\theta$ has degree one} (for the map $f$) if\r\n$\\theta_0(1_X)= 1_Y\\in H^0(Y)$ \\cite[p. 238]{Spanier}.\r\n\r\n\r\n\\medskip $(iii)$ Let $V$ be an irreducible, quasi-projective variety\r\nof complex dimension $n$. We say that $V$ is an {\\it $\\mathbb A$-homology\r\nmanifold} if for all $y\\in Y$ and for all $i\\neq 2n$ one has\r\n$H_{i}(Y, Y\\backslash\\{y\\})=0$, and $H_{2n}(Y,\r\nY\\backslash\\{y\\})\\cong \\mathbb A$ \\cite{BM}, \\cite{Brasselet} (by\r\n$H_{i}(Y, Y\\backslash\\{y\\})$ we denote the singular homology of a\r\npair). This is equivalent to say that $\\mathbb A_Y[n]$ is self-dual,\r\nor that $\\mathbb A_Y[n]\\cong IC_{Y}^{\\bullet}$ \\cite[p. 804-805]{Brasselet}.\r\n\r\n\r\n\\medskip $(iv)$ An element $\\theta\\in H^i(X\\stackrel{f}\\to Y)$\r\nis called a {\\it strong orientation of codimension $i$} for the morphism\r\n$f:X\\to Y$ if, for all morphisms $g:Z\\to X$, the morphism\r\n$$\r\n H^{\\bullet}(Z\\stackrel{g}\\to X)\\stackrel{\\bullet\\,\\theta}\\to H^{\\bullet}(Z\\stackrel{f\\circ g}\\to Y)\r\n$$\r\nis an isomorphism \\cite[p. 26]{FultonCF}, \\cite[p. 803]{Brasselet}.\r\n\r\n\r\n\r\n\\bigskip\r\n\\begin{remark}\\label{remintro}\r\n$(i)$ Observe that  {\\it $\\theta$ has degree one if and only if\r\n$\\theta$ is a section of the pull-back $f^*:\\mathbb A_Y\\to\r\nRf_*\\mathbb A_X$}, i.e.\r\n$$\r\n\\theta_0(1_X) =1_{Y} \\iff \\theta\\circ f^*={\\text{id}}_{\\mathbb A_Y}.\r\n$$\r\n\r\nIn fact,  assume that $\\theta$ is of degree one.   For every $y\\in\r\nH^{\\bullet}(Y)$ one has (\\cite[p. 26, (G4), (i)]{FultonCF}, \\cite[Spanier,\r\np. 251, 9]{Spanier}):\r\n$$\r\n\\theta_*(f^*(y))=\\theta_*({1_X}\\cup f^*(y))=\\theta_*(1_X) \\cup\r\ny=1_{Y}\\cup y=y\r\n$$\r\nfor every $y\\in H^{\\bullet}(Y)$. By functoriality, this means that the\r\nmorphism $\\theta \\circ f^*$ induces the identity on the\r\ncohomology groups ${{\\rm{id}}_{H^{\\bullet}(Y)}}=\\theta_*\\circ f^*:H^{\\bullet}(Y)\\to\r\nH^{\\bullet}(Y)$. On the other hand, we have $ \\theta\\circ f^*\\in\r\n{\\rm{Hom}}_{D^b_c(Y)}(\\mathbb A_Y, \\mathbb A_Y)\\cong H^0(Y).$ It\r\nfollows that $\\theta\\circ f^*={\\text{id}}_{\\mathbb A_Y}$.\r\n\r\nConversely, if $\\theta\\circ f^*={\\text{id}}_{\\mathbb A_Y}$, then the\r\ncomposite $ H^0(Y)\\stackrel{f^*}\\to H^0(X)\\stackrel{\\theta_0}\\to\r\nH^0(Y)$ is the identity of $H^0(Y)$. Since $f^*(1_Y)=1_X$, it\r\nfollows that $\\theta_0(1_X)=1_{Y}$, i.e. $\\theta$ has degree one.\r\n\r\n\r\n\\smallskip\r\n$(ii)$ Let $f:X\\to Y$ be a proper map. Let $\\theta\r\n\\in H^0(X\\stackrel{f}\\to Y)$ be a bivariant class. If\r\n$\\theta_0(1_X)=d\\cdot 1_Y\\in H^0(Y)$, and if $d$ is a unit in\r\n$\\mathbb A$, then $d^{-1}\\cdot\\theta$ is a bivariant class of degree\r\none. Moreover, let $i:W\\subseteq Y$ be a non-empty subspace of $Y$,\r\nand let $g:f^{-1}(W)\\to W$ be the restriction of $f$ on $f^{-1}(W)$.\r\nDenote by $\\theta'=i^*(\\theta)\\in H^0(f^{-1}(W)\\stackrel{g}\\to W)$\r\nthe pull-back of $\\theta$. By \\cite[(G2), $(ii)$, p. 26]{FultonCF}, we\r\nsee that $i^*\\theta_0(1_X)=\\theta'_0j^*(1_X)$, where\r\n$j:f^{-1}(W)\\subseteq X$ denotes the inclusion. Therefore,\r\n$1_W=\\theta'_0(1_{f^{-1}(W)})\\in H^0(W)$. This proves that the\r\npull-back of a bivariant class of degree one, is again of degree\r\none. And, conversely, if $Y$ is path-connected, and  $\\theta'$ is of\r\ndegree one, then also $\\theta$ is of degree one.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\\smallskip\r\n$(iii)$ Assume that $f:X\\to Y$ is a projective, locally complete\r\nintersection morphism between complex irreducible quasi-projective\r\nvarieties, and that $f$ is birational (e.g. $f$ is the blowing-up of\r\n$Y$ at a locally complete intersection subvariety $W\\subset Y$\r\n\\cite[p. 114]{FultonIT}). Let $\\theta\\in H^0(X\\stackrel{f}\\to Y)$ be\r\nthe {\\it orientation class} of $f$ \\cite[p. 114]{FultonIT}, \\cite[p.\r\n131]{FultonCF}. Then $\\theta$ has degree one. In fact, let $U$ be a\r\nnon-empty Zariski open set of $Y$, such that $f$ induces an\r\nisomorphism $f^{-1}(U)\\cong U$. Let $\\theta'$ be the restriction of\r\n$\\theta$ on $f^{-1}(U)\\to U$. Since $\\theta'$ is the orientation\r\nclass of $f^{-1}(U)\\to U$ \\cite[Lemma 19.2, (a), p. 379]{FultonIT},\r\nand $f^{-1}(U)\\cong U$, it follows that $\\theta'$ has degree one. By\r\nremark $(ii)$ above,  also $\\theta$ has degree one . Compare with\r\n\\cite[p. 137]{BFMP} and \\cite[p. 12]{Verdier}.\r\n\r\n\\smallskip\r\n$(iv)$ If $Y$ is a quasi-projective {$\\mathbb A$-homology manifold},\r\nand $f: X\\rightarrow Y$ is a {resolution of singularities}  of $Y$,\r\nthen there exists a {unique} bivariant class $\\theta\\in\r\nHom_{D^{b}_{c}(Y)}(Rf_*\\mathbb A_X, \\mathbb A_Y)$ of degree one. See\r\nTheorem \\ref{homology} above.\r\n\r\n\\smallskip\r\n$(v)$ Let $f:X\\to Y$ be a projective map between irreducible,\r\ncomplex quasi-projective varieties of the same dimension $n$. Assume\r\nthat $Y$ is smooth (or, more generally, that $Y$ is an $\\mathbb\r\nA$-homology manifold). In this case one has (compare with\r\n\\cite[3.1.4, p. 34]{FultonCF}, \\cite[Lemma 2, p. 217]{FultonYT}, and\r\nthe proof of Theorem \\ref{homology} below):\r\n$$\r\nH^0(X\\stackrel{f}\\to Y)\\cong H_{2n}(X)\\cong H^0(X).\r\n$$\r\nBy remark $(i)$ above, if there exists a bivariant class of degree\r\none for $f$, then, for every $k$,  $H^k(Y)$ is contained, via\r\npull-back, in $H^k(X)$. Therefore, if $\\mathbb A=\\mathbb Z$ and\r\n$h^k(Y)>h^k(X)$ for some $k$, then it  happens that\r\n$H^0(X\\stackrel{f}\\to Y)\\neq 0$, but $\\theta_0=0$, for every\r\nbivariant class $\\theta$.  However, if, in addition, $f$ is\r\nbirational, then the bivariant class $\\theta$ corresponding to\r\n$1_X\\in H^0(X)$ is a bivariant class of degree one. In fact, if $U$\r\nis a Zariski open subset of $Y$ such that $f^{-1}(U)\\cong U$, the\r\nrestriction of $\\theta$ on $f^{-1}(U)\\to U$ has degree one. Observe\r\nthat, if $Y$ is singular, it is no longer true. For instance, let\r\n$C\\subset \\mathbb P^3$ be a projective non-singular curve of genus\r\n$\\geq 1$. Let $Y\\subset \\mathbb P^4$ be the cone over $C$, and let\r\n$f:X\\to Y$ be the blowing-up of $Y$ at the vertex. Then one has\r\n$H^0(X\\stackrel{f}\\to Y)\\neq 0$, but there is no a bivariant class\r\nof degree one of $f$. This is a consequence of Theorem\r\n\\ref{homology}. For more details, see Remark \\ref{remfin}, $(iii)$.\r\n\r\n\r\n\r\n\\smallskip\r\n$(vi)$ Let $f:X\\to Y$ be a projective map between irreducible\r\nquasi-projective varieties. Assume there exists a bivariant class\r\n$\\theta$ of degree one. Put $n=\\dim X$, and $m=\\dim Y$. Since\r\n$f_*\\circ \\theta^*={\\rm{id}_{H_{\\bullet}(Y)}}$, the push-forward map\r\n$f_*$ induces an inclusion $H_{\\bullet}(Y)\\subseteq H_{\\bullet}(X)$.\r\nIt follows that $m\\leq n$. Moreover, $f$ is surjective, otherwise\r\nthe push-forward $f_*:H_{2m}(X)\\to H_{2m}(Y)$ vanishes. Since\r\nrestricting $\\theta$ to some special fibre, we obtain again a\r\nbivariant class of degree one, in general it may happen that $n>m$.\r\nIt is clear that, if $n=m$, then $f$ is birational.\r\n\\end{remark}\r\n\r\n\r\n\r\n\r\n\r\n\\bigskip\r\n\\section{Bivariant class of degree one and decompositions.}\r\n\r\n\r\n\\bigskip\r\nIn this section we are going to prove Theorem \\ref{Crossintro} stated\r\nin the Introduction.\r\n\r\nTo this purpose, we need some preliminaries.\r\nThe first one is the following lemma.\r\n\r\n\r\n\\begin{lemma}\\label{l1} Let $\\mathcal T$ be a triangulated category, and  $f^*\\in\r\n{\\rm{Hom}}_{\\mathcal T}(A, B)$ be a morphism in $\\mathcal T$. Assume\r\nthat $f^*$ if left-invertible, i.e. that there exists $\\theta \\in\r\n{\\rm{Hom}}_{\\mathcal T}(B, A)$ such that $\\theta\\circ f^*=1_A$. Then\r\nwe have $B\\cong A\\oplus C$ for some $C\\in Ob (\\mathcal T)$.\r\n\\end{lemma}\r\n\\begin{proof}[Proof of Lemma \\ref{l1}]\r\nThe axiom TR1 $(iii)$ of triangulated categories implies that $f^*$\r\ncan be completed to a distinguished triangle\r\n$$A \\stackrel{f^*}{\\longrightarrow} B \\longrightarrow C $$\r\n\\cite[p. 12]{Huy}. Thus, combining the hypothesis $\\theta\\circ\r\nf^*=1_A$ with axioms TR1 and TR3, we have a commutative diagram of\r\ndistinguished triangles\r\n\\[\r\n\\xymatrix{\r\nA \\ar[r]^{f^*} \\ar[d]^{1_{A}}& B \\ar[d]^{\\theta} \\ar[r]& C  \\ar[d]^{}\\\\\r\nA  \\ar[r]^{1_A} & A \\ar[r]  & 0. }\r\n\\]\r\nThe axiom TR2 provides also the following commutative diagram of\r\ndistinguished triangles\r\n\\[\r\n\\xymatrix{\r\nC \\ar[r]^{\\delta} \\ar[d]& A[1] \\ar[d]^{1_{A[1]}} \\ar[r]& B[1]  \\ar[d]^{\\theta[1]}\\\\\r\n0  \\ar[r] & A[1] \\ar[r]  & A[1], }\r\n\\]\r\nfrom which we argue that $\\delta$ vanishes.  We conclude at once by\r\n\\cite[Exercise 1.38]{Huy}.\r\n\\end{proof}\r\n\r\n\\smallskip\r\nWe are in position to prove that $(i)$ is equivalent to $(iii)$ in\r\nTheorem \\ref{Crossintro}.\r\n\r\n\r\nTo this purpose, first assume there exists a bivariant class\r\n$\\theta: Rf_*\\mathbb A_X\\to \\mathbb A_Y$ of degree one, and let\r\n$f^*:\\mathbb A_Y\\to Rf_*\\mathbb A_X$ be the pull-back morphism. By\r\nRemark \\ref{remintro}, $(i)$, we know that $\\theta\\circ\r\nf^*=1_{\\mathbb A_Y}$. Therefore, we may apply previous Lemma\r\n\\ref{l1}, with $\\mathcal T=D^b_c(Y)$, $A=\\mathbb A_Y$,\r\n$B=Rf_*\\mathbb A_X$, with the morphism $f^*$ as the pull-back, and\r\n$\\theta$ as the given bivariant class. It follows a decomposition\r\nlike $Rf_*\\mathbb A_X \\cong \\mathbb A_Y \\oplus \\mathcal K$.\r\n\r\n\r\nConversely, suppose there exists a decomposition $Rf_*\\mathbb A_X\r\n\\cong \\mathbb A_Y \\oplus \\mathcal K$. By projection, it induces a\r\nbivariant class $\\eta: Rf_*\\mathbb A_X \\to \\mathbb A_Y$. Since the\r\nrestriction $\\eta'$ of $\\eta$ on $U$ is an automorphism of ${\\mathbb\r\nA}_U$, and $U$ is nonempty, it follows that $\\eta'_0(1_U)=d\\cdot\r\n1_U\\in H^0(U)$,  with some unit $d\\in\\mathbb A$. Therefore,\r\n$d^{-1}\\cdot \\eta$ is a bivariant class of degree one (compare with\r\nRemark \\ref{remintro}, $(ii)$).\r\n\r\nThis concludes the proof that $(i)$ is equivalent to $(iii)$ in\r\nTheorem \\ref{Crossintro}.\r\n\r\n\\medskip\r\n\\begin{remark}\\label{nonempty}\r\nIn order to prove that $(i)$ implies $(iii)$, we do not need the\r\nexistence of $U$.\r\n\\end{remark}\r\n\r\nNow we are going to prove that $(i)$ is equivalent to $(ii)$.\r\n\r\n\\smallskip\r\nObserve that the same argument we just used to prove that $(iii)$\r\nimplies $(i)$,  proves that $(ii)$ implies $(i)$. In fact, suppose\r\nthere exists a decomposition $Rf_*\\mathbb A_X\\oplus \\mathbb A_W\\cong\r\nRf_*\\mathbb A_{\\widetilde W}\\oplus \\mathbb A_Y$. By projection, it\r\ninduces a bivariant class $\\eta: Rf_*\\mathbb A_X \\to \\mathbb A_Y$.\r\nSince both $\\mathbb A_W$ and $Rf_*\\mathbb A_{\\widetilde W}$ are\r\nsupported on $W$, the restriction of $\\eta$ on $U$ is an\r\nautomorphism of ${\\mathbb A}_U$. And now we may conclude as before.\r\n\r\n\\smallskip\r\nIn order to conclude the proof of Theorem \\ref{Crossintro}, we only have\r\nto prove that $(i)$ implies $(ii)$. Also in this case, we need some\r\npreliminaries.\r\n\r\nConsider the following natural commutative diagram\r\n\\begin{equation}\\label{diagram}\r\n\\xymatrix{\r\n\\widetilde{W} \\ar[d]^g \\ar[r]^j& X \\ar[d]^f &U \\ar[l]_{\\partial_X} \\ar[d]^1\\\\\r\nW \\ar[r]^i &Y  & U \\ar[l]_{\\partial_Y} }\r\n\\end{equation}\r\nwhere $g:\\widetilde W\\to W$ denotes the restriction of $f$,  and the\r\nother maps are the inclusions. Denote by $\\ac$ (resp. $\\bc$)  the\r\nfull subcategory of ${\\text{Sh}}(X)$ (resp. ${\\text{Sh}}(Y)$)\r\nsupported on $U$.\r\n\\begin{lemma}\\label{l2}\r\n\\label{equivalence} On the category ${\\rm{Sh}}(U)$ we have $f_*\r\n\\circ\r\n\\partial_{X\\, !}=\\partial_{Y\\, !}$. Furthermore, $f_*$ is an exact\r\nequivalence between $\\ac$ and $\\bc$, whose inverse is the pull-back\r\n$f^*$.\r\n\\end{lemma}\r\n\\begin{proof}\r\nFirst we prove that $f_* \\circ    \\partial_{X\\, !}=\\partial_{Y\\, !}$\r\non ${\\text{Sh}}(U)$.\r\n\r\nLet $\\fc$ be a sheaf on $U$   and let $V\\subseteq Y$ be an open\r\nsubset. By \\cite[Definition 6.1, p. 106]{Iversen}, we have\r\n$$f_*(\\partial_{X !}(\\fc))(V)= \\{s\\in \\Gamma\\left( f^{-1}(V) \\cap U, \\fc\\right) \\mid  \\,\\, \\text{supp($s$) is closed in} \\,\\, f^{-1}(V)\\}$$\r\nand\r\n$$\\partial_{Y !}(\\fc)(V)= \\{s\\in \\Gamma\\left( V\\cap U, \\fc\\right) \\mid  \\,\\, \\text{supp($s$) is closed in} \\,\\, V\\}.$$\r\nSince $f$ is continuous, we have $\\partial_{Y !}(\\fc)(V)\\subseteq\r\nf_*(\\partial_{X !}(\\fc))(V)$. Hence, $ \\partial_{Y !}(\\fc)$ is a\r\nsubsheaf of  $f_*(\\partial_{X !}(\\fc))$.\r\nAs for the opposite inclusion, we argue as follows. By the local\r\ncompactness of $Y$, we can assume that the closure of $V$ is compact\r\nin $Y$. Fix $s\\in f_*(\\partial_{X !}(\\fc))(V)$, and set $C:=\r\n{\\rm{supp}}(s)$, so that $C$ is closed in $f^{-1}(V)$. It suffices\r\nto prove that $f(C)$, which is homeomorphic to $C$, is closed in\r\n$V$. Since  $f$ is a proper morphism, $f^{-1}(\\overline V )$ is\r\ncompact and  the map $f^{-1}(\\overline V ) \\rightarrow \\overline V$\r\nis closed. Then we have\r\n$$C=f(C)= f(\\overline C \\cap f^{-1}(V))=f(\\overline C) \\cap V $$\r\nand we are done.\r\n\r\nWe are left with the proof that $f_*$ induces an exact equivalence\r\nbetween $\\ac$ and $\\bc$. By \\cite[Proposition 6.4, p. 107]{Iversen},\r\nwe already know that $f_*$ induces an equivalence between $\\ac$ and\r\n$\\bc $, whose inverse is the pull-back.\r\nAs for the exactness, $f_*$ first of all is left-exact by \\cite[p.\r\n97]{Iversen}. Now, consider an exact sequence of sheaves in $\\ac$:\r\n$\\dc \\rightarrow \\hc \\rightarrow 0$. By \\cite[Proposition 6.4, p.\r\n107]{Iversen}, we can assume\r\n$\\dc= \\partial_{X !}\\dc _U, \\,\\,\\,\\, \\hc= \\partial_{X !}\\hc _U$,\r\nfor suitable and well determined sheaves $\\dc _U, \\, \\hc _U \\in\r\n{\\rm{Sh}}\\, (U)$. Therefore, taking into account we just proved that\r\n$f_* \\circ\r\n\\partial_{X\\, !}=\\partial_{Y\\, !}$,\r\nby \\cite[(6.3) p. 106]{Iversen} we deduce\r\n\\[\r\n\\xymatrix{\r\nf_*\\partial_{X !}\\dc _U \\ar[d] _= \\ar[r]& f_*\\partial_{X !}\\hc _U  \\ar[d]^= \\ar[r] & 0  \\ar[d]\\\\\r\n\\partial_{Y !}\\dc _U \\ar[r] & \\partial_{Y !}\\hc _U \\ar[r] & 0\r\n}\r\n\\]\r\nand we are done.\r\n\\end{proof}\r\n\r\n\r\n\\begin{lemma}\\label{l3}\r\nConsider a triangulated category $\\tc$,  and two  commutative\r\ndiagram of distinguished triangles in $\\tc$\r\n$$\r\n\\xymatrix{\r\nA  \\ar[r]^{\\partial} & B_1  \\ar[r]& C_1  \\\\\r\nA \\ar[u]^{1_A} \\ar[r]^{\\partial} &B \\ar[u]^{f^*} \\ar[r]  & C\r\n\\ar[u]^{g^*}} \\quad\\quad \\xymatrix{\r\nA \\ar[r]^{\\partial} \\ar[d]_{1_A}& B_1 \\ar[r] \\ar[d]_{\\theta}& C_1  \\ar[d]_{\\eta}\\\\\r\nA  \\ar[r]^{\\partial} & B \\ar[r]  & C.}\r\n$$\r\nAssume moreover that $\\theta\\circ f^*=1_{B}$,  $\\eta\\circ\r\ng^*=1_{C}$, and that ${\\rm{Hom}}_{\\tc}(A, C_1[-1])=0$. Then we have\r\na \\lq\\lq cross\\rq\\rq \\,isomorphism\r\n$$\r\nB_1\\oplus C\\cong B\\oplus C_1.\r\n$$\r\n\\end{lemma}\r\n\r\n\\begin{remark}\\label{injectives}\r\nIf the category $\\tc$ is the derived category of an abelian category\r\n$\\ac$ with enough injectives (e.g. $D^b_c(Y)$), and\r\n$A\\in{\\text{Ob}}(\\ac)$, and $C_1$ is a complex in degree $\\geq 0$,\r\nthen the assumption ${\\rm{Hom}}_{\\tc}(A, C_1[-1])=0$ is verified.\r\n\\end{remark}\r\n\r\n\r\n\\begin{proof}[Proof of Lemma \\ref{l3}]\r\nConsider the following commutative diagram:\r\n$$\\xymatrix{\r\nA \\ar[r]^{1_A} \\ar[d]^{\\partial} &A \\ar[r] \\ar[d]^{\\partial}&0 \\ar[r] \\ar[d]^{}&A\\left[ 1 \\right] \\ar[d]\\\\\r\nB \\ar[r]^ {f^{*}} \\ar[d] &B_1 \\ar[r] \\ar[d] & B_2 \\ar[r] \\ar[d] &B\\left[ 1 \\right] \\ar[d]\\\\\r\nC \\ar[r]^{g^{*}} \\ar[d] &C_1 \\ar[r] \\ar[d] & C_2 \\ar[r] \\ar[d] &C\\left[ 1 \\right] \\ar[d]\\\\\r\nA\\left[ 1 \\right] \\ar[r]^{1_A[1]} &A\\left[ 1 \\right] \\ar[r] &0\r\n\\ar[r] &A\\left[ 2 \\right]}\r\n$$\r\nwhere the first and second columns are the ones given in the\r\nhypothesis, and the fourth column is obtained by the first one by\r\nmeans of TR2. The first row, which gives the fourth one by means of\r\nTR2, is given by TR1. The second and third rows are given by\r\ncompletion of $f^{*}$ and $g^{*}$, respectively, by means of TR1.\r\nLastly, the arrows in the third column are given by TR3. Observe\r\nthat the third column, a priori, is not  a distinguished triangle.\r\n\r\nSince $\\theta\\circ f^*=1_{B}$ and  $\\eta\\circ g^*=1_{C}$, by Lemma\r\n\\ref{l1} and its proof, we know that $B_1\\cong B\\oplus B_2$, and\r\nthat $C_1\\cong C\\oplus C_2$. Therefore, it suffices to prove that\r\n$B_2\\cong C_2$. To this purpose, we are going to use TR4 \\cite[p.\r\n11]{Dimca2} as follows.\r\n\r\n\r\nCorresponding to the composition $A \\stackrel\r\n{\\partial}{\\rightarrow} B \\rightarrow B_1$ at the top left square in\r\nthe diagram, and to the distinguished triangles given by the first\r\ncolumn, the second row, and the second column, TR4 says there exist\r\na distinguished triangle\r\n\\begin{equation}\\label{secondtriangle}\r\nC \\stackrel{\\gamma}\\rightarrow C_1 \\rightarrow B_2 \\rightarrow C\r\n\\left[ 1 \\right]\r\n\\end{equation}\r\nand a triangle morphism:\r\n$$\r\n\\xymatrix{\r\nA \\ar[r] \\ar[d]^{=} &B \\ar[r] \\ar[d]^{f^*} & C \\ar[r] \\ar[d]^{\\gamma} &A \\left[ 1 \\right] \\ar[d]^{=}\\\\\r\nA \\ar[r] &B_1 \\ar[r] &C_1 \\ar[r] &A \\left[ 1 \\right].}\r\n$$\r\nThe same diagram appears in our assumptions, with $g^{*}$ instead of\r\n$\\gamma$. It follows that $g^{*}=\\gamma$, because\r\n${\\rm{Hom}}_{\\tc}(A, C_1[-1])=0$ \\cite[Proposition 1.1.9., p.\r\n23]{BBD}. Now, comparing (\\ref{secondtriangle}) with the third row\r\nof the diagram at the beginning of the proof, we see that $B_2\\cong\r\nC_2$, because the third object in a distinguished triangle is\r\nunique, up to isomorphism.\r\n\\end{proof}\r\n\r\n\r\n\r\nWe are in position to prove that $(i)$ implies $(ii)$ in Theorem\r\n\\ref{Crossintro}. We keep the notations introduced in the diagram\r\n(\\ref{diagram}).\r\n\r\nFirst notice that the pull-back induces a natural commutative\r\ndiagram of distinguished triangles in $D^b_c(Y)$ \\cite[p.\r\n46]{Dimca2}:\r\n\\begin{equation}\\label{pb}\r\n\\xymatrix{\r\nRf_*(\\partial_{X\\, !}\\mathbb A_U)  \\ar[r]^{\\partial_X} & Rf_*\\mathbb A_X  \\ar[r]^{j^*}& Rf_*\\mathbb A_{\\widetilde W}  \\\\\r\n\\partial_{Y\\, !}\\mathbb A_U \\ar[u]^{1} \\ar[r]^{\\partial_Y} &\\mathbb\r\nA_Y \\ar[u]^{f^*} \\ar[r]^{i^*}  & \\mathbb A_W. \\ar[u]^{g^*}}\r\n\\end{equation}\r\nIn view of Lemma \\ref{l2}, the vertical map $\\partial_{Y\\, !}\\mathbb\r\nA_U \\stackrel {1}{\\longrightarrow} Rf_*(\\partial_{X\\, !}\\mathbb\r\nA_U)$  on the left is an isomorphism in $D^b_c(Y)$. Now consider the\r\nfollowing diagram:\r\n\\[\r\n\\xymatrix{ Rf_*(\\partial_{X\\, !}\\mathbb A_U)  \\ar[r]^{\\partial_X}\r\n\\ar[d]_{1}&\r\nRf_*\\mathbb A_X \\ar[r]^{j^*} \\ar[d]_{\\theta}& Rf_*\\mathbb A_{\\widetilde W}  \\\\\r\n\\partial_{Y\\, !}\\mathbb A_U  \\ar[r]^{\\partial_Y} & \\mathbb\r\nA_Y  \\ar[r]^{i^*}  & \\mathbb A_W.}\r\n\\]\r\nSince the pull-back diagram is commutative, and $\\theta$ has degree\r\none (so $\\theta\\circ f^*=1_{\\mathbb A_Y}$), it follows that previous\r\nsquare commutes. In fact:\r\n$$\r\n\\theta\\circ\\partial_X=\\theta\\circ\\left(f^*\\circ\\partial_Y\\circ\r\n1\\right)=\\left(\\theta\\circ f^*\\right)\\circ\\partial_Y\\circ\r\n1=1_{\\mathbb A_Y}\\circ\\partial_Y\\circ 1=\\partial_Y\\circ 1.\r\n$$\r\nThen, by axiom TR3,  previous diagram extends to a \\lq\\lq\r\nGysin\\rq\\rq \\, morphism of triangles, induced  by the bivariant\r\nclass $\\theta$:\r\n\\begin{equation}\\label{Gy}\r\n\\xymatrix{ Rf_*(\\partial_{X\\, !}\\mathbb A_U)  \\ar[r]^{\\partial_X}\r\n\\ar[d]_{1}&\r\nRf_*\\mathbb A_X \\ar[r]^{j^*} \\ar[d]_{\\theta}& Rf_*\\mathbb A_{\\widetilde W}   \\ar[d]_{\\eta}\\\\\r\n\\partial_{Y\\, !}\\mathbb A_U  \\ar[r]^{\\partial_Y} & \\mathbb\r\nA_Y  \\ar[r]^{i^*}  & \\mathbb A_W.}\r\n\\end{equation}\r\nIn this diagram, by \\cite[loc. cit.]{BBD} (compare with Remark\r\n\\ref{injectives}), the morphism $\\eta$ is unique. For the same\r\nreason, since composing this diagram with the diagram induced by the\r\npull-back, we get the identity on both $\\partial_{Y\\, !}\\mathbb A_U$\r\nand $\\mathbb A_Y $, we also have $\\eta\\circ g^*=1_{\\mathbb A_W}$. At\r\nthis point, it is clear that the decomposition appearing in $(ii)$\r\nfollows from Lemma \\ref{l3} and Remark \\ref{injectives}. This\r\nconcludes the proof of Theorem \\ref{Crossintro}.\r\n\r\n\\begin{remark}\\label{eta}\r\nBivariant Theory provides a pull-back morphism $\\eta_1:=i^*(\\theta)$\r\n\\cite[(3), p. 19]{FultonCF}, with:\r\n$$\r\n\\eta_1:Rf_*\\mathbb A_{\\widetilde W}\\to \\mathbb A_W.\r\n$$\r\nWe are not able to prove that $\\eta=\\eta_1$, i.e. that the Gysin\r\ndiagram, with $\\eta_1$ instead of $\\eta$, commutes. However, we will\r\nprove, later, that $\\eta$ and $\\eta_1$ induce the same morphism  in\r\n(co)homology. Notice that also $\\eta_1$ has degree one, and\r\ntherefore we also  have $\\eta_1\\circ g^*=1_{\\mathbb A_W}$.\r\nTherefore, if a morphism of degree one was unique, then\r\n$\\eta=\\eta_1$.\r\n\\end{remark}\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\\bigskip\r\n\\section{Consequences for the (co)homology.}\r\nKeep the same assumption of Theorem \\ref{Crossintro}, and suppose there\r\nis a bivariant class of degree one for $f$. Then we have a cross\r\nisomorphism $Rf_*\\mathbb A_X\\oplus \\mathbb A_W\\cong Rf_*\\mathbb\r\nA_{\\widetilde W}\\oplus \\mathbb A_Y$. Taking hypercohomology\r\n(hypercohomology with compact support resp.), we deduce isomorphisms\r\nin cohomology (Borel-Moore homology resp.):\r\n$$\r\nH^{\\bullet}(X)\\oplus H^{\\bullet}(W)\\cong H^{\\bullet}(\\widetilde\r\nW)\\oplus H^{\\bullet}(Y),\\quad\\quad H_{\\bullet}(X)\\oplus\r\nH_{\\bullet}(W)\\cong H_{\\bullet}(\\widetilde W)\\oplus H_{\\bullet}(Y).\r\n$$\r\nUsing the triangle morphisms (\\ref{pb}) and (\\ref{Gy}), we may\r\nexplicit this isomorphisms as follows.\r\n\r\nFirst, taking hypercohomology \\cite[p. 46]{Dimca2}, the triangle\r\nmorphisms (\\ref{pb}) and (\\ref{Gy}) induce  commutative diagrams\r\nwith exact rows:\r\n$$\r\n\\xymatrix{ H^k(X,\\widetilde W) \\ar[r] \\ar[d]  & H^k(X) \\ar[r]^{j^*}\r\n& H^k(\\widetilde W) \\ar[r]^{\\partial_X}\r\n&H^{k+1}(X,\\widetilde W) \\ar[d]^{}\\\\\r\nH^k(Y,W)  \\ar[r] \\ar[u]^{=}  &H^k(Y) \\ar[r]^{i^*} \\ar[u]^{f^*}\r\n&H^k(W) \\ar[r]^{\\partial_Y} \\ar[u]^{g^*} &H^{k+1}(Y, W) \\ar[u]^{=}}\r\n$$\r\nand\r\n$$\r\n\\xymatrix{ H^k(X,\\widetilde W) \\ar[r] \\ar[d]^{=} & H^k(X)\r\n\\ar[r]^{j^*} \\ar[d]^{\\theta_*} & H^k(\\widetilde W)\r\n\\ar[r]^{\\partial_X} \\ar[d]^{\\eta_*}\r\n&H^{k+1}(X,\\widetilde W) \\ar[d]^{=}\\\\\r\nH^k(Y,W) \\ar[r] \\ar[u]^{}&H^k(Y) \\ar[r]^{i^*} &H^k(W)\r\n\\ar[r]^{\\partial_Y} &H^{k+1}(Y, W) \\ar[u]^{}}\r\n$$\r\nfor every $k\\in\\mathbb Z$. Since these diagrams commute, and\r\n$\\theta_*\\circ f^*={\\text{id}}_{H^{\\bullet}(Y)}$ and $\\eta_*\\circ\r\ng^*={\\text{id}}_{H^{\\bullet}(W)}$, a chase diagram shows that the\r\nsequence:\r\n$$\r\n0\\to H^k(X)\\stackrel{\\alpha^*}\\to H^k(\\widetilde W)\\oplus\r\nH^k(Y)\\stackrel{\\beta^*}\\to H^k(W)\\to 0,\r\n$$\r\nwith\r\n$$\r\n\\alpha^*(x):=(j^*(x),\\, -\\theta_*(x)), \\quad \\beta^*(\\widetilde\r\nw,\\,y):=\\eta_*(\\widetilde w)+i^*(y),\r\n$$\r\nis exact (compare with \\cite[Proposition 6.7, (e), p.\r\n114-115]{FultonIT}). Moreover, the map\r\n$$\r\nw\\in H^k(W) \\to (g^*(w),0)\\in H^k(\\widetilde W)\\oplus H^k(Y)\r\n$$\r\nis a right section for the sequence, and so we get an explicit\r\nisomorphism:\r\n\\begin{proposition}\\label{crossexpl}\r\nThe map\r\n$$\r\n\\varphi^*:H^k(X)\\oplus H^k(W)\\to H^k(\\widetilde W)\\oplus H^k(Y),\r\n$$\r\nwith\r\n$$\r\n\\varphi^*(x,\\, w):=(j^*(x)+g^*(w),\\,-\\theta_*(x)),\r\n$$\r\nis an isomorphism.\r\n\\end{proposition}\r\nWe may interpret the map $\\varphi^*$ as a matrix product (compare\r\nwith \\cite[p. 328]{Jouanolou}):\r\n$$\r\n\\left[\\begin{matrix} \\widetilde w \\\\\r\ny\r\n\\end{matrix}\\right]=\\left[\\begin{matrix} j^* & g^*\\\\\r\n-\\theta_* & 0\r\n\\end{matrix}\\right]\\cdot \\left[\\begin{matrix} x \\\\\r\nw\r\n\\end{matrix}\\right].\r\n$$\r\nSince\r\n$$\r\n\\varphi^*(-f^*y,\\,i^*y)=(0,y),\r\n$$\r\nthe matrix defining the inverse map $({\\varphi^*})^{-1}$ has the\r\nfollowing form:\r\n$$\r\n\\left[\\begin{matrix} x \\\\\r\nw\r\n\\end{matrix}\\right]=\\left[\\begin{matrix} \\lambda_* & -f^*\\\\\r\n\\mu_* & i^*\r\n\\end{matrix}\\right]\\cdot \\left[\\begin{matrix} \\widetilde w \\\\\r\ny\r\n\\end{matrix}\\right],\r\n$$\r\nwhere the functions:\r\n$$\r\n\\lambda_*: H^{\\bullet}(\\widetilde W)\\to H^{\\bullet}(X),\\quad \\mu_*:\r\nH^{\\bullet}(\\widetilde W)\\to H^{\\bullet}(W)\r\n$$\r\nare uniquely determined by the condition that the two matrices above\r\nare the inverse each other, i.e. by the equations:\r\n\\begin{equation}\\label{eqc1}\r\n\\begin{cases}\r\n\\lambda_*\\circ\r\nj^*+f^*\\circ\\theta_*={\\rm{id}_{H^{\\bullet}(X)}}\\\\\r\n\\lambda_*\\circ g^*=0\\\\\r\n\\mu_*\\circ j^*-i^*\\circ \\theta_*=0\\\\\r\n\\mu_*\\circ g^*={\\rm{id}_{H^{\\bullet}(W)}},\r\n\\end{cases}\r\n\\end{equation}\r\nwhich in turn are equivalent to the equations:\r\n\\begin{equation}\\label{eqc2}\r\n\\begin{cases}\r\nj^*\\circ\\lambda_*+g^*\\circ\\mu_*={\\rm{id}_{H^{\\bullet}(\\widetilde\r\nW)}}\\\\\r\n\\theta_*\\circ \\lambda_*=0\\\\\r\nj^*\\circ f^*=g^*\\circ i^*\\\\\r\n\\theta_*\\circ f^*={\\rm{id}_{H^{\\bullet}(Y)}}.\r\n\\end{cases}\r\n\\end{equation}\r\nSince we also have $\\eta_*\\circ j^*-i^*\\circ \\theta_*=0$ and\r\n$\\eta_*\\circ g^*={\\rm{id}_{H^{\\bullet}(W)}}$, by the uniqueness, it\r\nfollows that $\\eta_*=\\mu_*$.\r\n\r\n\\smallskip\r\n\\begin{remark}\\label{eta2}\r\nLet $\\eta_1:=i^*(\\theta)$ be the pull-back of $\\theta$ on $W$. By\r\nproperties of bivariant classes \\cite[(G2), p. 26]{FultonCF}, we see\r\nthat $(\\eta_1)_*\\circ j^*-i^*\\circ \\theta_*=0$ and $(\\eta_1)_*\\circ\r\ng^*={\\rm{id}_{H^{\\bullet}(W)}}$. As before, this proves that\r\n$\\eta_*=(\\eta_1)_*$. Similarly, for the maps induced in homology,\r\none sees that $\\eta^*=(\\eta_1)^*$ (see below).\r\nRecall that we do not know whether\r\n$\\eta=\\eta_1$ (compare with Remark \\ref{eta}).\r\n\\end{remark}\r\n\r\n\r\nUsing these equations, we are able to explicit also the isomorphism\r\ninduced in cohomology by the decomposition appearing in $(iii)$ of\r\nTheorem \\ref{Crossintro}. First observe that, since $\\eta_*\\circ\r\ng^*={\\rm{id}_{H^{\\bullet}(W)}}$, we may see $H^k(W)$, via $g^*$,  as\r\na direct summand of $H^k(\\widetilde W)$ for every integer $k$. Denote by\r\n$$\\frac{H^k(\\widetilde W)}{H^k(W)}$$ the corresponding quotient.\r\n\\begin{proposition}\\label{decexpl}\r\nFor every $k$, the map\r\n$$\r\nx\\in H^k(X)\\to (\\theta_*x,\\, j^*x)\\in H^k(Y)\\oplus\r\n\\left[\\frac{H^k(\\widetilde W)}{H^k(W)}\\right]\r\n$$\r\nis an isomorphism, whose inverse is the map\r\n$$\r\n(y,\\, \\widetilde w)\\in H^k(Y)\\oplus \\left[\\frac{H^k(\\widetilde\r\nW)}{H^k(W)}\\right]\\to f^*(y)+\\lambda_* \\widetilde w\\in H^k(X).\r\n$$\r\n\\end{proposition}\r\n\\begin{proof} First observe that the map\r\n$$\r\nx\\in H^k(X)\\to (\\theta_*x,\\, x-f^*\\theta_*x)\\in H^k(Y)\\oplus \\ker\r\n\\theta_*\r\n$$\r\nis an isomorphism. Next, observe that previous equations (\\ref{eqc1}) and (\\ref{eqc2})\r\nimply that\r\n$j^*$ induces an isomorphism\r\n$$\r\nj^*: \\ker \\theta_* \\to \\ker \\eta_*,\r\n$$\r\nwhose inverse acts as $\\lambda_*$. On the other hand, we also have\r\nan isomorphism:\r\n$$\r\n\\widetilde w\\in  \\ker\\eta_*\\to\\widetilde w\\in \\frac{H^k(\\widetilde\r\nW)}{H^k(W)}.\r\n$$\r\n\\end{proof}\r\n\r\n\r\n\r\nSimilarly, taking hypercohomology with compact support, the triangle\r\nmorphisms (\\ref{pb}) and (\\ref{Gy}) induce commutative diagrams with\r\nexact rows involving Borel-Moore homology:\r\n$$\r\n\\xymatrix{ H_{k+1}(U) \\ar[r]^{\\partial_X} \\ar[d]^{=} &\r\nH_k(\\widetilde W) \\ar[r]^{j_*} \\ar[d]^{g_*} & H_k(X) \\ar[r]\r\n\\ar[d]^{f_*}\r\n&H_{k}(U) \\ar[d]^{=}\\\\\r\nH_{k+1}(U) \\ar[r]^{\\partial_Y} \\ar[u]^{}&H_k(W) \\ar[r]^{i_*} &H_k(Y)\r\n\\ar[r] &H_{k}(U) \\ar[u]^{}}\r\n$$\r\nand\r\n$$\r\n\\xymatrix{ H_{k+1}(U) \\ar[r]^{\\partial_X} \\ar[d]  & H_k(\\widetilde\r\nW) \\ar[r]^{j_*} & H_k(X) \\ar[r]\r\n&H_{k}(U) \\ar[d]^{}\\\\\r\nH_{k+1}(U)  \\ar[r]^{\\partial_Y} \\ar[u]^{=}  &H_k(W) \\ar[r]^{i_*}\r\n\\ar[u]^{\\eta^*} &H_k(Y) \\ar[r] \\ar[u]^{\\theta^*} &H_k(U) \\ar[u]^{=}}\r\n$$\r\nfor every $k\\in\\mathbb Z$. Since these diagrams commute, and\r\n$f_*\\circ \\theta^*={\\text{id}}_{H_{\\bullet}(Y)}$ and $g_*\\circ\r\n\\eta^*={\\text{id}}_{H_{\\bullet}(W)}$, a chase diagram shows that the\r\nsequence:\r\n$$\r\n0\\to H_k(W)\\stackrel{\\alpha_*}\\to H_k(\\widetilde W)\\oplus\r\nH_k(Y)\\stackrel{\\beta_*}\\to H_k(X)\\to 0,\r\n$$\r\nwith\r\n$$\r\n\\alpha_*(w):=(\\eta^*(w),\\, -i_*(w)), \\quad \\beta_*(\\widetilde\r\nw,\\,y):=j_*(\\widetilde w)+\\theta^*(y),\r\n$$\r\nis exact (compare with \\cite[pp. 264-266, Proposition 2.5]{RCMP}).\r\nMoreover, the map\r\n$$\r\n(\\widetilde w,\\, y)\\in H_k(\\widetilde W)\\oplus H_k(Y)\\to\r\ng_*\\widetilde w\\in H_k(W)\r\n$$\r\nis a left section for the sequence, and so we get an explicit\r\nisomorphism:\r\n\\begin{proposition}\\label{crossexpl2}\r\nThe map\r\n$$\r\n\\varphi_*:H_k(\\widetilde W)\\oplus H_k(Y)\\to H_k(X)\\oplus H_k(W),\r\n$$\r\nwith\r\n$$\r\n\\varphi_*(\\widetilde w,\\, y):=(j_*(\\widetilde\r\nw)+\\theta^*(y),\\,g_*(\\widetilde w)),\r\n$$\r\nis an isomorphism.\r\n\\end{proposition}\r\nWe may interpret the map $\\varphi_*$ as a matrix product:\r\n$$\r\n\\left[\\begin{matrix} x \\\\\r\nw\r\n\\end{matrix}\\right]=\\left[\\begin{matrix} j_* & \\theta^*\\\\\r\ng_* & 0\r\n\\end{matrix}\\right]\\cdot \\left[\\begin{matrix} \\widetilde w \\\\\r\ny\r\n\\end{matrix}\\right].\r\n$$\r\nSince\r\n$$\r\n\\varphi_*(\\eta^*w,\\,-i_*w)=(0,w),\r\n$$\r\nthe matrix defining the inverse map $(\\varphi_*){^{-1}}$ has the\r\nfollowing form:\r\n$$\r\n\\left[\\begin{matrix} \\widetilde w \\\\\r\ny\r\n\\end{matrix}\\right]=\\left[\\begin{matrix} \\lambda^* & \\eta^*\\\\\r\n\\mu^* & -i_*\r\n\\end{matrix}\\right]\\cdot \\left[\\begin{matrix} x \\\\\r\nw\r\n\\end{matrix}\\right],\r\n$$\r\nwhere the functions:\r\n$$\r\n\\lambda^*: H_{\\bullet}(X)\\to H_{\\bullet}(\\widetilde W), \\quad \\mu^*:\r\nH_{\\bullet}(X)\\to H_{\\bullet}(Y)\r\n$$\r\nare uniquely determined by the condition that the two matrices above\r\nare the inverse each other, i.e. by the equations:\r\n\\begin{equation}\\label{eqo1}\r\n\\begin{cases}\r\nj_*\\circ \\lambda^*+\\theta^*\\circ \\mu^*={\\rm{id}_{H_{\\bullet}(X)}}\\\\\r\nj_*\\circ \\eta^* -\\theta^*\\circ i_*=0\\\\\r\ng_*\\circ \\lambda^*=0\\\\\r\ng_*\\circ \\eta^*={\\rm{id}_{H_{\\bullet}(W)}},\r\n\\end{cases}\r\n\\end{equation}\r\nwhich in turn are equivalent to the equations:\r\n\\begin{equation}\\label{eqo2}\r\n\\begin{cases}\r\n\\lambda^*\\circ j_*+\\eta^*\\circ g_*={\\rm{id}_{H_{\\bullet}(\\widetilde\r\nW)}}\\\\\r\n\\lambda^*\\circ \\theta^*=0\\\\\r\n\\mu^*\\circ j_*=i_*\\circ g_*\\\\\r\n\\mu^*\\circ \\theta^*={\\rm{id}_{H^{\\bullet}(Y)}}.\r\n\\end{cases}\r\n\\end{equation}\r\nIn particular, it follows that $\\mu^*=f_*$. Using these equations,\r\nwe are able to explicit the isomorphism induced in Borel-Moore\r\nhomology by  $(iii)$ of Theorem \\ref{Crossintro}. First, observe\r\nthat, since $g_*\\circ\\,\\eta^*={\\rm{id}_{H_{\\bullet}(W)}}$, we may\r\nsee $H_k(W)$, via $\\eta^*$,  as a direct summand of $H_k(\\widetilde\r\nW)$ for every integer $k$. Denote by\r\n$$\\frac{H_k(\\widetilde W)}{H_k(W)}$$ the corresponding quotient.\r\n\\begin{proposition}\\label{decexpl2}\r\nFor every $k$, the map\r\n$$\r\nx\\in H_k(X)\\to (f_*x,\\, \\lambda^* x)\\in H_k(Y)\\oplus\r\n\\left[\\frac{H_k(\\widetilde W)}{H_k(W)}\\right]\r\n$$\r\nis an isomorphism, whose inverse is the map\r\n$$\r\n(y,\\, \\widetilde w)\\in H_k(Y)\\oplus \\left[\\frac{H_k(\\widetilde\r\nW)}{H_k(W)}\\right]\\to \\theta^*(y)+j_*\\lambda^* j_* \\widetilde w\\in\r\nH_k(X).\r\n$$\r\n\\end{proposition}\r\n\\begin{proof} First observe that the map\r\n$$\r\nx\\in H_k(X)\\to (f_*x,\\, x-\\theta^*f_*x)\\in H_k(Y)\\oplus \\ker f_*\r\n$$\r\nis an isomorphism. Next, observe that previous equations (\\ref{eqo1}) and (\\ref{eqo2}) imply that\r\n$\\lambda^*$ induces an isomorphism\r\n$$\r\n\\lambda^*: \\ker f_* \\to \\ker g_*,\r\n$$\r\nwhose inverse acts as $j_*$. On the other hand, we also have an\r\nisomorphism:\r\n$$\r\n\\widetilde w\\in  \\ker g_*\\to\\widetilde w\\in \\frac{H_k(\\widetilde\r\nW)}{H_k(W)}.\r\n$$\r\n\\end{proof}\r\n\r\n\r\n\r\n\r\n\r\n\\bigskip\r\n\\section{Behaviour under the duality morphism.}\r\nOne may ask how previous decompositions given in Proposition\r\n\\ref{decexpl} and Proposition \\ref{decexpl2}, behave under the cap\r\nproduct with a homology class. In this section  we consider only the\r\ncase of the fundamental class, and algebraic maps.\r\n\r\n\\smallskip\r\nConsider a map $f:X\\to Y$ as in Theorem \\ref{Crossintro}, and assume\r\nthere exists a bivariant class of $f$ of degree one. Moreover,\r\nassume that $f$ is onto, and that $X$ and $Y$ are open subsets of complex quasi-projective\r\nvarieties of the same complex dimension $n$. Let $[X]\\in H_{2n}(X)$\r\nbe the fundamental class of $X$, and consider the map\r\n\\begin{equation}\\label{dm}\r\n\\mathcal D_X: x\\in H^{k}(X)\\to x\\cap [X]\\in H_{2n-k}(X)\r\n\\end{equation}\r\ngiven by the cap product with $[X]$. When $X$ is a circuit, this map\r\nis called {\\it the duality morphism} \\cite[p. 150]{McCrory}. If, in\r\naddition, $X$ is smooth, then $\\mathcal D_X$ is the Poincar\\'e\r\nDuality isomorphism. In view of the decompositions given in\r\nProposition \\ref{decexpl} and Proposition \\ref{decexpl2}, the map\r\n$\\mathcal D_X$ identifies with a map\r\n$$\r\n\\mathcal D_X:H^k(Y)\\oplus \\left[\\frac{H^k(\\widetilde\r\nW)}{H^k(W)}\\right]\\to H_{2n-k}(Y)\\oplus\r\n\\left[\\frac{H_{2n-k}(\\widetilde W)}{H_{2n-k}(W)}\\right]\r\n$$\r\nwhich acts as follows:\r\n$$\r\n\\mathcal D_X(y,\\,\\widetilde w)=(f_*([X]\\cap(f^*y+\\lambda_*\\widetilde\r\nw)),\\, \\lambda^*([X]\\cap(f^*y+\\lambda_*\\widetilde w))).\r\n$$\r\nThe map $\\mathcal D_X$ induces two projections\r\n$$\r\nP_1:y\\in H^{k}(Y)\\to f_*([X]\\cap f^*y)\\in H_{2n-k}(Y),\r\n$$\r\n$$\r\nP_2:\\widetilde w\\in \\left[\\frac{H^k(\\widetilde W)}{H^k(W)}\\right]\\to\r\n\\lambda^*([X]\\cap \\lambda_*\\widetilde w)\\in\r\n\\left[\\frac{H_{2n-k}(\\widetilde W)}{H_{2n-k}(W)}\\right].\r\n$$\r\nObserve that, by the projection formula \\cite[p. 24]{FultonCF}, we have\r\n$$\r\n f_*([X]\\cap f^*y)=[Y]\\cap y.\r\n$$\r\nTherefore, $P_1= \\mathcal D_Y$, i.e. $P_1$ is nothing but the\r\nduality morphism on $Y$.\r\n\r\n\\begin{corollary}\\label{duality}\r\nThe duality morphism $\\mathcal D_X:H^{k}(X)\\to H_{2n-k}(X)$ is the\r\ndirect sum of $\\mathcal D_Y$ and $P_2$, i.e.\r\n$$\r\n\\mathcal D_X=\\mathcal D_Y\\oplus P_2.\r\n$$\r\n\\end{corollary}\r\n\r\n\\begin{proof}\r\nWe have to prove that:\r\n\r\n\\medskip\r\n$\\bullet$) for every $\\widetilde w\\in \\frac{H^i(\\widetilde\r\nW)}{H^i(W)}$ one has $f_*([X]\\cap \\lambda_* \\widetilde w)=0$, and\r\n\r\n\\smallskip\r\n$\\bullet$) for every $y\\in H^i(Y)$ one has $\\lambda^*([X]\\cap\r\nf^*y)=0$.\r\n\r\n\\medskip\r\nTo this purpose, first observe that $\\theta^*([Y])=[X]$, i.e. the\r\nGysin map sends the fundamental class of $Y$ in the fundamental\r\nclass of $X$. In fact, from the equations (\\ref{eqo1}) we obtained in homology\r\n(recall that $\\mu^*=f_*$),\r\nwe know that\r\n$\\theta^*([Y])=\\theta^*f_*[X]=[X]-(j_*\\circ\\lambda^*)([X])=[X]$\r\nbecause $\\lambda^*[X]=0\\in H_{2n}(\\widetilde W)=\\{0\\}$ for\r\ndimensional reasons.\r\n\r\n\\medskip\r\n $\\bullet$) Now, by \\cite[p. 26, G4, (ii)]{FultonCF}, we have:\r\n$$\r\nf_*([X]\\cap \\lambda_* \\widetilde w)=f_*(\\theta^*[Y]\\cap \\lambda_*\r\n\\widetilde w)=(\\theta_*\\lambda_*\\widetilde w)\\cap [Y]\r\n$$\r\nwhich is zero because, from the equations (\\ref{eqc2}) we obtained in cohomology,\r\nwe know that $\\theta_*\\circ\\lambda_*=0$ .\r\n\r\n\\smallskip\r\n$\\bullet$) Next, by \\cite[p. 26, G4, (iii)]{FultonCF}, we have:\r\n$$\\lambda^*([X]\\cap f^*y)=\r\n\\lambda^*(\\theta^*[Y]\\cap f^*y)=\\lambda^*(\\theta^*(Y\\cap y))\r\n$$\r\nwhich is zero because, from the equations (\\ref{eqo2}) we obtained in homology,\r\nwe know that  $\\lambda^*\\circ \\theta^*=0$.\r\n\\end{proof}\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\\bigskip\r\n\\section{Resolution of singularities of a homology manifold.}\r\n\r\nIn this section we are going to prove Theorem \\ref{homology} stated\r\nin the Introduction. Observe that it applies to a resolution of\r\nsingularities of $Y$.\r\n\r\n\\medskip\r\nFirst assume that $Y$ is an $\\mathbb\r\nA$-homology manifold.\r\n\r\nBy \\cite[Definition 3.1, Theorem 3.7]{Brasselet}, we know that the\r\nfundamental class of $Y$\r\n$$\r\n[Y]\\in H_{2n}(Y)\\cong H^{-2n}(Y\\stackrel{}\\to pt)\r\n$$\r\nis a strong orientation. Therefore, we have\r\n$$\r\nHom_{D^{b}_{c}(Y)}(Rf_*\\mathbb A_X, \\mathbb A_Y)\\cong\r\nH^0(X\\stackrel{f}\\to Y)\\stackrel{\\bullet [Y]}\\cong\r\nH^{-2n}(X\\stackrel{}\\to pt)\\cong H_{2n}(X)\\cong H^0(X).\r\n$$\r\nSince $f$ is birational, the bivariant class corresponding to\r\n$1_X\\in H^0(X)$ is a bivariant class of degree one for $f$, and it\r\nis unique (compare with Remark \\ref{remintro}, $(ii)$ and $(v)$). By\r\nTheorem \\ref{Crossintro}, we know there exists a decomposition\r\n\\begin{equation}\\label{D1}\r\nRf_*\\mathbb A_X[n] \\cong \\mathbb A_Y[n] \\oplus \\mathcal K[n].\r\n\\end{equation}\r\nIt is clear that $\\mathcal K$ is supported on $W$. Passing to\r\nVerdier dual, we get:\r\n\\begin{equation}\\label{D2}\r\nD\\left(Rf_*\\mathbb A_X[n]\\right) \\cong D\\left(\\mathbb A_Y[n]\\right)\r\n\\oplus D\\left(\\mathcal K[n]\\right). \\end{equation} Now let $$[X]\\in\r\nH_{2n}(X)$$ be the fundamental class of $X$. We have \\cite[p.\r\n804-805]{Brasselet}:\r\n$$\r\n[X]\\in H_{2n}(X)\\cong H^{-2n}(X\\stackrel{}\\to pt.)\\cong\r\nHom_{D^{b}_{c}(X)}(\\mathbb A_X[n],D\\left(\\mathbb A_X[n]\\right)).\r\n$$\r\nTherefore, $[X]$ corresponds to a morphism\r\n\\begin{equation}\\label{is}\r\n\\mathbb A_X[n]\\to D\\left(\\mathbb A_X[n]\\right),\r\n\\end{equation}\r\nwhose induced map in hypercohomology is nothing but the duality\r\nmorphism (\\ref{dm}). If we assume that $X$ is an $\\mathbb\r\nA$-homology manifold, the morphism (\\ref{is}) is an isomorphism\r\n\\cite[Proof of Theorem 3.7]{Brasselet}. Since $D\\left(Rf_*\\mathbb\r\nA_X[n]\\right)\\cong Rf_* D\\left(A_X[n]\\right)$ \\cite[p. 69]{Dimca2},\r\nit induces an isomorphism\r\n\\begin{equation*}\r\nRf_*\\mathbb A_X[n]\\to D\\left(Rf_*\\mathbb A_X[n]\\right),\r\n\\end{equation*}\r\nwhich in turn, via the previous decompositions (\\ref{D1}) and\r\n(\\ref{D2}), induces two projections\r\n$$\r\n\\mathbb A_Y[n] \\to D\\left(\\mathbb A_Y[n]\\right), \\quad  \\mathcal\r\nK[n]\\to D\\left( \\mathcal K[n]\\right).\r\n$$\r\nBy Corollary \\ref{duality}, we know that the maps induced in\r\nhypercohomology by $\\mathcal K[n]\\to D\\left( \\mathcal K[n]\\right)$\r\nare isomorphisms, and this holds true when restricting to every open\r\nsubset of $Y$. Therefore, we have $ \\mathcal K[n]\\cong\r\nD\\left(\\mathcal K[n]\\right)$, i.e. $\\mathcal K[n]$ is self-dual.\r\n\r\n\r\n\r\n\\medskip\r\nConversely, assume  there exists a bivariant class $\\theta$ of\r\ndegree one. Arguing as before, by Corollary \\ref{duality}, we know\r\nthat the isomorphism (\\ref{is}) induces an isomorphism $\\mathbb\r\nA_Y[n]\\cong D\\left(\\mathbb A_Y[n]\\right)$. This is equivalent to say\r\nthat $Y$ is an $\\mathbb A$-homology manifold \\cite[loc.\r\ncit.]{Brasselet}.\r\n\r\nThis concludes the proof of Theorem \\ref{homology}.\r\n\r\n\r\n\r\n\r\n\r\n\\begin{remark}\\label{remfin}\r\n$(i)$ With the notations as in Theorem \\ref{homology}, assume  there\r\nexists a bivariant class $\\theta$ of degree one. When the\r\ncoefficients are in a field, we may prove that $Y$ is an $\\mathbb\r\nA$-homology manifold in a different manner, using the Decomposition\r\nTheorem \\cite[p. 161]{Dimca2}. In fact, by the Decomposition\r\nTheorem, there exists a certain decomposition\r\n$$Rf_*\\mathbb A_X[n] \\cong  IC_Y^{\\bullet} \\oplus \\mathcal H.$$\r\nComparing with the decomposition given by Theorem \\ref{Crossintro}\r\n$$Rf_*\\mathbb A_X[n] \\cong \\mathbb A_Y[n] \\oplus \\mathcal K[n],$$\r\nit follows a non-zero endomorphism $IC_Y^{\\bullet}\\to \\mathbb\r\nA_Y[n]\\to IC_Y^{\\bullet}$. On the other hand,  $IC_Y^{\\bullet}$\r\nbelongs to the core of $D^b_c(Y)$, which is an abelian subcategory\r\nof $D^b_c(Y)$. In this category, $IC_Y^{\\bullet}$ is a simple\r\nobject. Therefore, by Schur's Lemma, the composition\r\n$IC_Y^{\\bullet}\\to \\mathbb A_Y[n]\\to IC_Y^{\\bullet}$ is an\r\nautomorphism. Observe that also the composition $\\mathbb A_Y[n]\\to\r\nIC_Y^{\\bullet}\\to \\mathbb A_Y[n]$ is an automorphism, because\r\n$Hom_{D^{b}_{c}(Y)}(\\mathbb A_Y, \\mathbb A_Y)\\cong H^0(Y)$. So,\r\n$IC_Y^{\\bullet}\\cong \\mathbb A_Y[n]$.\r\n\r\n\\smallskip\r\n$(ii)$ Since $\\mathcal K[n]$ is self-dual, it follows that\r\n$$\r\nh^{2n-i}(\\widetilde W)-h_i(\\widetilde W)=h^{2n-i}(W)-h_i(W)\r\n$$\r\nfor every $i\\in\\mathbb Z$.\r\n\r\n\\smallskip\r\n$(iii)$ The following example shows there exist projective\r\nbirational maps $f:X\\to Y$ such that $H^0(X\\stackrel{f}\\to Y)\\neq\r\n0$, without bivariant classes of degree one. The coefficients are in\r\n$\\mathbb Q$.\r\n\r\nLet $C\\subset \\mathbb P^3$ be a projective non-singular curve of\r\ngenus $g\\geq 1$. Let $Y\\subset \\mathbb P^4$ be the cone over $C$,\r\nand let $f:X\\to Y$ be the blowing-up of $Y$ at the vertex $y\\in Y$.\r\nBy the Decomposition Theorem (see e.g. \\cite{DGF3}) we have\r\n$$\r\nRf_*\\mathbb Q_X=\\mathbb Q_y[-2]\\oplus IC_Y^{\\bullet}[-2].\r\n$$\r\nOn the other hand, combining \\cite[9.13, p. 128]{Iversen} with\r\n\\cite[Remark 2.4.5, (i), p. 46]{Dimca2}, we have\r\n$$\r\nHom_{D^b_c(Y)}(\\mathbb Q_y, \\mathbb Q_Y[2])\\cong\r\nH^2(Y,Y\\backslash\\{y\\})\\cong H^1(L),\r\n$$\r\nwhere $L$ is the link of $Y$ at the vertex $y$. The Hopf fibration\r\n$L\\to C$ induces a Gysin sequence\r\n$$\r\n0\\to H^1(C)\\to H^1(L)\\to H^0(C)\\to H^2(C)\\to\\dots\r\n$$\r\nfrom which we get $h^1(L)=h^1(C)=2g\\geq 2$. It follows that\r\n$H^0(X\\stackrel{f}\\to Y)\\cong Hom_{D^{b}_{c}(Y)}(Rf_*\\mathbb A_X,\r\n\\mathbb A_Y)\\neq 0$, and that $Y$ is not a homology manifold. In\r\nparticular, since $X$ is smooth, in view of Theorem \\ref{homology},\r\nthere is no a bivariant class of degree one.\r\n\\end{remark}\r\n\r\n\r\n\\begin{corollary}\\label{strong}\r\nLet $f:X\\to Y$ be a projective birational morphism between\r\nirreducible and quasi-projective complex varieties of the same\r\ncomplex dimension $n$. Let $\\theta\\in H^0(X\\stackrel{f}\\to Y)$ be a\r\nbivariant class. If $\\theta$ is a strong orientation for $f$, then\r\n$\\theta$ is a bivariant class of degree one for $f$, up to\r\nmultiplication by a unit. Moreover, if $X$ is an $\\mathbb\r\nA$-manifold and $\\theta$ is a bivariant class of degree one for $f$,\r\nthen $\\theta$ is a strong orientation for $f$.\r\n\\end{corollary}\r\n\r\n\r\n\\begin{proof} First assume that $\\theta$ is a strong orientation for\r\n$f$.\r\n\r\nLet $U\\subset Y$ be a Zariski non-empty open subset of $Y$ such that\r\n$f^{-1}(U)\\cong U$ via $f$.  Product by $\\theta$ gives an\r\nisomorphism:\r\n$$\r\nH^0(f^{-1}(U)\\to X)\\stackrel{\\bullet \\theta}\\to H^0(U\\to Y).\r\n$$\r\nOn the other hand, by Verdier Duality \\cite[p. 803]{Brasselet}, and\r\n\\cite[Corollary 3.2.12., p. 65]{Dimca2}, we have:\r\n$$\r\nH^0(f^{-1}(U)\\to X)\\cong H^0(f^{-1}(U)), \\quad {\\text{and}}\\quad\r\nH^0(U\\to Y)\\cong H^0(U).\r\n$$\r\nTherefore, $\\theta$ induces an isomorphism $H^0(f^{-1}(U))\\to\r\nH^0(U)$. It follows that, up to multiplication by a unit, $\\theta$\r\nis a bivariant class of degree one.\r\n\r\nConversely, assume $X$ is an $\\mathbb A$-manifold, and $\\theta$ is a\r\nbivariant class of degree one for $f$.\r\n\r\nIn this case, by Theorem \\ref{homology}, we know that also $Y$ is an\r\n$\\mathbb A$-homology manifold, and that $\\theta$ corresponds to\r\n$1_X$ in the isomorphism $H^0(X\\stackrel{f}\\to Y)\\cong H^0(X)$.\r\nSince $X$ and $Y$ are $\\mathbb A$-manifolds,  we get:\r\n$$\r\nf^{!}(\\mathbb A_Y)=D(f^*(D(\\mathbb A_Y)))=D(f^*(\\mathbb\r\nA_Y[2n]))=D(\\mathbb A_X[2n])=\\mathbb A_X.\r\n$$\r\nTherefore, $\\theta$ corresponds to an isomorphism in\r\n$$\r\nHom_{D^b_c(X)}(\\mathbb A_X,f^{!}\\mathbb A_Y)\\cong\r\nHom_{D^b_c(X)}(\\mathbb A_X,\\mathbb A_X)\\cong H^0(X).\r\n$$\r\nBy \\cite[7.3.2, proof of Proposition, p. 85]{FultonCF}, we deduce\r\nthat $\\theta$ is a strong orientation for $f$.\r\n\\end{proof}\r\n\r\n\r\n\r\n\\begin{proposition}\\label{strong2}\r\nLet $f:X\\to Y$ be a projective birational morphism between\r\nirreducible and quasi-projective complex varieties of the same\r\ncomplex dimension $n$. Let $\\theta\\in H^0(X\\stackrel{f}\\to Y)$ be a\r\nbivariant class. If $\\theta$ is a strong orientation for $f$, and\r\n$Y$ is an $\\mathbb A$-homology manifold, then also $X$ is so.\r\n\\end{proposition}\r\n\r\n\\begin{proof} Since $Y$ is an $\\mathbb A$-homology manifold, we have:\r\n$$\r\nf^{!}(\\mathbb A_Y)=D(f^*(D(\\mathbb A_Y)))=D(f^*(\\mathbb\r\nA_Y[2n]))=D(\\mathbb A_X[2n]).\r\n$$\r\nOn the other hand, if $\\theta$ is a strong orientation, then\r\n\\cite[loc. cit.]{FultonCF}\r\n$$\r\nf^{!}(\\mathbb A_Y)\\cong \\mathbb A_X.\r\n$$\r\nTherefore, we get $D(\\mathbb A_X[2n])\\cong \\mathbb A_X$. This means\r\nthat $\\mathbb A_X[n]$ is self-dual, i.e. $X$ is an $\\mathbb\r\nA$-homology manifold \\cite[proof of Theorem 3.7]{Brasselet}.\r\n\\end{proof}\r\n\r\n\\begin{remark}\\label{strong3}\r\nLet $f:X\\to Y$ be a birational, projective local complete\r\nintersection morphism between complex irreducible quasi-projective\r\nalgebraic varieties. Let $\\theta\\in H^0(X\\stackrel{f}\\to Y)$ be the\r\n{orientation class} of $f$. Then $\\theta$ has degree one (Remark\r\n\\ref{remintro}, $(iii)$). But, in general, in view of previous\r\nProposition \\ref{strong2}, $\\theta$ cannot be a strong orientation.\r\n\\end{remark}\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "meta": {"timestamp": "2021-09-07T02:38:18", "yymm": "2109", "arxiv_id": "2109.02585", "language": "en", "url": "https://arxiv.org/abs/2109.02585"}}
{"text": "\\section{Introduction}\n\nA long-standing science puzzle has been to explain the origin of the heaviest elements in the Universe, and more particularly the production of the elements heavier than iron up to uranium. One of the nucleosynthesis processes called for to explain the origin of these heavy elements is the rapid neutron-capture process or r-process \\citep{burbidge1957,cameron1957,arnould2020,arnould2007}.  It can account for the stable (and some long-lived radioactive) neutron-rich nuclides observed in stars of various metallicities, as well as in the Solar system, {\\it i.e.} about half of the elements heavier than Fe. However, for decades there existed no direct evidence of where in the Universe this process could occur \\citep{arnould2007,cowan2021}. \n\nThe r-process remains the most complex nucleosynthetic process to model from the astrophysics as well as nuclear-physics points of view. Early studies of core-collapse supernovae have raised a lot of excitement about the so-called neutrino-driven wind environment \\citep{qian1996,hoffman1997,Arcones2007}.\nHowever, until now a successful r-process cannot be obtained {\\it ab initio} without tuning the relevant parameters (neutron excess, entropy, expansion timescale) in a way that is not supported by increasingly more sophisticated existing models \\citep{witti1994,takahashi1994,roberts2010,wanajo2011,janka2012,wanajo2018c}.\nAlthough these scenarios remain promising, especially in view of their potential to contribute to the galactic enrichment significantly, they remain affected by large uncertainties associated mainly with the still complex mechanism responsible for the  supernova explosion and the persistent difficulties to obtain suitable r-process conditions in self-consistent dynamical explosion and neutron-star (NS) cooling models \\citep{janka2012,hudepohl2010,fischer2010,Janka2016,Muller2017,Bollig2017,Bolling2021,Pinedo2012}. \nMoreover, a subclass of core-collapse supernovae, the so-called jet-driven or jet-associated collapsars corresponding to the death of rapidly rotating, massive stars, some of which might also be the origin of observed long $\\gamma$-ray bursts \\citep{Woosley1993,macfadyen1999}, could represent another, promising r-process site. In these systems, r-process viable conditions may be found in magnetically driven jets \\citep{cameron2003,winteler2012,Nishimura2015,Mosta2018,Reichert2021,Grimmett2020} or in the ejecta launched from the central black-hole accretion disk \\citep{Cameron2001,Pruet2004, Surman2004, Siegel2019, miller2019a,just2021}.\n\nSpecial attention has also been paid to an alternative site, namely NS mergers following the initial study of the decompression of cold neutronized matter by \\cite{lattimer1974,lattimer1977,meyer1989}, who first proposed that merging binary NSs could eject neutron-rich material and be a suitable site where the r-process nucleosynthesis could take place. Recent studies including hydrodynamical simulations of NS merger systems have established that a significant amount of material ($\\sim10^{-3}-10^{-1}$ $M_{\\odot}$) can be ejected during such events \\citep[e.g.][]{rosswog1999, ruffert2001, oechslin2007, hotokezaka2013, bauswein2013, just2015,radice2018b,foucart2016}. \nIt was found that the neutron-rich conditions in the ejected material enable a robust r-process, producing a solar like abundance distribution up to the 3rd r-process peak and the actinides \\citep[e.g.][]{freiburghaus1999, goriely2011, korobkin2012, wanajo2014, just2015, radice2018b}. \n\nThe r-nuclide enrichment is predicted to originate from both the dynamical (prompt) material expelled during the NS-NS or NS-BH merger phase and from the outflows generated during the evolution of the merger remnant, which is a massive NS or a BH surrounded by an accretion torus. However, the detailed composition of each ejecta component, and therefore its role for the galactic chemical evolution, remains uncertain at this point, which is mainly owing to the computational difficulties associated with the treatment of neutrino interactions and details of the ejection mechanism. The impact of neutrinos is particularly challenging to assess for the dynamical ejecta, first because the phase of the merger requires a three-dimensional, general relativistic description and cannot be approximated by an axisymmetric evolution, and second, the evolution during the merger phase is extremely violent and characterized by short dynamical timescales and strong shocks. Early studies \\citep[e.g.][]{goriely2011,korobkin2012,mendoza-temis2015} found very neutron-rich ($Y_e\\la 0.1$) dynamical outflows and a significant solar-like production of $A>140$ elements. However, the impact of weak processes on the ejecta was either neglected in those simulations or was relatively minor because of dominant tidal ejection \\citep{korobkin2012} and, under such an approximation, the r-process nucleosynthesis yields were shown to be rather insensitive to the initial astrophysical conditions, such as the masses of the binary NS or the nuclear equation of state (EoS), due to fission recycling \\citep{goriely2011}.\n\nThis picture changed when weak interactions started to be included in both the hydrodynamical simulations as well as the nucleosynthesis calculations \\citep{wanajo2014, goriely2015a, martin2018, radice2016, foucart2016}. The weak reactions that are most important are the $\\beta$-interactions of electron neutrinos ($\\nu_e$) with free neutrons ($n$) and electron antineutrinos ($\\bar{\\nu}_e$) with free protons ($p$) as well as their inverse reactions,\n\\begin{equation}\n\\begin{aligned}\n & \\nu_e+ n \\rightleftharpoons p + e^- , \\label{eq:betareac}  \\\\\n & \\bar{\\nu}_e + p \\rightleftharpoons n + e^+  .\n\\end{aligned}\n\\end{equation}\nIn studies incorporating the above neutrino emission/absorption reactions the neutron richness of the dynamical ejecta was shown to be substantially altered by the neutrinos emitted by the post-merger remnant, driving the electron fraction up to $\\sim0.3-0.4$ and even $\\sim0.5$ in the polar regions. However, the exact impact of these nucleonic weak processes in NS mergers still remains unclear for several reasons. Firstly, there is a large spread in the applied approximations for the neutrino treatment in the literature or simply many studies do not take neutrino absorption into account \\citep[e.g.][]{wanajo2014, palenzuela2015,lehner2016,sekiguchi2015,foucart2016,bovard2017,radice2016,martin2018}. Secondly, the quantitative effects of weak reactions may depend on the details of the merger dynamics and the shock heating strength during the merging, and therefore on the adopted EoS \\citep{sekiguchi2015} and possibly, to some extent, on numerics. In addition, flavour conversions of the neutrino species can also affect the neutron richness, hence the nucleosynthesis in the merger ejecta \\citep{wu2017,george2020}. \n\nThe decay of freshly produced radioactive r-process elements can power a ``kilonova'', which is potentially observable in the optical electromagnetic spectrum in the aftermath of a NS merger event \\citep[e.g.][]{li1998, roberts2011, metzger2010, goriely2011, barnes2013,Kulkarni2005,Tanaka2013}. Indeed, such an electromagnetic counterpart, AT2017gfo \\citep{abbott2017d} was recently observed for the first time accompanying the gravitational wave signal, GW170817 \\citep[e.g.][]{abbott2017c}, from two merging NSs. Fits to the observed kilonova light curve provided a strong indication that r-process elements have been synthesized in this event \\citep[e.g.][]{kasen2017}. Recently, \\cite{watson2019} also reported the spectroscopic identification of strontium, providing the first direct evidence that NS mergers eject material enriched in heavy elements. The fact that strontium was present implies that weak interactions may play a prominent role to shape the composition of the ejecta, since weak interactions alter the initial composition towards higher $Y_e$ values.\n\nModels of the kilonova observed in the aftermath of GW170817 have indicated the presence of (at least) two kilonova components, the so-called ``blue'' and ``red''  components\\footnote{Note that the ``blue\" and ``red\" kilonova components did not exhibit spectral peaks in the blue optical and red optical bands, but rather in the red optical and near infrared bands, respectively.},  which are presumably associated with ejecta material having $A<140$ and $A>140$, respectively \\citep[e.g.][]{kasen2017}.  However, given the complexity of modelling the merger and its remnant the identification of these kilonova components with the basic ejecta components (dynamical vs. winds from a hypermassive NS or from a remnant BH-disk) is not yet unambiguous. In particular, the role of the dynamical ejecta, e.g. whether they contribute more to the blue or more to the red kilonova component, is not finally settled \\citep[e.g.][]{kasen2017,Kawaguchi2018}.\n\nThis work will contribute to the question of whether a more accurate description of neutrino interactions can significantly affect the r-process abundance yields in NS mergers.  \nTo this end, r-process nucleosynthesis network calculations will be performed on the full set of ejected trajectories from four relativistic hydrodynamical simulations using the so-called improved leakage-equilibration-absorption scheme (ILEAS) for neutrino transport  \\citep{ardevol-pulpillo2019}. ILEAS takes into account neutrino equilibration with matter in optically thick regions, uses an improved prescription of the diffusion timescale that is consistent with the diffusion law, and describes the transition to free streaming via a flux-limiter. Moreover, ILEAS includes re-absorption in optically thin regions. Hence, ILEAS should provide a fairly realistic picture of the impact of neutrino interactions on the early ejecta in NS mergers.\nThe present study is divided into two papers. Part I, the present paper, focuses on the r-process nucleosynthesis and the subsequent heating rate produced in the dynamical ejecta, while Part II \\citep{just2021b} studies consistently ({\\it i.e.} on the basis of our four NS merger models and their determined nucleosynthesis) the resulting electromagnetic signal expected from the dynamical ejecta. \nSince we will focus here on the dynamical ejecta only, neglecting the possible contribution from the hypermassive NS or the remnant BH-disk winds, no attempt is made to compare our results with observations, neither of kilonovae nor from spectroscopy of r-process-rich stars. This aspect will be studied in another forthcoming paper.\n \nIn Sect.~\\ref{sect_weak}, we introduce our NS merger models and their basic properties. We discuss the ejecta electron fraction and show that the neutron richness is significantly affected by the presence of neutrinos, in particular when compared to cases neglecting neutrino absorption and emission. The results of the nucleosynthesis calculations are discussed in Sect.~\\ref{sec_nuc_q}, describing both the composition and the radioactive decay heat of the ejected material. Special attention is paid to the angular and velocity dependence of our nucleosynthesis results which are analysed in detail in Sect.~\\ref{sec_ang_vc}. A comparison of our results to other simulations including nucleonic weak interactions, similar NS mass configurations and EoSs is performed in Sect.~\\ref{sect_comp}.\nFinal remarks and conclusions are given in Sect. \\ref{sect_concl}.\n\n\\section{NS merger models}\n\\label{sect_weak}\n\nIn the present study we analyse the composition of material dynamically ejected from two binary NS merger systems using two different EoSs,  giving in total four models. \nThe hydrodynamical models are based on the work of \\citet{ardevol-pulpillo2019} that includes simulations of symmetric binary merger systems consisting of 1.35-1.35$M_{\\odot}$\\ NSs, which have been extended to include asymmetric systems consisting of 1.25-1.45$M_{\\odot}$\\ NSs.  \nIn these simulations, the recent ILEAS method is coupled to a relativistic smoothed-particle-hydrodynamics (SPH) code that assumes a conformally flat spatial metric to reduce the complexity of the Einstein equations. \nAs shown for instance in \\citet{Ciolfi20}, the impact of magnetic fields on the properties of the dynamical ejecta can be neglected.\nILEAS is designed to follow the neutrino effects in the hot and dense astrophysical plasma \\citep{ardevol-pulpillo2019}. This approximation improves the lepton number and energy losses of traditional leakage descriptions by a novel prescription of the diffusion time-scale based on a detailed energy integral of the flux-limited diffusion equation. The leakage module is also supplemented by a neutrino-equilibration treatment that ensures the proper evolution of the total lepton number and medium plus neutrino energies as well as neutrino-pressure effects in the neutrino-trapping domain. In addition,  a simple and straightforwardly applicable ray-tracing algorithm is used for including re-absorption of escaping neutrinos especially in the decoupling layer and during the transition to semitransparent conditions.  All details can be found in \\cite{ardevol-pulpillo2019} where it was shown in particular that ILEAS can satisfactorily reproduce local losses and re-absorption of neutrinos as found in more sophisticated transport calculations on the level of 10\\%\\footnote{The tests in \\cite{ardevol-pulpillo2019} were performed for a spherically symmetric proto-NS configuration and a BH-torus. The accuracy for the merger case is likely similar though not exactly known due to the lack of detailed reference solutions. }.\nAt the start of the hydrodynamical simulation, a few orbits before the merging of the two NSs, equilibrium is assumed to hold between electrons, positrons and neutrinos. \nThe Cartesian grid covering the NS in which the neutrino interactions are calculated expands 100~km in all six directions from the centre of mass of the system, with a resolution of 0.738~km\\footnote{\nAlthough detailed resolution tests have yet to be conducted, the rather modest resolution of 0.738~km can be justified by three main arguments. First, the density gradients at the neutron star surfaces are steep only prior to the collision of the two stars, {\\it i.e.} at times when the neutrino emission is still low. Steep density gradients develop again at late post-merger times after considerable neutrino cooling of a stable merger remnant. Such a late evolution phase, however, is not reached in our present set of merger models. Second, our neutrino treatment is not based on numerically determined gradients but employs space integrals that are usually less sensitive to fine spatial structures. Third, the possibility of resolving steep density gradients or local density variations on small scales is essentially set by the numerical limits from SPH hydrodynamics rather than from the neutrino treatment.}. \nAfter the merger, as the remnant torus expands, the grid size is progressively increased keeping the whole remnant covered at all times and the same resolution (cell size) throughout the simulation \\citep{ardevol-pulpillo2019}.\nThese relativistic hydrodynamic simulations also provide the temperature evolution to which no post-processing is applied \\cite[in contrast to][]{goriely2011} to retain consistency between the neutrino properties and the other thermodynamical properties.\nIn these simulations, either the temperature-dependent DD2 \\citep{hempel2010,typel2010} or SFHo EoS \\citep{steiner2013} is adopted.\nFor clarity,  from now on the DD2 1.35-1.35$M_{\\odot}$\\ and 1.25-1.45$M_{\\odot}$\\ merger models will be referred to by DD2-135135 and DD2-125145, respectively,  and similarly the SFHo 1.35-1.35$M_{\\odot}$\\ and 1.25-1.45$M_{\\odot}$\\ models as SFHo-135135 and SFHo-125145, respectively. \n\nFor the four models, the number of unbound mass elements are 783,  1263,  1290 and 4398 with a total ejected mass of $2\\times 10^{-3}$$M_{\\odot}$, $3.2\\times10^{-3}$$M_{\\odot}$, $3.3\\times10^{-3}$$M_{\\odot}$\\ and $8.6\\times10^{-3}$$M_{\\odot}$\\ for  DD2-135135,  DD2-125145, SFHo-135135 and SFHo-125145, respectively. \nIt should, however, be stressed that defining the exact total amount of mass dynamically ejected from the system is far from being an easy task \\citep[see e.g.][]{Foucart2021}, since different criteria for gravitationally unbound conditions can be considered, and furthermore, the adopted criterion can be applied at different times or radii. \nWe employ the positive stationary energy criterion  ($\\varepsilon_{\\rm stationary}>0$) stating that a mass element becomes unbound if at a chosen time $t_{\\rm ej}$ and beyond a certain radius $r_{\\rm ej}$, the kinetic plus thermal energy exceeds the gravitational one \\citep[modulo relativistic corrections, see, e.g., ][for a detailed derivation of this criterion]{oechslin2007}.\nIn Fig.~\\ref{fig_mejec} we compare for each model the growth of $M_{\\rm ej}$ with time, where $M_{\\rm ej}(t)$ is the mass of all particles that fulfill $\\varepsilon_{\\rm stationary}>0$ and $r >\nr_{\\rm ej}$, and $r_{\\rm ej}$ is varied between 0, 100~km, 200~km, 300~km, and 400~km. Independent of the chosen value of $r_{\\rm ej}$, the total ejected mass is found to grow with time and not to saturate at the end of the simulations.  In order to select ejecta particles for our nucleosynthesis calculations, an ejection\nradius of 100~km and time of 7~ms are adopted, as shown by the crosses in Fig.~\\ref{fig_mejec}.\nA finite value of $r_{\\rm ej}$ is chosen to ensure the correct\nidentification of expanding material, however, as visible in Fig.~\\ref{fig_mejec}, the choice of $r_{\\rm ej}=100$~km is equivalent to applying no radius criterion at all. The similar, nearly parallel, line shape for different values of $r_{\\rm ej}$ indicates the robustness of using a low value of $r_{\\rm ej}$ in the sense that a larger value of $r_{\\rm ej}$ would count the same ejecta material but just at a correspondingly later time $t_{\\rm ej}$ (the time needed for material to reach that particular  $r_{\\rm ej}$). We indeed verified that, for model DD2-135135, nearly the same mass elements are counted when using $t_{\\rm ej}=10$~ms and $r_{\\rm ej}=220$~km instead of 7~ms and 100~km, respectively. \nNote that the time $t_{\\rm ej}$ is chosen in the middle of the shallow, nearly linear growth since at later times the ejecta are more likely to be affected by magneto-hydrodynamical and secular effects in the merger remnant. The exact physical reasons behind such a continuous increase in the mass loss has not been determined, but it could be associated with various mechanisms, such as a growing \n$m=1$ mode \\citep{nedora2019}, neutrino-driven winds, or viscous effects and angular momentum redistribution in the merger remnant.\nThe ejected masses for the different models are summarised in Table~\\ref{tab:prop}. See Sect. ~\\ref{sect_comp} for a comparison to other works.\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=\\textwidth]{ejectaevolution_rcrit_4models.pdf}\n\\caption{(Color online). \nTime evolution of the dynamically ejected masses for our four merger\nmodels as measured by the criterion $\\varepsilon_{\\rm stationary}>0$ applied beyond radii of $r_{\\rm ej}=0$, 100~km, 200~km, 300~km, 400~km. The $t=0$ reference time corresponds to the phase of the merger when the remnant is maximally compressed about 1~ms after the stars first touched. The fact that lines for different values of $r_{\\rm ej}$ are similar to each other indicates that material flagged\nunbound at low radii keeps expanding and truly ends up as ejected\nmaterial. The crosses at time $t=t_{\\rm ej}\\sim 7$~ms for lines with $r_{\\rm ej}=100$~km correspond to the ejecta material that is adopted for nucleosynthesis calculations in this study (see also Table~\\ref{tab:prop}).\n}\n\\label{fig_mejec}\n\\end{center}\n\\end{figure*}\n\n\nFor each of these relativistic hydrodynamical simulations, a post-processing reaction network is used to calculate the composition and radioactive decay heat of the ejected material.\nThe nucleosynthesis calculation starts as soon as the temperature drops below  $T=10^{10}$~K and the density is below the neutron-drip density $\\rho_\\mathrm{drip}\\simeq 4.2\\times 10^{11}$\\,g\\,cm$^{-3}$, at which the initial abundances of heavy nuclei are determined by nuclear statistical equilibrium at the given electron fraction, density and temperature. \nThe density at which the full reaction network is initiated is referred to as $\\rho_\\mathrm{net}$.  \nTo take advantage of the complete ILEAS calculation,  the initial $Y_e$ adopted by the network calculation corresponds to the value estimated by the hydrodynamical model at the end of the simulation, {\\it i.e.} some 10~ms after the plunge. Note that the evolution of the $Y_e$ rapidly reaches a plateau before the end of the hydrodynamical simulation and before the beginning of the network calculation, for which reason the difference in $Y_e$ at $\\rho_\\mathrm{net}$ and at the last hydrodynamical time step is found to be smaller than 0.01. Only a handful of trajectories have a $Y_e$ difference larger than 0.01, though always smaller than 0.05. \n\nFor the first 10\\,ms after the plunge the density history is consistently followed by the numerical \nsimulation. Afterwards the ejected matter is assumed to expand freely with constant velocity. The radii of the ejecta clumps thus grow linearly with time $t$ and consequently their densities drop like $1/t^3$. As soon as the full reaction network is initiated at $\\rho=\\rho_\\mathrm{net}$, the temperature evolution is determined on the basis of the laws of thermodynamics, allowing for possible nuclear heating through $\\beta$-decays, fission, and $\\alpha$-decays \\citep{meyer1989}.\nDuring this extrapolation,  the weak interactions of nucleons (Eqs.~\\ref{eq:betareac}) are neglected since the hydrodynamical simulation shows that the $Y_e$ has reached a plateau and is not affected anymore by these weak interactions.\n\nThe corresponding $Y_e$ distributions at $\\rho=\\rho_\\mathrm{net}$, {\\it i.e.} initial to the full reaction network computation of the nucleosynthesis, together with the mean value $\\langle Y_e \\rangle$ are shown in Fig.~\\ref{fig_yedist} for the four NS-NS merger models. Each of these distributions are used as initial condition to estimate the final composition of the ejecta through a full reaction network (see Sect.~\\ref{sec_nuc}). \n\n\\begin{figure}\n\\includegraphics[scale=0.33]{yedist_mods.png}\n\\caption{(Color online). Fractional mass distributions of the\n    matter ejected as a function of $Y_e$ at the time of $\\rho=\\rho_\\mathrm{net}$  together \n    with the mean electron fraction $\\langle Y_e \\rangle$. From the top: \n    a) DD2-125145, b)  DD2-135135, c) SFHo-125145\n    and d) SFHo-135135 NS-NS merger models. \n    }\n\\label{fig_yedist}\n\\end{figure}\n\nTo study the impact of nucleonic weak processes, we have created a reference case in which no such interactions are included after the time when the  lapse function reached a first minimum, which is defined as $t=0$, as in Fig.~\\ref{fig_mejec}.\nThis model follows the same density and temperature evolution as the original DD2-135135 model, but considers the electron fraction at the time  $t=0$, as estimated from the hydrodynamical simulation, and assumes no more weak interaction (Eqs.~\\ref{eq:betareac}) with nucleons to take place afterwards. \nThus, $Y_e$ is assumed to remain unaffected by nucleonic weak processes in all ejected mass elements after $t = 0$. \nThis approximation is referred to as the 'no neutrino' case.\nIn Fig.~\\ref{fig_yedist_cases}, the $Y_e$ distributions at $\\rho=\\rho_\\mathrm{net}$ together with the mean values $\\langle Y_e \\rangle$ are displayed for the DD2-135135 cases with and without neutrinos.  It should be noted that the $Y_e$ distribution without neutrinos does not correspond to the one found in cold NSs  \\citep[which was assumed in][]{goriely2011} since matter has been affected by neutrino absorption and emission between the start of the simulation and  time $t=0$. In particular, a significant amount of ejected material has a $Y_e >0.1$, as seen in Fig.~\\ref{fig_yedist_cases}a.\n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{yedist_cases.png}\n  \\caption{(Color online). Same as Fig.~\\ref{fig_yedist} for the two cases, without (a) or with (b) weak nucleonic interactions, of the DD2-135135 model discussed in the text.  Note that the ILEAS case with neutrinos corresponds to Fig.~\\ref{fig_yedist}b.\n  }\n\\label{fig_yedist_cases}\n\\end{figure} \n\nSome properties of the ejecta are summarised in Table~\\ref{tab:prop}, including in particular the total ejected mass, the mass of its high-velocity component ({\\it i.e.} with $v\\ge 0.6c$) or the mean electron fraction at $\\rho=\\rho_{\\rm net}$. Additional properties resulting from the nucleosynthesis calculations described in Sect.~\\ref{sec_nuc_q} are also given in Table~\\ref{tab:prop}. \nThese properties are also estimated within the three different angular regions, namely the polar ($0^\\circ < |90^\\circ - \\theta| \\leq 30^\\circ$), middle ($30^\\circ < |90^\\circ - \\theta| \\leq 60^\\circ$) or equatorial ($60^\\circ < |90^\\circ - \\theta| \\leq 90^\\circ$) regions, highlighting the angular dependence of the ejecta composition, as discussed in Sect.~\\ref{sec_ang_vc} and of particular relevance to understand the kilonova light curve studied in Part II \\citep{just2021b}.\n\n\n\\begin{table*}\n\\centering\n\\caption{Summary of the ejecta properties for the five models considered in the present study, including the contribution stemming from three different angular regions, namely the polar ($0^\\circ < |90^\\circ - \\theta| \\leq 30^\\circ$), middle ($30^\\circ < |90^\\circ - \\theta| \\leq 60^\\circ$) or equatorial ($60^\\circ < |90^\\circ - \\theta| \\leq 90^\\circ$)  regions. These properties correspond to the total ejected mass $M_{\\rm ej}$, the mass of the high-velocity ejecta $M_{\\rm ej}^{v \\ge 0.6c}$,  the mean velocity $\\langle v/c \\rangle$, the mean $Y_e$ at $\\rho=\\rho_{\\rm net}$, the neutron mass fraction $X_n$ at $t=20$~s, the lanthanide plus actinide mass fraction $X_{LA}$ ({\\it i.e.} with respect to the total ejecta), the relative amount nuclei $x$ with respect to the mass in each region $x_{A>69}$ and $x_{A>183}$ for r-process nuclei and third-r-process peak nuclei, respectively, and the $^{232}$Th to $^{238}$U ratio. }\n\\begin{tabular}{llccccccccc}\n\\hline \\hline\nModel & Region & $M_{\\rm ej}$ & $M_{\\rm ej}^{v \\ge 0.6c}$ & $\\langle v/c \\rangle$ & $\\langle Y_{e,\\rho_{net}}\\rangle$ & $X_n^{t=20s}$ & $X_{LA}$ & $x_{A>69}$ & $x_{A>183}$ & Th/U \\\\\n  &   & [$10^{-3}$ $M_{\\odot}$] & [$10^{-4}$ $M_{\\odot}$] &   &   & [$10^{-3}$] &   &   &   &   \\\\\n\\hline\n  DD2-125145 & Total        & 3.20 & 1.77   & 0.25   & 0.22   & 7.57   & 0.152   & 0.90   & 0.15   & 1.39 \\\\\n             & Equatorial   & 2.20 & 0.91   & 0.24   & 0.19   & 1.60   & 0.126   & 0.97   & 0.30   & 1.40 \\\\\n             & Middle       & 0.78 & 0.62   & 0.24   & 0.26   & 4.22   & 0.022   & 0.95   & 0.11   & 1.35 \\\\\n             & Polar        & 0.22 & 0.23   & 0.27   & 0.33   & 1.76   & 0.004   & 0.78   & 0.05   & 1.30 \\\\\n\\hline\n  DD2-135135 & Total        & 1.99 & 0.87   & 0.25   & 0.27   & 7.21   & 0.107   & 0.88   & 0.11   & 1.35 \\\\\n             & Equatorial   & 1.35 & 0.46   & 0.25   & 0.25   & 4.71   & 0.086   & 0.95   & 0.17   & 1.36 \\\\\n             & Middle       & 0.45 & 0.23   & 0.24   & 0.29   & 1.72   & 0.017   & 0.93   & 0.09   & 1.30 \\\\\n             & Polar        & 0.20 & 0.18   & 0.27   & 0.34   & 0.78   & 0.003   & 0.75   & 0.07   & 1.33 \\\\\n\\hline\n SFHo-125145 & Total        & 8.67 & 2.56   & 0.24   & 0.24   & 2.75   & 0.114   & 0.95   & 0.12   & 1.38 \\\\\n             & Equatorial   & 5.05 & 1.46   & 0.24   & 0.22   & 0.32   & 0.079   & 0.97   & 0.20   & 1.38 \\\\\n             & Middle       & 2.89 & 0.79   & 0.24   & 0.27   & 1.22   & 0.030   & 0.95   & 0.11   & 1.38 \\\\\n             & Polar        & 0.73 & 0.31   & 0.26   & 0.30   & 1.21   & 0.005   & 0.92   & 0.05   & 1.34 \\\\\n\\hline\n SFHo-135135 & Total        & 3.31 & 1.53   & 0.29   & 0.26   & 4.76   & 0.115   & 0.92   & 0.11   & 1.35 \\\\\n             & Equatorial   & 2.17 & 0.93   & 0.31   & 0.24   & 0.46   & 0.088   & 0.95   & 0.18   & 1.36 \\\\\n             & Middle       & 0.86 & 0.51   & 0.25   & 0.29   & 2.54   & 0.020   & 0.93   & 0.08   & 1.34 \\\\\n             & Polar        & 0.28 & 0.09   & 0.26   & 0.30   & 1.76   & 0.007   & 0.89   & 0.07   & 1.26 \\\\\n\\hline\n  DD2-135135 & Total        & 1.99 & 0.87   & 0.25   & 0.13   & 9.17   & 0.246   & 0.97   & 0.43   & 1.31 \\\\\n no neutrino & Equatorial   & 1.35 & 0.46   & 0.25   & 0.11   & 5.51   & 0.174   & 0.98   & 0.48   & 1.31 \\\\\n             & Middle       & 0.45 & 0.23   & 0.24   & 0.14   & 2.83   & 0.048   & 0.97   & 0.42   & 1.29 \\\\\n             & Polar        & 0.20 & 0.18   & 0.27   & 0.18   & 0.83   & 0.024   & 0.96   & 0.40   & 1.28 \\\\\n\\hline \\hline\\end{tabular}\n\\label{tab:prop}\n\\end{table*}\n\n\n\\section{Nucleosynthesis and radioactive decay heat}\n\\label{sec_nuc_q}\n\n\\subsection{Nucleosynthesis}\n\\label{sec_nuc}\n\nThe nucleosynthesis is followed with a full reaction network including all 5000 species from protons up \nto $Z=110$ lying  between the valley of $\\beta$-stability and the neutron-drip line \\citep[for more details, see][]{goriely2011,bauswein2013,just2015}. All charged-particle \nfusion reactions on light and medium-mass elements that play a role when the nuclear statistical \nequilibrium freezes out are included in addition to radiative neutron captures and photodisintegrations. \nThe reaction rates on light species are taken from the NETGEN library, which includes all the latest \ncompilations of experimentally determined  reaction rates \\citep{xu2013}.  By default,  experimentally unknown reactions \nare estimated  with the TALYS code \\citep{goriely2008,koning2012} on the basis of the HFB-21 nuclear masses \\citep{goriely2010}, the HFB plus combinatorial nuclear level densities \\citep{goriely2008} and the QRPA E1 strength functions \\citep{goriely2004}. \nFission and $\\beta$-decay processes, including neutron-induced fission, spontaneous fission, $\\beta$-delayed \nfission, as well as $\\beta$-delayed neutron emission, are considered as detailed in \\citet{goriely2015}.  All fission processes are estimated on the basis of the HFB-14 \nfission paths \\citep{goriely2007} and the full calculation of the corresponding barrier penetration \n\\citep{goriely2009}.  The fission fragment distribution is taken from the microscopic scission-point model,  known as the SPY model,  as described \nin \\citet{lemaitre2019}.  The  $\\beta$-decay processes are taken from the mean field plus relativistic QRPA calculation of \\citet{marketin2016},  when not available \nexperimentally. This nuclear physics set represents our standard input. A sensitivity analysis of our results to the nuclear ingredients is postponed to a future study.\n\nFig.~\\ref{fig_rpro_aa} shows the final isotopic abundance distributions obtained if we adopt the initial ILEAS $Y_e$ distributions of Fig.~\\ref{fig_yedist} for the four hydrodynamical merger models. Given the similar and relatively wide initial $Y_e$ distributions, the resulting abundance distributions are almost identical and reproduce rather well the solar system r-abundance distribution above $A\\ga 90$. For all models, we have an efficient r-process nucleosynthesis with the production of lanthanides, second- and third-peak nuclei. The lanthanide plus actinide mass fraction $X_{LA}$, the relative amount of r-process nuclei $x_{A>69}$ and of third-r-process peak nuclei $x_{A>183}$,({\\it i.e.} with $A>183$) are summarised in Table~\\ref{tab:prop} for each model. In particular, the ejecta of all four systems can be seen to consist of 88 up to 95\\% of $A>69$ r-process material with lanthanides plus actinides ranging between 11 and 15\\% in mass. In all four models, the third r-process peak is rather well produced and includes between 11 to 15\\% of the total mass. The DD2-125145 model has a relatively larger production of the heaviest r-process elements, as indicated by a larger value of $x_{A>183}$, which reaches about 30\\% in the equatorial region.\n\nFor the DD2-135135 model, Fig.~\\ref{fig_rpro_aa_cases} shows the final isotopic abundance distributions of the case without neutrinos compared to the case where neutrino interactions are included.\n If we assume the initial $Y_e$ distribution to be unaffected by weak interactions (Fig.~\\ref{fig_yedist_cases}a),  the resulting distribution is characteristic of what has been obtained by most of the calculations neglecting neutrino absorption, {\\it i.e.} the production of $A \\ga 130-140$ is considerably enhanced due to the dominance of $Y_e<0.1$ trajectories and an efficient fission recycling. The production of $A\\simeq 130$ nuclei in the second r-process peak is linked to the non-negligible presence of $Y_e> 0.15 $ trajectories (see Fig.~\\ref{fig_yedist_cases} and the discussion in Sect.~\\ref{sect_weak}). The ``no neutrino'' case is found to be composed of 2.1 (4.7) times more lanthanides (actinides) and a significantly more pronounced third r-process peak (Table ~\\ref{tab:prop}).\n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{rpro_aa.png}\n  \\caption{(Color online).  Final mass fractions of the material ejected as a function of the atomic mass $A$ for our DD2 1.25--1.45$M_{\\odot}$\\,  DD2 1.35--1.35$M_{\\odot}$\\, SFHo 1.25--1.45$M_{\\odot}$\\ and  SFHo 1.35--1.35$M_{\\odot}$\\ NS-NS merger models.  The solar system r-abundance distribution  (open circles) from \\citet{goriely1999} is shown for comparison and arbitrarily normalised to the DD2 asymmetric model at the third r-process peak ($A\\simeq 195$).}\n\\label{fig_rpro_aa}\n\\end{figure} \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{rpro_aa_cases.png}\n  \\caption{(Color online).  Mass fractions of the $2\\times 10^{-3}$$M_{\\odot}$\\ of material\n    ejected in our 1.35--1.35$M_{\\odot}$\\ NS-NS merger model with DD2 EoS as a function of the atomic mass $A$ for the two cases studied here, {\\it i.e} with (ILEAS) or without (no neutrinos) weak nucleonic interactions.  The solar system abundance distribution is normalised as in Fig.~\\ref{fig_rpro_aa}. }\n\\label{fig_rpro_aa_cases}\n\\end{figure} \n\nThe final elemental abundance distributions obtained from the four hydrodynamical models including weak processes are shown in Fig.~\\ref{fig_rpro_zz}. As for the isotopic distributions, there are only minor differences between the four elemental distributions. In particular,  the production of actinides is larger for the two asymmetric merger models. However, the $^{232}$Th to $^{238}$U ratio remains rather constant and equal to 1.35--1.39 for all four models (Table~\\ref{tab:prop}), a property of particular interest to cosmochronometry \\citep[e.g][]{goriely2016}.\n\nThe elemental distributions of the DD2-135135 cases with and without neutrinos are presented in Fig.~\\ref{fig_rpro_zz_cases}.  We can see that a rather different prediction is obtained when including weak processes, in particular, a significantly smaller amount of $Z \\ga 50$ elements is produced.  \nHowever, although the actinide production for the ILEAS case is significantly smaller compared to the reference neutrino-less simulation, the elemental ratio Th/U remains rather constant.  \nFor the ILEAS case, the Ni to Zr region dominates the ejecta,  and the production of Sr ($Z=38$) is 14 times larger compared to the case without neutrinos.  Sr is of special interest after its identification by \\citet{watson2019} in the AT2017gfo spectrum.  Such a different elemental distribution impacts the observed kilonova light curve,  as discussed in Part II \\citep{just2021b}.\n\n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{rpro_zz.png}\n  \\caption{(Color online). Same as Fig.~\\ref{fig_rpro_aa} for the elemental abundance distributions given by the molar fractions. The solar distribution is normalized to the DD2-125145 prediction of the third r-process peak.}\n\\label{fig_rpro_zz}\n\\end{figure} \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{rpro_zz_cases.png}\n  \\caption{(Color online). Same as Fig.~\\ref{fig_rpro_aa_cases} for the elemental abundance distributions given by the molar fractions. The solar distribution is normalized as in Fig.~\\ref{fig_rpro_zz}.}\n\\label{fig_rpro_zz_cases}\n\\end{figure} \n\n\\subsection{Radioactive decay heat}\n\\label{sec_q}\nThe energy release by radioactive decay is estimated consistently within the same nucleosynthesis network for each of the models considered. No thermalisation efficiency is included at this stage and only the contribution from neutrino energy loss in each of the $\\beta$-decays are removed following the prescription of \\citet{Fowler1975},  \n{\\it i.e.} what we denote as radioactive heating rate $Q$ in this study is the total energy carried by $\\alpha$, $\\beta$, and $\\gamma$ particles as well as fission products.  \nFig.~\\ref{fig_Q_t} shows the time evolution of the radioactive heating rate $Q$ for the four hydrodynamical models including weak interactions. \nAs indicated by the arrow, a bump corresponding to the ejection of free neutrons that decay after $t=10$~min is visible. \nIn particular, the models using the DD2 EoS have the largest ejection of free neutrons with a mass fraction of 0.7\\% at late time $t\\simeq 20$~s (see  Table~\\ref{tab:prop}). \nThe absence of neutrino interactions would provide even larger quantities, typically a $\\sim 30$\\% increase of ejected free neutrons.\nFor both SFHo models the ejected amount of free neutrons is smaller, corresponding to 0.3\\% and 0.5\\% of the ejected material for the SFHo-125145 and SFHo-135135 models, respectively. \nIn these cases, lower ejection velocities in comparison to the DD2 models are found, and therefore a less noticeable feature linked to neutron decay is observed.\nThe decay of free neutrons may power a precursor signal of the kilonova,  as discussed in \\citet{metzger2015}. The inclusion of neutrino interaction may, however, decrease its strength, though it is essentially linked to the very fast ejection of a few neutron-rich mass elements.  \n\nIn general, the heating rate is dominated by a large number of neutron-rich $A \\la 200$ nuclei that $\\beta$-decay towards the valley of stability. Even after several days, the main contribution to the heating rate comes from a few $\\beta$-unstable isotopes with half-lives on the order of days. However, at late time ($t>10$~d), the contribution from $\\alpha$-decay and spontaneous fission of trans-Pb species starts to become significant (Fig.~\\ref{fig_Q_t_cases}). \nMore specifically, the $\\alpha$-decay chains starting from $^{222}$Rn, $^{223-224}$Ra and, in particular, $^{225}$Ac significantly contribute to the heating rate at time $t>10$~d.  In addition,  the $\\alpha$-decay of $^{253}$Es and $^{255}$Es produces an important amount of heat, however,  several orders of magnitude less than the four aforementioned-mentioned $\\alpha$-decay chains since their daughter nuclei quickly $\\beta$-decay to long-lived Cf. \nAmong the fissioning species, $^{254}$Cf is the only notable isotope, in fact, it has one of the largest heating rates across all decay modes and isotopes for $t>10$~d. \nThe production of trans-Pb are often taken as  the signature of r-processing beyond the heaviest stable elements \\citep{wanajo2018,wu2019,zhu2018}.\nAmong our four hydrodynamical models, it is the asymmetric DD2 model that shows the most pronounced contribution to the heating rate by trans-Pb species for  $t > 10$~d, as also shown by the large production of Th and U in Fig.~\\ref{fig_rpro_aa}. \n\nIn Fig.~\\ref{fig_Q_t_cases} the heating rate of our two models with and without neutrino interactions are compared, highlighting the relative contribution from the main decay modes $\\beta$, $\\alpha$ and fission.  In general, the time evolution of the heating rate due to $\\alpha$-decay and spontaneous fission follow the same trend for both models, where the heating rate of the no-neutrinos model is up to one order of magnitude higher. For the total heating rate, the main differences occur around $t\\sim7$~h, 30~d and after 150~d (see Fig.~\\ref{fig_dQ} for the heating rate versus mass number at these three time points). \nAt 7~h after the merger,  the discrepancies are caused by an excess of heat for the ILEAS model mainly generated by the $\\beta$-decay of  $^{88}$Kr-$^{88}$Rb and $^{92}$Y (top panel Fig.~\\ref{fig_dQ}).  \nThis can be seen in Fig.~\\ref{fig_rpro_zz_cases} where an excess amount of elements in the $33 \\le Z \\le\\ 39$ region are produced in the simulation including neutrinos and are responsible for the large radioactive heat through $\\beta$-decays at times between 0.1 and 1 day (Fig.~\\ref{fig_Q_t_cases}).  The same effect can be seen after $t>60$~d where in the no-neutrinos case $\\beta$-decay heating rate starts to decrease, while this does not occur within the ILEAS model due to heat generated by $A<130$ nuclei with longer half-lives. \nAfter $t>10$d the signature of trans-actinides production start to differ significantly. The ILEAS model is still dominated by $\\beta$-decays of $A \\la 200$ species, however,  for the no-neutrinos model the contribution from the $A=222-225$ $\\alpha$-decay chains and spontaneous fission of $^{254}$Cf becomes important.  In addition, the case without weak interactions has a larger production of elements with $A>130$ (see Fig.~\\ref{fig_rpro_zz_cases}), leading to more heat generated by $\\beta$-decays, in particular by $^{140}$Ba and $^{140}$La  (see middle panel Fig.~\\ref{fig_dQ}).\nLater at $t=150$~d (bottom panel Fig.~\\ref{fig_dQ}), the major trans-actinide contributor is $^{254}$Cf, with a heating rate about 4 times larger in the no-neutrinos model, while the $\\beta$-decay of $A<130$ nuclides powers the heating rate in the ILEAS model.\n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{Q_t.png}\n  \\caption{(Color online). Time evolution of the radioactive heating rate $Q$  for the four hydrodynamical models including weak interactions. The arrow at $t=10$~min corresponds to the decay half-life of the neutron.}\n\\label{fig_Q_t}\n\\end{figure} \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{Q_t_c_dmodes.png}\n  \\caption{(Color online). Same as Fig.~\\ref{fig_Q_t} for the symmetric DD2 model with (solid lines) and without (dashed lines) weak nucleonic interactions. The total heating rate is shown in black, the $\\beta$-, $\\alpha$- and fission contributions in green, orange and purple colours, respectively. }\n\\label{fig_Q_t_cases}\n\\end{figure} \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{dQ_A_c12.png}\n  \\caption{(Color online).  Contribution to the radioactive heating rate $Q$ stemming from individual nuclear decays versus mass number $A$ at $t=7$~h, 30~d and 150~d in the top, middle and bottom panels, respectively. The no-neutrinos model is represented by a dot and the ILEAS model by a cross and both predictions are connected by a line. The colours represent the decay modes as in Fig.~\\ref{fig_Q_t_cases}.  }\n\\label{fig_dQ}\n\\end{figure} \n\n\n\n\n\n\\subsection{Sensitivity to the electron fraction}\n\\label{sect_sens}\n\nAs briefly discussed in Sect.~\\ref{sect_weak}, NS merger simulations still suffer from numerous uncertainties connected to, among others, the neutrino transport and the equation of state. Those uncertainties inevitably affect the initial conditions for the nucleosynthesis calculations within the model described in Sect.~\\ref{sect_weak}. In particular, the initial $Y_e$ at the time of $\\rho=\\rho_\\mathrm{net}$ may differ by $\\pm 0.05$ or even $\\pm 0.10$ with respect to the present modelling. To test the sensitivity with respect to the many uncertainties affecting still the hydrodynamical simulations, we have considered extreme cases in which the  $Y_e$ at the time of $\\rho=\\rho_\\mathrm{net}$ is systematically increased or decreased by 0.05 or even 0.10. We show in Figs.~\\ref{fig_rpro_dye}-\\ref{fig_Q_dye} the impact of such different initial conditions on the final composition of the ejected material as well as the time evolution of the radioactive heating rate and the lanthanide+actinide mass fraction $X_{LA}$ for the DD2-135135 model.\n\nAs seen in Fig.~\\ref{fig_rpro_dye}, a systematic increase of $Y_e$ by 0.1 is not sufficient to give rise to the production of the first r-process peak, but rather favours the synthesis of the Fe, Ni or Zn  even-$N$ isotopes in the $A=58-66$ region, as well as $^{88}$Sr and $^{84,86}$Kr. As a consequence, a significant reduction in the production of the second and third r-process peaks, as well as lanthanides and actinides, can be observed (Fig.~\\ref{fig_Q_dye}). \nNote, however, that even in the least neutron-rich case where $Y_e$ is increased by 0.10 the mass fraction of $X_{LA}\\simeq 0.02$ remains not negligible.\nThe radioactive decay heat $Q$ is found to follow rather well the empirical approximation $Q_0=10^{10}~t[{\\rm d}]^{-1.3}$ \\citep{metzger2010} in all the 5 cases shown in Fig.~\\ref{fig_Q_dye}.  The differences seen in the various $Y_e$ cases in Fig.~\\ref{fig_Q_dye} can be explained by the same effects as those found in the  comparison between the cases with and without weak interactions described in Sect.~ \\ref{sec_q}.\nIn the most neutron-rich case ($Y_e-0.10$), the $Q$ enhancement around $t=10^{-2}$~d and $t>10$~d is due to the decay of free neutrons and actinides, respectively. \nFor the least neutron-rich case, the trans-actinides signal at $t>10$~d completely disappears and the heating rate is essentially powered by $\\beta$-decay of $A\\leq 140$ nuclei, except for the spontaneous fission of $^{254}$Cf. \n\nThe highest $Y_e+0.10$ case with the lowest $X_{LA}$ gives rise to a lower heating rate $Q$ with respect to the most neutron-rich case, in contrast to what is obtained by \\citet{martin2015} (see their Fig.13) who found,  relative to $Q_0$, a heating rate enhancement by a factor of 2.5 at $t\\sim 4$~h for their neutrino-driven wind producing the light r-process elements. Our heating rate is also significantly lower than the value found by \\citet{perego2017} (see in particular their Eq. 2, and Part II for a detailed comparison with their heating rates) for $Y_e \\ga 0.25$ ejecta. \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{rpro_aa_yepm.png}\n  \\caption{(Color online). Final abundance distribution in the dynamical ejecta of the DD2-135135 model assuming the initial $Y_e$ is systematically modified with respect to ILEAS prediction by $\\pm 0.05$ or $\\pm 0.10$. The solar system abundance distribution is normalised as in Fig.~\\ref{fig_rpro_aa}.}\n\\label{fig_rpro_dye}\n\\end{figure} \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{Q_t_XLA_yepm.png}\n  \\caption{(Color online). Time evolution of the radioactive heating rate $Q$  and the lanthanide+actinide mass fraction $X_{LA}$ for the DD2-135135 model assuming the initial $Y_e$ is systematically modified with respect to ILEAS prediction by $\\pm 0.05$ or $\\pm 0.10$. The black dotted line corresponds to the approximation $Q_0=10^{10}~(t[{\\rm d}])^{-1.3}$ \\citep{metzger2010}.}\n\\label{fig_Q_dye}\n\\end{figure} \n\n\n\n\\section{Angular and velocity dependence of the ejecta\n\\label{sec_ang_vc}\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=0.99\\textwidth]{snapshots.pdf}\n\\caption{(Color online).  2D snapshots of the density distributions of the DD2-135135 simulation in the equatorial plane at six different times.\nThe locations of the 783 finally ejected mass elements are indicated by coloured dots, projected onto the equatorial plane, with the final $Y_e$-values colour-coded within the following ranges: dark\nred: $Y_e < 0.1$, red: $0.1 \\le Y_e < 0.2$, purple: $0.2 \\le Y_e < 0.3$, blue: $0.3 \\le Y_e < 0.4$, light blue: $0.4 \\le Y_e < 0.5$, white: $Y_e \\ge 0.5$. Note that the high-$Y_e$ particles move mostly perpendicular to the plane of the figure.\n}\n\\label{fig_snapshot}\n\\end{center}\n\\end{figure*}\n\nThe three-dimensional implementation of the weak nucleonic reactions (Eqs. \\ref{eq:betareac}) within the ILEAS framework of the hydrodynamical simulations allows us to study the angle and velocity distributions of the ejecta and nucleosynthesis yields. \nAs shown in Fig.~\\ref{fig_snapshot} and by previous studies \\citep[see e.g.][]{goriely2011,sekiguchi2015}, two major mechanisms during the merger phase are responsible for the dynamical mass ejection, namely tidal stripping preferentially in the orbital plane and the more isotropic mass ejection by shock compression at the NS contact interface.\nMost of the high-$Y_e$ mass elements are seen to originate from the collision interface. \nIn the equatorial plane mostly lower-$Y_e$ material is ejected, whereas regions at higher polar angles contain a mix of low-$Y_e$ and high-$Y_e$ ejecta.\nIndeed, Fig.~\\ref{fig_yedist_ang} displays the electron fraction distribution of the ejected matter as a function of polar angle $\\theta$ for the four merger models. It can be seen that the material at the poles is significantly less neutron-rich, in particular for the DD2-125145, SFHo-125145 and DD2-135135 models. It is consequently of particular interest for observation \\citep[see][]{just2021b} to study in more detail the composition and decay heating rate obtained as a function of the polar angle, but also as a function of the expansion velocity. Some properties of the ejecta in the different angular regions, namely the polar, middle or equatorial regions, are given in Table~\\ref{tab:prop}.\nIn the following section a detailed analysis of the angle and velocity distribution of the material as well as the resulting nucleosynthesis is presented for the DD2-135135 model. If the other models deviate from its trend, a special mention will be made.\n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{yedist_ang_1panel.png}\n  \\caption{(Color online).  Average electron fraction per polar angle bin $d\\theta$\n   at $\\rho=\\rho_\\mathrm{net}$ \n  as predicted by the ILEAS hydrodynamical simulation for the four NS-NS merger models: \n  DD2-125145, DD2-135135, SFHo-125145 and SFHo-135135. \n  }\n\\label{fig_yedist_ang}\n\\end{figure} \n\n\nFig.~\\ref{fig_dM2d}a illustrates the mass distribution of the ejecta in the two-dimensional plane of the polar angle $\\theta$ and  velocity $v/c$.  Figs.~\\ref{fig_dM2d}b and \\ref{fig_dM2d}c give the corresponding projection of the integrated mass fraction on the $\\theta$ and $v/c$ axes, respectively.  Fig.~\\ref{fig_dM2d}b shows that most of the material is ejected in the equatorial region ($60^\\circ < \\theta \\leq 120^\\circ$) of the NSs merger plane. The angular distribution is seen to be roughly following a $\\rm{sin}^2(\\theta)$ function, as pointed out by \\citet{perego2017}. However, compared to the $\\rm{sin}^2(\\theta)$ distribution the simulations tend to produce a pattern that is slightly more peaked towards $\\theta=90^\\circ$.\n\nIn Fig.~\\ref{fig_dM2d}c, we see that most of the ejected material has a velocity below $0.3c$, and that the high-velocity tail ($v/c>0.6$) contains only a minor fraction of the total ejected mass (between 1-5\\% of the total mass, see also Table~\\ref{tab:prop}) and is represented by 63 (22) and 37 (13) particles for the DD2 and SFHo asymmetric (symmetric) models, respectively. These fast-moving particles contain over 99\\% of the $X_n^{t=20\\mathrm{s}}$ mass.\nThe high-velocity ejecta are not isotropic, but more concentrated in the equatorial plane, though they are distributed over all angles, especially for the DD2 EoS models (Fig.~\\ref{fig_dM2d}a).\n\nIn general, the further away from the equatorial plane the mass element, the larger its minimum velocity. In particular,  the minimum velocity in the extreme polar regions ($\\theta \\simeq 0^\\circ$ and $\\theta \\simeq 180^\\circ$) is larger than $0.2c$, while $v/c\\simeq 0.03$ is the lower limit in the equatorial plane.\nFig.~\\ref{fig_ye2d}a shows the electron fraction distribution in the $\\theta-v/c$ plane, and Figs.~\\ref{fig_ye2d}b and \\ref{fig_ye2d}c the projection of the average $Y_e$ on the $\\theta$ and $v/c$ axes, respectively.  The ejecta in the equatorial regions is the most neutron rich.  As seen in Fig.~\\ref{fig_ye2d}c, most of the high-$Y_e$ mass elements have velocities in the range $\\sim 0.1-0.4c$.  \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{dM_vc_angle_dd2_sym.png}\n  \\caption{(Color online).  Mass distribution of the ejecta a) in the two-dimensional plane of the polar \n  angle v.s. velocity at the end of the DD2-135135 NS-NS hydrodynamical simulation. b) and c) panels represent the one-dimensional projections of the integrated ejected masses on the $\\theta$ and $v/c$ axes, respectively. In panel a), the white\nregions depict the absence of any  trajectory. The green line in panel b) corresponds to the $\\rm{sin}^2(\\theta)$ function, arbitrarily normalized.\n  }\n\\label{fig_dM2d}\n\\end{figure} \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{Ye_vc_angle_dd2_sym.png}\n  \\caption{(Color online).  Same as Fig.~\\ref{fig_dM2d} for the electron fraction distribution at $\\rho=\\rho_\\mathrm{net}$ \n  as predicted by the ILEAS hydrodynamical simulation. b) and c) panels represent the mass-averaged $Y_e$ along the $\\theta$ and $v/c$ axes, respectively.\n  }\n\\label{fig_ye2d}\n\\end{figure} \n\nIn Fig.~\\ref{fig_rpro_aa_angs} the composition of the ejecta in the polar,  middle and equatorial regions are shown as a function of mass number for the DD2-135135 model.  Fig.~\\ref{fig_rpro_aa_angs}a emphasises the differences in ejected yields in the separate angle bins,  with a particular small contribution from the polar regions. Fig.~\\ref{fig_rpro_aa_angs}b compares the nucleosynthesis results of the angle regions renormalized such that in that region $\\sum_A X_A=1$, showing that the production of $A>140$ nuclei are proportionally smaller in the polar regions compared to the other angle regions.\nAs can be seen in Fig.~\\ref{fig_yedist_ang}, the material in the middle and polar regions has a significantly higher $Y_e$ and is therefore expected to yield a weaker r-process.  However, since the material ejected in both the polar  and middle regions is less massive (see Table~\\ref{tab:prop}),  the global effect of the high-$Y_e$ trajectories remains small. In the case of the symmetric SFHo system, a more isotropic distribution of $Y_e$ is found (Fig.~\\ref{fig_yedist_ang}), so that a rather similar pattern is observed in all directions. In this case, like in the others, the mass ejected along the poles remains, however, rather low, {\\it i.e.} smaller than typically 10\\% of the total ejected mass (Table~\\ref{tab:prop}).\n\nThe velocity dependence of the isotopic composition  is given in Fig.~\\ref{fig_rpro_aa_vc} where panel a) shows the yields relative to the total ejecta mass, and b)  the renormalized mass fractions, so that the sum over the atomic mass for each velocity bin is 1.  The mass elements with the highest velocities ($v\\ge 0.6c$) have a small contribution to the overall production of all isotopes.  Comparing only the shape of the abundance distributions in Fig.~\\ref{fig_rpro_aa_vc}b, those $v/c>0.6$ trajectories have their second and third r-process peaks shifted to higher mass numbers and a relatively low content in lanthanides with respect to the slow ejecta. This specific distribution is related to the fact that for the fast ejecta not all neutrons are captured and the final composition results from the competition between neutron captures and $\\beta$-decays during the fast expansion \\citep{goriely2016b}.\n\nThe distribution of ejected lanthanides and actinides, $X_{LA}$,  in the $\\theta-v/c$ plane is presented in Fig.~\\ref{fig_Xla2d}a for the DD2-135135 model, where b) and c) shows the projection of the average $X_{LA}$ fraction on the $\\theta$- and $v/c$ axes, respectively.  Both Figs.~\\ref{fig_Xla2d}b and \\ref{fig_rpro_aa_angs}a show that the production of lanthanides and actinides are the largest in the equatorial plane (see also Table~\\ref{tab:prop}). However, as displayed in Fig.~\\ref{fig_Xla2d}c, the production of $X_{LA}$ does not seem to have a large dependence on the velocity for $v/c<0.6$ while the production falls rapidly for the fastest escaping mass elements, as mentioned above. \n\n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{rpro_aa_angles_dd2_sym.png}\n  \\caption{(Color online).  Final mass fractions of the material ejected as a function of the atomic mass $A$\n  divided into six angle bins for the DD2-135135 model.  a) Shows the ejected mass fraction $X_\\theta$ \n   relative to the total ejected mass, while in b) the mass fractions are renormalised so that the sum over the atomic\n     mass numbers are equal to one for each angle bin.}\n\\label{fig_rpro_aa_angs}\n\\end{figure} \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{rpro_aa_vc_dd2_sym.png}\n  \\caption{(Color online).  Same as Fig.~\\ref{fig_rpro_aa_angs} for four velocity bins. }\n\\label{fig_rpro_aa_vc}\n\\end{figure} \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{Xla_vc_angle_dd2_sym.png}\n  \\caption{(Color online).  Same as Fig.~\\ref{fig_dM2d} for the ejected mass fraction of lanthanides and actinides $X_{LA}$. \n  }\n\\label{fig_Xla2d}\n\\end{figure} \n\n\nThe time evolution of the heating rate in six different angle bins are displayed in Fig.~\\ref{fig_Q_t_angles} for the DD2-135135 model. The shape of the curves follows more or less the same $t^{-1/3}$ dependence and are rather similar for the different viewing angles. Some variations are found for the polar directions due to a relatively different composition as shown in Fig.~\\ref{fig_rpro_aa_angs}.\nOn the contrary,  the velocity dependence of the heating rate displayed in Fig.~\\ref{fig_Q_t_vc} differs for the highest escape velocities, especially at specific times.  In particular, for the high-velocity ejecta ($v/c>0.6$), the decay of free neutrons at $t\\sim 10$~min creates a large bump in the heating rate, and the decay of largely produced $^{132}$Te and $^{132}$I (see Fig.~\\ref{fig_rpro_aa_vc}) is responsible for the smaller bump at $t\\simeq 10$~d.\nIn Fig.~\\ref{fig_rpro_aa_vc}b, a relative enhancement in the actinide production is also observed for the largest velocity bin ($v/c>0.6$), compared to the low velocities, which leads to an increase in the heating rate for $t \\ga 30$~d due to the decay of $^{254}$Cf. \nTo a smaller extent, the same features as for the high-velocity ejecta can be seen in the second largest velocity bin ($0.4 < v/c<0.6$), and are explained by the same effects. A rather similar evolution of the heating rate is found for the SFHo-135135 model, and the asymmetric models. We note that the high-velocity tail is discussed as a possible source of the late-time synchrotron emission that was recently observed in GW170817 \\citep{hajela2021,nedora2021}.\n\n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{Q_t_angles_dd2_sym.png}\n  \\caption{(Color online). Same as Fig.~\\ref{fig_Q_t} for the heating rate for the symmetric DD2 model in the six angle bins.   }\n\\label{fig_Q_t_angles}\n\\end{figure} \n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{Q_t_vc_dd2_sym.png}\n  \\caption{(Color online). Same as Fig.~\\ref{fig_Q_t_angles} for the four velocity bins of the DD2-135135 model.  }\n\\label{fig_Q_t_vc}\n\\end{figure} \n\n\n\\section{Comparison to other studies}\n\\label{sect_comp}\n\nIn recent years a variety of publications have focused on the dynamically ejected material from NS-NS mergers \\citep[e.g.][]{wanajo2014, palenzuela2015,lehner2016,sekiguchi2015,foucart2016,foucart2020,bovard2017,radice2018a,nedora2021a,martin2018}. However, only a subset of the simulations available in the literature includes neutrino absorption and in addition performs r-process nucleosynthesis network calculations. In the following, we highlight the most relevant differences between our simulations and those of others that also use approximately the same NS mass configurations and EoSs as we do.\n\nThe works of \\citet{radice2018a} and more recently \\citet{nedora2021a} perform fully general relativistic hydrodynamical calculations with neutrino absorption.\nTheir calculations are performed with the grid-based, adaptive mesh code WhiskyTHC \\citep{radice2014} in contrast to our simulations based on the Smoothed Particle Hydrodynamics (SPH) code \\citep{oechslin2007,bauswein2013} described in Sect.~\\ref{sect_weak}. \nIn the optically thick regions they also implement a leakage scheme for the weak interactions, but, in contrast to our ILEAS scheme, without ensuring that the neutrino fluxes obey the diffusion-law and without conserving lepton number in neutrino-trapping regions. In order to describe net neutrino absorption in optically thin regions, they evolve free-streaming neutrinos using a one-moment (\"M0\") scheme, whereas our ILEAS scheme employs a ray-tracing algorithm. Lacking, to our knowledge, comparisons with reference solutions or with other approximate schemes, we are unable to assess the accuracy of their M0 scheme. We note that while ILEAS has not been benchmarked by Boltzmann solutions in a binary NS merger environment, it showed excellent agreement with accurate neutrino transport solutions for the case of a proto-neutron star formed in a core-collapse supernova \\citep{ardevol-pulpillo2019}.\n\nFor the r-process nucleosynthesis, \\citet{radice2018a} and \\citet{nedora2021a} use a mapping technique to calculate the ejecta composition, {\\it i.e.} the expansion history of every trajectory is parametrized depending on its density, entropy, electron fraction and velocity at a fixed instant of time (when crossing the sphere of 443~km). On this basis the composition is deduced from the corresponding pre-computed r-process calculation of \\citet{lippuner2015}.  In our simulations we perform one r-process calculation per unbound SPH mass element, following its detailed expansion history. \nA subset of models by \\citet{radice2018a} as well as \\citet{nedora2021a} employs general-relativistic large eddy simulations (GRLES) calibrated to mimic viscosity due to magnetohydrodynamic turbulence. Since the GRLES simulations of \\citet{radice2018a} do not include neutrino absorption, we only compare to the calculations without GRLES in the following.  \nNote that some complementary information about models discussed in \\citet{radice2018a} is provided in  \\citet{perego2017}.\n\nNumerical simulations in full general relativity have also been performed by \\citet{sekiguchi2015,sekiguchi2016} with a leakage-plus-M1 scheme for neutrino cooling and absorption using finite-volume methods on a nested grid. The corresponding nucleosynthesis calculations are reported in \\citet{wanajo2014}. \nWhen calculating the r-process composition, \\citet{wanajo2014} only include ejecta from the orbital plane and choose a representative particle from each $Y_e$-bin from 0.09 to 0.44 giving in total 35 particles.\n\nThe recent study by \\citet{foucart2020} applied a sophisticated Monte-Carlo scheme for neutrino transport to directly solve the transport equations in low-density regions for a 1.27 -- 1.58~$M_{\\odot}$\\ NS-NS merger system. The simulations ran for 5~ms on a Cartesian grid, using high-order finite volume shock-capturing methods with the general relativistic SpEC code, and did not include nucleosynthesis calculations. We note here that the total mass of the binary system in \\citet{foucart2020} is 0.15~$M_{\\odot}$\\ larger than the total mass of the models discussed above, and in addition a different mass configuration was used. \n\n\\begin{table}\n\\centering\n\\caption{Summary of the compared ejecta properties from \\citet{radice2018a}, \\citet{nedora2021a}, \\citet{sekiguchi2015}, \\citet{sekiguchi2016}, \\citet{foucart2020}, abbreviated as R18, N21, S15, S16, F20, respectively, and this work. All systems have the same total mass $M_1+M_2=2.7$$M_{\\odot}$, except for F20 where it is 0.15~$M_{\\odot}$\\ larger. The symmetric mergers have $q=M_1/M_2=1$ for the NSs star masses $M_1$ and $M_2$, while the asymmetric have $q=0.86$, except for F20 where $q=0.80$. }\n\\begin{tabular}{llcccc}\n\\hline \\hline \n Ref. & EoS & $q$ & $M_{\\rm ej}$         &  $M_{\\rm ej}^{v \\ge 0.6c}$ & $\\langle Y_e\\rangle$ \\\\ \n       &  &         & [$10^{-3}$ $M_{\\odot}$] & [$10^{-5}$ $M_{\\odot}$] &  \\\\ \n\\hline\nR18 & DD2 & 1 & 1.4 & 0.2 & 0.23 \\\\ \nN21 & DD2 & 1 & 1.1 & - & 0.25 \\\\ \nS15 & DD2 & 1 & 2.1 & - & 0.29 \\\\ \nThis work & DD2 & 1 & 2.0 & 8.7 & 0.27 \\\\ \n \\hline\n R18 & SFHo & 1 & 4.2 & 3.3 & 0.22 \\\\ \n N21 & SFHo & 1 & 2.8 & - & 0.23 \\\\ \nS15 & SFHo & 1 & 11 & - & 0.31 \\\\ \nThis work & SFHo & 1 & 3.3 & 15.3 & 0.26\\\\ \n \\hline\nF20 & DD2 & 0.80 & 8.3 & - & 0.13 \\\\\nS16 & DD2 & 0.86 & 5 & - & 0.20 \\\\\nThis work & DD2 & 0.86 & 3.2 & 17.7 & 0.22\\\\ \n \\hline\nS16 & SFHo & 0.86 & 11 & - & 0.18 \\\\ \nThis work & SFHo & 0.86 & 8.7 & 25.6 & 0.24\\\\ \n\\hline \\hline \n\\end{tabular} \n\\label{tab:comp}\n\\end{table}\n\n\nAs is common in grid-based codes, the NSs in the simulations of \\citet{radice2018a,nedora2021a,sekiguchi2015,foucart2020} are placed in a low-density artificial atmosphere. If not checked carefully,  the atmosphere material may suppress mass ejection or decelerate parts of the ejecta \\citep{sekiguchi2016}. This problem is not present in SPH-based simulations where no artificial atmosphere needs to be used. In general, it is assuring that very different numerical schemes, i.e. particle-based and grid-based hydrodynamics methods, yield ejecta properties in the same ballpark noting that, in addition, we employ the conformal flatness approximation, whereas the aforementioned studies consider a fully relativistic evolution. Note that the differences due to the use of the conformal flatness condition are likely to be small, since calculations employing this approximation reproduce post-merger gravitational-wave frequencies of full-general relativity simulations quantitatively very well, implying that our simulations capture well the gravitational field and dynamics of the bulk matter ~\\citep[e.g.][]{bauswein2012b}. Also compared to the range of literature data on, for instance, total ejecta masses, our simulations do not seem to feature some systematic offset from fully relativistic simulations.\nWe emphasize that direct quantitative comparisons with other works remain difficult and should be taken with a grain of salt, as discussed below, in particular when dealing with the total ejected mass. \n\nWe first consider the ejecta masses summarised in Table~\\ref{tab:comp}. \nWhen comparing our results to previously published studies, it should be kept in mind, as already discussed in Sect.~\\ref{sect_weak}, that estimating the exact amount of ejected mass remains a difficult task, since it depends on the extraction time as well as on the criterion adopted to count gravitationally unbound material and the radius or volume at which this criterion is evaluated. While we use the $\\varepsilon_{\\rm stationary}>0$ criterion (see Sect. ~\\ref{sect_weak}), \\citet{radice2018a} and  \\citet{sekiguchi2015} use a more restrictive so-called geodesic criterion which defines a mass element as unbound, if its kinetic energy only is larger than the gravitational one (or, more precisely, if the time component of the fluid four velocity $u_t$ is smaller than $-1$). \nIn this case, we expect the ejecta to be less massive than determined by applying our criterion.  \nThe way how this criterion is applied also differs between published\nworks. \nWhile we apply it at a given time $t_{\\rm ej}$ in a given volume (with $r> r_{\\rm ej}=100$~km), \\citet{radice2018a} applies the criterion at a sphere of a given radius (443~km) and time integrates the fluxes at this radius to obtain the ejecta mass. \n\\citet{sekiguchi2015,sekiguchi2016} followed our approach, except that\nthey use $r_{\\rm ej}=0$\\footnote{One can see in Fig.~\\ref{fig_mejec}, however, that our ejecta masses would be very similar if we had used $r_{\\rm ej}=0$.} and ran their simulations for a longer time.\n\\citet{foucart2020} applies a specific version of the Bernoulli criterion to identify unbound ejecta, namely version B of \\citet{Foucart2021}, where a variety of ejection criteria are compared and discussed. \n(Note that most merger simulations ---in line with our work--- have not followed the potential ejecta to very large distances, in which case their gravitationally unbound state could not be unambiguously diagnosed.)  For all these reasons, the comparison of the quoted ejected masses should be taken with care. \n\nThis being said, our dynamically ejected mass for the symmetric 1.35--1.35$M_{\\odot}$\\ system with the DD2 EoS is found to be 40-80\\% larger than the one obtained by \\citet{radice2018a} and recently \\citet{nedora2021a}, while, with the SFHo EoS, it is  20\\% lower than the values found by \\citet{radice2018a} and up to 20\\% larger than those of \\citet{nedora2021a}.  Compared to the results of \\citet{sekiguchi2015},  our ejected mass is found to be similar for the DD2 EoS, but 3 times lower for the SFHo EoS. \nWhen comparing the symmetric and asymmetric merger simulations with the SFHo EoS in \\citet{sekiguchi2016}, they yield the same dynamical ejecta mass, while we observe about three times more ejected mass in our asymmetric case.  For the DD2 EoS, their ejected mass is over two times larger for the asymmetric merger,  while our ejected mass only increases by 60\\%. For the angular distribution of the ejected mass, \\citet{perego2017} (see their Fig.~1) report a $\\sin^2\\theta$ dependence for the SFHo symmetric model including neutrino heating,  which corresponds well with our SFHo EoS results (see Fig.~\\ref{fig_dM2d} for the DD2 EoS).\nThe seemingly large ejected mass of \\citet{foucart2020} for the asymmetric DD2 EoS could also be due to the larger total mass or the smaller mass ratio $q$ of their merger system.\n\nLooking now at the mean electron fractions (cf. Table~\\ref{tab:comp} and Fig.~\\ref{fig_yedist}), \\citet{radice2018a} and \\citet{nedora2021a} found with their M0 scheme for the symmetric systems using the DD2 (SFHo) EoSs,  values that are lower compared to our mean $Y_e$ by by 0.04 and 0.02 (0.04 and 0.03), respectively, and \\citet{sekiguchi2015} with their leakage-plus-M1 scheme found larger values, namely by 0.02 (0.05). \nHowever, the $Y_e$ distributions in \\citet{radice2018a}, \\citet{nedora2021a}, \\citet{sekiguchi2015}  and our in Fig.~\\ref{fig_yedist} are quite similar since they all show a wide $Y_e$ range spanning from about 0.4 or 0.5 down to at least 0.05. \nIn \\citet{sekiguchi2016} they also report the $Y_e$ values for asymmetric mergers and find a decrease of the average value relative to symmetric mergers by 0.09 (0.13) compared to 0.05 (0.02) in our case for the DD2 (SFHo) EoS. \nThe mean $Y_e$ of 0.13 reported by \\citet{foucart2020} for their asymmetric DD2 system is relatively low compared to the other simulations discussed here. While the $Y_e$ distribution of \\citet{foucart2020} spans a similar range as in Fig.~\\ref{fig_yedist}a,  their distribution peaks at the lowest $Y_e$-value around $\\sim 0.05$ (compared to $\\sim 0.2$ in our calculations). It remains, however,  difficult to draw conclusions here on the impact of the neutrino treatment since the discrepancy between their $\\langle Y_e \\rangle$ and ours can also be due  to the different NS masses and, in particular, their lower mass ratio leading to a larger fraction of low-$Y_e$ tidal ejecta. \nAs can be observed in Fig.~1 of \\citet{perego2017} for the SFHo symmetric merger, the hydrodynamical simulations of \\citet{radice2018a} including neutrino interactions produce larger $Y_e$ values in the polar regions compared to the equatorial plane, which can also be seen in our Fig.~\\ref{fig_yedist_ang}. However,  as pointed out in Sect.~\\ref{sec_ang_vc} (see Fig.~\\ref{fig_ye2d}), the mass of the polar ejecta is smaller than the equatorial ejecta, which is also what \\citet{perego2017} report. \n\nWhen comparing our abundance distribution in Fig.~\\ref{fig_rpro_aa} with the simulation of \\citet{radice2018a} (see their Fig.~20), some differences can be observed. \nIn particular, a different abundance distribution is obtained for $A>90$ nuclei that probably finds its origin in the different nuclear data used as input to the nuclear reaction network (e.g. fission fragment distributions) adopted to estimate the nucleosynthesis yields. However, our isotopic composition is characterised by a systematically smaller production of $A \\le 80$ nuclei despite our higher mean electron fraction. The DD2-135135 (SFHo-135135) model is characterised by $\\langle Y_e\\rangle$=0.27 (0.26) versus 0.23 (0.22) for the equivalent model in \\citet{radice2018a}. It remains unclear whether this discrepancy is connected to the nuclear network solver, or the electron fractions, or other trajectory properties that may be differently predicted by \\citet{radice2018a}. The mapping technique applied by \\citet{radice2018a} and \\citet{nedora2021a} to calculate the ejecta nucleosynthesis and taken from the parametrised r-process calculation of \\citet{lippuner2015} is also likely to contribute to such differences. The updated calculations of \\citet{nedora2021a} show similar nucleosynthesis results as \\citet{radice2018a}.\n\nThe nucleosynthesis results reported in \\citet{wanajo2014} should be compared to our equatorial composition for the SFHo EoS. As can be seen in Fig.~\\ref{fig_rpro_aa_angs} and Fig.~\\ref{fig_rpro_aa}, the equatorial ejecta dominate the shape of the final abundance distribution for the DD2 EoS, and the same is also true for the SFHo EoS.  Similar to our results, \\citet{wanajo2014} report a distribution covering the range of mass numbers from 240 down to $\\sim 80$, which is a consequence of the wide $Y_e$ distribution when including neutrino absorption.  \\citet{wanajo2014} predict a larger production of $A\\sim 80-120$ nuclei compared to our composition, but follow the solar r-process distribution fairly well for $A>120$. Differences in the predicted composition may also originate from the use of different $\\beta$-decay rates and fission fragment distributions~\\citep{goriely2015,Lemaitre2021}.\n\nAs discussed in Sect.~\\ref{sec_ang_vc}, the high-velocity ($v/c>0.6$) tail in our simulations contains only a minor fraction of the total ejected mass. Compared to some of our previous simulations \\citep{bauswein2013} ignoring neutrino emission and absorption, the inclusion of neutrinos does not give rise to an increase of the total ejecta mass, but it does enhance the mass of ejecta with $v/c>0.6$ by a factor of about two. \nFor comparison, \\citet{radice2018a} (see their Table 2) find that simulations with the M0 scheme compared to a leakage scheme not accounting for neutrino absorptions tend to increase the total ejected mass as well as the mass of the fast ejecta significantly. Still, in the case of the 1.35-1.35$M_{\\odot}$\\ models, they predict masses of fast ejecta with $M_{\\rm ej}^{v \\ge 0.6c}=1.7\\times 10^{-6}$ and $3.3\\times 10^{-5}$$M_{\\odot}$\\ for the DD2 and SFHo EoSs, respectively, which are about 50 and 5 times lower than those given in Table~\\ref{tab:comp}. Similarly, the recent calculations of \\citet{nedora2021} (see their Table 1) give masses about 90 and 5 times lower than our results for the fast ejecta of the DD2 and SFHo EoSs, respectively.\nThese difference are likely to be connected to limitations in the affordable numerical resolution in both our SPH models as well as the grid-based models of \\citet{radice2018a}, such that details of the numerical methods have a strong impact. This includes also a possible influence of the aforementioned atmosphere treatment in grid-based codes as well as potentially the way how ejecta properties are extracted, {\\it i.e.} on a fixed sphere in \\citet{radice2018a} compared to Lagrangian tracers in SPH. Future, better resolved, simulations will have to reduce this uncertainty. \n\n\n\\section{Conclusions}\n\\label{sect_concl}\n\nIn the present work we have studied r-process nucleosynthesis and the subsequent radioactive heating rate produced by material dynamically ejected from four NS-NS merger models including a proper description of neutrino interactions. \nIn contrast to more ``conventional'' leakage schemes,  the ILEAS framework coupled to the hydrodynamical calculations follows the detailed evolution of the electron fraction and its changes due to weak interactions through a neutrino-equilibration treatment in the high-density regions and an absorption module in semitransparent conditions \\citep{ardevol-pulpillo2019}.  This allows us to study the impact of the neutrino interactions with nucleons on the composition of the dynamical ejecta of NS mergers.\nOur four merger models include two EoSs, namely DD2 and SFHo, and for each EoS we have included symmetric and asymmetric merger cases, {\\it i.e.}  systems of 1.35-1.35$M_{\\odot}$\\ and 1.25-1.45$M_{\\odot}$\\ NSs, which all lead to the formation of a longer-lived neutron star surrounded by an accretion torus as remnant.\nThe second part of this work \\citep{just2021} presents kilonova light curve calculations that follow from the nucleosynthesis results presented here.\n\n\nThe main conclusions from our study can be summarised as follows:\n\\begin{itemize}\n\\item A proper inclusion of neutrino interactions yields dynamical ejecta with a solar-like composition down to $A \\simeq 90$ compared to 140 without neutrino interactions. This includes in particular a significant enrichment in Sr. This conclusion is rather similar to the one drawn initially by \\citet{wanajo2014}, though the final abundance distribution obtained in our case with different nuclear physics inputs is found to be in much closer agreement with the solar system distribution. \n\n\\item The composition of the ejected matter as well as the corresponding heating rate are found to be rather independent of the system mass asymmetry and the EoS. In the four hydrodynamical models, despite differences in the initial electron fraction distributions, rather similar nucleosynthesis results are found.   This approximate degeneracy in abundance pattern and heating rates can be favourable for extracting the ejecta properties (such as masses, opacities, expansion velocities) from kilonova observations.  \nHowever, our study has two clear limitations. First, only the dynamical ejecta are taken into account. The hypermassive NS or BH-torus wind ejecta may not follow the same trend with EoS and mass ratio as the dynamical ejecta. Second, here we only consider delayed-collapse cases in which a NS remnant survives for a longer period of time after the merger. Cases of prompt BH formation may exhibit different nucleosynthesis signatures, albeit the ejecta in these cases are typically much less massive \\citep[e.g.][]{bauswein2013}. \n\n\\item Uncertainties in the initial electron fraction impact not only the composition and in particular the amount of lanthanides and actinides ejected, but also the heating rate, mainly  10~min, 7~h or 30~d after merger through the $\\beta$-decay of residual nuclei. Spontaneous fission of $^{254}$Cf or $A=222-225$ $\\alpha$-decay chains contribute significantly less to the heating rate when weak nucleonic processes are taken into account.\n\n\\item  The nucleonic weak processes have the largest impact on the composition of the material ejected in the polar regions but also affect the r-process efficiency in the equatorial plane.\n\n\\item Even with nucleonic weak processes, the fast expanding layers are found to include a non-negligible amount of free neutrons that should leave an observable signature in the light curve and may contribute to late time X-ray emission \\citep{hajela2021}.\n\n\n\n\n\\end{itemize}\n\nIn addition to the important role of  nucleonic $\\beta$-processes on the final composition of the ejecta, the final abundance distribution as well as decay heat are known to be affected by nuclear uncertainties. Despite numerous works dedicated to the impact of nuclear physics uncertainties in this context, the proper inclusion of  neutrino interactions may change the conclusion of such sensitivity studies. In particular, as already shown by \\citet{Lemaitre2021}, the role of fission may be significantly reduced if neutrino interactions are included and found to affect the initial neutron richness. \nFinally, before drawing conclusions on the contribution of NS mergers to the Galactic enrichment, the outflow generated during the evolution of the merger remnant must be consistently included in the simulations and the discussion \\cite[see][]{just2015}, which will be the objective of a forthcoming paper.  Much work remains to be performed in this respect.\n\n\n\n\n\n\n\\section*{Acknowledgments}\nSG acknowledges financial support from F.R.S.-FNRS (Belgium). This work has been supported by the Fonds de la Recherche Scientifique (FNRS, Belgium) and the Research Foundation Flanders (FWO, Belgium) under the EOS Project nr O022818F. \n The present research benefited from computational resources made available on the Tier-1 supercomputer of the F\u00e9d\u00e9ration Wallonie-Bruxelles, infrastructure funded by the Walloon Region under the grant agreement n$^\\circ$1117545 and the Consortium des \u00c9quipements de Calcul Intensif (C\u00c9CI), funded by the Fonds de la Recherche Scientifique de Belgique (F.R.S.-FNRS) under Grant No. 2.5020.11 and by the Walloon Region. \nOJ and  AB acknowledge support by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme under grant agreement No. 759253. AB was supported by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 279384907 - SFB 1245 and - Project-ID 138713538 - SFB 881 (``The Milky Way System'', subproject A10) and acknowledges the support by the State of Hesse within the Cluster Project ELEMENTS. OJ acknowledges computational support by the HOKUSAI supercomputer at RIKEN.\nAt Garching, funding by the European Research Council through Grant ERC-AdG No.~341157-COCO2CASA and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)\nthrough Sonderforschungsbereich (Collaborative Research Centre) SFB-1258 ``Neutrinos and Dark Matter in Astro- and Particle Physics (NDM)'' and under Germany's Excellence Strategy through\nCluster of Excellence ORIGINS (EXC-2094)---390783311 is acknowledged.\n\n\\section*{Data availability}\nThe data underlying this article will be shared on reasonable request to the corresponding author.\n\n\\bibliographystyle{mnras}\n", "meta": {"timestamp": "2021-11-23T02:13:33", "yymm": "2109", "arxiv_id": "2109.02509", "language": "en", "url": "https://arxiv.org/abs/2109.02509"}}
{"text": "\\section{Mathematical background}\n\\noindent This sections presents the main concepts, definitions, and notation on multi-objective optimization, the $\\epsilon-$constrain approach, and some fundamental concepts related to the representation of the whole set of outcome vectors. \n\n\\subsection{The multi-objective integer programming problem}\n\\noindent Consider the following multi-objective integer linear programming model. \n\n\\begin{equation}\\label{MOO}\n    \\begin{array}{rcl}\\tag{$P$}\n        \\max z_1(x) & = &  (c^1)^\\top x \\\\\n        \\vdots      &   &  \\vdots \\\\\n        \\max z_k(x) & = &  (c^k)^\\top x \\\\\n        \\vdots      &   &  \\vdots \\\\\n        \\max z_p(x) & = &  (c^p)^\\top x \\\\\n                    &   &  \\\\\n        \\mbox{subject to:} &   & x \\in X.\n    \\end{array}\n\\end{equation}\n\n\\noindent where $x = (x_1,\\ldots,x_j,\\ldots,x_n)$ is an $n-$vector of non-negative and integer \\textit{decision variables}, $(c^k)^\\top = (c_{1}^k,\\ldots,c_{j}^k,\\ldots,c_{j}^k)$ is an $n$ row vector composed of the \\textit{coefficients} of the decision variables in the \\textit{objective functions}, $k=1,\\ldots,p$ (we assume these coefficients are integer values or can be converted into integer), and $X$ is the \n\\textit{feasible region} in the \\textit{decision space}, $\\mathbb{Z}^{n}$. Let $Z = z(X)$ denote the image of the \\textit{feasible region} according to the objective functions. The set $Z$ is called the \\textit{feasible region} in the \\textit{objective space}, i.e., $\\mathbb{Z}^{k}$ along with the order relation imposed by the objective functions in this set.\n\nProblem \\ref{MOO} can be presented in a more compact form as follows. \n\n\\begin{equation}\\label{eq:compact}\n    \\begin{array}{rcl}\\tag{$P^{c}$}\n        \\mbox{``}\\max\\mbox{''} z(x) & = &  Cx \\\\\n                 \\mbox{subject to:} &   & x \\in X.\n    \\end{array}\n\\end{equation}\n\n\\noindent where ``$\\max$'' means that all functions are to be maximized, $z(x) = \\big(z_1(x),\\ldots,z_k(x),\\ldots, z_p(x)\\big)$ is the vector of the $p$ objective functions, and  $C$ is an $n\\times k$ matrix, each line being composed of the coefficients of each objective function.  \n\n\\begin{definition}{(Dominance)}\nLet $z^\\prime$ and $z^{\\prime\\prime}$ denote two outcome vectors, or solutions, in the objective space. The vector $z^\\prime$ \\textit{(weakly) dominates} $z^{\\prime\\prime}$, iff $z^{\\prime} \\geq z^{\\prime\\prime}$ and $z^{\\prime} \\neq z^{\\prime\\prime}$ (i.e., $z_{k}^{\\prime} \\geqslant z_{k}^{\\prime\\prime}$, with at least a strict inequality, for $k=1,\\ldots,p$).  When all the inequalities are strict it is said that $z^\\prime$ \\textit{strictly dominates} $z^{\\prime\\prime}$.  \n\\end{definition}\n\nLet $z^{\\ast}$ denote a feasible outcome vector,  i.e., $z^{\\ast} \\in Z$. This vector is a \\textit{weak non-dominated outcome vector} if an only if there is no another $z \\in Z$ such that $z$ strictly dominates $z^{\\ast}$. Some of weak non-dominated outcome vectors are of no interest since we can find another weak non-dominated outcome vector with better performances in at least one objective function (they can thus be discarded). If there is no vector $z \\in Z$, such that $z$ weakly dominates $z^{\\ast}$, the vector $z^{\\ast}$ is simply called a \\textit{non-dominated outcome vector}. Whenever $z^\\ast$ can be obtained as a weighted-sum of the $p$ objective functions with strictly positive weighing factors, $z^\\ast$ is called a \\textit{supported outcome vector}. Otherwise, $z^\\ast$ is said to be an \\textit{unsupported outcome vector}. Let $N(Z)$ denoted the whole set of outcome non-dominated vectors or solutions, also called \\textit{Pareto front}. \n\nThe same kind of concepts can be applied in the decision space. Thus, a feasible solution $z^{\\ast}$ is called a \\textit{weakly efficient solution} if and only if we cannot find another $x \\in X$ such that $Cx \\geq Cx^\\ast$ and $Cx \\neq Cx^\\ast$. The concepts of \\textit{efficient solutions}, \\textit{supported efficient solutions}, and \\textit{unsupported efficient solutions} are easy to define. Finally, let $E(X)$ denote the set of all efficient solutions. For more details about these definitions see \\cite{Ehrgott2005} and \\cite{Steuer1986}.\n\n\n\n\n\\subsection{The $\\epsilon-$constraint approach}\n\\noindent This subsection is devoted to present an approach for identifying the whole Pareto front. This approach is based on the resolution of a sequence of problems of the following form.\n\n\n\\begin{equation}\\label{e-MOO}\n    \\begin{array}{rl}\\tag{$P^\\epsilon$}\n        {\\displaystyle \\max_{x}}   & \\big\\{z_q(x)\\big\\}   \\\\\n        \\mbox{subject to:} &   z_k(x) \\geqslant \\epsilon_k, \\;\\; k = 1,\\ldots,p,\\; k \\neq q\\\\\n                      & x \\in X.\n    \\end{array}\n\\end{equation}\n\n\\begin{theorem}{(\\citeauthor{HaimesEtAl1971}, 1971)}\\label{Theo_HaimesEtAl71}\nIf $x^\\ast$ is an optimal solution of Problem \\ref{e-MOO}, for some $q$, then $x^\\ast$ is a weakly efficient solution of Problem \\ref{MOO}.  \n\\end{theorem}\n\n\\begin{proof}\nSee  \\cite{ChankongHaines2008}, \\cite{Ehrgott2005}, or \\cite{HaimesEtAl1971},. \n\\end{proof}\n\n\\vspace{0.5cm}\n\nAlgorithm \\ref{alg:e-constraint_algo} can be implemented for solving a sequence of \\ref{e-MOO} instances. This algorithm requires as input the data or Problem \\ref{eq:compact}, $C$ and $X$, and a small enough strictly positive parameter value, $\\eta$, and it provides as output the whole Pareto front, $N(Z)$. This algorithm makes use of four internal procedures:\n\n\\begin{enumerate}[label={--}]\n    \\item $Ideal(P)$: which computes the ideal point, $z^\\ast = (z^{\\ast}_1,\\ldots,z^{\\ast}_k,\\ldots,z^{\\ast}_p)$, trough an individual maximization of each objective function. \n    \\item $ApproxNadir(P)$: which computes the nadir point or an approximation of it, $z^{nad} = (z^{nad}_1,\\ldots,z^{nad}_k,\\ldots,z^{nad}_p)$.\n    \\item From the previous two calculations we can identify the ranges of possible values, for each objective function, which are bounded from below by $z^{nad}_k$ and from above by $z^{\\ast}_k$, for $k=1,\\ldots,p$.   \n    \\item $Solve(P^\\epsilon)$: which makes use of an integer linear programming solver to solve $P^\\epsilon$, and provides the outcome vector $\\bar{z} = (\\bar{z}_1,\\ldots, \\bar{z}_k, \\ldots, \\bar{z}_p)$.\n    \\item $Filter(\\hat{N}(Z))$: which makes the filtering of an auxiliary set, $\\hat{N}(Z)$, which may contain weakly non-dominated outcome vectors, and provides as output the Pareto front set, $N(Z)$.\n\\end{enumerate}\n\n\n\\begin{algorithm}[!htbp]\n    \\mbox{\\bf{Input}:}{ $C,X, \\eta$}\\;\n    \\mbox{\\bf{Output}:}{ $N(Z)$}\\;\n    $\\hat{N}(Z) \\gets \\{\\}$\\;\n    $z^\\ast \\gets Ideal(P)$\\;\n    $z^{nad} \\gets ApproxNadir(P)$\\;\n    Select $q$ from $\\{1,\\dots,p\\}$ and build $P^\\epsilon$\\; \n    \\For{$(k=1,\\; k \\neq q)$ \\mbox{\\bf{to}} $(p)$}\n    {\n        $\\epsilon_k \\gets z_{k}^{nad} + \\eta$, ~for $k=1,\\ldots,p$\\;\n        $\\hat{z}_k \\gets -\\infty$\\;\n        \\While{$(\\hat{z}_k < z_{k}^{\\ast})$} \n        {\n         \t $\\hat{z} \\gets Solve(P^\\epsilon)$\\;\n         \t $\\hat{N}(Z) \\gets \\hat{N}(Z) \\cup \\{\\hat{z}\\}$\\;\n         \t $\\epsilon_k \\gets \\hat{z}_k + \\eta$\\;\n         }\n    }\n    $N(Z) \\gets Filter(\\hat{N}(Z))$\\;\n    \\mbox{\\bf{return}}{$({N}(Z))$}\\;\n\\caption{Computing the Pareto front, $({N}(Z))$.}   \n\\label{alg:e-constraint_algo}\n\\end{algorithm}\n\n    \n\\vspace{0.5cm}\n\nThe $\\epsilon-$constraint approach consists of solving a sequence of \\ref{e-MOO} instances by adjusting successively (increasing) the parameters $\\epsilon_k$, for $k=1,\\ldots,p$ with $k \\neq q$.  Any basic algorithm designed for implementing this approach (as for example Algorithm \\ref{alg:e-constraint_algo}) suffers from three major drawbacks, independently of the solver used for optimizing an instance of \\ref{e-MOO}:\n\n\\begin{enumerate}\n    \\item (Model structure) It leads to solve unnecessary instances of Problem \\ref{e-MOO}, since it may find some weakly efficient solutions with no interest (i.e.,  disposable weak efficient solutions). This is due to the shape of the model objective function and appears as a conclusion of Theorem \\ref{Theo_HaimesEtAl71}.  \n    \\item (Model structure) It leads to a large amount of CPU time \\citep{EhrgottRyan2003}. This is due to the nature of the $\\epsilon$ constraints; they lack of some flexibility \\citep{EhrgottRyan2003, EhrgottRuzika2008}. \n    \\item (Model technical parameter adjustment) It may miss some weakly efficient solutions of interest. This is due to the fact that the solutions obtained are largely dependent on the values chosen for the $\\epsilon$ vector \\citep{LaumannsEtAl2006}.  \n\\end{enumerate}\n\nThe third drawback was addressed by \\cite{LaumannsEtAl2006}. These authors shown in detail the serious issues related with the \\textit{a priori} technical adjustment of the $\\epsilon$ vector, and propose an adaptive based algorithm with an interesting scheme for determining the adequate values for the $\\epsilon$ parameters, when sequentially solving each instance of Problem \\ref{MOO}. \n\nThe second drawback was reported and firstly addressed by \\cite{EhrgottRyan2003}. These authors proposed an elastic technique for changing the nature of $\\epsilon$ type constraints. As a consequence the following model was proposed (some improved versions of this model can be seen in \\citealt{EhrgottRuzika2008}). \n\n\\begin{equation}\\label{e2s-MOO}\n    \\begin{array}{rl}\\tag{$P^{\\epsilon e}$}\n         {\\displaystyle \\max_{x,e^{-},e^{+}}}  & {\\displaystyle \\left\\{ z_q(x)  - \\sum_{k=1, \\, k\\neq q}^{p}p_ke_{k}^{-} \\right\\}}\\\\\n        \\mbox{subject to:} &    z_k(x) + e_{k}^{-} - e_{k}^{+} = \\epsilon_k, \\;\\; k = 1,\\ldots,p,\\; k \\neq q\\\\\n                    &   e_{k}^{-}, e_{k}^{+} \\geqslant 0, \\;\\; k = 1,\\ldots,p,\\; k \\neq q\\\\\n                    &   x \\in X.\n    \\end{array}\n\\end{equation}\n\nThe following result did not change the nature of the output of any algorithm, and the first drawback persists. \n\n\\begin{theorem}{(\\cite{EhrgottRyan2003})}\nIf $p_k >0$, for $k=1,\\ldots,p$ with $k \\neq p$, and $(x^\\ast,e^{-\\ast},e^{+\\ast})$ is an optimal solution of Problem \\ref{e2s-MOO}, for some $q$, then $x^\\ast$ is a weakly efficient solution of Problem \\ref{MOO}.  \n\\end{theorem}\n\n\\begin{proof}\nSee \\cite{EhrgottRyan2003}, or \\cite{EhrgottRuzika2008}\n\\end{proof}\n\nThe first drawback was properly addressed by \\cite{Mavrotas2009} with the proposal of a new model. This new model is based on the introduction of slack variables and it seems to mitigate also the second drawback. A new improvement for multi-objective integer linear programming which makes a slight change on the objective function was proposed by \\cite{MavrotasFlorios2013}. This change will not alter the theoretical results presented in \\cite{Mavrotas2009}. The improved model can be stated follows. \n\n\\begin{equation}\\label{es-MOO}\n    \\begin{array}{rl}\\tag{$P^{\\epsilon s}$}\n        {\\displaystyle \\max_{x,s}}  & {\\displaystyle \\left\\{z_q(x)  + \\rho\\sum_{k=1, \\, k\\neq q}^{p}10^{k-1}\\frac{s_k}{r_k}\\right\\}}\\\\\n        \\mbox{subject to:} &    z_k(x) - s_{k}  = \\epsilon_k, \\;\\; k = 1,\\ldots,p,\\; k \\neq q\\\\\n                    &   s_{k},  \\geqslant 0, \\;\\; k = 1,\\ldots,p,\\; k \\neq q\\\\\n                    &   x \\in X.\n    \\end{array}\n\\end{equation}\n\\noindent where $r_k$ is the amplitude of the range values for each objective function, $k = 1,\\ldots,p,\\, k \\neq q$.\n\nThe nature of the output of any algorithm designed to solve the previous problem changes as it can be see in the following theoretical result.\n\n\\begin{theorem}{(\\citealt{Mavrotas2009})}\nIf $(x^\\ast,s^\\ast)$ is an optimal solution of Problem \\ref{es-MOO}, for some $q$ and for $\\rho >>0$, then $x^\\ast$ is an efficient solution of Problem \\ref{MOO}.  \n\\end{theorem}\n\n\\begin{proof}\nSee \\cite{Mavrotas2009}.\n\\end{proof}\n\nProblem \\ref{es-MOO} will be used in our algorithms. \n\n\\subsection{On the representation of the Pareto front}\\label{Background representation}\n\\noindent A discrete representation of the Pareto front, $R(N)$, is a finite subset of the Pareto front, $N(Z)$. The quality of representations, in terms of how well its subset captures the characteristics of the full set, has been the focus of many studies \\citep[see][]{Sayin2000,FaulkenbergWiecek2010,AudetEtAl2021}. These studies proposed dimensions of interest in their evaluation of the representation. \\cite{Sayin2000} proposes that the quality of solution representations should be assessed through three criteria: \n\\begin{enumerate}\n\\item \\textit{Coverage}: How well does the representation covers all regions of the objective space $Z$ included in $N(Z)$.\n\\item \\textit{Uniformity}: How diverse and equally spaced are the points in the representation, i.e., how spread are the points (outcome vectors) included in the representation.\n\\item \\textit{Cardinality}: The number of outcome vectors considered in the representation, $\\Pi\\big(R(N)\\big)$. \n\\end{enumerate}\n\nCoverage and uniformity are dependent on the distance between points in the representation. There are multiple distance metrics, the most common being as follows:\n\n\\begin{equation*}\n    d(z,z') = \\begin{cases}\n    \\left( |z_1-z'_1|^t +\\dots+|z_k-z'_k|^t +\\dots +  |z_p-z'_p|^t\\right)^{1/t}, & t \\geqslant 1, \\\\\n    ~~ \\\\\n    \\max \\left(|z_1-z'_1|,\\dots,|z_k-z'_k|,\\dots, |z_p-z'_p|\\right), & t = \\infty.\n    \\end{cases}\n\\end{equation*}\n\n\\noindent where, if $t=1$ the distance metric used is the Manhattan distance, if $t=2$ it corresponds to the Euclidean distance, and if $t=\\infty$ the Chebyshev distance is applied. The higher the $t$ value, the smaller the distance value measured. Considering a distance metric, the coverage error of the representation can be stated as follows:\n\\begin{equation}\n    \\Gamma\\big(R(N), N(Z)\\big) = \\max_{z\\in N(Z)} \\min_{z'\\in R(N)} d(z,z'),\n    \\label{eq: coverage_gamma_gen}\n\\end{equation}\nwhich corresponds to the maximum distance from $z\\in N(Z)$ to its closest point in the representation $z'\\in R(N)$. For the sake of simplicity, from hereinafter, let us use the notation $\\Gamma\\big(R(N)\\big)$ instead of $\\Gamma\\big(R(N), N(Z)\\big)$. A representation $R(N)$ is said to have coverage $\\gamma$, if $\\Gamma\\big(R(N)\\big)\\leqslant\\gamma$. To increase coverage, the coverage error $\\Gamma\\big(R(N)\\big)$ must be minimized. Since the representation $R(N)$ is a finite subset of $N(Z)$, minimizing the maximum distance from $z\\in N(Z)$ to its closest point in the representation $z'\\in R(N)$ can be done by minimizing the maximum distance between each two consecutive points in the representation. Furthermore, by guaranteeing that all consecutive points in the representation $R(N)$ are distanced by a value lower or equal to $\\gamma$, there is also the guarantee that $R(N)$ has, at most, a coverage error of $\\gamma$.\n\nThe uniformity level corresponds to the minimum distance between any two points in the representation:\n\\begin{equation}\n    \\Delta\\big(R(N)\\big) = \\min_{z,z'\\in R(N), z\\neq z'}d(z,z')\n\\end{equation}\n\nA representation $R(N)$ is said to have uniformity $\\delta$, if $\\Delta\\big(R(N)\\big)\\geqslant \\delta$. To increase uniformity, the uniformity level $\\Delta\\big(R(N)\\big)$ should be maximized. As a consequence of the aforementioned, the discrete representation problem, \\ref{DRP}, can be stated as a three-objective optimization problem \\citep{ShaoEhrgott2016}:\n\n\\begin{equation}\\label{DRP}\n    \\begin{array}{l}\\tag{$P^{DRP}$}\n        \\min \\; \\Pi\\big(R(N)\\big) \\\\\n        \\max \\; \\Delta\\big(R(N)\\big) \\\\\n        \\min \\; \\Gamma\\big(R(N)\\big)  \\\\\n        \\mbox{subject to:} \\;\\; R(N) \\subset N(Z), |R(N)|>\\infty \n    \\end{array}\n\\end{equation}\n\nThe objectives considered in Problem \\ref{DRP} are interrelated. \\cite{Sayin2000} noted that, by improving the representation coverage, cardinality would increase too. As uniformity increases, potentially, the opposite effect on cardinality may emerge. Furthermore, \\cite{KiddEtAl2020} proved that, when comparing two representation, $R^{\\prime}(N)$ and $R^{\\prime\\prime}(Z)$, with the same cardinality, where $R^{\\prime\\prime}(Z)$ is an equidistant representation, then $\\Gamma\\big(R^{\\prime\\prime}(Z)\\big)\\leqslant\\Gamma(R^{\\prime}(N))$ and $\\Delta\\big(R^{\\prime\\prime}(Z)\\big)\\geqslant\\Delta(R^{\\prime}(N))$. \n\nIn this work, we make use of the Chebyshev norm to compute $\\Gamma(R(N))$ and $\\Delta(R(N))$. Although other metrics may be used, the choice is supported by three main aspects: (1) Chebyshev norm allows the decoupling of the distance for each criterion, in the sense that all coordinate distances will be at most the Chebyshev distance, i.e., if $|z_k - z'_k|\\leqslant 1$  for all $k = 1,\\ldots, p$, then $d_{l^\\infty}(z,z')\\leqslant 1$; (2) From the previous fact, it also derives that both the coverage error and the uniformity level have a direct correspondence to the coordinate distances without requiring the combination of multiple coordinates; this allows greater control over the representation criteria; and (3) The combination of coordinates, a fundamental characteristic of the other metrics, may distort the distance metric, consequently the coverage error and uniformity level, if the ranges of each criterion are significantly different \\citep{Sayin2000}.\n\n\n\\subsection{The design of the experiments}\n\\noindent  The generation of the instances used in these experiments was performed through a generator developed by the second author of this study\\footnote{For more details please contact Jos\u00e9 Rui Figueira at figueira@tecnico.ulisboa.pt}. This generator makes use of the random number generation procedure of NETGEN generator proposed in \\cite{KlingmanEtAl1974} and requires as input the following elements:\n\\begin{itemize}[label={--}]\n    \\item The number of variables (\\textit{n}), objective functions (\\textit{p}), and constraints (\\textit{m}).\n    \\item The ranges for the parameter values (coefficients of the constraints and the weights/profits of the objective functions). These parameters are randomly generated by using a uniform distribution with a specific seed number for each constraint/objective function. \n    \\item In this study, all seed numbers are increased by a fixed value, for each instance generated. \n    \\item The right-hand-side of each constraint is bounded by half the sum of the constraint's coefficients.\n\\end{itemize}\n\nThe algorithm's performance is essentially compared in terms of solution quality and running time, or a proxy of the latter, such as the number of iterations \\textit{per} supported outcome vector computed. As such, since both running time and its proxies are often not normally distributed, each problem type was tested on 30 instances. The choice of 30 instances was motivated by the conclusion of \\cite{CoffinSaltzman2000} that, for those measures of performance, 30 to 40 observation would guarantee statistically significant results. Apart from the aforementioned observation, \\cite{CoffinSaltzman2000} also provided other suggestions to draw meaningful conclusions from the comparison of algorithms that were followed in this work, such as the use of sets of problems where certain parameters are varied, and the use of mean and variance of metrics for comparison. As a result, a total of 570 different uncorrelated instances were generated.\n\n\n\\subsection{Implementation issues}\\label{sec: implementation issues}\n\\noindent All algorithms, aside from \\textit{AUGM-2}, can overcome poor nadir estimations, since the constraints' right-hand-side is computed based on the obtained solution and not beforehand. Hence, for those algorithms, in these experiments, we simply use the single minimization of each objective function individually as a nadir point. However, since the improvements on the constrained objective functions are weighted by the amplitude of the objective's range, due to some solvers' sensitivity, the value of parameter $\\rho$ in Problem \\ref{es-MOO} may have to be adapted for some instances. The consequence of not adapting parameter $\\rho$ is the failure to compute some non-dominated solutions.\n\nAlgorithm \\textit{AUGM-2}, however, requires good quality nadir estimations. For such a purpose, as suggested in \\cite{MavrotasFlorios2013}, the lexicographic pay-off table is used to provide an approximation of the nadir point. However, as it has been discussed in literature, lexicographic pay-off tables do not guarantee a good estimation of nadir points for more than two objective functions \\citep[see, e.g.,][]{IsermannSteuer1988,AlvesClimaco2007}. For this implementation reason, in some instances, \\textit{AUGM-2} does not explore the full Pareto front and, consequently, skips non-dominated solutions.\n\n\\subsection{Experiments on generating the whole Pareto front}\n\\noindent To evaluate the computational and accuracy behaviour of different algorithms, when computing the complete Pareto front, experiments will be performed on binary and integer instances, considering both single and multi-dimensional cases. Additionally, $180$ tests are run on multi-objective general integer programming instances. This is done in order to, not only compare the algorithms, but understand how changes in the number of objective functions, constraints, variables, and problem types affect the performance of the algorithms. \n\nIn the remaining of this section, analysis of the results for the aforementioned experiments are presented. This analysis is accompanied by summary tables, where underlined are results that show algorithms' drawbacks, while in bold are results that show algorithms' strengths. Algorithms are evaluated on the average and standard deviation over the number of unique non-dominated solutions found ($\\vert \\overline{N(Z)} \\vert$ and $\\sigma_{N(Z)}$) and  CPU time, as well as the average number of iterations computed for each non-dominated solution found ($\\overline{iter/\\hat{z}}$). Although the latter corresponds to a proxy of the CPU time, it is included in this study since it is not correlated to the number of solutions in the Pareto front while the CPU time typically is, better conveying the influence of the remaining problem parameters on the algorithm's performance.\n\n\\subsubsection{Multi-objective $\\{0,1\\}-$knapsack instances}\\label{sec: Multi-objective 0,1-knapsack instances}\n\\noindent The first set of problems studied were multi-objective binary knapsack instances with three and four objective functions, considering one constraint. The three-objective instances have $50$, $75$, and $100$ variables, while the four-objective instances were generated with just $50$ variables. Both the objective function's profits and the constraint's coefficients are drawn from a uniform distribution of the type $U[1,100]$. The initial seed numbers for the three-objective instances with $50$ variables were $128$, $888$ and $6$ for each objective function profit, and $40$ for the constraint's coefficients. For the remaining three-objective instances, the seed numbers for the objective functions' profits were $47$, $28$ and $626$, respectively, whilst the seed number $135$ was used for the constraint's coefficients. At last, for the four-objective instances, the seed numbers of the objective functions' profits are $47$, $28$, $626$ and $135$, and $298$ for the constraint's coefficients. All seed numbers are increased by $5$ on each instance generated of each type. For each set of problems, $30$ instances were generated. Table \\ref{table: 0-1 knapsack - 1 const} presents experiments results for the above described instances.\n\n\n\\begin{table}[!htbp]\n\\centering\n\\begin{tabular}{rrlrrrrr} \n \\hline\\hline\n $p$ & $n$ & Algorithm & $\\vert \\overline{N(Z)} \\vert$ & $\\sigma_{N(Z)}$ & $\\overline{iter/\\hat{z}}$  & $CPU$ (sec) & $\\sigma_{CPU}$ \\TBstrut\\\\[0.5ex] \n \\hline\\hline\n 3 & 50 & \\textit{AUGM-2}  &  \\underline{403.47}   &  209.13   &  \\underline{78.76} &  \\underline{1133.90}  &  773.31   \\\\\n   &    & \\textit{S-AUGM}  &  408.27   &  210.30   &  $\\mathbf{1.89}$  &   58.36    &  34.51    \\\\\n   &    & \\textit{GPBA-A}  &  408.27   &  210.30   &  \\underline{14.37} &  \\underline{394.56}   &  238.80   \\\\\n   &    & \\textit{GPBA-B}  &  408.27   &  210.30   &   $\\mathbf{1.89}$  &  $\\mathbf{58.36}$    &  $\\mathbf{34.04}$    \\\\\n   &    & \\textit{GPBA-C}  &  408.27   &  210.30   &   $\\mathbf{1.89}$  &  58.61   &  34.64    \\\\\\hline\n   & 75 & \\textit{AUGM-2}  &  \\underline{1512.63}  &  915.60   &  \\underline{52.09} &  \\underline{3628.31}  &  1885.52  \\\\\n   &    & \\textit{S-AUGM}  &  1518.83  &  920.27   &   $\\mathbf{1.82}$  &  263.34   &  193.64   \\\\\n   &    & \\textit{GPBA-A}  &  1518.83  &  920.27   & \\underline{11.69}  &  \\underline{1771.25}  &  1378.51  \\\\\n   &    & \\textit{GPBA-B}  &  1518.83  &  920.27   &   $1.83$  &   $\\mathbf{260.03}$   &  190.42   \\\\\n   &    & \\textit{GPBA-C}  &  1518.83  &  920.27   &   $1.83$  &  260.18   &  189.67   \\\\\\hline \n   &100 & \\textit{AUGM-2}  &  \\underline{3343.37}  &  1609.82  & \\underline{42.82}  &  \\underline{9237.81} &  4656.32 \\\\\n   &    & \\textit{S-AUGM}  &  3356.40  &  1628.35  & \\textbf{1.79}   &  666.86 &  411.67 \\\\\n   &    & \\textit{GPBA-A}  &  3356.40  &  1628.35  & \\underline{9.28}  & \\underline{4501.94} &  2674.97  \\\\\n   &    & \\textit{GPBA-B}  &  3356.40  &  1628.35  & \\textbf{1.79}   & \\textbf{661.26} &   427.21  \\\\\n   &    & \\textit{GPBA-C}  &  3356.40  &  1628.35  & \\textbf{1.79}   &  662.42 &  427.22 \\\\ \\hline\n  4  & 50 & \\textit{S-AUGM}\t&\t2655.43\t&\t1392.73\t&\t$\\mathbf{4.74}$\t&\t\\textbf{9940.89}\t\t&\t7008.43 \\\\\n     &    & \\textit{GPBA-B} &\t2655.43\t&\t1392.73\t&\t$\\mathbf{4.74}$\t&\t\\underline{10104.69}\t&\t7152.10\\\\\n     &    & \\textit{GPBA-C} & 2655.43\t&\t1392.73\t&\t\\underline{4.77}\t&\t10032.92   & 7205.45\\\\\n \\hline\\hline\n\\end{tabular}\n\\caption{Multi-objective 0-1 knapsack instances}\n\\label{table: 0-1 knapsack - 1 const}\n\\end{table}\n\nRegarding the instances with three objectives, the computational effort, in terms of computational time, increases significantly as the number of solutions in the Pareto front and problem variables also increases. However, the increase in the number of variables does not impact on the number of iterations \\textit{per} non-dominated solution computed. All algorithms apart from \\textit{AUGM-2}, were able to compute the full Pareto front. Algorithm \\textit{AUGM-2} failed to obtain some non-dominated solutions due to the strategy used to provide the nadir point, which can generate poor quality nadir estimations (refer to Section \\ref{sec: implementation issues}, for more details). Additionally, \\textit{AUGM-2} is the least efficient method among the ones studied, both in terms of computational time and in terms of number of iterations \\textit{per} non-dominated solution obtained. The reason for this is that, in this algorithm, iteration skipping and early exit mechanisms are only applied to the innermost loop. \\textit{GPBA-A} also does not present competitive results since more iterations are performed \\textit{per} non-dominated solution computed for this algorithm when compared to the others, which is also reflected in the computational time. \\textit{GPBA-A} presents this behaviour due to the fact that the accelerated early exit strategy cannot be applied to this algorithm. The results displayed by algorithms \\textit{AUGM-2} and \\textit{GPBA-A} justify the need and added value of the integration of the acceleration strategies (Figures \\ref{fig:flow_check_redundancy} and \\ref{fig:early_exit}). Consequently, algorithms \\textit{AUGM-2} and \\textit{GPBA-A} were discarded from further analysis. At last, when comparing the performance of \\textit{S-AUGM} and the other two proposed algorithms, \\textit{GPBA-B} and \\textit{GPBA-C}, the presented results are similar and all appear to be competitive.  \n\nIn the case of the four-objective instances, for the aforementioned reasons, only algorithms \\textit{GPBA-B} and \\textit{GPBA-C} are compared with \\textit{S-AUGM}. From the analysis of the results it is possible to observe that the three compared algorithms show a high sensitivity to the increase in the number of objectives, having increased the average number of iterations needed to compute a non-dominated solution. Furthermore, although all algorithms remained competitive, \\textit{GPBA-C} appears to have the lowest efficiency having performed, on average, the most iterations per non-dominated solution found. This could be explained by possibly needing a readjustment of parameter $\\rho$ in Problem \\ref{es-MOO}, as explained in Section \\ref{sec: implementation issues}. However, when comparing in terms of computational time, \\textit{GPBA-C} is slightly more efficient than \\textit{GPBA-B}.\n\n\n\\subsubsection{Multi-objective multi-dimensional $\\{0,1\\}-$knapsack instances}\\label{sec: Multi-objective multi-dimensional 0-1}\n\\noindent For the multi-objective multi-dimensional binary knapsack problems, instances were generated varying the number of constraints between $2$ and $5$, with a fixed number of objectives at $3$ and variables at $25$. Both the objective function's profits and the constraint's coefficients follow a uniform distribution bounded between 1 and 100. The seed numbers used for the objective function's profits were $47$, $63$ and $728$, while for each constraint equation's coefficient the values used were 626, 135, 28, 17, 758, respectively. Table \\ref{table: 0-1 knapsack - multi-dimendion} displays the results for algorithms \\textit{S-AUGM}, \\textit{GPBA-B} and \\textit{GPBA-C}. For the reasons conveyed in Section \\ref{sec: Multi-objective 0,1-knapsack instances}, related to poor performance, algorithms \\textit{AUGM-2} and \\textit{GPBA-A} are not considered in this comparison.\n\n\n\\begin{table}[!htbp]\n\\centering\n\\begin{tabular}{rrrlrrrrr} \n \\hline\\hline\n $p$ & $n$ &  $m$ &Algorithm & $\\vert \\overline{N(Z)} \\vert$ & $\\sigma_{N(Z)}$ & $\\overline{iter/\\hat{z}}$  & $CPU$ (sec) & $\\sigma_{CPU}$ \\TBstrut\\\\[0.5ex]\n \\hline\\hline\n  3  & 25 & 2  & \\textit{S-AUGM} &\t80.03\t&\t38.39\t&\t$\\mathbf{1.91}$\t&\t$\\mathbf{8.06}$\t&\t4.53 \\\\\n     &    &    & \\textit{GBPA-B} &\t80.03\t&\t38.39\t&\t$\\mathbf{1.91}$\t&\t8.11\t&\t4.49 \\\\\n     &    &    & \\textit{GBPA-C} &\t80.03\t&\t38.39\t&\t$\\mathbf{1.91}$\t&\t8.30\t&\t4.74 \\\\\\hline\n     &    & 3  & \\textit{S-AUGM} &\t84.83\t&\t40.56\t&\t$\\mathbf{1.92}$\t&\t$\\mathbf{8.95}$\t&\t4.81 \\\\\n     &    &    & \\textit{GPBA-B} &\t84.83\t&\t40.56\t&\t$\\mathbf{1.92}$\t&\t8.99\t&\t4.95 \\\\\n     &    &    & \\textit{GPBA-C} &\t84.83\t&\t40.56\t&\t$\\mathbf{1.92}$\t&\t9.03\t&\t4.90 \\\\ \\hline \n     &    & 4  & \\textit{S-AUGM} &\t88.30\t&\t44.96\t&\t$\\mathbf{1.92}$\t&\t$\\mathbf{9.67}$\t&\t5.49 \\\\\n     &    &    & \\textit{GPBA-B} &\t88.30\t&\t44.96\t&\t$\\mathbf{1.92}$\t&\t9.78\t&\t5.58 \\\\\n     &    &    & \\textit{GPBA-C} &\t88.30\t&\t44.96\t&\t$\\mathbf{1.92}$\t&\t9.73\t&\t5.56 \\\\ \\hline\n     &    & 5  & \\textit{S-AUGM} &\t90.73\t&\t37.97\t&\t$\\mathbf{1.92}$\t&\t$\\mathbf{9.94}$\t&\t4.71 \\\\\n     &    &    & \\textit{GPBA-B} &\t90.73\t&\t37.97\t&\t$\\mathbf{1.92}$\t&\t10.02\t&\t4.85 \\\\\n     &    &    & \\textit{GPBA-C} &\t90.73\t&\t37.97\t&\t$\\mathbf{1.92}$\t&\t10.12\t&\t4.89 \\\\\n \\hline\\hline\n\\end{tabular}\n\\caption{Multi-objective multi-dimensional 0-1 knapsack instances}\n\\label{table: 0-1 knapsack - multi-dimendion}\n\\end{table}\n\nIt is possible to observe that the performance of the algorithms is not affected by the increase in the number of constraints. Additionally, the three tested algorithms, \\textit{S-AUGM}, \\textit{GPBA-B} and \\textit{GPBA-C}, are competitive, finding the full Pareto front in a short amount of time. \\textit{S-AUGM} appears to be slightly more efficient when comparing average computational time, however differences between algorithms represent, in the worst case, $1.8\\%$.\n\n\\subsubsection{Multi-objective integer knapsack instances}\n\\noindent To test the algorithms on integer multi-objective knapsack instances, the instances generated in Section \\ref{sec: Multi-objective 0,1-knapsack instances} with three objectives and 50 variables were solved using integer variables. Instance number 27 that uses seed numbers 158, 1018 and 136 for the objective function's profits and 170 for the constraint's coefficients, was discarded since, by having many Pareto front solutions, was taking too long to solve. Hence, the results presented in Table \\ref{table: Multi-objective integer knapsack instances} consider 29 instances. \n\n\n\\begin{table}[!htbp]\n\\centering\n\\begin{tabular}{rrlrrrrr} \n \\hline\\hline\n $p$ & $n$ & Algorithm & $\\vert \\overline{N(Z)} \\vert$ & $\\sigma_{N(Z)}$ & $\\overline{iter/\\hat{z}}$  & $CPU$ (sec) & $\\sigma_{CPU}$ \\TBstrut\\\\[0.5ex]\n \\hline\\hline\n 3\t&\t50\t&\t\\textit{S-AUGM}\t&\t3152.59\t&\t9061.66\t&\t\\underline{1.98}    &\t$\\mathbf{2590.92}$\t&\t6666.79\t\\\\\n\t&\t\t&\t\\textit{GPBA-B}\t&\t3152.59\t&\t9061.66\t&\t$\\mathbf{1.71}$\t&\t$2662.00$\t&\t7017.92\t\\\\\n\t&\t\t&\t\\textit{GPBA-C}\t&\t3152.59\t&\t9061.66\t&\t$\\mathbf{1.71}$\t&\t$2673.95$\t&\t7051.64\t\\\\\n \\hline\\hline\n\\end{tabular}\n\\caption{Multi-objective integer knapsack instances}\n\\label{table: Multi-objective integer knapsack instances}\n\\end{table}\n\nAlthough all algorithms are able to compute the full Pareto front, the performance differs. If on one hand \\textit{S-AUGM} takes less computational time than the other two algorithms, around $3\\%$ less than \\textit{GPBA-C}, it is also the least efficient method when assessing the number of iteration needed to compute each non-dominated solution. This suggests that, both \\textit{GPBA-B} and \\textit{GPBA-C} computational time performance could be bounded by code efficiency.\n\n\\subsubsection{Multi-objective multi-dimensional integer knapsack instances}\\label{sec: Multi-objective multi-dimensional integer knapsack instances}\n\\noindent To assess the impact of the increase in dimensions on the algorithms performance on integer instances, the instances from Section \\ref{sec: Multi-objective multi-dimensional 0-1} were applied using integer type variables. Table \\ref{table: Integer multi-dimensional} presents the results for this case.\n\n\\begin{table}[!htbp]\n\\centering\n\\begin{tabular}{rrrlrrrrr} \n \\hline\\hline\n $p$ & $n$ &  $m$ &Algorithm & $\\vert \\overline{N(Z)} \\vert$ & $\\sigma_{N(Z)}$ & $\\overline{iter/\\hat{z}}$  & $CPU$ (sec) & $\\sigma_{CPU}$ \\TBstrut\\\\[0.5ex]\n \\hline\\hline\n  3\t&\t25\t&\t2\t&\t\\textit{S-AUGM} &\t167.07\t&\t269.14\t&\t\\underline{1.93}\t&\t\\underline{30.13}\t&\t66.06\\\\\n\t&\t\t&\t\t&\t\\textit{GPBA-B} &\t167.07\t&\t269.14\t&\t$\\mathbf{1.89}$\t&\t$\\mathbf{28.72}$\t&\t63.53\\\\\n\t&\t\t&\t\t&\t\\textit{GPBA-C} &\t167.07\t&\t269.14\t&\t$\\mathbf{1.89}$\t&\t28.79\t&\t62.97\\\\ \\hline\n\t&\t\t&\t3\t&\t\\textit{S-AUGM} &\t292.57\t&\t302.10\t&\t\\underline{1.94}\t&\t\\underline{50.94}\t&\t58.59\\\\\n\t&\t\t&\t\t&\t\\textit{GPBA-B} &\t292.57\t&\t302.10\t&\t$\\mathbf{1.88}$\t& $\\mathbf{28.72}$\t&\t54.10\\\\\n\t&\t\t&\t\t&\t\\textit{GPBA-C} &\t292.57\t&\t302.10\t&\t$\\mathbf{1.88}$\t&\t28.79\t&\t53.83\\\\ \\hline\n\t&\t\t&\t4\t&\t\\textit{S-AUGM} &\t285.50\t&\t232.03\t&\t\\underline{1.93}\t&\t\\underline{53.51}\t&\t44.51\\\\\n\t&\t\t&\t\t&\t\\textit{GPBA-B} &\t285.50\t&\t232.03\t&\t$\\mathbf{1.92}$\t&\t$\\mathbf{49.55}$\t&\t44.51\\\\\n\t&\t\t&\t\t&\t\\textit{GPBA-C} &\t285.50\t&\t232.03\t&\t$\\mathbf{1.92}$\t&\t49.66\t&\t44.33\\\\ \\hline\n\t&\t\t&\t5\t&\t\\textit{S-AUGM} &\t238.30\t&\t169.77\t&\t$\\mathbf{1.94}$\t&\t\\underline{42.30}\t&\t32.86\\\\\n\t&\t\t&\t\t&\t\\textit{GPBA-B} &\t238.30\t&\t169.77\t&\t$\\mathbf{1.94}$\t&\t$\\mathbf{40.04}$\t&\t31.53\\\\\n\t&\t\t&\t\t&\t\\textit{GPBA-C} &\t238.30\t&\t169.77\t&\t$\\mathbf{1.94}$\t&\t40.08\t&\t31.96\\\\ \n \\hline\\hline\n\\end{tabular}\n\\caption{Multi-objective multi-dimensional integer knapsack instances}\n\\label{table: Integer multi-dimensional}\n\\end{table}\n\nAlthough the impact of the increase in dimension does not appear to be significant, the same trend as in Table \\ref{table: Multi-objective integer knapsack instances} can be observed where \\textit{S-AUGM} is less efficient than \\textit{GPBA-B} and \\textit{GPBA-C}. In this case, \\textit{GPBA-B} and \\textit{GPBA-C} outperform consistently \\textit{S-AUGM}, both in terms of number of iterations solved \\textit{per} non-dominated solution obtained and in terms of computational time. The latter indicator also suggests that \\textit{GPBA-B} is more efficient than \\textit{GPBA-C}. The worse performance of \\textit{S-AUGM} in integer instances, when compared to the other algorithms, can probably be attributed to the $\\epsilon$-constraint model formulation used on \\textit{S-AUGM} not using elastic constraints as proposed by \\cite{EhrgottRyan2003}. However, it is possible to observe on Table \\ref{table: Integer multi-dimensional} that, as the number of constraints increase, the performance difference between \\textit{S-AUGM} and the other algorithms appears to diminish.\n\n\\subsubsection{Multi-objective general integer programming instances}\n\\noindent Instances for general integer problems were generated, setting a uniform distribution ranging between 0 and 20 for the objective function's profits and the constraints' coefficients. The instances were generated for both three and four objectives with multiple constraints and 10, 15 and 20 variables. The generation of objective function's profits started with seed numbers 47, 63, 728 and 11, one for each objective function. The initial seed numbers used to generate the constraints' coefficients was 634, 17, 28, 626, 135, 34, 78, 55, 783, 945, 823, 362, 133, 92 and 41, one for each constraint equation. Seed numbers were increased by 5 on each instance generated of each type. Results on solving these instances using \\textit{S-AUGM}, \\textit{GPBA-B} and \\textit{GPBA-C} are displayed in Table \\ref{table: GenInteger}\n\n\\begin{table}[!htbp]\n\\centering\n\\begin{tabular}{rrrlrrrrr} \n \\hline\\hline\n $p$ & $n$ &  $m$ & Algorithm & $\\vert \\overline{N(Z)} \\vert$ & $\\sigma_{N(Z)}$ & $\\overline{iter/\\hat{z}}$  & $CPU$ (sec) & $\\sigma_{CPU}$ \\TBstrut\\\\[0.5ex]\n \\hline\\hline\n  3    &    10   &    5    &    \\textit{S-AUGM}     &    19.83     &    10.14     &    $\\mathbf{1.80}$     &    \\underline{1.54}    &    1.06\\\\\n     &         &         &    \\textit{GPBA-B}     &    19.83     &    10.14     &    $\\mathbf{1.80}$     &    $\\mathbf{1.37}$     &    0.64\\\\\n     &         &         &    \\textit{GPBA-C}     &    19.83     &    10.14     &    $\\mathbf{1.80}$     &    1.45                &    0.75\\\\\\hline\n     &    15   &    10   &    \\textit{S-AUGM}     &    38.93     &    20.13     &    \\underline{1.79}    &    \\underline{3.72}    &    2.16 \\\\\n     &         &         &    \\textit{GPBA-B}     &    38.93     &    20.13     &    $\\mathbf{1.78}$     &    $\\mathbf{3.36}$     &    1.97\\\\\n     &         &         &    \\textit{GPBA-C}     &    38.93     &    20.13     &    $\\mathbf{1.78}$     &    3.45                &    1.96\\\\\\hline\n     &    20   &    15   &    \\textit{S-AUGM}     &    75.13     &    31.07     &    $\\mathbf{1.73}$     &    \\underline{9.03}    &    3.91 \\\\\n     &         &         &    \\textit{GPBA-B}     &    75.13     &    31.07     &    $\\mathbf{1.73}$     &    $\\mathbf{8.74}$     &    3.95\\\\\n     &         &         &    \\textit{GPBA-C}     &    75.13     &    31.07     &    $\\mathbf{1.73}$     &    8.91                &    3.85\\\\\\hline\n4    &    10   &    5    &    \\textit{S-AUGM}     &    30.67     &    13.31     &    $\\mathbf{3.54}$     &    $\\mathbf{7.73}$     &    4.71\\\\\n     &         &         &    \\textit{GPBA-B}     &    30.67     &    13.31     &    \\underline{3.58}    &    7.75                &    4.76\\\\\n     &         &         &    \\textit{GPBA-C}     &    30.67     &    13.31     &    3.55                &    \\underline{7.86}    &    4.75\\\\\\hline\n     &    15   &    10   &    \\textit{S-AUGM}     &    107.77    &    74.05     &    $\\mathbf{3.60}$     &    $\\mathbf{43.39}$    &    37.44\\\\\n     &         &         &    \\textit{GPBA-B}     &    107.77    &    74.05     &    \\underline{3.64}    &    44.23               &    39.23\\\\\n     &         &         &    \\textit{GPBA-C}     &    107.77    &    74.05     &    3.63                &    \\underline{44.70}   &    38.86\\\\\\hline\n     &    20   &    15   &    \\textit{S-AUGM}     &    311.30    &    175.06    &    $\\mathbf{3.52}$     &    $\\mathbf{152.01}$   &    94.09\\\\\n     &         &         &    \\textit{GPBA-B}     &    311.30    &    175.06    &    \\underline{3.56}    &    156.39              &    97.36\\\\\n     &         &         &    \\textit{GPBA-C}     &    311.30    &    175.06    &    3.55                &    \\underline{156.43}  &    97.56\\\\ \n \\hline\\hline\n\\end{tabular}\n\\caption{General multi-objective integer linear instances}\n\\label{table: GenInteger}\n\\end{table}\n\nRegarding the results on the three objective instances, it is possible to observe the same trend as in Section \\ref{sec: Multi-objective multi-dimensional integer knapsack instances} in which \\textit{GPBA-B} and \\textit{GPBA-C} outperform \\textit{S-AUGM}, although not as significantly. The less significant difference in performance between \\textit{S-AUGM} and the remaining algorithms may be attributed to these problems having more constraints than the ones presented in the previous section, where it was observed that the performance gap was lower as the number of constraints increased. Finally, \\textit{GPBA-B} is consistently the best performing algorithm among the three.\n\nWhen looking at the four objective instances, the same trend as before is not observed. On the contrary, \\textit{S-AUGM} performs better than \\textit{GPBA-B} and \\textit{GPBA-C} in all sets of problems, both in terms of computational time and in terms of number of iterations \\textit{per} non-dominated solution computed. \\textit{GPBA-B}, for all sets of problems, requires the most iterations \\textit{per} non-dominated solution, while \\textit{GPBA-C} takes the most time. \n\n\\subsection{Experiments on representation}\n\\noindent To assess the quality of the proposed algorithms for the representation problem, experiments were carried out on the binary knapsack problem instances, Section \\ref{sec: Multi-objective 0,1-knapsack instances}, varying the target cardinality for each objective ($c_k$, for all $k = {1,\\dots,p}$, $k\\neq q$). As a pre-computing stage for each instance, the acceptable uniformity level, $\\delta_k$, and the acceptable coverage level, $\\gamma_k$, are defined as $(z^*_k - z^{nad}_k)/c_k$, where the estimation of the nadir point $z^{nad}_k$ was, only for this purpose of defining the acceptable uniformity and coverage level, obtained using the lexicographic pay-off table. Table \\ref{table: Representation} presents the results for these experiments. Apart from the already introduced performance measures, the obtained representations are evaluated in terms of the  coverage error, $\\Gamma\\big(R(N)\\big)$, the uniformity level, $\\Delta\\big(R(N)\\big)$, and the cardinality, $\\Pi\\big(R(N)\\big)$. In this section, only the three proposed algorithms are compared since \\textit{S-AUGM} was developed exclusively to compute the whole Pareto front and not a representation of it.\n\n\\begin{table}[!htbp]\n\\centering\n\\begin{tabular}{rrrlrrrrr} \n\\hline\\hline\n$p$ & $n$ & $c_k$ & Algorithm & $\\vert\\Pi\\big(R(N)\\big)\\vert$ & $\\vert\\Gamma\\big(R(N)\\big)\\vert$ & $\\vert\\Delta\\big(R(N)\\big)\\vert$ & $\\overline{iter/\\hat{z}}$ & $CPU$(s)\\TBstrut\\\\[0.5ex] \\hline\\hline\n3 & 50 & 5 & \\textit{GPBA-A} & \\underline{25.83} & \\textbf{186.67} & \\underline{29.43} & \\underline{1.42} & \\underline{1.45} \\\\\n &  &  & \\textit{GPBA-B} & \\textbf{13.60} & \\underline{274.33} & \\textbf{46.53} & \\textbf{1.16} & \\textbf{0.78} \\\\\n &  &  & \\textit{GPBA-C} & 15.33 & 205.00 & 42.93 & 1.22 & 0.89 \\\\ \\hline\n &  & 17 & \\textit{GPBA-A} & \\underline{113.70} & \\textbf{113.53} & \\underline{8.20} & \\underline{1.80} & \\underline{8.64} \\\\\n &  &  & \\textit{GPBA-B} & \\textbf{67.40} & \\underline{160.90} & \\textbf{14.40} & \\textbf{1.20} & \\textbf{3.65} \\\\\n &  &  & \\textit{GPBA-C} & 112.20 & 114.53 & 8.30 & 1.36 & 6.86 \\\\ \\hline\n &  & 25 & \\textit{GPBA-A} & 153.53 & 98.77 & 6.70 & \\underline{2.13} & \\underline{13.80} \\\\\n &  &  & \\textit{GPBA-B} & \\textbf{102.17} & \\underline{143.63} & \\textbf{10.03} & \\textbf{1.25} & \\textbf{5.71} \\\\\n &  &  & \\textit{GPBA-C} & \\underline{168.83} & \\textbf{94.13} & \\underline{5.87} & 1.57 & 11.56 \\\\ \\hline\n3 & 75 & 5 & \\textit{GPBA-A} & \\underline{31.50} & \\textbf{279.17} & \\underline{27.03} & \\underline{1.33} & 1.99 \\\\\n &  &  & \\textit{GPBA-B} & \\textbf{15.20} & \\underline{425.13} & 50.27 & \\textbf{1.15} & \\textbf{0.97} \\\\\n &  &  & \\textit{GPBA-C} & 16.07 & 313.57 & \\textbf{52.07} & 1.19 & \\textbf{0.97} \\\\ \\hline\n &  & 17 & \\textit{GPBA-A} & \\underline{199.33} & \\textbf{157.10} & \\underline{6.80} & \\underline{1.41} & \\underline{14.35} \\\\\n &  &  & \\textit{GPBA-B} & \\textbf{99.90} & \\underline{214.50} & \\textbf{12.30} & \\textbf{1.14} & \\textbf{6.05} \\\\\n &  &  & \\textit{GPBA-C} & 152.13 & 175.33 & 8.40 & 1.18 & 9.50 \\\\ \\hline\n &  & 25 & \\textit{GPBA-A} & \\underline{284.73} & \\textbf{137.10} & \\underline{4.97} & \\underline{1.57} & \\underline{22.65} \\\\\n &  &  & \\textit{GPBA-B} & \\textbf{169.17} & \\underline{186.13} & \\textbf{8.27} & \\textbf{1.16} & \\textbf{10.26} \\\\\n &  &  & \\textit{GPBA-C} & 274.93 & 139.73 & 5.23 & 1.26 & 18.31 \\\\ \\hline\n3 & 100 & 5 & \\textit{GPBA-A} & \\underline{32.17} & \\textbf{334.40} & \\underline{29.00} & \\underline{1.31} & 2.25 \\\\\n &  &  & \\textit{GPBA-B} & \\textbf{15.13} & \\underline{530.47} & \\textbf{87.00} & \\textbf{1.17} & \\textbf{1.09} \\\\\n &  &  & \\textit{GPBA-C} & 16.17 & 397.20 & 55.07 & 1.20 & \\textbf{1.09} \\\\ \\hline\n &  & 17 & \\textit{GPBA-A} & \\underline{232.10} & \\textbf{189.33} & \\underline{6.80} & \\underline{1.27} & \\underline{17.11} \\\\\n &  &  & \\textit{GPBA-B} & \\textbf{109.90} & \\underline{277.87} & \\textbf{11.47} & \\textbf{1.12} & \\textbf{7.59} \\\\\n &  &  & \\textit{GPBA-C} & 167.73 & 217.97 & 8.03 & \\textbf{1.12} & 11.53 \\\\ \\hline\n &  & 25 & \\textit{GPBA-A} & \\underline{361.07} & \\textbf{161.70} & \\underline{5.03} & \\underline{1.37} & \\underline{28.90} \\\\\n &  &  & \\textit{GPBA-B} & \\textbf{198.23} & \\underline{227.50} & \\textbf{10.20} & \\textbf{1.12} & \\textbf{13.43} \\\\\n &  &  & \\textit{GPBA-C} & 323.97 & 174.73 & 5.27 & 1.16 & 23.45 \\\\\n\\hline\\hline\n\\end{tabular}\n\\caption{Representation for multi-objective 0-1 knapsack instances}\n\\label{table: Representation}\n\\end{table}\n\nOn the one hand, in Table \\ref{table: Representation} it is possible to observe that, apart from few exceptions, \\textit{GPBA-A} presents the best results on coverage error and \\textit{GPBA-B} on uniformity level. However, both algorithms show the worst performance in the other measure, i.e. \\textit{GPBA-A} on uniformity level and \\textit{GPBA-B} on coverage error. On the other hand, algorithm \\textit{GPBA-C} proves to be an algorithm with a more balanced performance. When low values for the target cardinality are set, \\textit{GPBA-C} obtains a good uniformity level, while not compromising coverage as much as \\textit{GPBA-B}. For higher values of target cardinality, \\textit{GPBA-C} behaves more like \\textit{GPBA-A}, presenting low coverage errors and worsening the results on the uniformity level. Nevertheless, \\textit{GPBA-C} is much more efficient than \\textit{GPBA-A}, as can be observed by the number of models solved \\textit{per} non-dominated solution in the representation, and the computational time. \n\n\\subsection{Final comments and remarks}\n\\noindent All the three algorithms were tested on experiments set for computing the whole Pareto front and a representation of it. Regarding the computation of the whole Pareto front:\n\\begin{enumerate}\n   \n    \\item All the three algorithms outperformed \\textit{AUGM-2} algorithm. The reason for this is that \\textit{AUGM-2} only skips redundant solution and applies the early exit mechanism in the innermost loop. Furthermore, the redundant solution skip strategy is less sophisticated than the one applied on the other algorithms since it does not consider all past solutions, but just the last one. These factors imply that \\textit{AUGM-2} requires more models to be solved \\textit{per} Pareto front solution computed.\n   \n    \\item Algorithms \\textit{GPBA-B} and \\textit{GPBA-C} outperform \\textit{GPBA-A}. \\textit{GPBA-A}, due to the strategy used to determine the $\\epsilon$ vector, does not have the early exit mechanism, lagging behind the other algorithms in terms of number of models solved \\textit{per} Pareto front solution computed.\n   \n    \\item Algorithms \\textit{GPBA-B} and \\textit{GPBA-C} are competitive with respect to \\textit{S-AUGM}, being amongst the best in the literature. \\textit{GPBA-B} and \\textit{GPBA-C} showed very good results, similar to the ones presented by \\textit{S-AUGM}. This was to be expected since the acceleration mechanisms are common to the three algorithms. However, probably due to the model structure used, \\textit{S-AUGM} falls behind on integer knapsack instances with a low number of constraints, less than four. But, for general multi-objective integer instances, \\textit{S-AUGM} appears to be more efficient.\n\\end{enumerate}\n\n\\noindent Regarding the representation of the Pareto front, the three algorithms were not compared with others in literature since, to the best of our knowledge, this are the first $\\epsilon$-constraint based algorithms to target the representation problem for integer problems with more than two objectives. Furthermore, \\textit{S-AUGM} did not present any extension for generating a representation of the Pareto front and \\textit{AUGM-2} was inefficient in its search strategy. Hence, the analysis of the proposed algorithms performance on the representation of Pareto front showed that:\n\\begin{enumerate}\n   \n    \\item \\textit{GPBA-A} was the algorithm that presented the best coverage error, while having the worst uniformity level.\n   \n    \\item \\textit{GPBA-B} had the best uniformity level, presenting the worst coverage error.\n   \n    \\item Contrary to what is observed for \\textit{GPBA-A} and \\textit{GPBA-B}, \\textit{GPBA-C} behaviour changes depending on the target cardinality. When low cardinality is used, \\textit{GPBA-C} appears to privilege uniformity behaving as \\textit{GPBA-B}. When higher cardinality values are in place, \\textit{GPBA-C} privileges coverage, behaving as \\textit{GPBA-A} although more efficiently.\n\\end{enumerate}\n\n\\section{Introduction}\n\n\\noindent Most real-world problems, although being multi-objective by their very nature, are frequently modelled as single objective problems mainly due to the the lack of multi-objective tools and/or the computational complexity of solving multi-objective models. However, the manipulation of objectives in order to amalgamate them into a single objective function tends to oversimplify the problem and the conflicting nature among objectives. Indeed, a single objective model may not represent appropriately the decision-maker's (DM) real preferences and objectives \\citep{BrankeEtAl2008}. The aforementioned disadvantages together with the rapid rise of computational performance, both in terms of hardware and software, are making multi-objective models, and more specifically Multi-Objective Optimization (MOO), a more favoured approach. Nevertheless, multi-objective integer and mixed integer linear programming (MOILP and MOMILP), that arise in many real-world applications (such as logistics and production problems), have received less attention when compared to binary and continuous problems \\citep{AlvesClimaco2007}.\n\nMulti-objective models can be classified, depending on the stage at which the DM is involved to ascertain preferences between solutions, into \\textit{a priori} methods, interactive methods, and \\textit{a posteriori}/generation methods. In \\textit{a priori} methods the DM intervenes at the very beginning, stating her/his preferences and the problem is then solved by aggregating the objective functions. This approach not only carries much of the disadvantages presented above for the single objective models, but also most of the times, in complex functions, the DM struggles to provide parameters, as for example, the weights for each objective. In interactive methods, the DM iteratively states and adjusts preferences based on the results previously obtained. The drawback with this approach is two-folded: (1) it may take a long time for a satisfactory trade-off between objectives to be found; and (2) the DM, by never seeing the full Pareto front, also never clearly understands the relation between objectives and their impact on the solutions. At last, \\textit{a posteriori} or generation methods, which involve the DM only after finding the Pareto front, are the most advantageous. This approach clearly allows for defining the trade-off between all objectives, providing the DM a full view of the problem and all the tools for making a better informed decision. However, it may become not only more computationally heavy, but also risks being overwhelming for a DM that would not require so many alternatives.\n\nThe \\textit{a posteriori} approaches require significant computational effort and time to compute the full efficient/non-dominated set. Hence, apart from usually small and linear problems, the full non-dominated set is not practical to compute in a reasonable amount of time. As a result, there are two classes of generation methods, one that aims to determine the whole set of non-dominated solutions (the so called Pareto front), and the other  which focuses on obtaining a set of solutions representative of the non-dominated set without being overwhelming for the DM \\citep{AlvesClimaco2007,KiddEtAl2020}. To address the latter, the representation problem must be considered, which, in itself is a multi-objective problem with three objective functions: cardinality, the number of solutions presented to the DM; coverage, how well the complete Pareto frontier is being represented by the set of solutions; and uniformity, how well those solutions are spread through the Pareto front space \\citep{Sayin2000}.\n\nIn this work we propose three multi-objective \\textit{a posteriori} algorithms to solve MOILP/ MOMILP with more than two objectives. Common to the three algorithms, their structure presents strategies to overcome poor quality approximations for the bound of the Pareto front, and can be used either to generate the complete Pareto front or to compute a representation of it. To this end, the algorithms we put forward tackle each of the three dimensions of the representation problem.\n\n\nUsing a generation method on a MOILP/MOMILP is often more challenging than on a multi-objective linear programming since the former has a non-convex feasible region, which implies that there may exist unsupported non-dominated solutions. \\cite{ChalmetEtAl1986} proposed a modified weighted-sum method able to produce, apart from the set of supported non-dominated solutions, the whole set unsupported non-dominated solutions. This was achieved by adding constraints to the problem that bound the values of the objective functions. Although, all non-dominated solutions can be obtained with a full parametrization of the weights added to each objective function, it can lead to an extensive and computationally demanding optimization problems.\n\nSeveral other authors also proposed approaches based on a sequential reduction of the feasible region \\citep{KleinHannan1982,SylvaCrema2004,SylvaCrema2007}. \\cite{KleinHannan1982} proposes a reduction of the feasible region by sequentially adding constraints that eliminate solutions dominated by the previously found non-dominated solution. This strategy finds solutions spaced by at least a fixed amount (provided by a parameter), for each objective. However, it may skip interesting solutions from the DM preferences point of view, since only one of the objective is considered as the objective function to be optimized. \\cite{SylvaCrema2004} presented a variation of \\cite{KleinHannan1982}'s method where all the objectives are included in the objective function by using weighting parameters. \\cite{SylvaCrema2007} builds upon the \\cite{SylvaCrema2004} method by determining, at each iteration, the weights that maximizes the infinity-norm distance to the set dominated by the previously found solutions. Consequently, uniformly spaced solutions are found. All of these methods share the same main drawback, namely the fact that the problem size increases whenever a non-dominated solution is found, since  the new constraints and, in some cases, new variables have to be added to the model. \\cite{KiddEtAl2020} also presented a scalarization algorithm but targeted for generating representations, as opposed to the complete Pareto front, for bi-objective problems. To that end, an insertion method based on Voronoi cuts used to partition the search region was developed. This method was proven to achieve simultaneously good coverage and uniformity for a given cardinality. The algorithm continues to insert solutions until a desired cardinality level is reached.  \n\n\nOne of the most popular methods for solving \\textit{a posteriori} MOILP problems is the $\\epsilon$-constraint method. For constraining each objective, the method resorts to a virtual grid spaced, for each objective, by making use of an $\\epsilon$ parameter. \\cite{LaumannsEtAl2006} applied the $\\epsilon$-constraint method as a generation method by dividing, after each iteration, the objective space using the values of the previous computed solution and limiting the search space sequentially. Additionally, after each new solution is found, a lexicographic optimization is performed to ensure getting non-dominated solutions. \\cite{KirlikSayin2014} developed a method based on the \\cite{LaumannsEtAl2006} works, which makes use of a two stage model in order to guarantee non-dominated solutions. It is  a search approach based on the construction of rectangles, as in \\cite{LaumannsEtAl2006}, but it also removed rectangles in which no non-dominated solution can be found. This approach makes the search less exhaustive when compared to the \\cite{LaumannsEtAl2006}'s algorithm. \\cite{Mavrotas2009} adapted the original $\\epsilon$-constraint method by introducing slack variables in equality constraints and incorporating them in the objective function weighted by a factor which includes the range of each objective. To choose the $\\epsilon$ vector, the range of each objective is divided into evenly spaced intervals generating a uniform grid. Nonetheless, this approach produces several redundant solutions. To overcome such a problem, \\cite{MavrotasFlorios2013} extended the  \\cite{Mavrotas2009}'s method by exploring the values of the slack variables in order to skip redundant iterations, becoming computationally more efficient. \\cite{ZhangReimann2014} addressed the requirement to have the true nadir points, which are often difficult to compute \\citep[see][]{AlvesCosta2009,EhrgottRyan2003,KirlikSayin2015}. The proposed methodology skips the redundant iterations in a way that does not require more computations when using approximated nadir values. The disadvantage, however, is that it can only be used for the computation of the whole Pareto front and not for the generation of a representation. \n\n\\cite{EusebioEtAl2014} addresses the representation problem using the $\\epsilon$-constraint method for bi-objective problems. \\cite{EusebioEtAl2014} propose two algorithms, one for coverage and another for uniformity. The former is an insertion based method: at each iteration, the algorithm adds a solution in between the two most distant in the representation. The latter successively adds solutions spaced by a predetermined step to the representation. Both methods terminate when a desired level of coverage and uniformity are met. \n\nIn this work we take as baseline the work of \\cite{MavrotasFlorios2013} and combine it with the strategies presented in \\cite{ZhangReimann2014}, which not only allow to pass over redundant iterations, but also permits to overcome the need of computing the true nadir points without requiring the computation of the whole Pareto front. In this way, we can  address the aforementioned drawbacks of both works, while keeping their advantages. Furthermore, we develop three search strategy algorithms, one for coverage, another for uniformity, and a third one for cardinality, addressing each dimension of the Pareto front representation problem. The first two algorithms are based on the work by \\cite{EusebioEtAl2014} and are extended for MOILP problems with more than two objectives. The latter refines the virtual grid, introduced by \\cite{Mavrotas2009}, after finding each solution, trying to verify the cardinality level by focusing the search on the feasible region. This work, by the very nature of its model formulation and strategy to look for new solutions in the Pareto front, is independent of both the number of non-dominated solutions found and the objective functions ranges. \n\nThe remainder of this paper is organized as follows. Section 2 introduces the mathematical background, namely the type of problem addressed in this paper, the model used and the representation problem. Section 3 presents the proposed methodology, both the generic algorithm and the three proposed search strategies, as well as an illustrative example for each one of them. Section 4 provides the computational results for the strategies presented in Section 3, both the ones for computing the full Pareto front as well as those for the representation problem are presented and discussed. At last, Section 5 presents some concluding remarks and future work lines are put forward.\n\n\\subsection{Sectioning version}\n\\noindent This section presents the three algorithms developed for finding a representation of the Pareto front for MOILP Problems \\ref{MOO}: \\textit{GPBA-A}, \\textit{GPBA-B} and \\textit{GPBA-C}. Each algorithm targets one dimension of the discrete representation Problem \\ref{DRP} (coverage, uniformity, and cardinality) by employing different strategies when exploring the feasible region. The first two algorithms, \\textit{GPBA-A} and \\textit{GPBA-B}, are an extension of the ones proposed by \\cite{EusebioEtAl2014}, aiming to improve coverage and uniformity, respectively. The third algorithm, \\textit{GPBA-C}, represents an improvement over \\cite{MavrotasFlorios2013} algorithm, and concerns mostly cardinality. However, all algorithms share the same main procedure, detailed under Algorithm \\ref{alg:e-constraint_algo} and are represented in Figure \\ref{fig:flow_generic}, whilst solving Problem \\ref{es-MOO}.\n\n\\begin{enumerate}\n   \n    \\item The first step of the algorithm consist of the computation of the lower (pessimistic) and upper (optimistic) bounds of the Pareto front, $z^{nad}$ a and $z^\\ast$, respectively. Although the optimistic value is typically easily computed by maximizing  each objective function separately, the inverse cannot always be done to obtain the pessimistic values. Many studies in the literature show the difficulty of obtaining good estimations for those bounds \\citep[see][]{IsermannSteuer1988,AlvesCosta2009}. Consequently, nadir point overestimation is common. All three proposed methods are able to handle overestimated bounds, allowing any strategy to compute the pessimistic values for each objective function. \n   \n    \\item The objective function $q$ to be directly considered in Problem \\ref{es-MOO} is then chosen, as well as the representation algorithm (\\textit{GPBA-A}, \\textit{GPBA-B} or \\textit{GPBA-C}). Depending on the representation algorithm chosen, the desired characteristics of the representation must be defined: coverage error, uniformity level or the maximum cardinality of the representation.\n   \n    \\item The forth step consists of the initialization of the main loop by enforcing the parameters $\\epsilon_k$, in Problem \\ref{es-MOO}, to be the same as the pessimistic values for each objective with a small perturbation, resulting in \\ref{es-MOO} being in its most relaxed form. Additionally, the relative worst values for each objective function, $z^{wv}_k$, are also initialized as the ideal values.\n   \n    \\item The third step, consists of the comparison of the problem's parameters against the results of the previously solved problem, in order to check if a new outcome vector needs to be computed or if it is going to yield a redundant result. That procedure is shown in Figure \\ref{fig:flow_check_redundancy}:\n   \n    \\begin{enumerate}\n       \n        \\item Select from the list of previous iterations, for each $k=1,\\ldots,p,\\; k\\ne q$, a Problem $P'$ such that the $\\epsilon'$ vector is equal to $\\epsilon$ or closer to the the nadir point, i.e., $z^{nad}_k\\leqslant \\epsilon'_k \\leqslant \\epsilon_k,$ for all $k=1,\\ldots,p,\\; k\\ne q$.\n       \n        \\item If such a problem does not exist or, if it exists, its solution does not lay in the bounds of Problem \\ref{es-MOO} when using $\\epsilon$, then solve Problem \\ref{es-MOO} and return to the main procedure (see Figure \\ref{fig:flow_generic}).\n       \n        \\item If such a Problem $P'$ exists but it is infeasible, then return to the main procedure (see Figure \\ref{fig:flow_generic}), considering the problem as an infeasible one.\n       \n        \\item If such a Problem $P'$ exists and it is feasible, and its solution is within the bounds of Problem \\ref{es-MOO}, when using $\\epsilon$, i.e., $z'_k \\geqslant \\epsilon_k$, for all $k=1,\\ldots,p,\\; k\\ne q$, then return to the main procedure (see Figure \\ref{fig:flow_generic}) considering the solution of Problem $P'$ as the solution of Problem \\ref{es-MOO}.\n       \n    \\end{enumerate}\n   \n    \\item If the obtained (or selected from the set of previously computed solution) outcome vector is feasible, such a vector is saved and the new $z^{wv}_k$ updated for all objective functions, with the exception of function $q$ and the innermost loop $p$. Then, the new $\\epsilon_p$ parameter is readjusted according to the algorithm used. Otherwise, and in case the representation algorithm is not \\textit{GPBA-A}, the early exit loop is used (see Figure \\ref{fig:early_exit}).\n    \\item The process is repeated until the optimistic values is reached in all loops, i.e., until Problem \\ref{es-MOO} reaches its most constrained form.\n   \n\\end{enumerate}\n\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{images/generic_flow_v2.png}\n    \\caption{Flowchart for the generation algorithm}\n    \\label{fig:flow_generic}\n\\end{figure}\n\nThe aforementioned acceleration strategies, the redundancy checking (Figure \\ref{fig:flow_check_redundancy}) and the early exit from loop (Figure \\ref{fig:early_exit}) procedures, were originally proposed by \\cite{ZhangReimann2014}. While the latter is only based on Proposition \\ref{prop: infeasibility}, the former resorts to both Proposition \\ref{prop: infeasibility} and Proposition \\ref{prop: optimality}. Let $LP^{\\epsilon s}$ denote a linear relaxation of Problem \\ref{es-MOO}:\n\n\\begin{proposition}{(Infeasibility)}\\label{prop: infeasibility}\nIf Problem $LP^{\\epsilon s}$ is infeasible, then Problem \\ref{es-MOO} is also infeasible.\n\\end{proposition}\n\n\\begin{proposition}{(Optimality)}\\label{prop: optimality}\nIf $x^*$ is the optimal solution of Problem $LP^{\\epsilon s}$, and $x^*$ is in the feasible region of \\ref{es-MOO}, then $x^*$ is also the optimal solution of Problem \\ref{es-MOO}.\n\\end{proposition}\n\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{images/check_redundancy.png}\n    \\caption{Process to determine if the problem needs to be solved or can be skipped.}\n    \\label{fig:flow_check_redundancy}\n\\end{figure}\n\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{images/early_exit_v2.png}\n    \\caption{Accelerated exit algorithm.}\n    \\label{fig:early_exit}\n\\end{figure}\n\nIn the following subsections, we will details the algorithms to compute the parameters $\\epsilon_k$, for Problem \\eqref{es-MOO}.\n\n\\subsection{Coverage representation  (\\textit{GPBA-A})}\n\\noindent When looking for a representation that privileges the coverage of the Pareto front, the algorithm will aim at guaranteeing that each area of the Pareto front is represented. This may be achieved, as discussed in Section \\ref{Background representation}, by minimizing the maximum distance between any two consecutive points in the representation ($\\Gamma\\big(R(N)\\big)$), Equation \\ref{eq: coverage_gamma_gen}, i.e. the coverage error. The quality of the representation is controlled by the parameter $\\gamma$, such that $\\Gamma\\big(R(N)\\big)\\leqslant \\gamma$.\n\nParameter $\\gamma$, corresponding to the acceptable coverage error, is sometimes difficult to determine. An alternative to directly conveying the acceptable coverage error, is to define an acceptable cardinality in each objective ($c_k$, for all $k = {1,\\dots,p}$, $k\\neq q$) based on their range ($z^*_k - z^{nad}_k$, for all $k = {1,\\dots,p}$, $k\\neq q$). The ratio between the range and cardinality provides the acceptable coverage error for each objective: $\\gamma_k = (z^*_k - z^{nad}_k)/c_k$. The sum of the specified cardinality in each objective will correspond to the approximate cardinality of the representation, i.e., $\\Pi\\big(R(N)\\big) = \\sum_{k=1,\\, k\\neq q}^{p} c_k$. Note that, the cardinality of the representation $\\Pi\\big(R(N)\\big)$ is merely an approximation since the distribution of the Pareto front is unknown and it is not this parameter that controls the representation.\n\nAlgorithm \\ref{alg:GPBA-A} presents the method to adjust parameter $\\epsilon_k$ when aiming to have a representation that privileges coverage. It performs a single run for every iteration and for each objective $k$, i.e. for each loop, after solving a problem and obtaining, or not, a new outcome vector. The algorithm takes six inputs: (1) the desired coverage error over that loop, $\\gamma_k$; (2) the value of the $\\epsilon_k$ parameter of Problem \\ref{es-MOO} used in the current iteration; (3) the $k^{th}$ component of the outcome vector $z$, $z_k$, for the Problem \\ref{es-MOO} used in the current iteration; (4) the $k^{th}$ component of the ideal vector $z^{\\ast}$, $z^{\\ast}_k$; (5) the $k^{th}$ component of the approximation to the nadir vector $z^{nad}$, $z^{nad}_k$; and (6) the set $D$, corresponding to the ordered set of the discarded points for the current loop, which contains the points already obtained as well as the areas where redundant points will be obtained. The algorithm's outputs are the updated value of parameter $\\epsilon_k$ to be used in the next iteration, and the updated ordered set of discarded points, $D$, for the current loop. To this end, the first part of the algorithm updates $D$ based on one of the following two cases:\n\n\\begin{enumerate}\n    \\item If the Problem \\ref{es-MOO} solved in this iteration, when using $\\epsilon_k$, is infeasible (the feasible region is empty, $X^{^\\epsilon s} = \\{\\}$), then if the problem is more constrained it will be infeasible too, as stated in Proposition \\ref{prop: infeasibility}. As a result, all the points ranging from $\\epsilon_k$ to the $k^{th}$ component of the ideal vector, $z^{\\ast}_k$, can be discarded, and should be added to $D$ for the current loop.\n    \\item If the Problem \\ref{es-MOO} solved in this iteration, when using $\\epsilon_k$, is feasible, all the points ranging between $\\epsilon_k$ to the $k^{th}$ component of the obtained vector, $z_k$, can be discarded and should be added to $D$. This is based on Proposition \\ref{prop: optimality}. \n\\end{enumerate}\n\nAfterwards, using the updated set of discarded points, $D$, the algorithm computes the new value for parameter $\\epsilon_k$:\n\n\\begin{enumerate}\n    \\item In the case where the current value of $\\epsilon_k$ is the $k^{th}$ component of the pessimistic vector, $z^{nad}_k$, the problem solved in this iteration corresponds to the first extreme point. Hence, the following point to compute is the next extreme, the ideal value $z^{\\ast}_k$.\n   \n    \\item Otherwise, $D$ will have at least more than two points. As a result, the two consecutive points distancing the most from each other are drawn from set $D$. Then, on the one hand, if the distance between those points meets the desired coverage error, the loop may be exited. To do so, set $D$ is updated as empty and the $\\epsilon_k$ parameter is set to a value higher than the ideal $z^{\\ast}_k + 1$. On the other hand, if the distance between those points meets the desired coverage error, $\\epsilon_k$ is set to find a point that minimizes the coverage error, i.e., a point at least in between the two most distant consecutive points, $(z_1 - z_2)/2$.\n\\end{enumerate}\n\n\\begin{algorithm}[!ht]\n   \n    \\mbox{\\bf{Input}:}{ $\\gamma_k$, $\\epsilon_k$, $z_k$, $z^{\\ast}_k$, $z^{nad}_k$, $D$}\\;\n    \\mbox{\\bf{Output}:}{ $\\epsilon_k$, $D$}\\;\n   \n    \\eIf{$(X^{\\epsilon s} = \\{\\})$}{\n    $D \\gets D \\cup \\{\\lceil\\epsilon_k\\rceil,\\lceil\\epsilon_k\\rceil+1,\\dots,z^*_k\\}$\\;}\n    {\n    $D \\gets D \\cup \\{\\lceil\\epsilon_k\\rceil,\\lceil\\epsilon_k\\rceil+1,\\dots,z_k\\}$\\;}\n   \n    \\eIf{$(\\epsilon_k = z^{nad}_k)$}\n    {$\\epsilon_k \\gets z^*_k$\\;}{\n    {Let $z_1$ and $z_2$ be the most distant consecutive values in $D$}\\;\n     \\eIf{$(d(z_1,z_2) \\leqslant \\gamma_k)$}{$\\epsilon_k \\gets z^*_k+1$\\;\n     $D \\gets \\{\\}$\\;}\n    {$\\epsilon_k \\gets (z_{1} + z_{2})/2$\\;}}\n    \\mbox{\\bf{return}}{$(\\epsilon_k$, $D)$}\\;\n\\caption{Adjusting the parameter $\\epsilon_k$ in \\textit{GPBA-A}.}   \n\\label{alg:GPBA-A}\n\\end{algorithm}\n\n\\subsection{Uniformity representation (\\textit{GPBA-B})}\n\\noindent A representation that privileges uniformity is a representation which spread points as equally spaced as possible. By maximizing the minimum distance between two points in the representation, the uniformity level $\\Delta\\big(R(N)\\big)$ is increased. This representation is controlled by an acceptable uniformity level $\\delta$.\n\nSimilar to the acceptable coverage level, the acceptable uniformity level may be difficult to define. Hence, it is advised to derive it based on the range of each objective function values, ($z^*_k - z^{nad}_k$, for all $k = 1,\\dots,p$, $k\\neq q$) and the acceptable cardinality in each objective ($c_k$, for all $k = 1,\\dots,p$, $k\\neq q$). The ratio between the range and cardinality provides the acceptable uniformity level for each objective: $\\delta_k = (z^*_k - z^{nad}_k)/c_k$.\n\nThe procedure to determine $\\epsilon_k$, for the next iteration, is presented in Algorithm \\ref{alg:GPBA-B}. It takes as input the defined uniformity level $\\delta_k$ and the $k^{th}$ component of the obtained vector, $z_k$. The algorithm in each iteration adds $\\delta_k$ to $z_k$, until $\\epsilon_k$ is greater than the maximum value for objective $k$, $z^{\\ast}_k$. In that case the loop is exited since the problem becomes infeasible. \n\n\\begin{algorithm}[!ht]\n   \n    \\mbox{\\bf{Input}:}{ $\\delta_k$, $\\epsilon_k$}\\;\n    \\mbox{\\bf{Output}:}{ $\\epsilon_k$}\\;\n   \n    {$\\epsilon_k \\gets z_k + \\delta_k$\\;}\n    \\mbox{\\bf{return}}{$(\\epsilon_k)$}\\;\n\\caption{Adjusting the parameter $\\epsilon_k$ in \\textit{GPBA-B}.}   \n\\label{alg:GPBA-B}\n\\end{algorithm}\n\n\\subsection{Cardinality representation (\\textit{GPBA-C})}\n\n\\noindent The cardinality representation here proposed aims to provide a range insensitive search strategy that seeks to distribute the desired number of solutions in the feasible region. When the desired cardinality is specified, it is very common that the ranges of the objectives in which it is based are overestimated. As a result, the final cardinality of the representation will be significantly lower than the originally considered, and in which the determination of parameters, such as uniformity level $\\delta$ and coverage error $\\gamma$, may have been based. The cardinality representation algorithm addresses this drawback by defining the search region in a way that intends to maintain the originally determined cardinality, and provides a balance between uniformity and coverage. \n\nThe cardinality representation algorithm starts by defining a uniform grid between the ideal and nadir points. Whenever a solution is computed that skips a step on the grid, the grid is redefined from that point until the ideal value, considering only the remaining points left to compute. This results in a refinement of the search grid within the feasible region of the problem, and consequently in a range insensitive search strategy. The lower the desired cardinality is, the least amount of steps may need to be skipped, hence the more uniform the final representation will be. On the other hand, the higher the desired cardinality, the more solutions may need to be skipped and, consequently, the grid will be refined more times, leading to a representation with a better coverage and worst uniformity. \n\nAlgorithm \\ref{alg:GPBA-C} presents the procedure to adapt parameter $\\epsilon_k$. It takes six inputs: (1) the grid starting point for that loop, $z^{start}_k$; (2) the $k^{th}$ component of the ideal vector $z^{\\ast}$, i.e. the grid end point for that loop, $z^{\\ast}_k$; (3) the number of solutions to obtain from that grid, $c'_k$; (4) the position within the grid for this objective, i.e. the grid point number, $i_k$; (5) the $k^{th}$ component of the outcome vector $z$, $z_k$, for the Problem \\ref{es-MOO} used in the current iteration; and (6) the $k^{th}$ slack variable, $s_k$, for the Problem \\ref{es-MOO}. For the the first iteration, prior to entering Algorithm \\ref{alg:GPBA-C}, the grid start point, $z^{start}_k$, corresponds to the nadir approximation vector, $z^{nad}_k$, the position within the grid, $i_k$, to zero, and  the number of solutions to obtain, $c'_k$, to the cardinality for that objective minus 1, $c_k-1$. The algorithm outputs the new parameter $\\epsilon_k$ and the updated grid parameters, $z^{start}_k$, $c'_k$ and $i_k$. To that end, the following procedure is applied:\n\\begin{enumerate}\n    \\item The first step determines if the outcome vector from the problem solved in the previous iteration, $z_k$, makes any of the next grid points redundant according to Proposition \\ref{prop: optimality}, i.e. if a step on the grid may be skipped. Accordingly, the grid step size, $step$, is computed by dividing the grid range by the number of grid points in that grid. The integer part of the division between the slack variable, $s_k$, and the grid step size, $step$, determines how many grid points may be skipped.\n    \\begin{enumerate}\n        \\item In case there are steps on the grid to skip, the grid is redefined. The new starting point for the grid, $z^{start}_k$, will become the corresponding component of the outcome vector obtained in the current iteration, $z_k$. The updated number of grid points for the new grid, $c'_k$, corresponds to the number of points from the current grid subtracted the number of grid points already computed, $i_k$. The updated the position within the grid, $i_k$, will be one, i.e. at the beginning of the new grid. At last, the step according to the newly defined grid is computed.\n        \\item Otherwise, the grid is not updated and the position within the grid, $i_k$, is incremented.\n    \\end{enumerate}\n    \\item The second step, makes use of the grid parameters computed in the previous step, $z^{start}_k$, $i_k$ and $step$, the updated parameter $e_k$ is computed. In case $e_k$ surpasses the ideal vector value $z^{\\ast}_k$, the loop should be exited and, therefore, the grid parameter should return to their initial values: the grid start point, $z^{start}_k$, corresponds to the nadir approximation vector, $z^{nad}_k$; the number of solutions to obtain, $c'_k$, to the cardinality for that objective, $c_k$; and the position within the grid, $i_k$, to zero.\n\\end{enumerate}\n\n\\begin{algorithm}[!ht]\n   \n    \\mbox{\\bf{Input}:}{ $z^{start}_k$, $z^{\\ast}_k$, $c'_k$, $i_k$, $z_k$, $s_k$}\\;\n    \\mbox{\\bf{Output}:}{ $\\epsilon_k$, $i_k$, $c'_k$, $z^{start}_k$}\\;\n   \n    {$step \\gets \\max\\big\\{(z^{\\ast}_k-z^{start}_k)/c'_k,\\,1\\big\\}$\\;\n    $b \\gets \\lfloor|s_k/step|\\rfloor$\\;}\n   \n    \\eIf{$(b>0)$}{\n    $z^{start}_k \\gets z_k$\\;\n    $c'_k \\gets c'_k - i_k$\\;\n    $i_k \\gets 1$\\;\n    $step \\gets \\max\\big\\{(z^{\\ast}_k-z^{start}_k)/c'_k,\\,1\\big\\}$\\;}\n    {$i_k \\gets i_k +1$\\;}\n   \n    {$\\epsilon_k \\gets z^{start}_k + i_k\\times step$\\;}\n   \n    \\If{$(\\epsilon_k>z^{\\ast}_k)$}{\n    $z^{start}_k \\gets z^{nad}_k$\\;\n    $c'_k \\gets c_k$\\;\n    $i_k \\gets 0$\\;}\n    \n    \\mbox{\\bf{return}}{$(\\epsilon_k$, $z^{start}_k$, $i_k$, $c'_k)$}\\;\n\\caption{Adjusting the parameter $\\epsilon_k$ in \\textit{GPBA-C}.}   \n\\label{alg:GPBA-C}\n\\end{algorithm}\n\n\n\\subsection{An illustrative example}\\label{sec: An illustrative example}\n\\noindent Consider the following numerical example, originally presented in \\cite{IsermannSteuer1988}:\n\\[\n    \\begin{array}{rcrrcrcrcrcrcrcrcl}\n       \\max \\, z_1(x)   & = &  & 2x_1 &   &  &  & & - & 2x_4 & & & - & 2x_6 & - & 2x_7 & &  \\\\\n       \\max \\, z_2(x)   & = & - & 2x_1 & + & x_2 & + &  2x_3 & - & x_4 & + & x_5 & + & 2x_6  & - & x_7 & & \\\\\n       \\max \\, z_3(x)   & = & - & x_1 & - & 2x_2 &  & & - & 2x_4 & + & 3x_5 & + & x_6 & & & &   \\\\\n     \n    \\mbox{subject to:}  & & & x_1 & + & x_2 & + & 3x_3 & & & + & 3x_5 & + & 2x_6 & & & \\leqslant & 61 \\\\\n     & & & & & 3x_2 & + & 2x_3 & + & 4x_4 & & & & & & & \\leqslant & 72 \\\\\n     & & & 5x_1 & + & 3x_2 & & & & & + & 5x_5 & + & 4x_6 & + & 4x_7 & \\leqslant & 76 \\\\\n     & & & 4x_1 & + & 2x_2 & & & + & 4x_4 & & & + & 4x_6 & & & \\leqslant & 51  \\\\\n     & & & 5x_1 & + & 2x_2 & & & + & 3x_4 & + & x_5 & + & 4x_6 & & & \\leqslant & 66 \\\\\n     & & & 2x_1 & + & 2x_2 & & & + & 4x_4 & + & 4x_5 & + & 4x_6 & + & 5 x_7 & \\leqslant & 59 \\\\\n     & & & 3x_1 & & & + & 2x_3 & & & + & 5x_5 & + & x_6 & + & 2x_7 & \\leqslant & 77 \\\\ \n     & & & x_1, & & x_2, & & x_3, & & x_4, & & x_5, & & x_6, & & x_7 & \\in & \\mathbb{Z}_{0}^{+}.\\\\\n    \\end{array}\n\\]\n\nThe three representation algorithms, \\textit{GPBA-A}, \\textit{GPBA-B}, and \\textit{GPBA-C}, will be illustrated in this section. To that end, the first iteration over the outermost loop will be used. Common to all representations is the need to compute both the ideal point and the nadir point, or, alternatively, an approximation of the nadir point. In this case, the ideal point is $z^{\\ast} = (24, 49, 42)$, and the nadir approximation point, obtained through the minimization of each individual objective function, is $z^{nad} = (-28, -28, -48)$. Additionally, $z_1$ will be used as $z_q$, $z_2$ as the outermost loop, and $z_3$ as the innermost loop. Hence, all algorithms start with $\\epsilon_2 = -28$ and $\\epsilon_3 = -48$, as well as with the relative worst value for the outermost loop $z^{wv}_2 = 49$.\n\n\\subsubsection{Coverage representation (\\textit{GPBA-A})}\n\\noindent Assume the acceptable coverage error for the representation being computed is $\\gamma = 15$, for all objective functions. The first iteration over the outermost loop will have $\\epsilon_2 = -28$. Then, the iterations over the innermost loop, $k=3$, are the following:\n\\begin{enumerate}\n    \\item The first iteration over the innermost loop (Figure \\ref{fig:GPBA-A-1}) solves Problem \\ref{es-MOO} using $\\epsilon_2 = -28$ and $\\epsilon_3 = -48$. Point $z^{1}=(24,9,-14)$ is obtained and added to the representation $R(N)=\\{z^{1}\\}$. Next, the relative worst value for objective function $z_2$, $z^{wv}_2$, and the ordered set of discarded points, $D$, are updated to $z^{wv}_2=9$ and $D=\\{-48,-47,\\dots,-14\\}$. Since the current value for $\\epsilon_3$ is $z^{nad}_3$, the new value for $\\epsilon_3$ is $42$, corresponding to the ideal value.\n    \\item Since, according to Figure \\ref{fig:flow_check_redundancy}, Problem \\ref{es-MOO} needs to be solved for $\\epsilon_2 = -28$ and $\\epsilon_3 = 42$, the point obtained is $z^{2}=(0,20,42)$ (Figure \\ref{fig:GPBA-A-2}). Point $z^{2}$ is added to the representation, resulting in $R(N)=\\{z^{1},z^{2}\\}$. Since $z^{2}_2 \\geqslant z^{wv}_2$, the worst value for objective function $z_2$ does not need to be updated. To the set of discarded points, $z^{2}_3$ is added $D=\\{-48,-47,\\dots,-14,42\\}$. The two most distant consecutive values in $D$ are $-14$ and $42$, hence the new value of $\\epsilon_3$ is $(42+(-14))/2 = 14$.\n    \\item Again, according to Figure \\ref{fig:flow_check_redundancy}, Problem \\ref{es-MOO} is solved for $\\epsilon_2 = -28$ and $\\epsilon_3 = 14$, obtaining $z^{3}=(14,13,14)$ (Figure \\ref{fig:GPBA-A-3}), and updating the representation to $R(N)=\\{z^{1},z^{3}, z^{2}\\}$. Again, the worst value for objective function $z_2$, $z^{wv}_2$, does not require updating, while the set of discarded points is updated to $D=\\{-48,-47,\\dots,-14,14,42\\}$. At this point the maximum distance between points $D$ is $28$. As a result, the new value for $\\epsilon_3$ is $(14+(-14))/2 = 0$.\n    \\item Problem \\ref{es-MOO} is solved for $\\epsilon_2 = -28$ and $\\epsilon_3 = 0$, obtaining $z^{4}=(22,6,1)$ (Figure \\ref{fig:GPBA-A-4}), and updating the representation to $R(N)=\\{z^{1},z^{4}, z^{3}, z^{2}\\}$. The worst value for objective function $z_2$, $z^{wv}_2$, needs to be updated $z^{wv}_2 = 6$. The set of discarded points is updated to $D=\\{-48,-47,\\dots,-14,0,1,14,42\\}$. The new value for $\\epsilon_3$ is $(42+14)/2 = 28$.\n    \\item Using $\\epsilon_2 = -28$ and $\\epsilon_3 = 28$, Problem \\ref{es-MOO} is solved and $z^{5}=(8,13,29)$ is obtained (Figure \\ref{fig:GPBA-A-5}). Outcome vector $z^{5}$ is then added to the representation, $R(N)=\\{z^{1},z^{4}, z^{3}, z^{5},$ $z^{2}\\}$. The worst value for objective function $z_2$ remains the same, $z^{wv}_2 = 6$. The set of discarded points is updated to $D=\\{-48,-47,\\dots,-14,0,1,14,28,29,42\\}$. At this point, the maximum distance between points in the set of discarded points is $14< \\gamma$. Hence, the loop may stop, and the coverage error obtained for the first iteration over the outermost loop was $13 < \\gamma$.\n\\end{enumerate}\n\nFor the next iteration over the loop of objective function $z_2$, the value of $z^{wv}_2 = 6$ is used as $z_2$ for the discarded vector of that loop.\n\n\\subsubsection{Uniformity representation (\\textit{GPBA-B})}\n\\noindent Suppose the objective is to have a representation with at least a uniformity level of $\\delta = 10$ over all objective functions. The algorithm begins with $e_2=-28$, for the outermost loop, and the following iterations occur over the innermost loop:\n\\begin{enumerate}\n    \\item In the first iteration, Figure \\ref{fig:GPBA-B-1}, Problem \\ref{es-MOO} is solved using $\\epsilon_3=-48$. Point $z^1 = (24,9,-14)$ is obtained and added to the representation $R(N)=\\{z^1\\}$. The new $\\epsilon_3$ is updated according to the desired uniformity level as: $\\epsilon_3 = z^{1}_3 + \\delta_3 = -14+10 = -4$. Before the next iteration the worst value for objective function $z_2$, $z^{wv}_2$, is also updated to $9$.\n    \\item The second iteration, Figure \\ref{fig:GPBA-B-2}, uses $\\epsilon_3 =-4$ to solve Problem \\ref{es-MOO} and point $z^2 = (24,5,-3)$ is obtained and added to the representation, $R(N)=\\{z^1,z^2\\}$. The third component of the $\\epsilon$ vector is updated accordingly, becoming $\\epsilon_3 = 7$. Since $z^2_2 < z^{wv}_2$, the worst value for objective function $z_2$ is updated, resulting in $z^{wv}_2 = 5$.\n    \\item When solving Problem \\ref{es-MOO} with $\\epsilon_2 = -28$ and $\\epsilon_3 = 7$, Figure \\ref{fig:GPBA-B-3}, point $z^3 = (18,8,9)$ is obtained and added to the representation, $R(N)=\\{z^1,z^2,z^3\\}$. The $\\epsilon_3$ is updated as $\\epsilon_3=z^{3}_3 + \\delta_3 = 9+10 = 19$. Again, since $z^2_2\\geqslant z^{wv}_2$, the worst value for objective function $z_2$ is not updated.\n    \\item In the fourth iteration, Figure \\ref{fig:GPBA-B-4}, Problem \\ref{es-MOO} is solved using $\\epsilon_3=19$. The outcome vector is $z^4 = (12,11,21)$, which is added to the representation, $R(N)=\\{z^1,z^2,z^3, z^4\\}$. As a consequence, $\\epsilon_3$ is updated as $z^{4}_3 + \\delta_3 = 21+10 = 31$. In this iteration, because $z^2_2\\geqslant z^{wv}_2$, the worst value for objective function $z_2$ does not need to be updated, remaining at $z^{wv}_2 = 5$.\n    \\item For the fifth iteration, Figure \\ref{fig:GPBA-B-5}, Problem \\ref{es-MOO} is solved using $\\epsilon_3=31$. The resulting outcome vector is $z^5 = (6,14,33)$, which is added to the representation, becoming $R(N)=\\{z^1,z^2,z^3, z^4, z^5\\}$. Once more, $z^{wv}_2$ is not updated since $z^{wv}_2<14$. Then, $\\epsilon_3$ is updated according to Algorithm \\ref{alg:GPBA-B}, however, since the resulting $\\epsilon_3$ is $43>z^{\\ast}_3 = 42$ the loop is finished (see Figure \\ref{fig:flow_generic}).\n\\end{enumerate}\n\nFor the next iteration over the outermost loop, objective function $z_2$, the value of $z^{wv}_2$ is used as $z_2$ to compute the next value of $\\epsilon_2$, $\\epsilon_2 = z_2 + \\delta_2 = 5+10 = 15$.\n\n\\subsubsection{Cardinality representation (\\textit{GPBA-C})}\n\\noindent Consider a desired cardinality for the representation of $25$ points, corresponding to a division of each constrained objective $k$, $k=1,\\ldots,p,\\; k\\ne q$, into $5$. The first iteration over the outermost loop, $k=2$, starts with $\\epsilon_2 = z^{nad}_2 = -28$ and $z^{wv}_2 = z^{\\ast}_2 = 49$, and then iterates over the innermost loop in the following way:\n\\begin{enumerate}\n   \n    \\item For the first iteration, in Figure \\ref{fig:GPBA-C-1}, $\\epsilon_3$ is $-48$, corresponding to the nadir approximation value for that objective, $z^{nad}_3$. Solving Problem \\ref{es-MOO} with those parameters, yields to the outcome vector $z^1 = (24,9,-14)$, which is added to the representation $R(N)$, and we update $z^{wv}_2$ with $9$. Since this is the first iteration, the grid start point $z^{start}_3$ is $-48$, i.e.,  the nadir approximation value, the position within the grid, $i_3$, is $0$, and the number of solutions to obtain with such a grid, $c'_3$, is equal to the objective's cardinality minus one, $4$. The slack variable for Problem \\ref{es-MOO}, $s_3$, is the difference between $\\epsilon_3$ and the third component of the outcome vector, i.e., $s_3 = -14 - (-48) = 34$. To determine whether a step in the grid may be skipped or not, we firstly compute the step size: $step = \\max\\big\\{(42-(-48))/4,\\,1\\big\\} = 22.5$. Since the number of steps to skip, $b$, is the integer part of $34/22.5=1.51$, then $1$ step in the grid must be skipped. Consequently, the grid is refined from the obtained solution, $z^{start}_3 = z^1_3 = -14$, onwards, considering the number of points left to compute: $c'_3 = 4 - 0 = 4$, $step = \\max\\big\\{(42-(-14))/4,\\,1\\big\\} = 14$, $i_3 = 1$ (Figure \\ref{fig:GPBA-C-2}). Hence, the $\\epsilon_3$ parameter for the following iteration is $\\epsilon_3 = -14 + 1\\times 14 = 0$. \n   \n    \\item For the second iteration, in Figure \\ref{fig:GPBA-C-2}, we use $\\epsilon_3=0$ when solving Problem \\ref{es-MOO}, leading to the vector $z^2 = (22,6,1)$, which is added to the representation, $R(N)=\\{z^1,z^2\\}$. The worst value for objective function $z_2$ needs to be updated with $6$, since $z_2^2<z^{wv}_2$. As a result, the 3\\textsuperscript{rd} slack variable for Problem \\ref{es-MOO} is $s_3 = 1 - 0=1$. Since the step size is $14$, no solution is skipped, and it can be determined by the integer part of $b = \\lfloor 1/14\\rfloor = 0$. Hence the grid remains the same and the position within the grid is increased by one, $i_3 = 2$. Hence, the $\\epsilon_3$ parameter for the following iteration is $\\epsilon_3 = -14 + 2\\times 14 = 14$.\n   \n    \\item For the third iteration, Figure \\ref{fig:GPBA-C-3}, we solve Problem \\ref{es-MOO} with $\\epsilon_3 = 14$, leading to vector $z^3 = (14,13,14)$, which is added to the representation, $R(N)=\\{z^1,z^2,z^3\\}$. The worst value for objective function $z_2$, $z^{wv}_2$, remains the same. The slack variable for Problem \\ref{es-MOO} is $s_3 = 0$, hence no solution are skipped and the grid is kept. The position within the grid is increased by one, $i_3 = 3$, and $\\epsilon_3$ parameter is updated to $\\epsilon_3 = -14 + 3\\times 14 = 28$.\n   \n    \\item For the forth iteration, Figure \\ref{fig:GPBA-C-4}, we use $\\epsilon_3 =28$, which results in the outcome vector $z^4 = (8,13,29)$, and the representation $R(N)=\\{z^1,z^2,z^3,z^4\\}$.  The worst value for objective function $z_2$ keeps its value $z^{wv}_2=6$, because $z^4_2\\geqslant z^{wv}_2$. Since $z^4_3 - \\epsilon_3 = 1$ corresponds to the slack variable, there are no redundant solutions in the grid, $b = \\lfloor 1/14\\rfloor = 0$. As a result the grid remains the same and the position within the grid is increased by one, $i_3 = 4$. The $\\epsilon_3$ parameter is then updated to $\\epsilon_3 = -14 + 4\\times 14 = 42$.\n   \n    \\item For the fifth and last iteration in this loop, Figure \\ref{fig:GPBA-C-5}, we use $\\epsilon_3 =42$, which results in the outcome vector $z^5 = (0,20,42)$, updating the representation to $R(N)=\\{z^1,z^2,z^3,z^4,z^5\\}$. Since $\\epsilon_3 = z^5_3$ no solution was skipped and the position within the grid may be increased by one, $i_3 = 5$. Due to $i_3$ being greater than $c'_3$, the updated value of $\\epsilon_3 = -14 + 5\\times 14 = 56$ is greater than the ideal value $z^{ast}=42$. Hence, the loop will be exited and the grid control parameters for this objective, $k=3$, will return to the values of the first iteration.\n\\end{enumerate}\n\nFor the next iteration over the outermost loop, objective function $z_2$, the value of $z^{wv}_2=6$ is used as $z_2$ to compute the next value of $\\epsilon_2$, using Algorithm \\ref{alg:GPBA-C}. Because the slack variable $s_2 = -28-6=-34$ and the step size to $step = \\max\\big\\{(49-(-28))/4,\\,1\\big\\} = 19.25$, resulting in skipping one step, $b=\\lfloor|-34/19.25|\\rfloor=1$. The grid parameters are adapted to $z^{start}_2 = z_2 = 6$, $c'_2 = 4-0 = 4$ and $i_2 = 1$. The step size is updated to $step = \\max\\big\\{(49-6))/4,\\,1\\big\\} = 10.75$ and, at last, $\\epsilon_2 = 6 + 1\\times 10.75 = 16.75$.\n\n\\subsection{Generating the entire Pareto front}\n\\noindent The three algorithms presented in the above section, \\textit{GPBA-A}, \\textit{GPBA-B}, and \\textit{GPBA-C}, are based on solving a sequence of \\ref{es-MOO} problems, hence they can be used to generate the complete Pareto front under the condition that the objective function only takes integer values. Additionally the parameters that control each algorithm, the acceptable coverage error, the uniformity level, and the cardinality, must be chosen to ensure the production of the entire Pareto set. \n\nWhen using algorithm \\textit{GPBA-A} to compute the entire Pareto front, the desired acceptable coverage error, $\\gamma$, is $1$, guaranteeing that the distance between any two consecutive points in the Pareto front representation is at most unitary. Since this algorithms are applied to problems of type \\ref{MOO}, if $\\Gamma \\leqslant 1$, then $\\Gamma$ is, in fact, equal to $1$.\n\nIn case algorithm \\textit{GPBA-B} is used, the acceptable uniform level, $\\delta$, should be set to $1$ to achieve the computation of the entire Pareto front. Setting $\\delta=1$ ensures the algorithm will search for the next solution as close to the previous as a unitary step in the objective space. \n\nAt last, for the case of \\textit{GPBA-C}, when generating the entire Pareto front, the cardinality should be set for each objective as the respective range. As a result, a unitary step size is used for each objective function's loop and the algorithm restricts the search region for the next solution to a unitary distance in the objective space.\n\nAll three proposed algorithms will be tested in the computational experiments section generating the entire Pareto front, hence using the parameters setting described above. \n\\section{Representation of the Pareto Front}\n\\input{Representation}\n\n   \n\n\\section{Computational experiments}\n\\input{Experiments}\n\\section{Conclusions}\n\\input{Conclusions}\n\n\\section*{Acknowledgements}\n\\noindent Mariana Mesquita-Cunha acknowledges the support by national funds through FCT, under the research grant SFRH/BD/149441/2019. Jos\u00e9 Rui Figueira acknowledges the support by national funds through FCT, under DOME research project, PTDC/CCI-COM/31198/2017. Ana Paula Barbosa-P\u00f3voa and Mariana Mesquita-Cunha acknowledge the support by national funds through FCT, under the Data2Help research project, DSAIPA/AI/0044/2018, and the project 1801P.00740 PTDC/EGE-OGE/28071/2017 - LISBOA-01-0145-FEDER-028071.\n\\bibliographystyle{elsarticle-harv}\n", "meta": {"timestamp": "2021-09-07T02:39:59", "yymm": "2109", "arxiv_id": "2109.02630", "language": "en", "url": "https://arxiv.org/abs/2109.02630"}}
{"text": "\\section{Introduction}\n\n\n\nThe phase-field modeling technique involves finding and minimizing the free energy of one or more order parameters that describe a phase \\cite{Hohenberg1977,Provatas2010}. The method was originally introduced by Fix in 1983 \\cite{Fix1983} and has since been applied to modeling a wide range of problems including material microstructures \\cite{Chen2002,Li2017}, crack propagation \\cite{Spatschek_2011}, batteries \\cite{Wang2020}, biological membranes \\cite{Fan_2008}, cellular systems \\cite{Nonomura_2012,Palmieri2015} and even immune response \\cite{Najem2014}. Some recent advancements \ninclude the phase-field crystal model \\cite{Elder2002} and its applications, see e.g., Refs.~\\cite{Achim_2006,Emmerich_2011,Faghihi2013,Kocher_2019,Alster2020}, and phase-field damage models \\cite{Wu_2017}. \n\nThe same field-based approach can also used for problems in which a free energy description is not readily available. In such a case, the equations of motion are, instead, derived phenomenologically. A well-known class of such problems are reaction-diffusion systems, including Turing~\\cite{Turing1952} and Gray--Scott models~\\cite{Gray_1985}. These exhibit complex morphologies which mimic nature~\\cite{Lee_1994,Leppaenen2002,Maini_2012} with far-reaching applications to biological systems~\\cite{Murray1989}.\n\nWith the phase-field approach gaining popularity, there is a growing need for \nopen source phase-field simulation software that generalizes some numerical strategies as has been discussed by Hong and Viswanathan~\\cite{Hong2020}. Phase-field simulation software can generally be divided between numerical solvers using finite element or finite difference methods.\nOn the finite element side, some recent ones include, e.g.,\nPRISMS-PF \\cite{DeWitt2020}, which uses a matrix free approach, as well as\nSfePy \\cite{Cimrman_2019}, FEniCS \\cite{Alnaes2015} and\nMOOSE \\cite{Permann2020}, of which the latter offers symbolic algebra functionality and automatic differentiation, allowing for instance, specification of free energy equations. The package FiPy \\cite{Guyer2009} is equipped with an accessible Python interface and equation specification.\n\n\nWhile there are several packages for the finite element method to solve partial differential equations,\nmuch less is available for finite differences. \nCurrent open software includes\nthe Mesoscale Microstructure Simulation Project (MMSP) \\cite{Keller2019}, though development appears to have ceased shortly after its release, and \nOpenPhase \\cite{Tegeler2017}, which employs parallelization and sparse storage \nto solve large-scale multi-phase problems.\n\nTo improve upon the existing open source tools available using finite difference methods, we develop \\textit{SymPhas}{}, an API and software package \nthat aims at advancing the ability to implement numerical \nsolutions of general phase-field problems by maximizing accessibility, flexibility and performance. \\textit{SymPhas}{} uses a discrete-grid-based approach for the implementation of numerical solvers to allow\nsimulations to scale well with the number of grid points and number of order parameters, regardless of dimension. This is supplemented with parallelization via the C++{} standard library and OpenMP~\\cite{Dagum_1998}.\n\nThe \\textit{SymPhas}{} API allows the user to define and solve any phase-field model that can be formulated field-theoretically, up to three dimensions and with arbitrary numbers of order parameters and equations. This extends to reaction-diffusion problems as well.\nPhase-field problems are readily specified in the program using the dynamical equations provided in a completely unconstrained form using simple C++{}-macro-based grammar. We achieve this primarily in two ways: 1) Development of a symbolic algebra library to manipulate and transform mathematical constructs as expression trees and 2) a modular approach of Object Oriented Programming (OOP) that progressively layers more complexity as needed by a given application.\n\nThe symbolic algebra feature is implemented as compile-time constructs that directly formulate expression trees at the point of definition. This is a unique feature of \\textit{SymPhas}{} that is, to the best of our knowledge, not present in any other phase-field software package. \n\nA modular design is used to retain a simple interface for basic uses while simultaneously supporting complex tasks and implementations. This design applies template meta-programming to fully optimize program constructs and eliminate branching wherever possible. We also achieve considerable decoupling between elements, allowing individual functional elements of \\textit{SymPhas}{} to remain distinct; this has the added benefit of supporting community development. \n\nThe modular approach also facilitates another key feature of \\textit{SymPhas}{}: The ability to integrate a user-developed numerical solver into the workflow. The solver is seamlessly integrated via a class inheritance strategy\ndesigned to eliminate almost all API-specific restrictions on the solver implementation. In writing the solver, the user can leverage the significant set of features available in the symbolic algebra library and in the \\textit{SymPhas}{} API overall. \n\n\nThrough extensive documentation and adherence to best programming practices, we provide \\textit{SymPhas}{} as a codebase to be expanded and driven by community development.\nThis is further facilitated by managing the build process with CMake~\\cite{cmake}, which provides \\textit{SymPhas}{} with multi-platform support and grants users the ability to customize the compilation and installation procedure in a straightforward way.\n\n\n\\section{Methods}\n\nTo generate solutions to phase-field problems, an implementation that defines the problem and establishes the program control flow is written in C++{} using the \\textit{SymPhas}{} API. This consists of three components:\n\n\\begin{enumerate}\n\t\\item \\textbf{Model definitions file:} The phase-field description with the equations of motion are specified. These are written using C++{} macros provided by \\textit{SymPhas}{} and follow a simple grammar structure. Putting this in a separate file is optional.\n\t\\item \\textbf{Solver file:} The implementation of a specific method which solves a phase-field problem using the equations of motion.\n\t\\item \\textbf{Driver file:} Specifies the workflow, data inputs and outputs.\n\\end{enumerate}\n\nWe use OOP \nand extensively apply the programming paradigm\nknown as \\textit{template meta-programming}; the use of objects and functions defined with arbitrary parameters or data types~\\cite{Meyers2005}. \nThese abstractions are either implicitly (by the compiler) or explicitly (by the user) specialized for concrete types. The benefit of this approach is that the type specialization gives the compiler full information about the call stack, allowing it to make optimizations not possible in different approaches (e.g. virtual inheritance). The other advantage is added extendability through type dependent implementations. The drawback is that since each specialization is unique, the library and executable will take longer to compile and result in a larger size when many specializations are used. For example, compiling five phase-field models of one or two order parameters with one solver defined with all available finite difference stencils -- see Section~\\ref{methods:objects:stencils} -- results in a total size of approximately 3\\,MB.\nAs part of template meta-programming, we also apply the \\textit{expression template technique}~\\cite{Veldhuizen1995,Vandevoorde2003}, commonly referred to as the \\textit{curiously recurring template pattern} (CRTP) \\cite{Coplien1996}, mainly used in the implementation of the symbolic algebra functionality.\nA non-exhaustive list of familiar libraries using expression templates includes Armadillo~\\cite{Sanderson2016,Sanderson2018}, Blitz++~\\cite{Veldhuizen2000}, Boost $\\mu$BLAS~\\cite{Schaeling2011}, Dlib~\\cite{King2009}, Eigen~\\cite{GaeelGuennebaud2010}, Stan Math Library~\\cite{Carpenter2015} and xtensor~\\cite{xtensor}. \n\n\nThe OOP approach applies a modular design to program structure; this means that we minimize coupling and maximize cohesion~\\cite{Vanderfeesten_2008, Candela2016} of all classes in the API, as well as designing each element to support class inheritance or composition.\nThe former is primarily accomplished by following best programming principles such as applying the single-responsibility principle for objects \\cite{Martin2002}. The latter implies that objects designed under this modular framework can be readily extended or modified without refactoring the existing code. Moreover, modularity is used to reflect the real world representation of a phase-field problem. The overall aim is to simplify and streamline the future development of \\textit{SymPhas}{}.\n\nAdditionally, the build process is another aspect designed to be user friendly. Managed by CMake, the user has full control over program definitions and modules. The result of the build process is a shared library that can be linked in a g++ invocation or alternatively, imported into a separate user CMake \\cite{cmake} project.\n\n\\subsection{Overview of Modules}\n\nThere are four required and two supplementary modules as part of \\textit{SymPhas}{}. \nThe necessary modules constituting the \\textit{SymPhas}{} library are the basic functionality (\\modulename{lib}), datatypes (\\modulename{datatypes}), solution (\\modulename{sol}) and symbolic algebra (\\modulename{sym}) modules. The module \\modulename{lib} is a dependency of all other modules, since it introduces components such as objects and types used throughout the program.\nThere are two additional libraries which complete the feature set of \\textit{SymPhas}{}: the configuration (\\modulename{conf}) and the input/output (\\modulename{io}) modules.\n\nThe \\modulename{datatypes} module depends only on \\modulename{lib}. It implements objects used in the discrete grid representations, the most important of which is the \\lstinline{Grid} class, a managed array for storing data of 1-, 2- and 3-dimensional problems. \nThe symbolic algebra library (\\modulename{sym}) provides the core functionality \nto interpret mathematical expressions. The implementation of \\modulename{sym} contains all the mathematical objects and relations that are required to specify a phase-field equation of motion. It also specifies rules between these objects.\nThe solution module (\\modulename{sol}) provides the structural and functional framework used to describe and solve a phase-field problem. This is accomplished by defining two objects: One being the programmatic representation of a general phase-field problem and the other which implements a specific set of interface functions for time evolving the phase-field data. \n\n\n\\subsection{Objects in \\textit{SymPhas}{}} \\label{methods:objects}\n\n\nThe following is a list and brief description of the relevant objects\nin \\textit{SymPhas}{}:\n\n\\begin{itemize}\n\t\\item \\lstinline{Grid}: Basic array type for storing phase-field data of arbitrary type.\n\t\\item \\lstinline{Boundary}: Logical element for defining the properties of a grid boundary.\n\t\\item \\lstinline{Stencil}: Object which defines the finite difference stencils used to approximate derivatives in a uniform grid.\n\t\\item \\lstinline{OpExpression}: Interface object representing the node in an expression tree, based on CRTP.\n\t\\item \\lstinline{Solver}: Interface that is specialized for implementing the\n\tsolution procedure.\n\t\\item \\lstinline{Model}: Encapsulation of the problem representation, including the equations of motion. Primary interface for managing the solution of a phase-field model.\n\\end{itemize}\n\n\\subsubsection{Uniform Grid}\n\nThe data of a grid is initialized in computer memory as a one-dimensional array, and the desired system dimension is logically imposed according to row major order where the first dimension is always the horizontal ($x$-axis). This ensures fastest run time through memory localization. \n\nAn extension of \\lstinline{Grid} called \\lstinline{BoundaryGrid} enables the use of finite difference stencils at points near the boundary and implements routines to update boundary grid elements (for example, to apply periodic boundaries). This is accomplished by managing a list of indices that correspond to boundary elements. The number of layers of the boundary is predefined based on the extent of the largest finite difference stencil.\n\nThe design of the \\lstinline{Grid} class and any specialization thereof is based on template meta-programming, and allows the user to select the data type and the dimension.\n\n\n\\subsubsection{Finite Difference Stencils} \\label{methods:objects:stencils}\n\n\nStencils are finite difference approximations of derivatives of a specified order. In \\textit{SymPhas}{}, we implement second and fourth order accurate central-space stencils for various orders of derivatives. To this end, stencils are defined using three characteristics: 1) The order of derivative that is approximated, 2) the order of accuracy,\nand 3) the dimension of the system. An additional characterization is the number of points used in the approximation. A stencil family is a group of stencils with the same dimension and order of accuracy. We apply this categorization to the design of stencils in \\textit{SymPhas}{} by implementing specialized template classes for each family.\n\nStencils are CRTP-based template classes with member functions for each order of derivative up to fourth order, and a member function for the generalized implementation of higher orders. In particular, the Laplacian, bilaplacian, gradlaplacian and gradient derivatives are explicitly defined according to those derived in \\cite{Patra2006}, including both anisotropic- and isotropic-type stencils that are second and fourth order accurate in two dimensions, and second order accurate in three dimensions. Using CRTP in the stencil implementation eliminates branching in the member function invocations; a significant optimization that improves performance since stencils are applied multiple times at each grid point for every solution iteration. \n\nFor second order accuracy approximations, the Laplacian is implemented by 5 and 9 point stencils for 2D, and 7, 15, 19, 21 and 27 point stencils in 3D; the gradlaplacian is implemented by 6, 8, 12 and 16 point stencils, and 10, 12, 28, 36 and 40 point stencils in 3D; and the bilaplacian is implemented by 13, 17 and 21 point stencils for 2D, and 21, 25, 41, 52 and 57 point stencils for 3D. For fourth order accuracy approximations, which are only in 2D, the Laplacian is implemented by 9, 17 and 21 point stencils; the gradlaplacian is implemented by 14, 18, 26 and 30 point stencils. The bilaplacian is implemented by 21, 25, 33 and 37 point stencils. The wide selection ensures that appropriate approximations can be used a problem as necessary.\n\n\n\\subsubsection{Expressions} \\label{methods:objects:expression}\n\nOne of the primary features of \\textit{SymPhas}{} is the symbolic algebra library, which represents mathematical expressions with expression trees formulated at compile time. An expression tree representation allows the equations of motion to be treated as a single object, i.e., they can be persisted as an object state and passed as a function parameter. Moreover, expression trees provide the ability to reorganize and manipulate mathematical expressions. \nThe symbolic algebra functionality is used to interpret equations of motion that are provided in a general form, and is exposed in a user-friendly way via the \\textit{SymPhas}{} API. \n\nSymbolic algebra is implemented in \\textit{SymPhas}{} through an approach unique to phase-field simulations programs: CRTP is applied to generate compiled code for an expression tree evaluation so that it is as close as possible to writing the evaluation manually. In this regard, the symbolic algebra is considered ``compile-time constant'', since the expression representation is formulated at compile time. The type name of the CRTP base and expression tree node is \\lstinline{OpExpression}.\n\nOne motivation of this design choice is minimizing application runtime, since a design that applies a deterministic control flow to expression tree traversal can significantly increase performance. \nThe most significant corresponding improvement in performance is the reduction in the time spent by a numerical solver in evaluating the equation of motion of the phase-field model, which takes place for all points in the grid and for each iteration of the solver. \nAn implication\nis that, in general, each expression is a unique type (unique CRTP specializations of \\lstinline{OpExpression}).\n\n\\subsubsection{Solution Interface}\n\nThe base \\textit{SymPhas}{} library consisting of the six aforementioned modules does not contain a solver implementation, though two solver implementations that are detailed in Section~\\ref{methods:implementation} are provided in the \\textit{SymPhas}{} package obtained from Github (\\lstinline{https://github.com/SoftSimu/SymPhas}). Instead, the \\textit{solution interface}, \\lstinline{Solver}, declares three functions \nwhich a concrete numerical solver must implement.\nThe solver interface design uses CRTP and is based on applying a mediator design pattern via a special object that we refer to as the ``equation mediator object''. These are generated by the solver for each dynamical equation in a phase-field model before the simulation begins. Its purpose is to recast the equation of motion into a form that can be interpreted by the numerical scheme of the solver in order for the subsequent time index of the corresponding phase-field data to be computed. The equation mediator object is constructed by the solver member function \\lstinline{form_expr_one()}.\nBy taking advantage of the modular framework,\nwe design the solver interface to allow the equation mediator object to remain entirely specific to the implemented solver, maximizing third party development potential.\nA specialized solver implements the following three primary interface functions, where additional functions, such as derivatives, may also be written as necessitated:\n\\begin{itemize}\n\t\\item \\lstinline{form_expr_one()}: Given the set of equations of motion of a phase-field model, constructs the equation mediator object for a specified equation of motion.\n\tThis function is only called once. It performs as much computational work as possible to ensure maximum program performance.\n\t\\item \\lstinline{equation()}: Using the phase-field data and the equation mediator objects, performs an initial time evolution step,\n\ttypically writing intermediate results to working memory.\n\t\\item \\lstinline{step()}: Using the phase-field data and intermediate results computed by \\lstinline{equation()}, obtain the next iteration in the solution.\n\\end{itemize}\n\n\\subsubsection{Problem Encapsulation}\n\nTo represent the physical phase-field problem in the code domain, objects of basic functionality are successively encapsulated to build necessary functionality. For instance, the \\lstinline{Grid} class is encapsulated by \\lstinline{System} to add information such as the spatial intervals and discretization width. A further encapsulation will specialize \\lstinline{System} into \\lstinline{PhaseFieldSystem}, adding functionality such as data persistence and the ability to populate array values with initial conditions.\nUsing template meta-programming, the\\\\ \\lstinline{PhaseFieldSystem} object allows the user to select the data type and dimension used for the instantiated type. It also\nallows encapsulating \\lstinline{Grid} or any of its specializations to modify the basic implementation features as required by the problem or solver. \n\nAll of the aspects which constitute a phase-field model are encapsulated in the class \\lstinline{Model}, of which the responsibility is to manage the phase-field data and numerical solver and initialize all phase-field data in a standardized way. It is also the primary interface to the phase-field data and interacting with the solver. \n\\lstinline{Model} is itself specialized in order to manage a specific set of equations of motion corresponding to a phase-field problem. The user is responsible for this final specialization through the procedure defined in Section~\\ref{methods:capabilities:models}.\n\n\n\\subsection{Capabilities} \\label{methods:capabilities}\n\n\nTo produce solutions to \nphase-field problems, \\textit{SymPhas}{} offers a number of capabilities.\nThese include convenient parameter specification alongside a rich feature set for specifying the equations of motion and managing the phase-field problem. In this section, we briefly list the capabilities relevant in typical use cases and outline the steps for generating a simple driver file in \\textit{SymPhas}{}.\n\n\\subsubsection{Symbolic Algebra} \\label{methods:capabilities:algebra}\n\nData are used in the symbolic algebra by linking it with a specialized expression type that represents a ``variable'' term (e.g., in the context of an equation of motion, each order parameter is a ``variable'' linked to the respective phase-field data).\nThe symbolic algebra also defines value literals and spatial derivative operators. Value literals include special constructs representing the positive and negative multiplicative identity (the numbers $1$ and $-1$) and the additive identity (the number $0$). These are mainly used to facilitate symbolic algebra rules. Some common functions are defined as well, including $\\sin$, $\\cos$ and the exponential. For less common cases, the convolution operator is also defined. Since the structure of expression trees is managed at compile time, type-based rules defined by specific expression tree structures are applied to formulate expressions, such as when addition, subtraction, multiplication and division operations are used. Rules include simplification, distribution, factorization.\n\nWhile the primary purpose of the symbolic algebra feature is to represent an equation of motion for a phase-field problem and support the user in implementing a specialized solver, the same functionality can be applied in more general applications. \nFeatures such as the ability to name variables and print formatted expressions to an output stream in either simple text or \\LaTeX{} format are included. Moreover, the symbolic algebra can be used to perform high-performance pointwise operations on arrays, include new symbols to the algebra ruleset, and even define identities for the new and existing symbols that would be applied automatically.\n\n\n\\subsubsection{Defining New Phase-Field Models} \\label{methods:capabilities:models}\n\n\\textit{SymPhas}{} provides convenient C++{} macro-based grammar to allow a user to define a new phase-field model in a completely unconstrained way, a novel feature among phase-field simulations software. Each new definition generates a specialization of the \\lstinline{Model} class. Upon recompiling the program, the new model is fully functional without the pitfalls of verbose implementation details. Moreover, the same definition can be interpreted by all solvers available in \\textit{SymPhas}{}.\n\nA model is defined in three parts:\n1) The model name as it appears in the compilation unit, which must be unique for all models, 2) a list of the order parameters and their types, and 3) the equations of motion. An additional optional section between the order parameter type list and equations of motion can be specified to define virtual variables. These can be used to measure desired system quantities or used in the equations of motion to optimize the runtime by pre-computing values. An example of defining a two-phase model with a virtual variable is demonstrated in Figure~\\ref{fig:model_definition}. Specific details including all available macros are offered in the manual.\n\n\n\\begin{figure}\n    \\centering\n    \\footnotesize\n    \\begin{lstlisting}\nMODEL(MC, \n  (SCALAR, SCALAR),\n  PROVISIONAL_DEF(\n    (SCALAR),\n    var(1) = c5 * op(1) * op(2))\n  MODEL_PREAMBLE_DEF(\n    ( auto op13 = c2 * op(1) * op(1) * op(1);\n      auto op23 = c4 * op(2) * op(2) * op(2); )\n    dop(1) = lap(op(1)) + c1 * op(1) - op13 + lit(2.) * var(1),\n    dop(2) = -bilap(op(2)) - lap(c3 * op(2) - op23 + c5 * op(1) * op(1)))\n)\n    \\end{lstlisting}\n    \\caption{An example of specifying a phase-field model. Model~C~\\cite{Hohenberg1977}, which represents eutectic growth with two order parameters~\\cite{Elder1994}, is implemented. It is associated with the given name ``\\lstinline{MC}'', which defines the type alias of the model in the code and therefore must be unique. The keywords \\lstinline{op(N)} and \\lstinline{dop(N)} refer to the \\lstinline{N}th order parameter and its time derivative, respectively, and the keyword \\lstinline{var(N)} refers to the \\lstinline{N}th virtual variable. The keyword \\lstinline{SCALAR} specifies that the field types are real-valued. The keyword \\lstinline{lit(v)} is used to represent a numeric constant of value \\lstinline{v} in the symbolic algebra expression. The keywords \\lstinline{lap} and \\lstinline{bilap} apply the 2nd and 4th spatial derivative to their arguments, respectively. The enumerated terms \\lstinline{c1} to \\lstinline{c5} are parameters passed to the model upon instantiation. Variables can be defined before the equations of motion using the macro \\lstinline{MODEL_PREAMBLE_DEF}, demonstrated here with the cubic terms, \\lstinline{op13} and \\lstinline{op23}. This option exists chiefly for convenience and does not affect the structure of the expression tree formulated for the equation of motion. If this section is omitted and only the dynamical equations are provided, then the macro \\lstinline{MODEL_DEF} is used instead.}\n    \\label{fig:model_definition}\n\\end{figure}\n\n\n\n\\subsubsection{Creating Custom Solvers}\n\nThe user may develop their own solver using the \\textit{SymPhas}{} API and seamlessly integrate it into an existing workflow. Implementation of a solver entails extending the provided \\lstinline{Solver} interface. \nA major benefit of the design is provided by the equation mediator object, since it remains completely internal to the functionality of the implemented solver and does not interact with the other parts of the API or program. This allows the solver implementation to be decoupled as much as possible from the surrounding implementation and allows the user to leverage the capabilities of the API without being limited by extensive requisite knowledge. \nAdditionally, if the built-in \\lstinline{SolverSystem} and its specializations are insufficient,\nthe user may develop a new specialization. This has some constraints and requirements, including following a specific naming style, inheritance requirements and recompilation of the solution module \\modulename{sol}. \n\n\n\\subsubsection{Standardized Problem Parameter Management}\n\nThe parameters of the problem are managed by a specialized object tasked with managing data that fully describes a phase-field problem. \nThis information includes the initial conditions, the interval data and information about the boundary conditions, the latter which can be specified on an individual basis.\nThere are a number of phase-field initialization routines available to the user, each tuned by user-provided parameters. Initialization routines defined by the user can also be used. \n\nThis approach allows\nthe user to have a unified approach to initializing, accessing, and passing problem information, simplifying the workflow and ensuring flexibility.\n\n\n\\subsubsection{Input/Output}\n\nThe configuration module provides the user with the ability to write a configuration file with phase-field problem and data persistence parameters, which can be used to construct the problem parameters object.\n\n\nThe \\textit{SymPhas}{} API includes data persistence capabilities through the \\modulename{io} module, which introduces functions and objects for reading and writing phase-field data. The user is also provided with the ability to persist phase-field data at regular checkpoints throughout the simulation, which can later be used to recover simulation data from the last saved point if it is interrupted for any reason. This supports program reliability and convenience, particularly for extended simulations. \nCurrently, there are three output/input formats: 1) Plain text matrix (the matrix format is amenable to plotting utilities such as gnuplot), 2) plain text column (an ordered list of vectors and values), and 3) binary output in the xdrfile format, popularized by GROMACS \\cite{Lindahl2021}. This functionality is available to the user when \\modulename{io} is compiled with \\textit{SymPhas}{} through CMake.\n\n\nInput can also be given to \\textit{SymPhas}{} through the command line; this method allows configuring some program-level parameters. Unlike the configuration file, this a base component of \\textit{SymPhas}{}. Command line parameters allow \\textit{SymPhas}{} to change some of its behavior, largely with regards to the initial condition generation. The basic set of program level parameters are introduced in \\modulename{lib}, and \\modulename{io} introduces more parameters. The list of all configurable parameters and their details are provided in the manual.\n\n\n\\begin{figure}\n\t\\centering\n\t\\scriptsize\n\t\\begingroup%\n\t\\makeatletter%\n\t\\providecommand\\color[2][]{%\n\t\t\\errmessage{(Inkscape) Color is used for the text in Inkscape, but the package 'color.sty' is not loaded}%\n\t\t\\renewcommand\\color[2][]{}%\n\t}%\n\t\\providecommand\\transparent[1]{%\n\t\t\\errmessage{(Inkscape) Transparency is used (non-zero) for the text in Inkscape, but the package 'transparent.sty' is not loaded}%\n\t\t\\renewcommand\\transparent[1]{}%\n\t}%\n\t\\providecommand\\rotatebox[2]{#2}%\n\t\\newcommand*\\fsize{\\dimexpr\\f@size pt\\relax}%\n\t\\newcommand*\\lineheight[1]{\\fontsize{\\fsize}{#1\\fsize}\\selectfont}%\n\t\\ifx\\svgwidth\\undefined%\n\t\\setlength{\\unitlength}{419.73992097bp}%\n\t\\ifx\\svgscale\\undefined%\n\t\\relax%\n\t\\else%\n\t\\setlength{\\unitlength}{\\unitlength * \\real{\\svgscale}}%\n\t\\fi%\n\t\\else%\n\t\\setlength{\\unitlength}{\\svgwidth}%\n\t\\fi%\n\t\\global\\let\\svgwidth\\undefined%\n\t\\global\\let\\svgscale\\undefined%\n\t\\makeatother%\n\t\\begin{picture}(1,0.55151926)%\n\t\t\\lineheight{1}%\n\t\t\\setlength\\tabcolsep{0pt}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=1]{implementation_small_v2.pdf}}%\n\t\t\\put(0.16157335,0.46704034){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Initialize configuration\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=2]{implementation_small_v2.pdf}}%\n\t\t\\put(0.49225814,0.5319947){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Driver\\end{tabular}}}}%\n\t\t\\put(0.82377576,0.5319947){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}\\lstinline{Model}\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=3]{implementation_small_v2.pdf}}%\n\t\t\\put(0.16104047,0.42347701){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}[Load Checkpoint?]\\end{tabular}}}}%\n\t\t\\put(0.48547551,0.10795121){\\color[rgb]{0,0,0}\\makebox(0,0)[lt]{\\begin{minipage}{0.67051006\\unitlength}\\raggedright \\end{minipage}}}%\n\t\t\\put(0.16333496,0.5319947){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Solve Phase-Field Problem\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=4]{implementation_small_v2.pdf}}%\n\t\t\\put(0.49065199,0.39721209){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Define problem parameters\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=5]{implementation_small_v2.pdf}}%\n\t\t\\put(0.16099403,0.34815596){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Read backup file\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=6]{implementation_small_v2.pdf}}%\n\t\t\\put(0.48967667,0.34821582){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Initialize \\lstinline{Model} list\\end{tabular}}}}%\n\t\t\\put(0.49295187,0.22960868){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Select next \\lstinline{Model}\\end{tabular}}}}%\n\t\t\\put(0.23460284,0.38343563){\\color[rgb]{0,0,0}\\makebox(0,0)[lt]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{l}\\textit{No}\\end{tabular}}}}%\n\t\t\\put(0.16829113,0.37616843){\\color[rgb]{0,0,0}\\makebox(0,0)[lt]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{l}\\textit{Yes}\\end{tabular}}}}%\n\t\t\\put(0.49056795,0.31364469){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}[All models simulated?]\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=7]{implementation_small_v2.pdf}}%\n\t\t\\put(0.49659382,0.26355095){\\color[rgb]{0,0,0}\\makebox(0,0)[lt]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{l}\\textit{No}\\end{tabular}}}}%\n\t\t\\put(0.43775105,0.27644197){\\color[rgb]{0,0,0}\\makebox(0,0)[lt]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{l}\\textit{Yes}\\end{tabular}}}}%\n\t\t\\put(0.82129282,0.23042956){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Update system (\\lstinline{update})\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=8]{implementation_small_v2.pdf}}%\n\t\t\\put(0.82129282,0.18497542){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Compute dynamics (\\lstinline{equation})\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=9]{implementation_small_v2.pdf}}%\n\t\t\\put(0.82129282,0.1395214){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Time evolve (\\lstinline{step})\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=10]{implementation_small_v2.pdf}}%\n\t\t\\put(0.82110556,0.09609849){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}[Reached final index?]\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=11]{implementation_small_v2.pdf}}%\n\t\t\\put(0.82148597,0.01546429){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Persist solution data\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=12]{implementation_small_v2.pdf}}%\n\t\t\\put(0.78188778,0.34903378){\\color[rgb]{0,0,0}\\makebox(0,0)[t]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{c}Model parameters\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=13]{implementation_small_v2.pdf}}%\n\t\t\\put(0.76108252,0.05424335){\\color[rgb]{0,0,0}\\makebox(0,0)[lt]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{l}\\textit{No}\\end{tabular}}}}%\n\t\t\\put(0.83023066,0.04574204){\\color[rgb]{0,0,0}\\makebox(0,0)[lt]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{l}\\textit{Yes}\\end{tabular}}}}%\n\t\t\\put(0,0){\\includegraphics[width=\\unitlength,page=14]{implementation_small_v2.pdf}}%\n\t\\end{picture}%\n\t\\endgroup%\n\t\n\t\n\t\\caption{Control flow diagram for the \\textit{SymPhas}{} driver used to run simulations in this work. A list of models is generated using parameters from a configuration file. The number of models that is initialized depends on the configuration. \n\tThe \\textit{SymPhas}{} library provides a function to perform the solution loop illustrated under \\lstinline{Model}.\n\t}\n\t\\label{fig:flow4}\n\\end{figure}%\n\\begin{figure}\n    \\centering\n    \\footnotesize\n    \\begin{lstlisting}\n#include \"symphas.h\"\n#define psi op(1)\n#define dpsi dop(1)\nMODEL(EX, (SCALAR), MODEL_DEF(\n  dpsi = lap(psi) + (c1 - c2 * psi * psi) * psi))\n  \nint main(int argc, char* argv[]) {\n  double dt = 0.5;\n  symphas::problem_parameters_type pp{ 1 };\n  symphas::b_data_type bdata;\n  symphas::interval_data_type vdata;\n  symphas::init_data_type tdata{ Inside::UNIFORM, { -1, 1 } };\n  \n  symphas::interval_element_type interval;\n  interval.set_interval_count(0, 80, 128);\n  \n  bdata[Side::LEFT] = BoundaryType::PERIODIC;\n  bdata[Side::RIGHT] = BoundaryType::PERIODIC;\n  bdata[Side::TOP] = BoundaryType::PERIODIC;\n  bdata[Side::BOTTOM] = BoundaryType::PERIODIC;\n  vdata[Axis::X] = interval;\n  vdata[Axis::Y] = interval;\n  \n  pp.set_boundary_data(&bdata);\n  pp.set_initial_data(&tdata);\n  pp.set_interval_data(&vdata);\n  pp.set_problem_time_step(dt);\n\n  model_EX_t<2, SolverSP<Stencil2d2h<5, 9, 6>>> model{ pp };\n  symphas::find_solution(model, dt, 100);\n}\n    \\end{lstlisting}\n    \\caption{Example of a simple driver program which includes the model definition.\n    A phase-field problem of one order parameter named \\lstinline{EX} is defined. System boundaries are defined to be periodic on each edge and initial conditions are set by seeding valuing with the uniform distribution $\\mathcal{U}(-1, 1)$. Identical $x$ and $y$ intervals are defined and provided, resulting in a square $128\\times128$ grid. When the model is created, the solver (\\lstinline{SolverSP}) is passed as a template type parameter and the \\lstinline{find_solution} function is called to perform 100 solver iterations. This program can be compiled through CMake or by providing the directories of the installed header and library to gcc.}\n    \\label{fig:driver_implementation}\n\\end{figure}\n\n\\subsection{Implementation} \\label{methods:implementation}\n\nThe entire \\textit{SymPhas}{} API is available on Github (\\texttt{https://github.com/SoftSimu/SymPhas}) alongside two solvers (forward Euler, Sec.~\\ref{sec:feuler} and the Semi-Implicit Fourier Spectral, Sec.~\\ref{sec:spectral}), model definitions and driver file examples.\nThe program control flow of the driver file\nused to to generate the simulations in this paper is illustrated in Figure~\\ref{fig:flow}, and is found in the directory \\lstinline{examples/simultaneous-configs} relative to the source code root. \nThis driver performs several steps, including data persistence, as part of data collection, but a fully functional driver file can be as simple as is written in Figure~\\ref{fig:driver_implementation} and included with the source code in \\lstinline{examples/simple-driver}. It is presented here to illustrate relevant \\textit{SymPhas}{} API elements, but primarily demonstrate its ease of use.\n\n\n\\subsubsection{Forward Euler Solver}\n\\label{sec:feuler}\n\nA forward Euler solver is a well-known  explicit numerical method for partial differential equations. It is a first order method and it is known to have instabilities especially when solving stiff systems. It is provided as a base method. \nSince it is well-known and taught in virtually every course in numerical methods, we will not discuss it further but refer the reader to one of the standard references such as Press \\textit{et al.}~\\cite{Press1992}.\n\n\\subsubsection{Semi-Implicit Spectral Solver}\n\\label{sec:spectral}\n\n\nConsider a phase-field problem for the order parameter $\\psi = \\psi(\\vec{x},t)$; the equation of motion for this problem may be expressed in the form\n\\begin{equation}\n\t\\frac{\\partial \\psi}{\\partial t} = \\mathcal{L}(\\nabla^{n})\\left\\{\\psi\\right\\} + \\sum_i{{\\mathcal{N}}_i(\\nabla^{m_i})\\left\\{ f_i(\\psi)\\right\\}}\\,,\n\t\\label{eq:spectralphasefield}\n\\end{equation}\nwhere $\\mathcal{L}$ is a linear combination of derivatives up to order $n$ applied to $\\psi$, and each term in the sum over $i$ is a unique linear differential operator, $N_i$, of derivatives up to order $m_i$ that is applied to a nonlinear function $f_i$.\n\nUnder periodic boundary conditions, the semi-implicit Fourier spectral solver approximates the solution to $\\psi$ by first applying the Fourier transform of Equation~(\\ref{eq:spectralphasefield}):\n\\begin{align}\n\t\\frac{\\partial \\hat{\\psi}_{\\vec{k}}}{\\partial t} &= {L(k^n)}\\hat{\\psi}_{\\vec{k}} + \\sum_i{{N_i}(k^{m_i}) \\hat{f}_{i}(\\psi)_{\\vec{k}}} \n\t\\,,\n\t\\label{eq:phasefieldfourier}\n\\end{align}\nwhere $\\hat{\\phantom{a}}$ indicates the Fourier transform of the respective term, $\\hat{\\psi}_{\\vec{k}} = \\hat{\\psi}(\\vec{k}, t)$, $\\vec{k}$ is a vector in Fourier space and $k = |\\vec{k}|$. Also, since $\\vec{\\nabla} \\rightarrow i\\vec{k}$ (and correspondingly $\\nabla^2 \\rightarrow -|k|^2$) under a Fourier transform in an infinite domain, the linear operator $\\mathcal{L}$ becomes the function ${L}(k^n)$, a linear combination of the Fourier transformed derivatives, and likewise for $\\mathcal{N}$. \n\nA difference scheme \\cite{Provatas2010} to Equation~(\\ref{eq:phasefieldfourier}) is determined by solving it as a linear ordinary differential equation and approximating to 1st order, yielding\n\\begin{align}\n\t\\hat{\\psi}(t + \\Delta t) \\approx A\\hat{\\psi}_{\\vec{k}}(t) + B\\sum_i N_i(k^{m_i}) \\hat{f}_n(\\psi)_{\\vec{k}}\\,,\n\t\\label{eq:spectralscheme}\n\\end{align}\nwhere\n\\begin{gather}\n\tA = e^{{L}(k^n)\\Delta t} \\quad \\text{and} \\quad B = \\frac{e^{{L}(k^n)\\Delta t} - 1}{{L}(k^n)}\\,.\n\t\\label{eq:spectraloperators}\n\\end{gather}\n\nThe spectral solver produces Equation~(\\ref{eq:spectralscheme}) from any given equation of motion, a significant advantage that generalizes the spectral solver to a multitude of problems. This\ndemonstrates the adaptability of the solver and of the program design in general.\nThe spectral solver also computes the values of $A$ and $B$ \\textit{a priori} to minimize the runtime.\nThe procedure is as follows:\n\\begin{enumerate}\n\t\\item Split the equation into linear and nonlinear parts.\n\tCall the linear part $\\mathcal{L}$ and the nonlinear part $\\mathcal{N}$, analogous to the notation in Equation~(\\ref{eq:spectralphasefield}).\n\t\\item Further split the linear part by separating out terms which do not involve $\\psi$, along with terms that cannot be expressed using a linear operator. \n\tCall the expression formed by these terms $\\mathcal{L}_*$ and the expression formed by all other terms $\\mathcal{L}_\\psi$. Thus, $\\mathcal{L} = \\mathcal{L}_\\psi + \\mathcal{L}_*$.\n\t\\item Obtain ${L}_\\psi$ by removing $\\psi$ from terms in $\\mathcal{L}_{\\psi}$ and interchanging the derivative terms with the Fourier space transformed derivatives. Generate values for $A$ by evaluating ${L}_\\psi$.\n\t\\item Create the new expression ${{L}}_*$ by exchanging all order parameters in $\\mathcal{L}_*$ with the Fourier transformed counterparts. \n\n\t\\item \\label{step:make_D} Let ${{L}}_*$ be represented as the sum of its unique derivatives $d_n$ applied to expressions $e_n$, viz.: $$\t{L}_* = \\sum_n{d_n \\cdot e_n}\\,. $$ \n\tForm the set $\\mathbf{D}_* = \\{(d_n, e_n) \\mid n\\}$.\n\t\n\t\\item Apply Step~\\ref{step:make_D} for the terms of $\\mathcal{N}$, producing the list $\\mathbf{D}_{\\mathcal{N}}$. Form the set $\\mathbf{D}_N = \\{(d_n, \\hat{e}_n) \\mid (d_n, e_n) \\in \\mathbf{D}_{\\mathcal{N}} \\}$ where $\\hat{\\phantom{a}}$ denotes the Fourier transform of the respective term.\n\t\n\t\\item \n\tDefine the following sets: \n\t\\begin{align}\t\n\t\t\\mathbf{D}_1 &= \\{(d_n, e_n, e_m) \\mid (d_n, e_n) \\in \\mathbf{D}_{*}, (d_m, e_m) \\in \\mathbf{D}_{{N}}, d_n = d_m\\}  \\,, \\\\\n\t\t\\mathbf{D}_2 &= \\{(d_n, e_n, 0) \\mid (d_n, e_n) \\in \\mathbf{D}_{*}, (d_m, e_m) \\in \\mathbf{D}_{{N}}, d_n \\not\\in \\{d\\}_m\\} \\,, \\\\\n\t\t\\mathbf{D}_3 &= \\{(d_m, 0, e_m) \\mid (d_m, e_m) \\in \\mathbf{D}_{{N}}, (d_n, e_n) \\in \\mathbf{D}_{*}, d_m \\not\\in \\{d\\}_n\\}  \\,.\n\t\\end{align}\n\tDefine $\\mathbf{D} = \\mathbf{D}_1 \\,\\cup\\,\\mathbf{D}_2\\,\\cup\\,\\mathbf{D}_3$. In other words, generate elements of $\\mathbf{D}$ by pairing together the expressions in elements from $\\mathbf{D}_{*}$ and $\\mathbf{D}_{N}$ that match based on the derivatives $d_i$, and if there is no matching derivative in the other set, use 0 in place of the associated expression.\n\t\n\t\\item Define two sequences ${B}_i = B\\hat{d}_i$ where $B$ is as defined in Equation~(\\ref{eq:spectraloperators}) and ${E}_i = (e_{n_i}, {e}_{m_i})$, using the elements: $$(d_i, e_{n_i}, e_{m_i}) \\in \\mathbf{D}.$$\n\tWith respect to Equation~(\\ref{eq:spectralscheme}), ${B}_i = B {N}_i(k)$ and $e_{n_i} + {e}_{m_i} = \\hat{f}_i(\\psi)_{\\vec{k}}$.\n\t\\item Return the set $\\{A, \\{{B}\\}_i, \\{{E}\\}_i\\}$.\n\\end{enumerate}\n\nThe scheme applied by the implemented spectral solver is then given by:\n\\begin{equation}\n\t\\hat{\\psi}^{n+1} = A\\hat{\\psi}^n(t) + \\sum_i {B}_i \\left( {E}^0_i + {E}^1_i \\right)\\,,\n\\end{equation}\nwhere $\\hat{\\psi}^n$ is the approximate solution to $\\hat{\\psi}(\\vec{k}, t)$  ($t = n\\Delta t $ for time step $\\Delta t$), ${E}^0_i$ and ${E}^1_i$ are the sets of the first and second elements of ${E}_i$, respectively, and subscript $i$ represents the indexed elements of their respective sets.\n\n\n\n\\section{Simulations and Verification} \\label{sec:results}\n\nUsing the semi-implicit spectral solver, we performed simulations of the Allen--Cahn equation (Model A in the Hohenberg--Halperin classification \\cite{Hohenberg1977}) describing a non-conserved order parameter $\\psi = \\psi(\\vec{x}, t)$ \\cite{Allen1975}, the Cahn--Hilliard equation (Model B in the Hohenberg--Halperin classification \\cite{Hohenberg1977}) describing a conserved order parameter $m = m(\\vec{x}, t)$, and a two order-parameter problem which couples a conserved order parameter $m = m(\\vec{x}, t)$ with a non-conserved order parameter $\\psi = \\psi(\\vec{x}, t)$ (Model C in the Hohenberg--Halperin classification \\cite{Hohenberg1977}). The systems were simulated in both two and three dimensions. Since these systems are well-known and described in literature and since the main goal here is to demonstrate the software, we will not describe the above models in more detail but refer the reader to standard references such as the classic article by Hohenberg and Halperin \\cite{Hohenberg1977} and the book by Provatas and Elder \\cite{Provatas2010}. We also include a simulation of the phase-field crystal model of Elder et al.~\\cite{Elder2002}.\n\n\nThe Allen--Cahn equation describes the dynamics of a non-conserved order parameter $\\psi = \\psi(\\vec{x}, t)$ \\cite{Hohenberg1977,Provatas2010, Allen1975} as \n\\begin{equation}\n        \\frac{\\partial \\psi}{\\partial t} = \\nabla^2 \\psi + c_1\\psi -  c_3\\psi^3\\,.\n        \\label{eq:modelb}\n\\end{equation}\nThe Cahn--Hilliard equation, on the other hand, describes the phase separation dynamics of a conserved order parameter $m = m(\\vec{x}, t)$ \\cite{Hohenberg1977,Provatas2010,Cahn1958} as\n\\begin{equation}\n    \\frac{\\partial m}{\\partial t} = -\\nabla^4 m - \\nabla^2 \\left(c_1m - c_2m^3 \\right)\\,,\n    \\label{eq:modela}\n\\end{equation}\nIn addition to these, we simulated a two order parameter problem which couples a conserved order parameter $m = m(\\vec{x}, t)$ with a non-conserved order parameter $\\psi = \\psi(\\vec{x}, t)$ (Model C~\\cite{Hohenberg1977}) through a nonlinear term in the free energy functional:\n\\begin{align}\n    \\dfrac{\\partial \\psi}{\\partial t} &= \n    \\nabla^2 \\psi +c_1\\psi - c_2\\psi^3 + 2c_5 \\psi m\\\\\n    \\dfrac{\\partial m}{\\partial t} &= -\\nabla^4 m - \\nabla^2 \\left(c_3 m - c_4 m^3 + c_5 \\psi^2\\right).\n    \\label{eq:modelc}\n\\end{align}\nThis model has been used to describe eutectic growth~\\cite{Elder1994}.\n\nThe phase-field crystal model was developed to include periodic structure to the standard phase-field free energy functional in order to represent elastic and plastic interactions in a crystal \\cite{Elder2002, Elder_2004}.\nFor a conserved density field $n=n(\\vec{x}, t)$, the dynamical equation of a phase-field crystal model is given by:\n\\begin{equation}\n\t\\frac{\\partial n}{\\partial t} = \\nabla^2\\left( n^2 + n^3 + \\left((q_0 + \\nabla^2)^2 - \\varepsilon\\right)n \\right). \\label{eq:pfc}\n\\end{equation}\n\n\n\nCoarsening of the phase-field at any point during the transition can be quantified by the radial average of the static structure factor, $S(k)$. The static structure factor, $S(\\vec{k})$, measures incident scattering in a solid \\cite{Lovesey1984, Squires1978}. In the Born approximation for periodic systems, $S(\\vec{k}) = | \\hat{\\rho}_{\\vec{k}} |^2$ \\cite{Lindgard1994}. The term $\\hat{\\rho}_{\\vec{k}}$ is the Fourier transform of $\\rho(\\vec{r})$, the particle occupancy at the position $\\vec{r}$ in the lattice, which is 1 in the solid phase and 0 elsewhere. Correspondingly, for computing the structure factor of the continuous order parameter field $\\psi(\\vec{r})$, the positive phase is chosen to represent solidification, that is, $\\rho = 1$ when $\\psi > 0$ and $\\rho = 0$ otherwise. \n\nFor both the conserved and non-conserved phase-field models, the radial average of the structure factor should correspond to Porod's law~\\cite{Bray_2002}:\n\\begin{equation}\n    S(k) \\sim (Lk^{d+1})^{-1}\\,,\n    \\label{eq:porodslaw}\n\\end{equation}\nwhere $L$ is the size of the system and $d$ is the dimension. \nWe use this to establish that \\textit{SymPhas}{} generates correct solutions by validating that the relationship holds for $S(k)$ measured from simulations of the Allen--Cahn and Cahn--Hilliard models, representing the non-conserved and conserved dynamics, respectively~\\cite{Puri1997}. We proceed by computing $S(k)$ from the average of 10 independent simulations of these models in both 2D and 3D and verifying that the scaling is consistent to Porod's law, Equation~(\\ref{eq:porodslaw}), for the corresponding dimension. \n\nThe implementations of the simulated models with the \\textit{SymPhas}{} model definitions macros are provided in Figure~\\ref{fig:models_abc_definitions}. As the figure shows, models are defined in a compact and intuitive way.\n\nThe initial conditions of models A, B and C are uniformly distributed noise, and the equation parameters $c_1$, $c_2$, $c_3$, $c_4$ and $c_5$ are set to unity. For the phase-field crystal model, 128 randomly arranged seeds containing large fluctuations are initially distributed throughout the system, and the parameters of the equation and simulation were selected from Elder et al.~\\cite{Elder2002}.\nThe simulation results for the non-conserved Allen--Cahn model (Model A, Equation~(\\ref{eq:modela})) are displayed in Figure~\\ref{fig:modelab:a} and results for the conserved Cahn--Hilliard model (Model B, Equation~(\\ref{eq:modela})) are displayed in Figure~\\ref{fig:modelab:b}. The structure factor results of these two models are presented in Figure~\\ref{fig:model-a_sf} and Figure~\\ref{fig:model-b_sf}, respectively. The results for the eutectic model consisting of two coupled equations of motion~\\cite{Elder1994} (Model C, Equation~(\\ref{eq:modelc})) are shown in Figure~\\ref{fig:modelc} and the phase-field crystal model~\\cite{Elder2002} (Equation~(\\ref{eq:pfc})) with a conserved field is displayed in Figure~\\ref{fig:pfc}. \n\n\\begin{figure}\n    \\centering\n\\begin{subfigure}{1.\\textwidth}\n    \\footnotesize\n    \\begin{lstlisting}\nMODEL(MA, (SCALAR),\n  MODEL_DEF(\n    dpsi = lap(psi) + (c1 - c2 * psi * psi) * psi))\n    \\end{lstlisting}\n    \\caption{}\n    \\label{fig:modeldef:a}\n\\end{subfigure}\\hfill%\n\\begin{subfigure}{1.0\\textwidth}\n    \\footnotesize\n    \\begin{lstlisting}\nMODEL(MB, (SCALAR),\n  MODEL_DEF(\n    dpsi = -bilap(m) - lap((c1 - c2 * m * m) * m)))\n    \\end{lstlisting}\n    \\caption{}\n    \\label{fig:modeldef:b}\n\\end{subfigure}\n\\begin{subfigure}{1.0\\textwidth}\n    \\footnotesize\n    \\begin{lstlisting}\nMODEL(MC, (SCALAR, SCALAR),\n  MODEL_PREAMBLE_DEF(\n    ( auto psi3 = c2 * psi * psi * psi;\n      auto m3 = c4 * m * m * m; ),\n    dpsi = lap(psi) + c1 * psi - psi3 + lit(2.) * c5 * psi * m,\n    dm = -bilap(m) - lap(c3 * m - m3 + c5 * psi * psi)))\n    \\end{lstlisting}\n    \\caption{}\n    \\label{fig:modeldef:c}\n\\end{subfigure}\\hfill%\n\\begin{subfigure}{1.0\\textwidth}\n\\footnotesize\n\\begin{lstlisting}\nPFC_TYPE(PC, \n  DEFAULTS(\n    DEFAULT_DYNAMIC(PFC_CONSERVED)\n  ), \n  (SCALAR))\n   \n\\end{lstlisting}\n    \\caption{}\n    \\label{fig:modeldef:pfc}\n\\end{subfigure}\n    \\caption{Macro implementations of (\\subref{fig:modeldef:a}) the Allen--Cahn model \\cite{Allen1975} from Equation~(\\ref{eq:modela}),  (\\subref{fig:modeldef:b}) the Cahn--Hilliard model \\cite{Cahn1958} from Equation~(\\ref{eq:modelb}),  and  (\\subref{fig:modeldef:c}) Model C \\cite{Elder1994} from Equation~(\\ref{eq:modelc}). \n    The order parameter names in the macro specification are chosen to correspond to the variable names in the respective equations of motion, and the keys \\lstinline{c1} to \\lstinline{c5} correspond to the coefficients $c_1$ to $c_5$. (d) The phase-field crystal model (Equation~\\ref{eq:pfc}) specification uses different macro keywords that allow selecting the phase-field crystal model as a type of problem, dispensing with equation specification and allowing selection of the dynamics.\n    }\n    \n    \\label{fig:models_abc_definitions}\n\\end{figure}%\n\\begin{figure}\n    \\centering\n\\begin{minipage}{0.48\\textwidth}\n   \n    %\n    \\begin{subfigure}{\\textwidth}\n    \\centering\n   \n    \\begingroup\n   \n   \n   \n    \\inputencoding{cp1252}%\n    \\makeatletter\n    \\providecommand\\color[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage color not loaded in conjunction with\n    \t\tterminal option `colourtext'%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{Either use 'blacktext' in gnuplot or load the package\n    \t\tcolor.sty in LaTeX.}%\n    \t\\renewcommand\\color[2][]{}%\n    }%\n    \\providecommand\\includegraphics[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage graphicx or graphics not loaded%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}%\n    \t\\renewcommand\\includegraphics[2][]{}%\n    }%\n    \\providecommand\\rotatebox[2]{#2}%\n    \\@ifundefined{ifGPcolor}{%\n    \t\\newif\\ifGPcolor\n    \t\\GPcolortrue\n    }{}%\n    \\@ifundefined{ifGPblacktext}{%\n    \t\\newif\\ifGPblacktext\n    \t\\GPblacktexttrue\n    }{}%\n   \n    \\let\\gplgaddtomacro\\g@addto@macro\n   \n    \\gdef\\gplbacktext{}%\n    \\gdef\\gplfronttext{}%\n    \\makeatother\n    \\ifGPblacktext\n   \n    \\def\\colorrgb#1{}%\n    \\def\\colorgray#1{}%\n    \\else\n   \n    \\ifGPcolor\n    \\def\\colorrgb#1{\\color[rgb]{#1}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color[rgb]{1,0,0}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color[rgb]{0,1,0}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color[rgb]{0,0,1}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color[rgb]{1,0,1}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color[rgb]{0,1,1}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color[rgb]{1,1,0}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color[rgb]{0,0,0}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color[rgb]{1,0.3,0}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color[rgb]{0.5,0.5,0.5}}%\n    \\else\n   \n    \\def\\colorrgb#1{\\color{black}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color{black}}%\n    \\fi\n    \\fi\n    \\setlength{\\unitlength}{0.0500bp}%\n    \\ifx\\gptboxheight\\undefined%\n    \\newlength{\\gptboxheight}%\n    \\newlength{\\gptboxwidth}%\n    \\newsavebox{\\gptboxtext}%\n    \\fi%\n    \\setlength{\\fboxrule}{0.5pt}%\n    \\setlength{\\fboxsep}{1pt}%\n    \\begin{picture}(5182.00,1872.00)%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(1714,1924){\\makebox(0,0)[r]{\\strut{}index $500$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3341,1924){\\makebox(0,0)[r]{\\strut{}index $2,000$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(4963,1924){\\makebox(0,0)[r]{\\strut{}index $20,000$}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(218,0){\\makebox(0,0){\\strut{}$-1$}}%\n    \t\t\\put(1404,0){\\makebox(0,0){\\strut{}$-0.5$}}%\n    \t\t\\put(2590,0){\\makebox(0,0){\\strut{}$0$}}%\n    \t\t\\put(3776,0){\\makebox(0,0){\\strut{}$0.5$}}%\n    \t\t\\put(4963,0){\\makebox(0,0){\\strut{}$1$}}%\n    \t}%\n    \t\\gplbacktext\n    \t\\put(0,0){\\includegraphics{model-a-2d_data0_20000.pdf}}%\n    \t\\gplfronttext\n    \\end{picture}%\n    \\endgroup\n    \n    \\caption{}\n    \\label{fig:modelab:a:2d}\n    \\end{subfigure}\n    %\n    \\begin{subfigure}{\\textwidth}\n    \\centering\n    \\includegraphics[width=0.33\\textwidth]{modela50.png}\\hfill%\n    \\includegraphics[width=0.33\\textwidth]{modela200.png}\\hfill%\n    \\includegraphics[width=0.33\\textwidth]{modela2000.png}\n    \\caption{}\n    \\label{fig:modelab:a:cs}\n    \\end{subfigure}\\vspace{10pt}\n    %\n    \\begin{subfigure}{\\textwidth}\n    \\centering\n    \n   \n    \\begingroup\n   \n   \n   \n    \\inputencoding{cp1252}%\n    \\makeatletter\n    \\providecommand\\color[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage color not loaded in conjunction with\n    \t\tterminal option `colourtext'%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{Either use 'blacktext' in gnuplot or load the package\n    \t\tcolor.sty in LaTeX.}%\n    \t\\renewcommand\\color[2][]{}%\n    }%\n    \\providecommand\\includegraphics[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage graphicx or graphics not loaded%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}%\n    \t\\renewcommand\\includegraphics[2][]{}%\n    }%\n    \\providecommand\\rotatebox[2]{#2}%\n    \\@ifundefined{ifGPcolor}{%\n    \t\\newif\\ifGPcolor\n    \t\\GPcolortrue\n    }{}%\n    \\@ifundefined{ifGPblacktext}{%\n    \t\\newif\\ifGPblacktext\n    \t\\GPblacktexttrue\n    }{}%\n   \n    \\let\\gplgaddtomacro\\g@addto@macro\n   \n    \\gdef\\gplbacktext{}%\n    \\gdef\\gplfronttext{}%\n    \\makeatother\n    \\ifGPblacktext\n   \n    \\def\\colorrgb#1{}%\n    \\def\\colorgray#1{}%\n    \\else\n   \n    \\ifGPcolor\n    \\def\\colorrgb#1{\\color[rgb]{#1}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color[rgb]{1,0,0}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color[rgb]{0,1,0}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color[rgb]{0,0,1}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color[rgb]{1,0,1}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color[rgb]{0,1,1}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color[rgb]{1,1,0}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color[rgb]{0,0,0}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color[rgb]{1,0.3,0}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color[rgb]{0.5,0.5,0.5}}%\n    \\else\n   \n    \\def\\colorrgb#1{\\color{black}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color{black}}%\n    \\fi\n    \\fi\n    \\setlength{\\unitlength}{0.0500bp}%\n    \\ifx\\gptboxheight\\undefined%\n    \\newlength{\\gptboxheight}%\n    \\newlength{\\gptboxwidth}%\n    \\newsavebox{\\gptboxtext}%\n    \\fi%\n    \\setlength{\\fboxrule}{0.5pt}%\n    \\setlength{\\fboxsep}{1pt}%\n    \\begin{picture}(5182.00,1872.00)%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(1714,1924){\\makebox(0,0)[r]{\\strut{}index $50$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3341,1924){\\makebox(0,0)[r]{\\strut{}index $200$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(4963,1924){\\makebox(0,0)[r]{\\strut{}index $2,000$}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(218,0){\\makebox(0,0){\\strut{}$-1$}}%\n    \t\t\\put(1404,0){\\makebox(0,0){\\strut{}$-0.5$}}%\n    \t\t\\put(2590,0){\\makebox(0,0){\\strut{}$0$}}%\n    \t\t\\put(3776,0){\\makebox(0,0){\\strut{}$0.5$}}%\n    \t\t\\put(4963,0){\\makebox(0,0){\\strut{}$1$}}%\n    \t}%\n    \t\\gplbacktext\n    \t\\put(0,0){\\includegraphics{model-a-3d_data0_2000.pdf}}%\n    \t\\gplfronttext\n    \\end{picture}%\n    \\endgroup\n    \n    \n    \\caption{}\n    \\label{fig:modelab:a:3d}\n    \\end{subfigure}\n    \\caption{Snapshots from simulations of a 2D ($1024\\times1024$) and 3D ($256 \\!\\times256 \\times \\!256$) Allen--Cahn model \\cite{Allen1975}:\n    (\\subref{fig:modelab:a:2d}) the 2D system is shown at three intervals at solution index 500, 2,000 and 20,000.  (\\subref{fig:modelab:a:cs}) the 3D system is visualized using VTK \\cite{vtk} at solution index 50, 200 and 2,000 with a cross-section highlighted for visibility, where (\\subref{fig:modelab:a:3d}) shows the cross sections.\n    The simulations use a time step of $\\Delta t = 0.25$ and are initially seeded with random values between -1 and 1.\n    }\n    \\label{fig:modelab:a}\n\\end{minipage}\\hfil\n\\begin{minipage}{0.48\\textwidth}\n   \n   \n    \\centering\n    \\begin{subfigure}{\\textwidth}\n    \\centering\n    \n   \n    \\begingroup\n   \n   \n   \n    \\inputencoding{cp1252}%\n    \\makeatletter\n    \\providecommand\\color[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage color not loaded in conjunction with\n    \t\tterminal option `colourtext'%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{Either use 'blacktext' in gnuplot or load the package\n    \t\tcolor.sty in LaTeX.}%\n    \t\\renewcommand\\color[2][]{}%\n    }%\n    \\providecommand\\includegraphics[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage graphicx or graphics not loaded%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}%\n    \t\\renewcommand\\includegraphics[2][]{}%\n    }%\n    \\providecommand\\rotatebox[2]{#2}%\n    \\@ifundefined{ifGPcolor}{%\n    \t\\newif\\ifGPcolor\n    \t\\GPcolortrue\n    }{}%\n    \\@ifundefined{ifGPblacktext}{%\n    \t\\newif\\ifGPblacktext\n    \t\\GPblacktexttrue\n    }{}%\n   \n    \\let\\gplgaddtomacro\\g@addto@macro\n   \n    \\gdef\\gplbacktext{}%\n    \\gdef\\gplfronttext{}%\n    \\makeatother\n    \\ifGPblacktext\n   \n    \\def\\colorrgb#1{}%\n    \\def\\colorgray#1{}%\n    \\else\n   \n    \\ifGPcolor\n    \\def\\colorrgb#1{\\color[rgb]{#1}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color[rgb]{1,0,0}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color[rgb]{0,1,0}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color[rgb]{0,0,1}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color[rgb]{1,0,1}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color[rgb]{0,1,1}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color[rgb]{1,1,0}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color[rgb]{0,0,0}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color[rgb]{1,0.3,0}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color[rgb]{0.5,0.5,0.5}}%\n    \\else\n   \n    \\def\\colorrgb#1{\\color{black}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color{black}}%\n    \\fi\n    \\fi\n    \\setlength{\\unitlength}{0.0500bp}%\n    \\ifx\\gptboxheight\\undefined%\n    \\newlength{\\gptboxheight}%\n    \\newlength{\\gptboxwidth}%\n    \\newsavebox{\\gptboxtext}%\n    \\fi%\n    \\setlength{\\fboxrule}{0.5pt}%\n    \\setlength{\\fboxsep}{1pt}%\n    \\begin{picture}(5182.00,1872.00)%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(1714,1924){\\makebox(0,0)[r]{\\strut{}index $5,000$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3341,1924){\\makebox(0,0)[r]{\\strut{}index $20,000$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(4963,1924){\\makebox(0,0)[r]{\\strut{}index $200,000$}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(218,0){\\makebox(0,0){\\strut{}$-1$}}%\n    \t\t\\put(1404,0){\\makebox(0,0){\\strut{}$-0.5$}}%\n    \t\t\\put(2590,0){\\makebox(0,0){\\strut{}$0$}}%\n    \t\t\\put(3776,0){\\makebox(0,0){\\strut{}$0.5$}}%\n    \t\t\\put(4963,0){\\makebox(0,0){\\strut{}$1$}}%\n    \t}%\n    \t\\gplbacktext\n    \t\\put(0,0){\\includegraphics{model-b-2d_data0_200000.pdf}}%\n    \t\\gplfronttext\n    \\end{picture}%\n    \\endgroup\n    \n    \n    \\caption{}\n    \\label{fig:modelab:b:2d}\n    \\end{subfigure}\n    %\n    \\begin{subfigure}{\\textwidth}\n    \\centering\n    \\includegraphics[width=0.33\\textwidth]{modelb500.png}\\hfill%\n    \\includegraphics[width=0.33\\textwidth]{modelb2000.png}\\hfill%\n    \\includegraphics[width=0.33\\textwidth]{modelb20000.png}\n    \\caption{}\n    \\label{fig:modelab:b:cs}\n    \\end{subfigure}\\vspace{10pt}\n    %\n    \\begin{subfigure}{\\textwidth}\n    \\centering\n    \n   \n    \\begingroup\n   \n   \n   \n    \\inputencoding{cp1252}%\n    \\makeatletter\n    \\providecommand\\color[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage color not loaded in conjunction with\n    \t\tterminal option `colourtext'%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{Either use 'blacktext' in gnuplot or load the package\n    \t\tcolor.sty in LaTeX.}%\n    \t\\renewcommand\\color[2][]{}%\n    }%\n    \\providecommand\\includegraphics[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage graphicx or graphics not loaded%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}%\n    \t\\renewcommand\\includegraphics[2][]{}%\n    }%\n    \\providecommand\\rotatebox[2]{#2}%\n    \\@ifundefined{ifGPcolor}{%\n    \t\\newif\\ifGPcolor\n    \t\\GPcolortrue\n    }{}%\n    \\@ifundefined{ifGPblacktext}{%\n    \t\\newif\\ifGPblacktext\n    \t\\GPblacktexttrue\n    }{}%\n   \n    \\let\\gplgaddtomacro\\g@addto@macro\n   \n    \\gdef\\gplbacktext{}%\n    \\gdef\\gplfronttext{}%\n    \\makeatother\n    \\ifGPblacktext\n   \n    \\def\\colorrgb#1{}%\n    \\def\\colorgray#1{}%\n    \\else\n   \n    \\ifGPcolor\n    \\def\\colorrgb#1{\\color[rgb]{#1}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color[rgb]{1,0,0}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color[rgb]{0,1,0}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color[rgb]{0,0,1}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color[rgb]{1,0,1}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color[rgb]{0,1,1}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color[rgb]{1,1,0}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color[rgb]{0,0,0}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color[rgb]{1,0.3,0}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color[rgb]{0.5,0.5,0.5}}%\n    \\else\n   \n    \\def\\colorrgb#1{\\color{black}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color{black}}%\n    \\fi\n    \\fi\n    \\setlength{\\unitlength}{0.0500bp}%\n    \\ifx\\gptboxheight\\undefined%\n    \\newlength{\\gptboxheight}%\n    \\newlength{\\gptboxwidth}%\n    \\newsavebox{\\gptboxtext}%\n    \\fi%\n    \\setlength{\\fboxrule}{0.5pt}%\n    \\setlength{\\fboxsep}{1pt}%\n    \\begin{picture}(5182.00,1872.00)%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(1714,1924){\\makebox(0,0)[r]{\\strut{}index $500$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3341,1924){\\makebox(0,0)[r]{\\strut{}index $2,000$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(4963,1924){\\makebox(0,0)[r]{\\strut{}index $20,000$}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(218,0){\\makebox(0,0){\\strut{}$-1$}}%\n    \t\t\\put(1404,0){\\makebox(0,0){\\strut{}$-0.5$}}%\n    \t\t\\put(2590,0){\\makebox(0,0){\\strut{}$0$}}%\n    \t\t\\put(3776,0){\\makebox(0,0){\\strut{}$0.5$}}%\n    \t\t\\put(4963,0){\\makebox(0,0){\\strut{}$1$}}%\n    \t}%\n    \t\\gplbacktext\n    \t\\put(0,0){\\includegraphics{model-b-3d_data0_20000.pdf}}%\n    \t\\gplfronttext\n    \\end{picture}%\n    \\endgroup\n    \n    \n    \\caption{}\n    \\label{fig:modelab:b:3d}\n    \\end{subfigure}\n    \\caption{Snapshots from simulations of a 2D ($1024\\times1024$) and 3D ($128  \\! \\times \\! 128 \\! \\times \\!128$) Cahn--Hilliard model \\cite{Cahn1958}: (\\subref{fig:modelab:b:2d}) the 2D system is shown at three intervals at solution index 5,000, 20,000 and 200,000; and (\\subref{fig:modelab:a:cs}) the 3D system is visualized using VTK \\cite{vtk} at solution index 2,500, 5,000 and 20,000 with a cross-section highlighted for visibility, where (\\subref{fig:modelab:a:3d}) shows the cross sections.\n    The simulations use a time step of $\\Delta t = 0.05$, and are initially seeded with random values between -1 and 1.}\n    \\label{fig:modelab:b}\n    %\n\\end{minipage}\n\\end{figure}%\n\\begin{figure}\n    \\centering\n    \\begin{subfigure}{.5\\textwidth}\n    \\centering\n    \n   \n    \\begingroup\n   \n   \n   \n    \\inputencoding{cp1252}%\n    \\makeatletter\n    \\providecommand\\color[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage color not loaded in conjunction with\n    \t\tterminal option `colourtext'%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{Either use 'blacktext' in gnuplot or load the package\n    \t\tcolor.sty in LaTeX.}%\n    \t\\renewcommand\\color[2][]{}%\n    }%\n    \\providecommand\\includegraphics[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage graphicx or graphics not loaded%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}%\n    \t\\renewcommand\\includegraphics[2][]{}%\n    }%\n    \\providecommand\\rotatebox[2]{#2}%\n    \\@ifundefined{ifGPcolor}{%\n    \t\\newif\\ifGPcolor\n    \t\\GPcolortrue\n    }{}%\n    \\@ifundefined{ifGPblacktext}{%\n    \t\\newif\\ifGPblacktext\n    \t\\GPblacktexttrue\n    }{}%\n   \n    \\let\\gplgaddtomacro\\g@addto@macro\n   \n    \\gdef\\gplbacktext{}%\n    \\gdef\\gplfronttext{}%\n    \\makeatother\n    \\ifGPblacktext\n   \n    \\def\\colorrgb#1{}%\n    \\def\\colorgray#1{}%\n    \\else\n   \n    \\ifGPcolor\n    \\def\\colorrgb#1{\\color[rgb]{#1}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color[rgb]{1,0,0}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color[rgb]{0,1,0}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color[rgb]{0,0,1}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color[rgb]{1,0,1}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color[rgb]{0,1,1}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color[rgb]{1,1,0}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color[rgb]{0,0,0}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color[rgb]{1,0.3,0}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color[rgb]{0.5,0.5,0.5}}%\n    \\else\n   \n    \\def\\colorrgb#1{\\color{black}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color{black}}%\n    \\fi\n    \\fi\n    \\setlength{\\unitlength}{0.0500bp}%\n    \\ifx\\gptboxheight\\undefined%\n    \\newlength{\\gptboxheight}%\n    \\newlength{\\gptboxwidth}%\n    \\newsavebox{\\gptboxtext}%\n    \\fi%\n    \\setlength{\\fboxrule}{0.5pt}%\n    \\setlength{\\fboxsep}{1pt}%\n    \\begin{picture}(4320.00,5760.00)%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(682,704){\\makebox(0,0)[r]{\\strut{}-4}}%\n    \t\t\\put(682,1255){\\makebox(0,0)[r]{\\strut{}-3}}%\n    \t\t\\put(682,1806){\\makebox(0,0)[r]{\\strut{}-2}}%\n    \t\t\\put(682,2356){\\makebox(0,0)[r]{\\strut{}-1}}%\n    \t\t\\put(682,2907){\\makebox(0,0)[r]{\\strut{}0}}%\n    \t\t\\put(682,3458){\\makebox(0,0)[r]{\\strut{}1}}%\n    \t\t\\put(682,4009){\\makebox(0,0)[r]{\\strut{}2}}%\n    \t\t\\put(682,4560){\\makebox(0,0)[r]{\\strut{}3}}%\n    \t\t\\put(682,5110){\\makebox(0,0)[r]{\\strut{}4}}%\n    \t\t\\put(1054,484){\\makebox(0,0){\\strut{}$0.01$}}%\n    \t\t\\put(2138,484){\\makebox(0,0){\\strut{}$0.1$}}%\n    \t\t\\put(3221,484){\\makebox(0,0){\\strut{}$1$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(198,3121){\\rotatebox{-270}{\\makebox(0,0){\\strut{}$S(k)$}}}%\n    \t\t\\put(2368,154){\\makebox(0,0){\\strut{}$k$}}%\n    \t\t\\put(2835,5333){\\makebox(0,0){\\strut{}Model A, 2D}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4170){\\makebox(0,0)[r]{\\strut{}500}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4456){\\makebox(0,0)[r]{\\strut{}2,000}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4742){\\makebox(0,0)[r]{\\strut{}20,000}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,5028){\\makebox(0,0)[r]{\\strut{}$S(k) \\sim k^{-3}$}}%\n    \t}%\n    \t\\gplbacktext\n    \t\\put(0,0){\\includegraphics{model-a-2d_sa0_20000.pdf}}%\n    \t\\gplfronttext\n    \\end{picture}%\n    \\endgroup\n    \n    \n\t\\caption{}\n\t\\label{fig:modela:sf2d}\n    \\end{subfigure}%\n    \\begin{subfigure}{.5\\textwidth}\n    \\centering\n    \n   \n    \\begingroup\n   \n   \n   \n    \\inputencoding{cp1252}%\n    \\makeatletter\n    \\providecommand\\color[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage color not loaded in conjunction with\n    \t\tterminal option `colourtext'%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{Either use 'blacktext' in gnuplot or load the package\n    \t\tcolor.sty in LaTeX.}%\n    \t\\renewcommand\\color[2][]{}%\n    }%\n    \\providecommand\\includegraphics[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage graphicx or graphics not loaded%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}%\n    \t\\renewcommand\\includegraphics[2][]{}%\n    }%\n    \\providecommand\\rotatebox[2]{#2}%\n    \\@ifundefined{ifGPcolor}{%\n    \t\\newif\\ifGPcolor\n    \t\\GPcolortrue\n    }{}%\n    \\@ifundefined{ifGPblacktext}{%\n    \t\\newif\\ifGPblacktext\n    \t\\GPblacktexttrue\n    }{}%\n   \n    \\let\\gplgaddtomacro\\g@addto@macro\n   \n    \\gdef\\gplbacktext{}%\n    \\gdef\\gplfronttext{}%\n    \\makeatother\n    \\ifGPblacktext\n   \n    \\def\\colorrgb#1{}%\n    \\def\\colorgray#1{}%\n    \\else\n   \n    \\ifGPcolor\n    \\def\\colorrgb#1{\\color[rgb]{#1}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color[rgb]{1,0,0}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color[rgb]{0,1,0}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color[rgb]{0,0,1}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color[rgb]{1,0,1}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color[rgb]{0,1,1}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color[rgb]{1,1,0}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color[rgb]{0,0,0}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color[rgb]{1,0.3,0}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color[rgb]{0.5,0.5,0.5}}%\n    \\else\n   \n    \\def\\colorrgb#1{\\color{black}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color{black}}%\n    \\fi\n    \\fi\n    \\setlength{\\unitlength}{0.0500bp}%\n    \\ifx\\gptboxheight\\undefined%\n    \\newlength{\\gptboxheight}%\n    \\newlength{\\gptboxwidth}%\n    \\newsavebox{\\gptboxtext}%\n    \\fi%\n    \\setlength{\\fboxrule}{0.5pt}%\n    \\setlength{\\fboxsep}{1pt}%\n    \\begin{picture}(4320.00,5760.00)%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(682,704){\\makebox(0,0)[r]{\\strut{}-4}}%\n    \t\t\\put(682,1587){\\makebox(0,0)[r]{\\strut{}-2}}%\n    \t\t\\put(682,2471){\\makebox(0,0)[r]{\\strut{}0}}%\n    \t\t\\put(682,3354){\\makebox(0,0)[r]{\\strut{}2}}%\n    \t\t\\put(682,4238){\\makebox(0,0)[r]{\\strut{}4}}%\n    \t\t\\put(682,5121){\\makebox(0,0)[r]{\\strut{}6}}%\n    \t\t\\put(1707,484){\\makebox(0,0){\\strut{}$0.1$}}%\n    \t\t\\put(2984,484){\\makebox(0,0){\\strut{}$1$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(198,3121){\\rotatebox{-270}{\\makebox(0,0){\\strut{}$S(k)$}}}%\n    \t\t\\put(2368,154){\\makebox(0,0){\\strut{}$k$}}%\n    \t\t\\put(2835,5333){\\makebox(0,0){\\strut{}Model A, 3D}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4170){\\makebox(0,0)[r]{\\strut{}50}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4456){\\makebox(0,0)[r]{\\strut{}200}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4742){\\makebox(0,0)[r]{\\strut{}2,000}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,5028){\\makebox(0,0)[r]{\\strut{}$S(k) \\sim k^{-4}$}}%\n    \t}%\n    \t\\gplbacktext\n    \t\\put(0,0){\\includegraphics{model-a-3d_sa0_2000.pdf}}%\n    \t\\gplfronttext\n    \\end{picture}%\n    \\endgroup\n    \n    \n\t\\caption{}\n\t\\label{fig:modela:sf3d}\n    \\end{subfigure}\n    \\caption{The radially averaged structure factor, $S(\\vec{k}) \\! \\! = \\! | \\hat{\\psi}_{\\vec{k}} |^2$, for the Allen--Cahn model \\cite{Allen1975} in (\\subref{fig:modela:sf2d}) 2D and (\\subref{fig:modela:sf3d}) 3D, corresponding to the simulation parameters described in Figure~\\ref{fig:modelab:a} and computed using the average of 10 simulations. The results demonstrate scaling to Porod's law~(solid line, Equation~(\\ref{eq:porodslaw})) for $d=2$ and $d=3$, respectively~\\cite{Puri1997}.}\n    \\label{fig:model-a_sf}\n\\end{figure}\n\\begin{figure}\n    \\centering\n    \\begin{subfigure}{.5\\textwidth}\n    \\centering\n   \n    \\begingroup\n   \n   \n   \n    \\inputencoding{cp1252}%\n    \\makeatletter\n    \\providecommand\\color[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage color not loaded in conjunction with\n    \t\tterminal option `colourtext'%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{Either use 'blacktext' in gnuplot or load the package\n    \t\tcolor.sty in LaTeX.}%\n    \t\\renewcommand\\color[2][]{}%\n    }%\n    \\providecommand\\includegraphics[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage graphicx or graphics not loaded%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}%\n    \t\\renewcommand\\includegraphics[2][]{}%\n    }%\n    \\providecommand\\rotatebox[2]{#2}%\n    \\@ifundefined{ifGPcolor}{%\n    \t\\newif\\ifGPcolor\n    \t\\GPcolortrue\n    }{}%\n    \\@ifundefined{ifGPblacktext}{%\n    \t\\newif\\ifGPblacktext\n    \t\\GPblacktexttrue\n    }{}%\n   \n    \\let\\gplgaddtomacro\\g@addto@macro\n   \n    \\gdef\\gplbacktext{}%\n    \\gdef\\gplfronttext{}%\n    \\makeatother\n    \\ifGPblacktext\n   \n    \\def\\colorrgb#1{}%\n    \\def\\colorgray#1{}%\n    \\else\n   \n    \\ifGPcolor\n    \\def\\colorrgb#1{\\color[rgb]{#1}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color[rgb]{1,0,0}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color[rgb]{0,1,0}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color[rgb]{0,0,1}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color[rgb]{1,0,1}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color[rgb]{0,1,1}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color[rgb]{1,1,0}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color[rgb]{0,0,0}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color[rgb]{1,0.3,0}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color[rgb]{0.5,0.5,0.5}}%\n    \\else\n   \n    \\def\\colorrgb#1{\\color{black}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color{black}}%\n    \\fi\n    \\fi\n    \\setlength{\\unitlength}{0.0500bp}%\n    \\ifx\\gptboxheight\\undefined%\n    \\newlength{\\gptboxheight}%\n    \\newlength{\\gptboxwidth}%\n    \\newsavebox{\\gptboxtext}%\n    \\fi%\n    \\setlength{\\fboxrule}{0.5pt}%\n    \\setlength{\\fboxsep}{1pt}%\n    \\begin{picture}(4320.00,5760.00)%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(682,704){\\makebox(0,0)[r]{\\strut{}-4}}%\n    \t\t\\put(682,1671){\\makebox(0,0)[r]{\\strut{}-2}}%\n    \t\t\\put(682,2638){\\makebox(0,0)[r]{\\strut{}0}}%\n    \t\t\\put(682,3605){\\makebox(0,0)[r]{\\strut{}2}}%\n    \t\t\\put(682,4572){\\makebox(0,0)[r]{\\strut{}4}}%\n    \t\t\\put(682,5539){\\makebox(0,0)[r]{\\strut{}6}}%\n    \t\t\\put(1054,484){\\makebox(0,0){\\strut{}$0.01$}}%\n    \t\t\\put(2138,484){\\makebox(0,0){\\strut{}$0.1$}}%\n    \t\t\\put(3221,484){\\makebox(0,0){\\strut{}$1$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(198,3121){\\rotatebox{-270}{\\makebox(0,0){\\strut{}$S(k)$}}}%\n    \t\t\\put(2368,154){\\makebox(0,0){\\strut{}$k$}}%\n    \t\t\\put(2835,5333){\\makebox(0,0){\\strut{}Model B, 2D}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4170){\\makebox(0,0)[r]{\\strut{}5,000}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4456){\\makebox(0,0)[r]{\\strut{}20,000}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4742){\\makebox(0,0)[r]{\\strut{}200,000}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,5028){\\makebox(0,0)[r]{\\strut{}$S(k) \\sim k^{-3}$}}%\n    \t}%\n    \t\\gplbacktext\n    \t\\put(0,0){\\includegraphics{model-b-2d_sa0_200000.pdf}}%\n    \t\\gplfronttext\n    \\end{picture}%\n    \\endgroup\n    \n    \n\t\\caption{}\n\t\\label{fig:modelb:sf2d}\n    \\end{subfigure}%\n    \\begin{subfigure}{.5\\textwidth}\n    \\centering\n    \n   \n    \\begingroup\n   \n   \n   \n    \\inputencoding{cp1252}%\n    \\makeatletter\n    \\providecommand\\color[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage color not loaded in conjunction with\n    \t\tterminal option `colourtext'%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{Either use 'blacktext' in gnuplot or load the package\n    \t\tcolor.sty in LaTeX.}%\n    \t\\renewcommand\\color[2][]{}%\n    }%\n    \\providecommand\\includegraphics[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage graphicx or graphics not loaded%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}%\n    \t\\renewcommand\\includegraphics[2][]{}%\n    }%\n    \\providecommand\\rotatebox[2]{#2}%\n    \\@ifundefined{ifGPcolor}{%\n    \t\\newif\\ifGPcolor\n    \t\\GPcolortrue\n    }{}%\n    \\@ifundefined{ifGPblacktext}{%\n    \t\\newif\\ifGPblacktext\n    \t\\GPblacktexttrue\n    }{}%\n   \n    \\let\\gplgaddtomacro\\g@addto@macro\n   \n    \\gdef\\gplbacktext{}%\n    \\gdef\\gplfronttext{}%\n    \\makeatother\n    \\ifGPblacktext\n   \n    \\def\\colorrgb#1{}%\n    \\def\\colorgray#1{}%\n    \\else\n   \n    \\ifGPcolor\n    \\def\\colorrgb#1{\\color[rgb]{#1}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color[rgb]{1,0,0}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color[rgb]{0,1,0}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color[rgb]{0,0,1}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color[rgb]{1,0,1}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color[rgb]{0,1,1}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color[rgb]{1,1,0}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color[rgb]{0,0,0}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color[rgb]{1,0.3,0}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color[rgb]{0.5,0.5,0.5}}%\n    \\else\n   \n    \\def\\colorrgb#1{\\color{black}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color{black}}%\n    \\fi\n    \\fi\n    \\setlength{\\unitlength}{0.0500bp}%\n    \\ifx\\gptboxheight\\undefined%\n    \\newlength{\\gptboxheight}%\n    \\newlength{\\gptboxwidth}%\n    \\newsavebox{\\gptboxtext}%\n    \\fi%\n    \\setlength{\\fboxrule}{0.5pt}%\n    \\setlength{\\fboxsep}{1pt}%\n    \\begin{picture}(4320.00,5760.00)%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(682,823){\\makebox(0,0)[r]{\\strut{}-3}}%\n    \t\t\\put(682,1360){\\makebox(0,0)[r]{\\strut{}-2}}%\n    \t\t\\put(682,1897){\\makebox(0,0)[r]{\\strut{}-1}}%\n    \t\t\\put(682,2435){\\makebox(0,0)[r]{\\strut{}0}}%\n    \t\t\\put(682,2972){\\makebox(0,0)[r]{\\strut{}1}}%\n    \t\t\\put(682,3509){\\makebox(0,0)[r]{\\strut{}2}}%\n    \t\t\\put(682,4046){\\makebox(0,0)[r]{\\strut{}3}}%\n    \t\t\\put(682,4583){\\makebox(0,0)[r]{\\strut{}4}}%\n    \t\t\\put(682,5120){\\makebox(0,0)[r]{\\strut{}5}}%\n    \t\t\\put(1394,484){\\makebox(0,0){\\strut{}$0.1$}}%\n    \t\t\\put(2851,484){\\makebox(0,0){\\strut{}$1$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(198,3121){\\rotatebox{-270}{\\makebox(0,0){\\strut{}$S(k)$}}}%\n    \t\t\\put(2368,154){\\makebox(0,0){\\strut{}$k$}}%\n    \t\t\\put(2835,5333){\\makebox(0,0){\\strut{}Model B, 3D}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4170){\\makebox(0,0)[r]{\\strut{}500}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4456){\\makebox(0,0)[r]{\\strut{}2,000}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,4742){\\makebox(0,0)[r]{\\strut{}20,000}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3200,5028){\\makebox(0,0)[r]{\\strut{}$S(k) \\sim k^{-4}$}}%\n    \t}%\n    \t\\gplbacktext\n    \t\\put(0,0){\\includegraphics{model-b-3d_sa0_20000.pdf}}%\n    \t\\gplfronttext\n    \\end{picture}%\n    \\endgroup\n    \n    \n\t\\caption{}\n\t\\label{fig:modelb:sf3d}\n    \\end{subfigure}\n    \\caption{The radially averaged structure factor, $S(\\vec{k}) \\! = \\! | \\hat{\\psi}_{\\vec{k}} |^2$, for the Cahn--Hilliard model \\cite{Cahn1958} in (\\subref{fig:modelb:sf2d}) 2D and (\\subref{fig:modelb:sf3d}) 3D, corresponding to the simulation parameters described in Figure~\\ref{fig:modelab:b} and computed using the average of 10 simulations. The results demonstrate scaling to Porod's law~(solid line, Equation~(\\ref{eq:porodslaw})) for $d=2$ and $d=3$, respectively~\\cite{Puri1997}. }\n    \\label{fig:model-b_sf}\n\\end{figure}\n\n\\begin{figure}\n\\begin{minipage}{0.48\\textwidth}\n    \\centering\n    \\begin{subfigure}{\\textwidth}\n    \\centering\n   \n    \\begingroup\n   \n   \n   \n    \\inputencoding{cp1252}%\n    \\makeatletter\n    \\providecommand\\color[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage color not loaded in conjunction with\n    \t\tterminal option `colourtext'%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{Either use 'blacktext' in gnuplot or load the package\n    \t\tcolor.sty in LaTeX.}%\n    \t\\renewcommand\\color[2][]{}%\n    }%\n    \\providecommand\\includegraphics[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage graphicx or graphics not loaded%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}%\n    \t\\renewcommand\\includegraphics[2][]{}%\n    }%\n    \\providecommand\\rotatebox[2]{#2}%\n    \\@ifundefined{ifGPcolor}{%\n    \t\\newif\\ifGPcolor\n    \t\\GPcolortrue\n    }{}%\n    \\@ifundefined{ifGPblacktext}{%\n    \t\\newif\\ifGPblacktext\n    \t\\GPblacktexttrue\n    }{}%\n   \n    \\let\\gplgaddtomacro\\g@addto@macro\n   \n    \\gdef\\gplbacktext{}%\n    \\gdef\\gplfronttext{}%\n    \\makeatother\n    \\ifGPblacktext\n   \n    \\def\\colorrgb#1{}%\n    \\def\\colorgray#1{}%\n    \\else\n   \n    \\ifGPcolor\n    \\def\\colorrgb#1{\\color[rgb]{#1}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color[rgb]{1,0,0}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color[rgb]{0,1,0}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color[rgb]{0,0,1}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color[rgb]{1,0,1}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color[rgb]{0,1,1}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color[rgb]{1,1,0}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color[rgb]{0,0,0}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color[rgb]{1,0.3,0}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color[rgb]{0.5,0.5,0.5}}%\n    \\else\n   \n    \\def\\colorrgb#1{\\color{black}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color{black}}%\n    \\fi\n    \\fi\n    \\setlength{\\unitlength}{0.0500bp}%\n    \\ifx\\gptboxheight\\undefined%\n    \\newlength{\\gptboxheight}%\n    \\newlength{\\gptboxwidth}%\n    \\newsavebox{\\gptboxtext}%\n    \\fi%\n    \\setlength{\\fboxrule}{0.5pt}%\n    \\setlength{\\fboxsep}{1pt}%\n    \\begin{picture}(5182.00,1872.00)%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(1714,1924){\\makebox(0,0)[r]{\\strut{}$25,000$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3341,1924){\\makebox(0,0)[r]{\\strut{}$100,000$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(4963,1924){\\makebox(0,0)[r]{\\strut{}$1,000,000$}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(218,0){\\makebox(0,0){\\strut{}$-1$}}%\n    \t\t\\put(1404,0){\\makebox(0,0){\\strut{}$-0.5$}}%\n    \t\t\\put(2590,0){\\makebox(0,0){\\strut{}$0$}}%\n    \t\t\\put(3776,0){\\makebox(0,0){\\strut{}$0.5$}}%\n    \t\t\\put(4963,0){\\makebox(0,0){\\strut{}$1$}}%\n    \t}%\n    \t\\gplbacktext\n    \t\\put(0,0){\\includegraphics{model-c-2d_data0_1000000.pdf}}%\n    \t\\gplfronttext\n    \\end{picture}%\n    \\endgroup\n    %\n    \\vspace{15pt}\n   \n    \\begingroup\n   \n   \n   \n    \\inputencoding{cp1252}%\n    \\makeatletter\n    \\providecommand\\color[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage color not loaded in conjunction with\n    \t\tterminal option `colourtext'%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{Either use 'blacktext' in gnuplot or load the package\n    \t\tcolor.sty in LaTeX.}%\n    \t\\renewcommand\\color[2][]{}%\n    }%\n    \\providecommand\\includegraphics[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage graphicx or graphics not loaded%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}%\n    \t\\renewcommand\\includegraphics[2][]{}%\n    }%\n    \\providecommand\\rotatebox[2]{#2}%\n    \\@ifundefined{ifGPcolor}{%\n    \t\\newif\\ifGPcolor\n    \t\\GPcolortrue\n    }{}%\n    \\@ifundefined{ifGPblacktext}{%\n    \t\\newif\\ifGPblacktext\n    \t\\GPblacktexttrue\n    }{}%\n   \n    \\let\\gplgaddtomacro\\g@addto@macro\n   \n    \\gdef\\gplbacktext{}%\n    \\gdef\\gplfronttext{}%\n    \\makeatother\n    \\ifGPblacktext\n   \n    \\def\\colorrgb#1{}%\n    \\def\\colorgray#1{}%\n    \\else\n   \n    \\ifGPcolor\n    \\def\\colorrgb#1{\\color[rgb]{#1}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color[rgb]{1,0,0}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color[rgb]{0,1,0}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color[rgb]{0,0,1}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color[rgb]{1,0,1}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color[rgb]{0,1,1}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color[rgb]{1,1,0}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color[rgb]{0,0,0}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color[rgb]{1,0.3,0}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color[rgb]{0.5,0.5,0.5}}%\n    \\else\n   \n    \\def\\colorrgb#1{\\color{black}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color{black}}%\n    \\fi\n    \\fi\n    \\setlength{\\unitlength}{0.0500bp}%\n    \\ifx\\gptboxheight\\undefined%\n    \\newlength{\\gptboxheight}%\n    \\newlength{\\gptboxwidth}%\n    \\newsavebox{\\gptboxtext}%\n    \\fi%\n    \\setlength{\\fboxrule}{0.5pt}%\n    \\setlength{\\fboxsep}{1pt}%\n    \\begin{picture}(5182.00,1872.00)%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(1714,1924){\\makebox(0,0)[r]{\\strut{}$25,000$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(3341,1924){\\makebox(0,0)[r]{\\strut{}$100,000$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(4963,1924){\\makebox(0,0)[r]{\\strut{}$1,000,000$}}%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(218,0){\\makebox(0,0){\\strut{}$-1$}}%\n    \t\t\\put(1404,0){\\makebox(0,0){\\strut{}$-0.5$}}%\n    \t\t\\put(2590,0){\\makebox(0,0){\\strut{}$0$}}%\n    \t\t\\put(3776,0){\\makebox(0,0){\\strut{}$0.5$}}%\n    \t\t\\put(4963,0){\\makebox(0,0){\\strut{}$1$}}%\n    \t}%\n    \t\\gplbacktext\n    \t\\put(0,0){\\includegraphics{model-c-2d_data1_1000000.pdf}}%\n    \t\\gplfronttext\n    \\end{picture}%\n    \\endgroup\n    \n    \\caption{}\n    \\label{fig:modelc:2d}\n    \\end{subfigure}\n    \\begin{subfigure}{\\textwidth}\n    \\centering\n    \\includegraphics[width=0.33\\textwidth]{modelc02500.png}\\hfill%\n    \\includegraphics[width=0.33\\textwidth]{modelc010000.png}\\hfill%\n    \\includegraphics[width=0.33\\textwidth]{modelc0100000.png}\n    \\includegraphics[width=0.33\\textwidth]{modelc12500.png}\\hfill%\n    \\includegraphics[width=0.33\\textwidth]{modelc110000.png}\\hfill%\n    \\includegraphics[width=0.33\\textwidth]{modelc1100000.png}\n    \\caption{}\n    \\label{fig:modelc:3d}\n    \\end{subfigure}\n   \n   \n   \n   \n   \n   \n   \n    \\caption{Snapshots from simulations of 2D ($1024 \\! \\times \\!1024$) and 3D ($128 \\! \\times \\! 128 \\! \\times \\! 128$) Model C \\cite{Elder1994}, Equation~(\\ref{eq:modelc}):\n    (\\subref{fig:modelc:2d}) the 2D system is shown at three intervals at solution index 25,000, 100,000 and 1,000,000.  (\\subref{fig:modelc:3d}) the 3D system  visualized using VTK \\cite{vtk}, with a cross-section highlighted for visibility, at three intervals at solution index 2,500, 10,000 and 100,000.\n    The simulations use a time step of $\\Delta t = 0.025$, and were initially seeded with random values between -1 and 1. \n    The first row illustrates the evolution of the non-conserved field coalescing into droplet formations.\nThe conserved field illustrated on the second row represents the density of the material.\n    }\n    \\label{fig:modelc}\n\\end{minipage}\\hfill%\n\\begin{minipage}{0.48\\textwidth}\n    \\centering\n    \\begin{subfigure}{\\textwidth}\n    \\centering\n   \n    \\begingroup\n   \n   \n   \n    \\inputencoding{cp1252}%\n    \\makeatletter\n    \\providecommand\\color[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage color not loaded in conjunction with\n    \t\tterminal option `colourtext'%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{Either use 'blacktext' in gnuplot or load the package\n    \t\tcolor.sty in LaTeX.}%\n    \t\\renewcommand\\color[2][]{}%\n    }%\n    \\providecommand\\includegraphics[2][]{%\n    \t\\GenericError{(gnuplot) \\space\\space\\space\\@spaces}{%\n    \t\tPackage graphicx or graphics not loaded%\n    \t}{See the gnuplot documentation for explanation.%\n    \t}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}%\n    \t\\renewcommand\\includegraphics[2][]{}%\n    }%\n    \\providecommand\\rotatebox[2]{#2}%\n    \\@ifundefined{ifGPcolor}{%\n    \t\\newif\\ifGPcolor\n    \t\\GPcolortrue\n    }{}%\n    \\@ifundefined{ifGPblacktext}{%\n    \t\\newif\\ifGPblacktext\n    \t\\GPblacktexttrue\n    }{}%\n   \n    \\let\\gplgaddtomacro\\g@addto@macro\n   \n    \\gdef\\gplbacktext{}%\n    \\gdef\\gplfronttext{}%\n    \\makeatother\n    \\ifGPblacktext\n   \n    \\def\\colorrgb#1{}%\n    \\def\\colorgray#1{}%\n    \\else\n   \n    \\ifGPcolor\n    \\def\\colorrgb#1{\\color[rgb]{#1}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color[rgb]{1,0,0}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color[rgb]{0,1,0}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color[rgb]{0,0,1}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color[rgb]{1,0,1}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color[rgb]{0,1,1}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color[rgb]{1,1,0}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color[rgb]{0,0,0}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color[rgb]{1,0.3,0}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color[rgb]{0.5,0.5,0.5}}%\n    \\else\n   \n    \\def\\colorrgb#1{\\color{black}}%\n    \\def\\colorgray#1{\\color[gray]{#1}}%\n    \\expandafter\\def\\csname LTw\\endcsname{\\color{white}}%\n    \\expandafter\\def\\csname LTb\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LTa\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT0\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT1\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT2\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT3\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT4\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT5\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT6\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT7\\endcsname{\\color{black}}%\n    \\expandafter\\def\\csname LT8\\endcsname{\\color{black}}%\n    \\fi\n    \\fi\n    \\setlength{\\unitlength}{0.0500bp}%\n    \\ifx\\gptboxheight\\undefined%\n    \\newlength{\\gptboxheight}%\n    \\newlength{\\gptboxwidth}%\n    \\newsavebox{\\gptboxtext}%\n    \\fi%\n    \\setlength{\\fboxrule}{0.5pt}%\n    \\setlength{\\fboxsep}{1pt}%\n    \\begin{picture}(5182.00,4896.00)%\n    \t\\gplgaddtomacro\\gplbacktext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(671,979){\\makebox(0,0)[r]{\\strut{}$0$}}%\n    \t\t\\put(671,1400){\\makebox(0,0)[r]{\\strut{}$100$}}%\n    \t\t\\put(671,1822){\\makebox(0,0)[r]{\\strut{}$200$}}%\n    \t\t\\put(671,2243){\\makebox(0,0)[r]{\\strut{}$300$}}%\n    \t\t\\put(671,2664){\\makebox(0,0)[r]{\\strut{}$400$}}%\n    \t\t\\put(671,3086){\\makebox(0,0)[r]{\\strut{}$500$}}%\n    \t\t\\put(671,3507){\\makebox(0,0)[r]{\\strut{}$600$}}%\n    \t\t\\put(671,3928){\\makebox(0,0)[r]{\\strut{}$700$}}%\n    \t\t\\put(671,4350){\\makebox(0,0)[r]{\\strut{}$800$}}%\n    \t\t\\put(803,759){\\makebox(0,0){\\strut{}$0$}}%\n    \t\t\\put(1248,759){\\makebox(0,0){\\strut{}$100$}}%\n    \t\t\\put(1694,759){\\makebox(0,0){\\strut{}$200$}}%\n    \t\t\\put(2139,759){\\makebox(0,0){\\strut{}$300$}}%\n    \t\t\\put(2584,759){\\makebox(0,0){\\strut{}$400$}}%\n    \t\t\\put(3029,759){\\makebox(0,0){\\strut{}$500$}}%\n    \t\t\\put(3475,759){\\makebox(0,0){\\strut{}$600$}}%\n    \t\t\\put(3920,759){\\makebox(0,0){\\strut{}$700$}}%\n    \t\t\\put(4365,759){\\makebox(0,0){\\strut{}$800$}}%\n    \t}%\n    \t\\gplgaddtomacro\\gplfronttext{%\n    \t\t\\csname LTb\\endcsnam\n    \t\t\\put(803,181){\\makebox(0,0){\\strut{}$-1$}}%\n    \t\t\\put(1696,181){\\makebox(0,0){\\strut{}$-0.5$}}%\n    \t\t\\put(2590,181){\\makebox(0,0){\\strut{}$0$}}%\n    \t\t\\put(3483,181){\\makebox(0,0){\\strut{}$0.5$}}%\n    \t\t\\put(4377,181){\\makebox(0,0){\\strut{}$1$}}%\n    \t}%\n    \t\\gplbacktext\n    \t\\put(0,0){\\includegraphics{pfc_finished.pdf}}%\n    \t\\gplfronttext\n    \\end{picture}%\n    \\endgroup\n    \n    \\caption{}\n    \\label{fig:modelpfc:2d}\n    \\end{subfigure}\n    \\begin{subfigure}{\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{pfc3d_side.png}\n    \\caption{}\n    \\label{fig:modelpfc:3d}\n    \\end{subfigure}\n    \\caption{Snapshots from simulations of the phase-field crystal model. Parameters were taken from from Elder et al.~\\cite{Elder2002}, $h= \\pi/4$ and $\\Delta t = 0.01$, and the constants in the dynamical equation (Equation~(\\ref{eq:pfc})) were chosen to be $q_0=1$ and $\\epsilon=0.1$. (\\subref{fig:modelpfc:2d}) Shows the 2D ($1024\\times1024$) simulation at 700,000 iterations, representing approximately 700 diffusion time-lengths. (\\subref{fig:modelpfc:3d}) shows the visualization of the 3D system after 70,000 iterations, where a view of the interior is provided by removing a portion of the system along a sloped plane.\n    }\n    \\label{fig:pfc}\n\\end{minipage}\n\\end{figure}\n\n\\subsection{Performance}\n\nPerformance was measured on three different hardware and operating system platforms using Models A and C with\nthe solution taken after 10,000 iterations. The performance was measured by the runtime length of the entire program execution,\nin seconds. Since this includes all program activity between program initialization and termination, the data includes both time spent generating the spectral form and printing the final results to a file. All recorded measurements are listed in Table~\\ref{table:performance}, and the details of the hardware platforms are listed in Table~\\ref{table:hardware}.\n\nData was collected by repeating the simulations 10 times.\nFrom the system size, it is expected that the second test case is approximately 16 times longer than the first, and the third test case is similar to the second test case, for each model. Additionally, it is expected that Model C results are approximately twice as long as Model A (although the derivatives are of a higher order, and there is a coupling term). The data shows that typically the second test is much longer than the first test, which is the result of the ability for the processor to perform caching on the smaller system. Otherwise, these results correspond well to expectations.\n\n\n\\begin{table}\n    \\caption{Runtime data was recorded for three different hardware and operating system environments, for two models and three different grid sizes. The entries of the table show the results of taking the minimum and maximum runtime values (in seconds) for each individual configuration across 10 individual simulations, in the format ``minimum value/maximum value''. The hardware and operating system specifications of the environments used are given in Table~\\ref{table:hardware}.}\n    \\footnotesize\n    \\centering\n    \\begin{tabular}{|c|l|l|l|l|l|l|}\n        \\hline\n        \\multirow{2}{*}{Label} & \\multicolumn{3}{c}{Model A} & \\multicolumn{3}{|c|}{Model C} \\\\\n        & {$128 \\times 128$} & {$512 \\times 512$} & {$64 \\times 64 \\times 64$} & {$128 \\times 128$} & {$512 \\times 512$} & {$64 \\times 64 \\times 64$} \\\\ \\hline\n        Win7 & 2.0/2.4 & 33.9/36.4 & 37.1/40.1 & 5.0/5.7 & 118.5/125.1 & 113.9/122.6 \\\\ \n        Win10 & 1.7/1.8 & 32.6/33.4 & 31.1/31.6 & 4.1/4.2 & 102.8/106.3 & 96.7/97.3 \\\\\n        Arch & 2.0/2.0 & 49.8/50.1 & 45.8/46.2 & 5.5/5.7 & 175.0/177.8 & 158.7/160.3 \\\\\n        \\hline\n    \\end{tabular}\n    \n    \\label{table:performance}\n\\end{table}\n\n\\begin{table}\n    \\caption{The hardware and operating system specifications of the environments used to generate runtime data.}\n    \\footnotesize\n    \\centering\n    \\begin{tabular}{|c|l|l|}\n        \\hline\n        Label & {Clock speed} & {OS (Compiler)} \\\\ \\hline\n        Win7 & i7-6800K (3.40 GHz) & Windows 7 (msvc 14.28) \\\\ \n        Win10 & i7-7700HQ (2.80 GHz) & Windows 10 (msvc 14.27) \\\\\n        Arch & i5-6500 (3.20 GHz) & Arch Linux (gcc 10.2) \\\\\n        \\hline\n    \\end{tabular}\n    \\label{table:hardware}\n\\end{table}\n\n\n\n\\section{Requirements and Limitations}\n\\subsection{Hardware and Software Environment}\n\nTable~\\ref{table:compilers} lists compilers, operating systems and target architectures that were used in testing \\textit{SymPhas}{}.\nThe minimum C++{} standard is C++{}17.\nThe minimum gcc version required to build \\textit{SymPhas}{} is gcc7.5. This version notably introduces constructor template deduction guides, a necessary part of the compile time expression algebra.\nThe latest Microsoft Visual C++{} Compiler (abbreviated as MSVC++{} or MSVC) version is highly recommended. As a result of the heavy usage of meta-programming, earlier versions of MSVC++{} are not guaranteed to successfully build \\textit{SymPhas}{}.\n\n\nWhen compiling \\textit{SymPhas}{}, four of the modules are required for the minimum build and there is only one required external dependency, FFTW. These are listed in Table~\\ref{table:dependencies} alongside the minimum tested versions.\n\n\\begin{table}\n    \\caption{The environments used for testing and compiling \\textit{SymPhas}{}. The target architecture is specified in the third column. The minimum gcc version required to build \\textit{SymPhas}{} is gcc7.5. The latest MSVC++{} version is highly recommended. }\n    \\centering\n\\begin{tabular}{|l|l|l|l|}\n     \\hline\n     Compiler & Operating System & Target Arch. & Compiles?  \\\\\n     \\hline\n     MSVC++{} 14.28 & Windows 7 Professional (64-bit) & x64 & Yes \\\\\n     MSVC++{} 14.28 & Windows 10 Home (64-bit) & x64 & Yes \\\\\n     clang 11.0.1 & Arch Linux (64-bit) & x86-64 & Yes \\\\\n     g++ 10.2 & Arch Linux (64-bit) & x86-64 & Yes \\\\\n     g++ 7.5 & Arch Linux (64-bit) & x86-64 & Yes \\\\\n     g++ 5.5 & Arch Linux (64-bit) & x86-64 & No \\\\\n     \\hline\n\\end{tabular}\n    \\label{table:compilers}\n\\end{table}\n\n\\begin{table}\n    \\caption{List of the dependencies of each module. Modules that are required in the base build of \\textit{SymPhas}{} are emphasized using bold print. Optional modules are always an optional dependency. The version of the external dependency with which \\textit{SymPhas}{} has been tested is indicated in parentheses. The dependency tbb enables parallelism when using the execution header.}\n    \\centering\n    \\begin{threeparttable}\n        \\begin{tabular}{|l|l|l|}\n            \\hline\n            Module & Internal Dependency & External Dependency  \\\\ \n            \\hline\n            \\textbf{\\modulename{lib}} & None & FFTW (3.3.7) \\cite{Frigo_2005}, tbb* \\\\\n            \\textbf{\\modulename{datatypes}} & \\modulename{lib} & None  \\\\ \n            \\textbf{\\modulename{sym}} & \\modulename{datatypes} & None \\\\\n            \\textbf{\\modulename{sol}} & \\modulename{sym}, \\modulename{io} & VTK \\cite{vtk} (9.0)**  \\\\\n            \\modulename{io} & \\modulename{datatypes} & libxdrfile (2.1.2)** \\\\\n            \\modulename{conf} & \\modulename{sol}, \\modulename{io} & None \\\\\n            \\hline\n        \\end{tabular}\n        \\begin{tablenotes}\n            \\item\n            \\small\n            *Library is only required for compiling in Linux.\\\\\n            **Library is optional.\n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\label{table:dependencies}\n\\end{table}\n\n\\subsection{Limitations}\n\nWhen the equations of motion and equations for virtual variables are written, a corresponding expression tree will be constructed by the symbolic algebra functionality. However, virtual variables used in the equations of motion will be substituted directly as data variables rather than as expression trees, meaning that the expression tree associated with the virtual variable will not be used to construct the expression tree for the equation of motion.\nOne implication of this design is that computing the derivative of a virtual variable that is defined in terms of a derivative will apply a stencil twice, resulting in a poor approximation. \nThis also means that when using the spectral solver with equations of motion involving virtual variables, the spectral operators may be malformed if a virtual variable is written using a term necessary to correctly construct the operator. Refer to Section~\\ref{sec:spectral} which explains the procedure of constructing the operators.\n\nThe Euler solver will assume that it can approximate derivatives of any order, but is only able to compute derivatives for which the stencils are implemented. See Section~\\ref{methods:objects:stencils} for the exhaustive list of stencils.\n\nOnly scalar values can be provided to the model arguments for initializing the values of \\lstinline{c1}, \\lstinline{c2}, $\\ldots$, and cannot be other types like matrices or complex types. When the model equations should use other types, then an appropriate number of scalar arguments should be provided in order to construct the term in the model preamble.\n\n\n\\section{Conclusions}\n\nWith \\textit{SymPhas}{}, we have developed a high-performance, highly flexible API that allows a user to simulate phase-field problems in a straightforward way. This applies to any phase-field problem which may be formulated field-theoretically, including reaction-diffusion systems. Simulated models are written using the equations of motion in an unconstrained form specified through simple grammar\nthat can interpret mathematical constructs. Here, \\textit{SymPhas}{} was tested in both 2- and 3-dimensions against the well-known Cahn--Hilliard~\\cite{Cahn1958} and Allen--Cahn~\\cite{Allen_1972} models, a model of two coupled equations of motion for eutectic systems~\\cite{Elder1994}, and a phase-field crystal model~\\cite{Elder2002}. The results demonstrate that \\textit{SymPhas}{} produces correct solutions of phase-field problems.\nOverall, \\textit{SymPhas}{} successfully applies a modular design\nand supports the user by providing individual modules for each functional component alongside a highly detailed documentation.\n\n\nWith the growing interest in phase-field methods outside traditional materials and microstructure modeling, subjects such as wave propagation in cardiac activity and properties of biological tissues are other potential application fields  \\cite{Courtemanche_1996, Raina_2015, Gueltekin2016}. \\textit{SymPhas}{}  offers\nvery short definition-to-results workflow \nfacilitating rapid implementation of new models.\nIn addition to being a tool for direct simulations, \\textit{SymPhas}{} can generate large volumes of training data for new machine learning simulations of phase-field models. For instance, this type of approach has been applied in recent works that focused on formulating a free energy description from the evolving microstructure \\cite{Teichert_2019, Zhang_2020} and in developing machine-guided microstructure evolution models for spinodal decomposition \\cite{OcaZapiain2021}.\n\n\\textit{SymPhas}{} will continue to be developed and have features added over time, which will include items such as upgrades to performance, additional symbolic algebra functionality, stochastic options and more solvers. The software is available at \\texttt{https://github.com/SoftSimu/SymPhas}.\n\n\n\\medskip\n\n\\textbf{Supporting Information} \\par\nSupporting Information is available \nfrom the authors.\n\n\\medskip\n\n\\textbf{Conflict of Interest} \\par The authors declare no conflict of interest.\n\n\n\\medskip\n\\textbf{Acknowledgments} \\par \nM.K. thanks  the  Natural  Sciences  and  Engineering  Research  Council  of  Canada (NSERC) and Canada Research Chairs Program for financial support. \nS.A.S. thanks NSERC for financial support through the Undergraduate Student Research Award (USRA), the Canada Graduate Scholarships - Master's (CGSM) programs and Mitacs for support through the Mitacs Globalink Research Award. \nSharcNet  and  Compute  Canada  are acknowledged for  computational resources. \n\n\n\\medskip\n\n\n\\printbibliography\n\n\n\\end{document}\n", "meta": {"timestamp": "2021-09-07T02:38:34", "yymm": "2109", "arxiv_id": "2109.02598", "language": "en", "url": "https://arxiv.org/abs/2109.02598"}}
{"text": "\\section{Track 2 System Description}\n\nIn this challenge, we focused on the Track 4 task, however, we also validated two speaker recognition systems  for the fully supervised speaker verification (open, Track2). The first system  was a 34-layer ResNet \\cite{he2016deep} with a Squeeze-and-Excitation block  \\cite{hu2018squeeze} (ResNet34-SE), and the second was ECAPA-TDNN \\cite{desplanques2020ecapa}. Systems based on these two networks or their variations have achieved excellent results on last year VoxSRC 20, so we were interested in how they would perform this year.\n\nFor the ResNet34-SE network, the training data was VoxCeleb 2 dev part, and we used a 2-fold noise augmentation and 4-fold speed perturbation augmentation. The augmentation  was based on the Kaldi recipe. And the network was implemented on ASV-Subtools \\cite{tong2021asv}. We used the SGD optimizer with an initial learning rate set to 0.01 and iteratively trained 6 epochs on a 12 GB-memory GPU.\n\nFor the ECAPA-TDNN system, we used the pre-trained system on SpeechBrain \\cite{ravanelli2021speechbrain} directly. It was trained on the development set parts of VoxCeleb1 and VoxCeleb2, and online data augmentation was used during training. \nAt the back end, features are scored using cosine similarity after subtracting the mean and length normalization.\n\n\\begin{table}[htbp]\n  \\centering\n  \\caption{Evaluation results with different model structures}\n  \\label{tab:sv_tab}\n  \\setlength{\\tabcolsep}{1mm}{\n    \\begin{tabular}{rlrrr}\n    \\hline\n          & \\multicolumn{1}{c}{Feature} & \\multicolumn{1}{c}{System} & \\multicolumn{2}{c}{VoxSRC 2021 test} \\\\\n          &       &       & \\multicolumn{1}{r}{minDCF} & \\multicolumn{1}{r}{EER} \\\\\n    \\hline\n    1     &  Fbank 81 & \\multicolumn{1}{l}{ResNet34-SE } & 0.303 & 5.13 \\\\\n    2     & MFCC 80 & \\multicolumn{1}{l}{ECAPA-TDNN } & 0.304 & 5.41 \\\\\n    \\hline\n    3     & Fusion 1, 2  &       & 0.259 & 4.29 \\\\\n    \\hline\n    \\end{tabular}}\n  \\label{tab:addlabel}%\n\\end{table}%\nThe results of these two systems and their score fusions on the VoxSRC 21 test set are shown in Table~\\ref{tab:sv_tab}. The results show that this year's test set was even more difficult, to the extent that last year's optimal system did not show competitive results on that test set. Instead of further tuning on Track 2 for the next challenge, we focused more on Speaker diarisation Track4 task.\n\n\\section{Track 4 System Description}\n\nOur system was tuned on the development set of VoxConverse and reached a diarisation error rate (DER) of 2.92\\%. Compared with the baseline system \\cite{chung2020spot}, the DER of our system on VoxConverse evaluation set is 5.54\\% which is improved by 69\\%, while jaccard error rate (JER) is 27.11\\%.\n\n\\subsection{System overview}\nThe speaker diarisation system consists of the following modules\n\\begin{itemize}\n\\item Voice activity detection (VAD).\n\\item Speaker embedding extractor.\n\\item Initial clustering modules.\n\\item Resegmentation modules.\n\\item Overlap speech detection (OSD).\n\\end{itemize}\n\n\\subsection{Voice activity detection}\n\nWe design three kinds of voice activity detection, which play a key role in the whole system. We evaluated the following voice activity detection schemes:\n\\begin{enumerate}\n\\item An energy-based system. We tune energy threshold of frames on VoxConverse development set.\n\\item A bidirectional long short-term memory network (Bi-LSTM) \\cite{bredin2020pyannote} based system with two layers which is trained to output frames decisions. The module was pre-trained on the labeled DIHARD III development set and tuned on VoxConverse development set with 10 epochs.\n\\item A wav2vec2.0 \\cite{baevski2020wav2vec} based automatic speech recognition (ASR) system. The system was pre-trained on 53k hours unlabeled data. Phoneme classes corresponding to silence and garbage were considered silence for the purpose of VAD and the rest of the classes were considered speech.\n\\end{enumerate}\nBased on the above systems, we evaluate the performance with pyannote.metrics scoring tool \\cite{bredin2017pyannote}. The main evaluation indexes were accuracy, precision, recall and detection error rate (DetER). Table~\\ref{tab:VAD_tab} presents the comparison of results for each VAD system.\n\n\\begin{table*}[!htb]\n\\centering\n\\caption{Speaker 2nd indicates the second most probable speaker.\u201cBi-LSTM*\u201d means the module was fine tuned on VoxConverse development set with 50 epochs, while the rest was fine tuned with 5 epochs.}\n\\label{tab:VBx_tab}\n\\begin{tabular}{cccccccccc}\n\\hline\nSystem & VAD & Emb. & Clus. & Reseg. & Spk. 2nd & \\multicolumn{2}{c}{dev} & \\multicolumn{2}{c}{eval} \\\\ \\cline{7-10} \n &  &  &  &  &  & DER(\\%) & JER(\\%) & DER(\\%) & JER(\\%) \\\\ \\hline\n1 & \\multicolumn{5}{c}{Baseline \\cite{chung2020spot}} & - & - & 17.99 & 38.72 \\\\\n2 & ASR & ResNet101 & AHC & VBx & VBx & 3.91 & 19.65 & - & - \\\\\n3 & ASR & ResNet101 & AHC & VBx & heuristic & 3.82 & 19.21 & - & - \\\\\n4 & Bi-LSTM & ResNet101 & NME-SC & VBx & VBx & 4.27 & 24.32 & - & - \\\\\n5 & Bi-LSTM & ResNet101 & NME-SC & VBx & heuristic & 4.25 & 24.21 & - & - \\\\\n6 & Bi-LSTM & ResNet101 & AHC & VBx & - & 7.06 & 21.57 & 7.79 & 25.76 \\\\\n7 & Bi-LSTM & ResNet101 & AHC & VBx & VBx & 2.96 & 20.91 & - & - \\\\\n8 & Bi-LSTM & ResNet101 & AHC & VBx & heuristic & 2.92 & 20.84 & 5.54 & 27.11 \\\\\n9 & Bi-LSTM & ResNet152 & AHC & VBx & heuristic & 2.81 & 18.17 & - & - \\\\\n10 & Bi-LSTM* & ResNet152 & AHC & VBx & heuristic & 2.41 & 18.02 & 5.62 & 25.19 \\\\ \\hline\n11 & \\multicolumn{5}{c}{fusion of system 2,5,7,8} & 2.69 & 19.32 & 5.55 & 26.64 \\\\ \\hline\n\\end{tabular}\n\\end{table*}\n\n\\begin{table}[]\n\\caption{The main evaluation indexes were accuracy, precision, recall and detection error rate (DetER).}\n\\label{tab:VAD_tab}\n\\begin{tabular}{ccccc}\n\\hline\nSystem  & DetER(\\%) & Acc.(\\%) & Prec.(\\%) & Rec.(\\%) \\\\ \\hline\nEnergy  & 5.65  & 94.74    & 95.72     & 98.76  \\\\\nASR     & 3.43  & 96.81    & 97.70     & 98.90  \\\\\nBi-LSTM & 2.47  & 97.70    & 98.29     & 99.26  \\\\ \\hline\n\\end{tabular}\n\\end{table}\n\nBi-LSTM based system has the best performance when we removed silence shorter than 0.501s. Compared with systems that can be fine-tuned on the development set, ASR system has no development dataset to fine-tune. Considering this factor, the ASR system has great room for improvement. In subsequent experiments, we use Bi-LSTM based system to generate VAD label.\n\n\\subsection{Speaker embeddings}\n\nThe speaker embedding extractor is used to convert acoustic features into fixed dimensional feature vectors. We found that the Resnet34-SE and ECAPA-TDNN  extractor had inferior results to RensNet101 on the VoxSRC 21 Track4 Dev data, so we use ResNet101 as speaker embedding extractor whose inputs are 64 dimension log Mel filter bank features with 25ms frames and 10ms frames shift. In our system, the audio is cut into 1.44s segments with a 0.24s window shift. The 16 kHz x-vector extractor is trained using data from VoxCeleb1, VoxCeleb2 and CN-CELEB. We also apply ResNet152 as speaker embedding extractor that was trained on VoxCeleb1, VoxCeleb2 data set.\n\n\\subsection{Initial clustering}\n\nIn this stage, we evaluate different clustering algorithms, including agglomerative hierarchical clustering (AHC) and normalized maximum eigengap spectral clustering (NME-SC) \\cite{park2019auto}. After comparison, AHC can achieve better performance when combined with VBx. The 256 dimensional speaker embedding is reduced to 128 dimensions by linear discriminant analysis (LDA). Then, the cosine similarity matrix is used as the input of the AHC model, and the threshold is set as -0.015. \n\n\\subsection{Bayesian HMM for xvector clustering}\n\nThe detailed introduction of VBx model can be found in \\cite{landini2022bayesian}, which was used to improve the performance of initial clustering. The results of the initial clustering are used for VBx model initialization. Variational Bayes HMM at x-vector level BHMM is used to cluster x-vectors. The HMM states represent speakers, the transition between states represent the speaker turns and the state distributions are derived from as PLDA model pre-trained on labeled x-vectors. The hyper-parameters mentioned in VBx model were tuned on Voxconverse development set so that Fa=0.15, Fb=5.5, loopP=0.33. In addition, the output of VBx model contains the second speaker label which can be applied to overlap speech detection.\n\n\\subsection{Overlap speech detection}\n\nVoxconverse development set has two or more speakers speaking simultaneously. Considering the error of speech overlap is taken into account, we apply Bi-LSTM model as OSD. According to the output of OSD, we mask the second speaker labels. Bi-LSTM was trained on DIHARD III development set, and tuned on the development set of VoxConverse. Bi-LSTM takes overlap detection as a dichotomous classification problem that labels speech frames and non-speech frames. We set two thresholds to improve the robustness of the model. When the output probability of the model is higher than onset, the label changes from 0 to 1, while when the output probability of the model is lower than offset, the label changes from 1 to 0. Table~\\ref{tab:OSD_tab} presents the comparison of results for each OSD system that was trained on different datasets. Due to the small amount of audio in the development set, we fine tune it five epochs to prevent over fitting.\n\n\\begin{table}\n\\caption{OSD performance on the development set. Evaluation of pre-trained OSD modelsm, in terms of detection error (DetER\\%), accuracy, precision and recall.}\n\\label{tab:OSD_tab}\n\\setlength{\\tabcolsep}{0.5mm}{\n\\begin{tabular}{llllll}\n\\hline\nPre-train & Tune & DetER(\\%) & Acc.(\\%) & Prec.(\\%) & Reca.(\\%) \\\\ \\hline\nAMI & - & 248.53 & 91.41 & 70.50 & 28.53 \\\\\nDIHARD III & - & 89.50 & 96.88 & 53.93 & 72.10 \\\\\nAMI & VoxC. & 54.02 & 98.12 & 77.16 & 65.33 \\\\\nDIHARD III & VoxC. & 49.96 & 98.26 & 77.76 & 70.07 \\\\ \\hline\n\\end{tabular}}\n\\end{table}\n\nWe also apply a heuristic algorithm \\cite{otterson2007efficient} that considers the two closest speakers in times to infer a second speaker label. Table~\\ref{tab:VBx_tab} presents the comparison of results for VBx and the heuristic algorithm.\n\n\nWe observed that the Bi-LSTM tuned with 50 epochs had obvious over-fitting. When applying heuristic algorithm to get second speaker label, the performance of system had been slightly improved. It seems that our systems has high similarity, so that the performance of our fusion system has not been improved. The fusion algorithm named DOVER-lap was mentioned in \\cite{raj2021dover}.\n\n\n\\section{Conclusions}\n\nThis paper described the XMUSPEECH speaker recognition and diarisation systems for the VoxCeleb Speaker Recognition Challenge 2021. Our diarisation system consists of VAD, embedding extractor, initial clustering module, resegmentation module, and OSD. Finally, giving the credit to the VAD system, our best submission on the challenge obtained on the evaluation set DER 5.54\\% and JER 27.11\\%, while the performance on the development set is DER 2.92\\% and JER 20.84\\%. \n\n\\input{voxsrc21.bbl}\n\n\\bibliographystyle{IEEEtran}\n\n\n\n\\end{document}\n", "meta": {"timestamp": "2021-09-07T02:37:25", "yymm": "2109", "arxiv_id": "2109.02549", "language": "en", "url": "https://arxiv.org/abs/2109.02549"}}
{"text": "\\section{Introduction}\n\\label{sec:Introduction}\nThe lattice regularization of a Quantum Field Theory (QFT) enables\naccess to non-perturbati-ve aspects of the theory\nby numerical simulations. On the lattice physical quantities are\ndefined as high-dimensional integrals which can be\ncomputed via importance sampling by interpreting the factor $e^{-S}$ that appears in the integrands as a probability distribution.\nThis technique has proven to be very effective to understand, for instance, the phenomenon of confinement in QCD. \nUnfortunately there are very interesting theories whose action $S$ is\ncomplex, so that $e^{-S}$ is not a well-defined\nprobability distribution. This is the so-called sign problem and it affects for instance QCD at finite density. For this very\nreason the phase diagram of QCD remains largely unexplored by lattice simulations.\n\nNot so long ago, following earlier work by Witten \\cite{Witten1,Witten2}, it was proposed that one could evade the sign problem \nby regularizing a QFT on Lefschetz thimbles \\cite{Aurora,Kikukawa}. The idea is simple. After complexifying the degrees of freedom \none can define a set of manifolds attached to the critical\n(stationary) points of the theory. These manifolds are called Lefschetz thimbles.\nThe Lefschetz thimbles are a basis for the original integration circle in the homological sense, i.e. the original integrals can be\ndecomposed to a linear combination of integrals over the thimbles.\nIn other terms the thimble decomposition defines a convenient\ndeformation of the original domain of integration (as a result \nthe value of the integral does not change). It is a convenient deformation because on the thimbles the imaginary part of the action stays constant. Thus every\ncontribution is free of the sign problem, apart from a residual phase coming from the orientation of the thimble in the embedding manifold.\nThis gives rise to a residual sign problem, which is in practice \nfound to be a (quite) milder one. Also, the coefficients\nappearing in the decomposition (the so-called intersection numbers) are integers and they can be zero,\nmeaning that not all the thimbles do necessarily contribute.\n\nSemiclassical arguments would suggest that the thimble attached to the\ncritical point having the minimum real part of the action (aka the\nfundamental thimble) gives the\ndominant contribution. This contribution is expected to be further enhanced in the continuum limit. Since the theory formulated\non a single thimble has the same symmetries and the same perturbative expansion as the original theory, it was conjectured\nthat the contribution coming from the fundamental thimble could be considered a good approximation for the complete theory.\nThis conjecture is known as the single-thimble dominance hypothesis. \nSoon it was discovered that this hypothesis holds true in some cases,\nbut does not hold true in general. Various counter-examples have been found, such as \nthe ($0$+$1$)-dimensional QCD \\cite{QCD01} and heavy-dense QCD \\cite{HDqcd}, where the single-thimble approximation fails.\nTo our knowledge, the first counter-example that has been discovered is the one-dimensional Thirring model.\nIn refs. \\cite{ThirringKiku,StudyThirringKiku,Alexandru:2015xva,Alexandru:2015sua} \nit was shown that taking into account the fundamental thimble alone is not enough to reproduce\nthe correct results, not even in the continuum limit. Indeed how to address the failure of the single-thimble approximation for this theory\nhas been an open question that we want to settle in this paper. There\nis little doubt that a convenient thimble regularisation of the theory\nwill work, but this is expected to take into account contributions\nfrom many thimbles and of course ``How many?'' is a key issue. We will\nshow that we can indeed compute the solution of the theory by thimble\nregularisation (and the continuum limit can be taken). Quite interestingly, \nthis will be most effectively achieved in a way that\nto a large extent evades the subtleties of collecting contributions\nfrom many thimbles.\n\nThe problem of collecting the contributions from more than one thimble is non-trivial. The contributions can be individually\nestimated by importance sampling, but they must be properly weighted in the thimble decomposition. A universal and satisfactory\nway to obtain the weight is to date still missing. This was a major motivation for\nthe exploration of alternative formulations somehow inspired by\nthimbles, {\\em e.g.} the holomorphic\nflow \\cite{Alexandru:2015sua,Fukuma:2017fjq} or various definitions of \nsign-optimised manifolds, possibly selected by deep-learning techniques \n\\cite{PauloAndrei2,PauloAndrei3,Mori,ContourDefSUN}. Still, some proposals for\nmulti-thimbles simulations have been made in the literature \\cite{QCD01,HDqcd,reweightHeidelberg,Bielefeld}.\nIn particular, in ref. \\cite{HDqcd} we proposed a two-steps process in which one first computes the weights in the semiclassical approximation and then \ncomputes the relevant corrections. This worked fairly well for the simple version of heavy-dense QCD we had considered. \nNonetheless it turned out that this method doesn't work that well for the Thirring model.\nTherefore we turned back to the method we had thought of in ref. \\cite{QCD01}, where the weights\nare obtained by using a few known observables as normalization points. We also applied the method we have proposed\nin ref. \\cite{DiRenzo:2020cgp} in which observables are reconstructed by merging via Pad\\'e approximants different Taylor\nseries carried out around points where the single-thimble approximation is a good one. This last method proved\nto be more effective and it allowed to repeat the simulations towards the continuum limit.\n\nThe paper is organized as follows. In section \\ref{sec:ThimblesMC} we discuss the strategy we've put in place to perform\none-thimble simulations, in particular summarizing the algorithm we have adopted for the Monte Carlo integration.\nIn section \\ref{sec:ThirringSingleThimble} we report the numerical results we have obtained\nfor the Thirring model from one-thimble simulations. Not surprisingly we observe the failure of the single-thimble approximations\nas it was already reported in refs. \\cite{ThirringKiku,StudyThirringKiku,Alexandru:2015xva,Alexandru:2015sua}.\nIn section \\ref{sec:ThirringMultiThimble} we show\nhow we can address the issue by collecting the contributions from the sub-dominant thimbles. The relative weight between the dominant\nand the sub-dominant thimbles is obtained by the method proposed in ref. \\cite{QCD01}. Finally in section ref. \\ref{sec:ThirringTaylor}\nwe apply the strategy outlined in ref. \\cite{DiRenzo:2020cgp}. This\nstrategy allowed to carry out the study of the continuum limit.\n\n\\section{Monte Carlo integration on thimbles} \n\\label{sec:ThimblesMC}\nThe thimble decomposition for a given observable $\\langle O \\rangle$ can be stated as\n\\begin{subequations}\n\\label{eq:allVEV}\n\\begin{align}\n\\left\\langle O \\right\\rangle \\, & = \\, Z^{-1} \\, \\int dx \\; e^{-S(x)} \\, O(x) \\label{eq:basicComputation} \\\\\n       & = \\, \\frac{\\sum_{\\sigma} \\; n_{\\sigma} \\,\n  e^{-i\\,S_I(p_{\\sigma})} \\, \\int_{\\mathcal{J}_\\sigma} dz \\;\n  e^{-S_R(z)}\\; O(z)\\; e^{i\\,\\omega(z)}}{\\sum_{\\sigma} \\; n_{\\sigma} \\, e^{-i\\,S_I(p_{\\sigma})}\\, \\int_{\\mathcal{J}_\\sigma} dz \\;\n  e^{-S_R(z)}\\; e^{i\\,\\omega(z)}} \\label{eq:ThimbleDecomposition} \\\\\n       & = \\, \\frac{\\sum_{\\sigma} \\; n_{\\sigma} \\,\n  e^{-i\\,S_I(p_{\\sigma})} \\, Z_{\\sigma} \\left\\langle O\\,e^{i\\,\\omega} \\right\\rangle_{\\sigma}\n         }{\\sum_{\\sigma} \\; n_{\\sigma} \\, e^{-i\\,S_I(p_{\\sigma})}\\, \\,\n         Z_{\\sigma} \\left\\langle e^{i\\,\\omega} \\right\\rangle_{\\sigma}} \\label{eq:ThimbleDecompositionBis}\n\\end{align}\n\\end{subequations}\nEq. \\ref{eq:basicComputation} defines the physical quantity we want to calculate, where\n$x$ is a short-hand notation for the fields configurations and $S(x) = S_R(x) + i S_I(x)$\nis the complex-valued action of the system.\n\nIn eq. \\ref{eq:ThimbleDecomposition} the real degrees of freedom have been complexified, i.e. $x \\mapsto z$.\nThe integrals at the numerator and at the denominator have been rewritten as linear combinations of integrals over the \nstable thimbles $\\mathcal{J}_\\sigma$ attached to the critical points\n$p_\\sigma$ (in this context we call critical points \nthe stationary points of the complexified action, i.e. the points such that $\\partial_z S (p_\\sigma) = 0$).\nThe stable thimble attached to a given critical point $p_\\sigma$ is the\nunion of the solutions of the steepest-ascent (SA) equations stemming\nfrom the critical point,\n\\begin{equation}\n\\label{eq:saeqs}\n\\frac{d}{dt}z_i = \\frac{\\partial \\bar{S}}{\\partial \\bar{z}_i} \\mbox{ , \\hspace{0.25cm}} z_i (-\\infty) = p_{\\sigma,i} \\mbox{ . }\n\\end{equation}\nAlong each SA path the real part of the action is always increasing. On the other hand\nthe imaginary part of the action stays constant, therefore the expressions\n$e^{-i\\,S_I(p_{\\sigma})}$ have been factorized in front of the integrals.\nThe $n_\\sigma$ are integer coefficients \nknown as intersection numbers. These count the number of intersections between the original integration contour and\nthe unstable thimbles $\\mathcal{K_\\sigma}$, which are defined as the unions of the solutions of the steepest-descent (SD)\nequations leaving the critical point. Along each SD path the real part of the action is always decreasing.\nIn eq. \\ref{eq:ThimbleDecomposition} a so-called residual phase $e^{i\\,\\omega(z)}$ appears in the integrals.\nThis phase comes from the orientation of the thimble in the embedding manifold and it introduces\na residual sign problem, though this is usually found to be a mild one.\n\nIn eq. \\ref{eq:ThimbleDecompositionBis} the thimble decomposition has been reformulated in a way that is amenable to numerical simulations.\nWe have defined an expectation value on the thimble $\\mathcal{J}_\\sigma$,\n\\begin{equation}\\langle f \\rangle_\\sigma = \\frac{\\int_{\\mathcal{J}_\\sigma} dz ~ f ~ e^{-S_R}}{\\int_{\\mathcal{J}_\\sigma} dz ~ e^{-S_R}} =\n\\frac{\\int_{\\mathcal{J}_\\sigma} dz ~ f ~ e^{-S_R} }{Z_\\sigma} \\mbox{ , }\\label{eq:thimblecontribution}\\end{equation}\nthat we plan to evaluate stochastically. Notice that in order to make use of the thimble decomposition, one has to carry out two tasks.\nIn addition to computing the thimble contributions $\\langle f \\rangle_\\sigma$ one also has to calculate their associated weights $Z_\\sigma$.\nNow we summarize the procedure we have put in place to numerically compute $\\langle f \\rangle_\\sigma$. This is all that is needed\nin the single-thimble approximation, where one only takes into account the contribution coming from the fundamental thimble.\nLater on we will also discuss the issue of computing the weights in a\ngeneric thimble decomposition.\n\nBy solving the Takagi's problem for the Hessian of the action $H(S;z)$ \n$$ H(S;p_\\sigma)  \\, v^{(i)} = \\lambda_i \\, \\bar{v}^{(i)} $$\none can find a set of Takagi vectors $v^{(i)}$ and\ntheir associated Takagi values $\\lambda_i$. The Takagi vectors are a basis for the tangent space at the critical point.\nAfter having fixed a normalization $\\mathcal{R}$, the direction\nassociated to a given (infinitesimal) displacement from the critical point (a sort of initial\ncondition for a given SA path) can be expressed as $\\sum n_i v^{(i)}$,\nwith $\\hat{n} \\in \\mathcal{S}^{n-1}_\\mathcal{R}$, where $\\mathcal{S}^{n-1}_\\mathcal{R}$ is the $(n-1)$-dimensional hypersphere of radius $\\mathcal{R}$.\nA point on the thimble can be singled out by the initial direction of the flow $\\hat{n}$ and by the integration time $t$,\n$$z \\in \\mathcal{J_\\sigma} \\leftrightarrow (\\hat{n}, t) \\in \\mathcal{S}^{n-1}_\\mathcal{R} \\times \\mathbb{R} \\mbox{ . }$$\n\nLet's first consider the denominator of eq. \\ref{eq:thimblecontribution}, which we somehow improperly refer to as a partition\nfunction. We can rewrite it in terms of the integration measure $\\mathcal{D}\\hat{n} = \\prod_k dn_k \\delta (|\\hat{n}|^2 - \\mathcal{R}^2)$,\n$$Z_\\sigma = \\int_{\\mathcal{J}_\\sigma} dz ~ e^{-S_R(z)}  = \\int \\mathcal{D}\\hat{n} ~ \\int dt ~ \\Delta_{\\hat{n}}^\\sigma(\\hat{n},t) ~ e^{-S_R(\\hat{n},t)}\n\\equiv \\int \\mathcal{D}\\hat{n} ~ Z_{\\hat{n}}^\\sigma \\mbox{ , }$$\nwhere $\\Delta_{\\hat{n}}^\\sigma(\\hat{n},t)$ is a leftover from the change of variables. An explicit expression\nfor $\\Delta_{\\hat{n}}^\\sigma(\\hat{n},t)$ was worked out in ref. \\cite{thimbleCRM}.  One finds the following expression\nfor the \\textit{partial} partition function, \n$$Z_{\\hat{n}}^\\sigma = 2 (\\sum_i \\lambda_i n_i^2 ) \\int dt ~ e^{-S_{eff}(\\hat{n},t)} \\mbox{ . }$$\nNotice that we have defined the effective action $S_{eff}(\\hat{n},t) = S_R(\\hat{n},t) - log|det~V(\\hat{n},t)|$ from the real\npart of the action $S_R(\\hat{n},t)$ and the matrix $V(\\hat{n},t)$ having as columns the basis vectors of the tangent space at the time $t$.\nIn order to compute the partial partition function one has to parallel transport the basis vectors along the flow starting\nfrom the critical point, where a basis is known (the Takagi vectors). This requires integrating the parallel transport equations\n\\begin{equation}\n\\label{eq:pteqs}\n\\frac{dV_j^{(h)}}{dt} = \\sum_i \\overline{V}_i^{(h)} \\overline{\\partial^2_{z_i z_j} S} \\mbox{ . }\n\\end{equation}\nThe \\textit{partial} partition function can be interpreted as the contribution to the partition function coming from an entire SA\npath.  Similarly for the numerator one finds\n$$ \\int_{\\mathcal{J}_\\sigma} dz ~ f ~ e^{-S_R(z)} = \\int \\mathcal{D}\\hat{n} ~ f_{\\hat{n}}$$\n$$f_{\\hat{n}} = 2 (\\sum_i \\lambda_i n_i^2 ) \\int dt ~ f(\\hat{n},t) ~ e^{-S_{eff}(\\hat{n},t)} \\mbox{ . }$$\nAll in all the expectation value $\\langle f \\rangle_\\sigma$ becomes\n$$ \\langle f \\rangle_\\sigma = \\frac{\\int \\mathcal{D}\\hat{n} ~ f_{\\hat{n}}}{Z_\\sigma} = \\int \\mathcal{D}\\hat{n} ~ \\frac{Z_{\\hat{n}}^\\sigma }{Z_\\sigma} ~ \\frac{f_{\\hat{n}}}{Z_{\\hat{n}}^\\sigma}$$\nThis expectation value can be computed by importance sampling, sampling entire SA paths $\\propto \\frac{Z_{\\hat{n}}^\\sigma }{Z_\\sigma}$\nand estimating the result from the sample mean $\\frac{1}{N}\n\\sum_{\\hat{n}} \\frac{f_{\\hat{n}}}{Z_{\\hat{n}}}$. All in all, in the\nfollowing we have\nto think of a SA (parametrised by a direction $\\hat{n}$) as we think\nof a configuration in a standard Monte Carlo. \nThe algorithm proceeds in two steps.\n\\begin{enumerate}\n\\item First we make a Metropolis proposal starting from the previous\n  {\\em configuration} $\\hat{n}$. The proposal is generated\n      by making $N$ consecutive rotations in planes defined by random directions. That is we pick two random integers $i \\neq j$ and\n      we perform a rotation in the place singled out by the $i$-th and the $j$-th direction, i.e. we (propose an) update $(n_i, n_j) \\mapsto (n_i', n_j')$\n      subject to the normalization condition. Then we pick another random integer $k \\neq j$ and we perform a rotation\n      in the plane singled out by the $j$-th and the $k$-th direction. We iterate until we have made $N$ rotations. Each rotation is obtained by parametrizing\n      $(n_i, n_j)$ as\n      $$\n\\begin{cases}\nn_i = \\sqrt{C} ~ sin(\\phi) \\\\\nn_j = \\sqrt{C} ~ cos(\\phi) \\\\\n\\end{cases}\n$$\nand proposing\n$$\n\\begin{cases}\nn'_i = \\sqrt{C} ~ sin(\\phi + \\phi_0) \\\\\nn'_j = \\sqrt{C} ~ cos(\\phi + \\phi_0) \\mbox { , } \\\\\n\\end{cases}\n$$\nwhere $C = n_i^2 + n_j^2$ and $\\phi_0$ has been uniformly extracted in $[- \\alpha, \\alpha ]$.\n\n\\item Once we have proposed a new {\\em configuration} $\\hat{n}'$, we perform a Metropolis accept/reject test\nby accepting the proposal with probability $P_{acc}(\\hat{n}' \\leftarrow \\hat{n})\n= min \\left(1, \\frac{Z_{\\hat{n}'}^\\sigma}{Z_{\\hat{n}}^\\sigma} \\right) $.\nSince the Metropolis proposal is symmetric, this is enough to satisfy the detailed balance principle and\nthe Markov chain will converge to the targeted probability distribution $\\frac{Z_{\\hat{n}}^\\sigma }{Z_\\sigma}$.\n\n\\end{enumerate}\nThe key ingredients that we need to compute are the partial partition function $Z_{\\hat{n}}$\nand the (contribution from the current SA path to the) observable $f_{\\hat{n}}$.\nThese are time integrals whose computation requires to integrate the differential equations for both the fields\nand the basis vectors given in eq. \\ref{eq:saeqs} and eq. \\ref{eq:pteqs}\nstarting from an initial condition close to the critical point,\n$$\\begin{cases}\nz = z_\\sigma + n_i e^{\\lambda_i t} v^{(i)}\\\\\nV^{(h)} = v^{(h)} e^{\\lambda_h t} \\mbox{ . }\n\\end{cases}\n$$\nActually in our calculations we integrate in action instead of integrating in time. Since the real part of the action is monotonic in time,\none can make the change of variable\n$$\\frac{dS_R}{dt} = \\frac{1}{2}\\frac{d}{dt}(S + \\overline{S}) = |\\nabla S|^2 \\mapsto dt = dS_R ~ |\\nabla S|^{-2} = ds ~ |\\nabla S|^{-2} \\mbox{ . }$$\nIn the last step we have also defined a second change of variable $s = S_R - S_R(z_\\sigma)$. In terms of $s$,\nthe differential equations for the fields and the basis become\n$$\\frac{dz_i}{ds} = |\\nabla S|^{-2} ~ \\frac{\\partial \\overline{S}}{\\partial \\overline{z}_i}$$\n$$\\frac{dV_j^{(h)}}{ds} = |\\nabla S|^{-2} ~ \\sum_i \\overline{V}_i^{(h)} \\overline{\\partial^2_{z_i z_j} S} \\mbox{ . }$$\nThe change of variable is also performed for the time integrals defining $Z_{\\hat{n}}$ and $f_{\\hat{n}}$, yielding\n\\begin{eqnarray}\n\\label{eq:eqinaction}\nZ_{\\hat{n}} = 2 \\sum_i \\lambda_i n_i^2 e^{-S_R(z_\\sigma)} \\int_0^\\infty ds ~ |\\nabla S|^{-2} ~ e^{-s + log|det~V(s)|} \\\\\nf_{\\hat{n}} = 2 \\sum_i \\lambda_i n_i^2 e^{-S_R(z_\\sigma)} \\int_0^\\infty ds ~ f ~ |\\nabla S|^{-2} ~ e^{-s +  log|det~V(s)|} \\nonumber \\mbox{ . }\n\\end{eqnarray}\nThe advantage is twofold. First, the integration in action has the effect of decreasing the number of iterations required to reach convergence.\nSecond, one can immediately recognize from eqs. \\ref{eq:eqinaction} that the integrals can be calculated using\nthe Gauss-Laguerre quadrature, i.e.\n$$\\int_0^\\infty f(x) e^{-x} dx = \\sum w_i f(x_i) \\mbox{ . }$$\nwhere $\\{x_i\\}$ and $\\{w_i\\}$ are the quadrature points and their associated weights. The calculation of the determinant\n(which is computationally quite expensive) is required only at the quadrature points.\n\n\\section{One-thimble simulations}\n\\label{sec:ThirringSingleThimble}\nWe now discuss the thimble regularization of the one-dimensional Thirring model.\nAs we have remarked in the introduction, historically this is one of the very first examples that have shown the\ninadequacy of the single-thimble approximation (see refs \\cite{ThirringKiku,StudyThirringKiku,Alexandru:2015xva,Alexandru:2015sua}).\nThe lattice action for this theory can be written down as\n$$ S = \\beta \\sum_{n=1 \\ldots L} (1 - cos(\\phi_n)) - log~det~D \\mbox{,}$$\nwhere $det~D = \\frac{1}{2^{L-1}} \\left(cosh(L \\hat{\\mu} + i \\sum_n \\phi_n) + cosh(L~asinh(\\hat{m})) \\right)$ is the fermionic\ndeterminant. The parameters $\\hat{\\mu} = \\mu a$ and $\\hat{m} = ma$ are respectively the chemical potential and the fermion mass in lattice\nunits, $\\beta = (2g^2a)^{-1}$ is the inverse coupling constant and $\\phi_n$ is a scalar field discretized on a one-dimensional lattice\nof length $L$. The theory features a sign problem originating from the fermionic determinant, which is complex at finite $\\hat{\\mu}$.\n\nAn analytical solution is known for the partition function. This is given in term of the modified Bessel functions of the first kind $I_n(x)$,\n\n$$Z = \\frac{1}{2^{L-1}} e^{-L\\beta} \\left[I_1(\\beta)^L cosh(L\\hat{\\mu}) + I_0(\\beta)^L cosh(L~asinh(\\hat{m}))\\right] \\mbox{ . }$$\n\nFrom the partition function one can derive closed-form expressions for the physical observables, such the\nscalar condensate $\\langle \\bar{\\chi}\\chi \\rangle$ and the fermion density $\\langle n \\rangle$,\n\n$$\\langle \\bar{\\chi}\\chi \\rangle = \\frac{1}{L} \\frac{\\partial log~Z}{\\partial \\hat{m}}= \\frac{1}{cosh(asinh(\\hat{m}))} \\frac{I_0(\\beta)^L sinh(L~asinh(\n\\hat{m}))}{I_1(\\beta)^L cosh(L \\hat{\\mu}) + I_0(\\beta)^L cosh(L~asinh(\\hat{m}))} \\mbox{ . }$$\n\n$$\\langle n \\rangle = \\frac{1}{L} \\frac{\\partial log~Z}{\\partial \\hat{\\mu}} = \\frac{I_1(\\beta)^L sinh(L \\hat{\\mu})}{I_1(\\beta)^L cosh(L \\hat{\\mu}) + I_\n0(\\beta)^L cosh(L~asinh(\\hat{m}))}$$\n\nA solution by numerical methods, on the other hand, is difficult to obtain because of the sign problem.\nIn this paper we explore the feasibility of using the thimble regularization method to study the Thirring model.\nThe first step consists in complexifying the degrees of freedom. In this case, each real degree of freedom\n$\\phi_n$ is replaced by a complex degree of freedom $z_n$. The action is now given in terms of $z_n$,\n\n$$ \\beta \\sum_{n=1 \\ldots L} (1 - cos(z_n)) - log~detD \\mbox{ , }$$\n\nwhere $detD = \\frac{1}{2^{L-1}} \\left(cosh(L \\hat{\\mu} + i \\sum_n z_n) + cosh(L~asinh(\\hat{m})) \\right)$.\nThe critical points of the theory are found by requiring a vanishing gradient,\n\n\\begin{equation}\n\\frac{\\partial S}{\\partial z_n} = \\beta sin(z_n)  -  i~\\frac{sinh(L \\hat{\\mu} + i \\sum_i z_i)}{cosh(L \\hat{\\mu} + i \\sum_i z_i) + cosh(L ~ asinh(\\hat{m}))} = 0 \\mbox{ . }\n\\label{eq:zerograd}\n\\end{equation}\n\n\nFrom this condition one can see that $sin(z_n)$ takes for all $n=1\n\\ldots L$ always the same value, which we denote by\n$sin(z)$\\footnote{Here and in the following we adhere to the notation of \\cite{ThirringKiku}.}\nand which depends on $z_n$ only through the sum $\\sum_n z_n$. Therefore the critical points are given by field configurations\nwhere $z_i = z$ for any $i$ but a number $n_-$ of lattice points where $z_i$ can take the value $\\pi - z$ (without changing $sin(z_n)$).\nFor a fixed $n_-$ the values admitted for $z$ are found by numerically solving\n\n\\begin{equation}\nsin(z) = \\frac{i}{\\beta} \\frac{sinh(L \\hat{\\mu} + i (L - 2n_-) z + i n_- \\pi)}{cosh(L \\hat{\\mu} + i (L - 2n_-) z + i n_- \\pi) + cosh(L~asinh(\\hat{m}))} \\mbox { . }\n\\label{eq:zerograd2}\n\\end{equation}\n\n\nThis equation follows directly from eq. \\ref{eq:zerograd}. For instance let's consider the critical points \nin the $n_- = 0$ sector, which are expected to give the leading contributions \\cite{StudyThirringKiku}.\nThe solutions corresponding to the parameters $L=4$, $\\beta=1$ and $ma=1$ are graphically shown in fig. \\ref{fig:critp}.\nThe top left figure shows the solutions for $z$ of eq. \\ref{eq:zerograd2}, while\nthe top right figure shows how they move when we add a finite chemical potential.\n\n\\FloatBarrier\n\\begin{figure}[htb]\n        \\centering\n        \\includegraphics[scale=0.4]{files/CPSmu0-crop.pdf}\n        \\includegraphics[scale=0.4]{files/CPS-crop.pdf} \\\\ \\vspace{0.5cm}\n        \\includegraphics[scale=0.4]{files/SR-crop.pdf}\n        \\includegraphics[scale=0.4]{files/SI-crop.pdf}\n        \\caption{\\label{fig:critp} Critical points for $L=4$, $\\beta=1$ and $ma = 1$: solutions for $\\hat{\\mu} = 0$ (top left), solutions for $\\hat{\\mu} \\in [0.0, ~2.0]$ (top right), real part of the action as a function of $\\hat{\\mu}$ (bottom left) and imaginary part of the action as a function of $\\hat{\\mu}$ (bottom right).}\n\\end{figure}\n\\FloatBarrier\n\n\nNotice that only the left complex half-plane is shown. Since eq. \\ref{eq:zerograd2} is invariant under $z \\mapsto - \\bar{z}$,\nif $z$ defines a critical point so does $-\\bar{z}$. Therefore each critical point $\\sigma_i$ has a counterpart $\\sigma_{-i}$\nin the right complex half-plane. However these mirrored critical point do not give rise to independent contributions. Indeed \nthe action displays a symmetry $S(-\\bar{z}) = \\overline{S(z)}$ which ensures that the thimble contributions from the member\nof each pair $\\sigma_{i}$, $\\sigma_{-i}$ are conjugate contributions.\n\nIf we look at the bottom left picture of fig. \\ref{fig:critp} we can see that the fundamental thimble, the one attached to the critical point\nhaving the minimum (real part of the) action, is $\\mathcal{J}_{\\sigma_0}$. Actually from some chemical potential $\\mu_0$ onwards\nthe critical point $\\sigma_{\\bar{1}}$ has an even lower (real part of the) action, but since this is also lower than\nthe minimum real part of the action on the original domain, the thimble attached\nto this critical point cannot appear in the thimble decomposition.\n\n\n\nIn fig. \\ref{fig:res1t} we show the numerical results obtained from numerical simulations performed on the fundamental thimble $\\mathcal{J}_{\\sigma_0}$\nfor different values of $\\beta=1.0$, $1.5$, $2.0$, $4.0$. \\footnote{Actually a small imaginary part has been added to $\\beta$ in order to prevent a Stokes\nphenomenon between $\\mathcal{J}_{\\sigma_0}$ and $\\mathcal{J}_{\\sigma_{\\bar{0}}}$.}\nAt strong couplings (i.e. at low $\\beta$) the single-thimble approximation yields the wrong results, at least for high chemical\npotentials. This does not come as a surprise (it is exactly what other\nauthors found previously) and shows that the contribution from the sub-dominant thimbles cannot be neglected.\n\n\\FloatBarrier\n\\begin{figure}[h!]\n        \\centering\n        \\includegraphics[scale=0.4]{files/fig_thirring_L4C_COUPLING-crop.pdf}\n        \\caption{\\label{fig:res1t} Scalar condensate for $L=4$, $\\beta=1.0$, $1.5$, $2.0$, $4.0$ and $ma = 1$: results from one-thimble simulations on $\\mathcal{J}_{\\sigma_0}$.}\n\\end{figure}\n\\FloatBarrier\n\n\n\n\\section{Multi-thimble simulations}\n\\label{sec:ThirringMultiThimble}\n\nThe reason why the single-thimble approximation fails is that, when the chemical potential is increased starting from zero,\ndifferent Stokes phenomena take place and the thimble decomposition changes.\nAs a result thimbles other than the fundamental one need to be taken into account at high chemical potentials,\neven though at zero chemical potential the contribution from the fundamental thimble $\\mathcal{J}_{\\sigma_0}$ is\nthe only non negligible one.\n\nAll in all the sub-dominant contribution that one has to take into account is the one from the thimble\n$\\mathcal{J}_{\\sigma_1}$, which enters the decomposition at $\\hat{\\mu} \\approx 0.56$. Indeed at this chemical\npotential the imaginary parts of the action at $\\sigma_0$ and $\\sigma_1$ are the same, as shown in the\nbottom right picture of fig. \\ref{fig:critp}. This is a necessary condition for a Stokes phenomenon, that indeed\nhappens. For a very nice and thorough analysis of the Stokes phenomena and their consequences on the thimble decomposition, \nwe refer the reader to ref. \\cite{StudyThirringKiku}. \n\nIn order to properly collect the contributions from the fundamental thimble $\\mathcal{J}_{\\sigma_0}$ and\nthe sub-dominant thimble $\\mathcal{J}_{\\sigma_1}$, we have to determine the relative weights of the two\ncontributions. The approach we took is the one we've followed for ($0$+$1$)-dimensional QCD. \nSince we are considering only two independent thimble contributions, the expectation value of a generic observable $O$\ncan be written as\n$$\\langle O \\rangle = \\frac{n_0 e^{-iS_I(z_0)} Z_0 \\langle O e^{i \\omega_0} \\rangle_0\n+ n_{12} e^{-iS_I(z_{12})} Z_{12} \\langle O e^{i \\omega_{12}} \\rangle_{12}}\n{n_0 e^{-iS_I(z_0)} Z_0 \\langle O e^{i \\omega_0} \\rangle_0\n+ n_{12} e^{-iS_I(z_{12})} Z_{12} \\langle O e^{i \\omega_{12}} \\rangle_{12}} \\mbox{ . }$$\n\nHere we use the subscript $12$ to denote quantities that refer to the critical points $\\sigma_{1}$ and $\\sigma_{-1}$.\nAs we have already observed these critical points give rise to conjugate contributions (whose\nsum is purely real). When $\\mathcal{J}_{\\sigma_1}$ enters the thimble decomposition,\nso does $\\mathcal{J}_{\\sigma_{-1}}$, but we have only one independent contribution.\nNow if we divide both the numerator and the denominator by $n_0 e^{-iS_I(z_0)} Z_0$ we obtain\n$$\\langle O \\rangle = \\frac{\\langle O e^{i \\omega_0} \\rangle_0 + \\alpha \\langle O e^{i \\omega_{12}} \\rangle_{12}}\n{\\langle e^{i \\omega_0} \\rangle_0 + \\alpha \\langle e^{i \\omega_{12}} \\rangle_{12}} \\mbox{ . }$$\nwhere $\\alpha = \\frac{n_{12} e^{-iS_I(z_{12})} Z_{12}}{n_0 e^{-iS_I(z_0)} Z_0}$.\nSince $\\alpha$ only depends on the thimble structure of the theory, we can determine it by taking some observable\n$\\tilde{O}$ as a normalization point and then use such value to calculate any other observable.\n\nFor the Thirring model we fixed $\\alpha$ from the (analytical solution of the) number density and we\nused it to calculate the scalar condensate.\nThe numerical results are shown in fig. \\ref{fig:res2t}. The results are now in agreement with the analytical solution:\nthe contribution from the sub-dominant thimble fully accounts for the discrepancies observed in the results\nfrom one-thimble simulations (and that's why we could make a long\nstory short earlier, when we said that the sub-dominant contribution\nthat one has to take into account is the one from the thimble $\\mathcal{J}_{\\sigma_1}$).\nNotice that the statistical errors are quite large for $\\hat{\\mu} \\approx 0.6 \\div 0.75$, this is in part due to cancellations in the\ncalculation of $\\alpha$ and in part due to numerical difficulties in sampling the non-dominant thimble. \nThe partial partition function (i.e. the probability distribution for importance sampling) shows sharp spikes\nin some regions of $n_0$ (i.e. the initial direction of the SA path on the tangent space along the Takagi vector \nhaving the largest Takagi value). Within these regions the partial partition function varies by several\norder of magnitude and this makes it difficult to keep a good acceptance ratio. Moreover for $\\mathcal{J}_{\\sigma_{1}}$ the regions are also so\nthin that the regions of interest cannot represented in double precision and quadruple precision is needed in\nthe simulations, with a noticeable impact on the performance of the code. An example of this last issue\nis shown in fig. \\ref{fig:logZn}, where the logarithm of the partial partition function of $\\mathcal{J}_{\\sigma_{1}}$\nis shown as a function of $n_0$ for a $L=2$ lattice.\n\nIn the next section we will study the Thirring model using a strategy\nthat will turn out to be much more effective, since {\\em (a)} it does need\nto take a known result as a normalisation point and {\\em (b)} it does\nnot require to sample the sub-dominant thimble.\n\n\\FloatBarrier\n\\begin{figure}[h!]\n        \\centering\n        \\includegraphics[scale=0.4]{files/fig_thirring_L4C-crop.pdf}\n        \\caption{\\label{fig:res2t} Scalar condensate for $L=4$, $\\beta=1.0$, $1.5$, $2.0$ and $ma = 1$: results from multi-thimble simulations on \n$\\mathcal{J}_{\\sigma_0}$ and $\\mathcal{J}_{\\sigma_{\\pm 1}}$.}\n\\end{figure}\n\\FloatBarrier\n\n\\FloatBarrier\n\\begin{figure}[h!]\n        \\centering\n        \\includegraphics[scale=0.4]{files/cp1LogZNL2B1.pdf}\n        \\caption{\\label{fig:logZn} Logarithm of the partial partition function for the critical point $\\sigma_1$. This is \n                                 plotted as a function of $n_0$, i.e. the initial direction\n                                 of the SA path on the tangent space along the Takagi vector having the largest Takagi value. Parameters: $L=2$, $\\beta = 1$ and $ma=1$.}\n\\end{figure}\n\\FloatBarrier\n\n\\section{Taylor expansions on the fundamental thimble}\n\\label{sec:ThirringTaylor}\nIn this section we follow the approach we proposed in\nref. \\cite{DiRenzo:2020cgp} (the interested reader is referred to it\nfor further details on the method itself).\nThe main idea is that we can by-pass the need for multi-thimble simulations by calculating\nmultiple Taylor expansions around points where the single-thimble\napproximation holds true.\nAs a consequence, the coefficients of such expansions can be computed by one-thimble simulations.\nThe method relies on the fact that while the thimble\ndecomposition can be (highly) discontinuous as we sample different\nregions in the parameter space of the theory, physical observables (in\ngeneral) are not. Our strategy is to take advantage of the\nsingle-thimble approximation holding true in given regions and bridge\nthese different (disjoint) regions by Taylor expansions. Actually it\nturns out that the most efficient way to bridge these different regions\nis by Pad\\'e approximants. As a side-effect, having computed Pad\\'e approximants, we are able\nto probe the analytical structure of the observables, i.e. we can\nlocate their singularities. \nThis makes the approach a very powerful tool for theoretical investigations,\nwith applications beyond thimble regularization itself.\nIn a more general framework, one could say that bridging different\nregions by Pad\\'e approximants can be an effective way of studying the\nphase diagram of a theory for which direct computations can be\napproached only in given regions of the parameter space.\nIndeed such a strategy can be {\\em e.g.} applied to lattice QCD at imaginary chemical\npotential \\cite{BielePR0,BielePR}.\n\nFrom a numerical point of view the method of \\cite{DiRenzo:2020cgp}\nproved to be quite effective in our case. It allowed to study the Thirring model \nfor (much) larger/finer lattices than the (modest) $L=4$ lattice we have considered in the previous sections.\n\nLet's start by studying the theory using $L=8$, $\\beta = 1$ and $ma =\n2$ as parameters. We need a few expansion points where\nthe Taylor expansion can be calculated by one-thimble simulations:\n\n\\begin{enumerate}\n\\item As a first point we selected $\\frac{\\mu}{m} = 0.4$. This choice\ncan be understood from a simple argument. The range of $S_I$ on the real \ndomain of integration is limited. As a result, from explicit computation of\n$S_I^{\\sigma}(\\frac{\\mu}{m})$ we can conclude that below a given value\n$\\frac{\\mu_0}{m}$ of the chemical potential there are only two unstable thimbles\nthat can intersect the real domain of integration. These are the one attached\nto the fundamental critical point $\\sigma_0$ and the one attached to\nthe critical point $\\sigma_{\\bar{0}}$. But the contribution\nfrom $\\mathcal{J}_{\\sigma_{\\bar{0}}}$ can be neglected, as $S_R(\\sigma_{\\bar{0}}) \\gg S_R(\\sigma_{0})$.\n\n\\item As a second point we selected $\\frac{\\mu}{m} = 1.4$. For this\nvalue of the chemical potential, all but three the critical points\nother than the fundamental one have $S_R(\\sigma) \\gg S_R(\\sigma_{0})$. \nHence we can neglect them. We denote the three remaining critical points \nby $\\sigma_1$, $\\sigma_{\\bar{1}}$ and $\\sigma_{\\bar{2}}$. Two of them (namely $\\sigma_{\\bar{1}}$ and $\\sigma_{\\bar{2}}$)\nhave a real part of the action $S_R$ less than the minimum $S_R^{min}$ of the real action\non the original domain of integration. Hence their unstable thimble cannot intersect the original domain of integration.\nAs for $\\sigma_1$ one can explicitly check that the attached unstable thimble does not intersect the original\ndomain of integration. See the top picture of fig. \\ref{fig:nr5}. The critical point $\\sigma_0$ is represented by\nthe green point sitting at $Re(z) = 0$. The critical point $\\sigma_1$ is represented by the closest green point to\n$\\sigma_0$ to the left. The unstable thimbles are displayed in magenta.\n\n\\item The very same reasoning applied to $\\frac{\\mu}{m} = 0.4$ and $\\frac{\\mu}{m} = 1.4$ can be applied to\n$\\frac{\\mu}{m} = 0$ and $\\frac{\\mu}{m} = 1.8$ (actually at $\\frac{\\mu}{m} = 0$ there is no sign problem and thimble\nregularization is not even needed).\n\\end{enumerate}\n\nWe have computed by one-thimble simulations the Taylor coefficients for the scalar condensate\nat $\\frac{\\mu}{m} = 0.4$ and $\\frac{\\mu}{m} = 1.4$ respectively up to orders $2$ and $5$.\nThen we have constructed a (multi-points) Pad\\'e approximation using these coefficients as inputs, adding as extra constraints\nthe coefficients of order $0$ at the boundaries of the region we have considered,\ni.e. $\\frac{\\mu}{m} = 0$ and $\\frac{\\mu}{m} = 1.8$. These extra constraints were also calculated by one-thimble simulations.\n\nThe results are shown in the bottom left picture of fig. \\ref{fig:nr5}. The expansion points are displayed as black points.\nThe numerical results from the Pad\\'e approximants are displayed in\nblack (with error bars): they are in good agreement\nwith the analytical solution (the black line).\nThe bottom right picture of fig. \\ref{fig:nr5} shows the expansion points (the black points) on the complex $\\frac{\\mu}{m}$ plane.\nThe picture also shows the pole of the approximant (the blue point) and the true singularity of the observable (the green point).\nThe first matches quite well the latter.\n\n\\FloatBarrier\n\\begin{figure}[h!]\n        \\centering\n        \\includegraphics[width=\\textwidth, height=\\textheight, keepaspectratio]{files/thirringL8_3fig_bis.pdf}\n        \\includegraphics[width=\\textwidth, height=\\textheight, keepaspectratio]{files/thirringL8_3fig.pdf}\n        \\caption{\\label{fig:nr5} The top picture shows the thimble structure for $L=8$, $\\beta = 1.0$ and $ma=2$ at $\\frac{\\mu}{m} = 1.4$.\n                                 The critical points are represented by the green points, while the stable and unstable thimbles are\n                                 displayed in blue and magenta. The critical point $\\sigma_0$ is the one sitting at $Re(z) = 0$ and\n                                 the critical point immediately on the left is $\\sigma_1$. The unstable thimble attached to the former\n                                 intersects the original domain of integration (the real axis), while the unstable thimble attached to\n                                 the latter does not. The bottom left picture shows the results from Pad\\'e as black error bars, which\n                                 are in very good agreement with the expectations from the analytical solution (represented by the black solid line).\n                                 The expansion points are also shown as black points. In the bottom right picture the expansion points\n                                 are displayed on the complex $\\frac{\\mu}{m}$ plane. On top of these are also shown the singularity of the\n                                 condensate (the green point) and the stable pole of the Pad\\'e approximant (the blue point).}\n\\end{figure}\n\\FloatBarrier\n\nFinally the simulations have been repeated towards the continuum limit. For this theory the continuum limit \nis reached by increasing $L = \\frac{1}{Ta}$ and $\\beta = \\frac{1}{2g^2a}$ while keeping fixed\nthe dimensionless products $L \\hat{m}$ and $\\beta \\hat{m}$.\nIn particular we kept constant $L \\hat{m} = 16$ and $\\beta \\hat{m} = 2$.\nThe parameters used in the simulations are summarized in tab. \\ref{tab:clparams}.\n\nFor the finer lattices we proceeded as we did for the $L=8$ lattice. We selected two suitable expansion points\nand we calculated the Taylor coefficients up to order $2$ and $5$ by one-thimble simulations. \nAs extra constraints we also calculated the coefficients of order $0$ at the boundaries of the $\\hat{\\mu}$ range.\nThese data were used as inputs to construct the Pad\\'e approximants. For the $L=32,64$ lattices the statistical error\non the $5$th order Taylor coefficient was quite large and this resulted in a large indetermination on the\nPad\\'e approximant itself. Fortunately in building our (multi-points) \nPad\\'e approximants we have two different handles\nthat we can make use of: one handle is the order of the Taylor\nexpansions, the second one is the number of expansion points. \nFor the  $L=32,64$ lattices we decided to use an additional extra constraint in the low $\\hat{\\mu}$ region in place\nof the $5$th order Taylor coefficient.\n\nThe numerical results are shown in fig. \\ref{fig:continuum}. The\nbottom left and bottom right pictures show how the Pad\\'e approximants\n(the colored error bars) converge to the analytical solution (the red line) as we gradually increase the order of\nthe Taylor coefficients calculated at the two (central) expansion points. Specifically these pictures illustrate\nwhat happens in the two different cases ($L=16$ and $L=64$) where respectively two and three extra constraints were\nused and the highest order that has been calculated in the Taylor expansion was respectively $5$ and $4$. \n\nThe top picture of fig. \\ref{fig:continuum} summarizes the results we have obtained for all the lattices.\nDifferent colors denote different lattices. The analytical solutions are drawn as colored solid lines.\nThe results from the simulations are in good agreement with the analytical solutions\nand the statistical error are well under control up to $L=64$.\n\n\\begin{table}\n\\begin{center}\n\\begin{tabular}{ | c | c | c | }\n  \\hline\n  $L$ & $\\beta$ & $ma$ \\\\ \\hline\n  $8$ & $1.0$ & $2.00$ \\\\\n  $16$ & $2.0$ & $1.00$ \\\\\n  $32$ & $4.0$ & $0.50$ \\\\\n  $64$ & $8.0$ & $0.25$ \\\\ \\hline\n\\end{tabular}\n\\end{center}\n\\caption{Parameters used for the continuum limit analysis ($L \\hat{m}= 16$, $\\beta \\hat{m} = 2$).}\n\\label{tab:clparams}\n\\end{table}\n\n\\FloatBarrier\n\\begin{figure}[h!]\n        \\centering\n        \\includegraphics[width=\\textwidth, height=\\textheight, keepaspectratio]{files/thirring-continuo.pdf}\n        \\includegraphics[width=\\textwidth, height=\\textheight, keepaspectratio]{files/thirring-continuo_bis.pdf}\n        \\caption{\\label{fig:continuum} Results from the continuum limit analysis ($L \\hat{m} = 16$, $\\beta \\hat{m} = 2$). The bottom left and bottom right pictures\n                                 show how the Pad\\'e approximants converge to the correct solution for the $L=16$ and $L=64$ lattices.\n                                 The expansion points are shown as red points. Different colors are used to denote the Pad\\'e approximants\n                                 obtained by using a gradually increasing order of the Taylor coefficients for the two central expansion points.\n                                 Respectively two and three extra constraints on the $0$th order Taylor coefficients were used for the two lattices\n                                 and the highest order Taylor coefficient that was calculated is respectively $5$ and $4$. The top picture\n                                 summarizes the results we obtained in our analysis. Different colors correspond to different lattice sizes.\n                                 The analytical solutions are denoted by the colored solid lines.}\n\\end{figure}\n\\FloatBarrier\n\n\\section{Conclusions}\n\\label{sec:Conclusions}\nThe Thirring model has been extensively studied as a playground to\ntest our ability to tackle finite-density theories plagued by a sign problem.\nThis work aimed at settling an old story: is the thimble approach really\nfailing for the (supposedly simple) Thirring model? \\\\\nThimble regularisation of lattice field theories is a conceptually\nnice solution to the sign problem, but it can be strongly limited by \nthe need for multi-thimbles simulations. The Thirring model has indeed been\na first example of the failure of the simplest application of the\nthimble approach, {\\em i.e.} the one based on simulations on the\ndominant thimble alone. \nWhile there was little doubt of the conceptual solution to this failure\n(other thimbles provide an important contribution), an explicit proof\nof this has been till now missing.\nWe showed that by keeping into account more contributions (on top of\nthe dominant thimble) the (known) analytical solution can indeed be\nreconstructed. While this is conceptually clean, it is from a\nnumerical point of view quite cumbersome. As a result, the\ncomputations we reported on were restricted to a \nsystem of modest size. \\\\\nThere is nevertheless a more powerful method to achieve the result we\nwere interested in, which in the end avoids multi-thimbles\nsimulations. We showed that we could successfully take the continuum\nlimit in the computation of the Thirring model making use of the\nrecently introduced method of computing Taylor expansions on\nthimbles. After calculating multiple Taylor expansions around points \nwhere the single-thimble approximation holds true, we could bridge\nthese different (disjoint) regions by computing (multi-points) \nPad\\'e approximants. The latter also gave us access to the\nsingularity structure of the theory. While this settles the old story\nof the Thirring model in the thimble approach, at the same time\nsuggests that a similar strategy can be successfully  applied to problems beyond\nthimbles. Bridging different\nregions by Pad\\'e approximants can be an effective way of studying the\nphase diagram of a theory for which direct computations can be\napproached only in given regions of the parameter space.\n\n\n\\section*{Acknowledgments}\n\\par\\noindent\nThis work has received funding from the European Union\u2019s Horizon 2020 \nresearch and innovation programme under the Marie Sklodowska-Curie \ngrant agreement No. 813942 (EuroPLEx).\nWe also acknowledge support \nfrom I.N.F.N. under the research project {\\sl i.s. QCDLAT}.\nThe numerical work for this research has made use of the\nresources available to us under the INFN-CINECA agreement. \nIt also benefits from the HPC (High Performance Computing) \nfacility of the University of Parma, Italy. \n\n", "meta": {"timestamp": "2021-09-07T02:35:46", "yymm": "2109", "arxiv_id": "2109.02511", "language": "en", "url": "https://arxiv.org/abs/2109.02511"}}
{"text": "\\section{Experimentation Details}\n\nWhen {\\bf continuing pre-training} BERT-continue/RoBERTa-continue in Table~\\ref{tab:continue}, we follow~\\cite{kocijan19acl} and set learning rate to $1e-5$, batch size to $64$, and train the model for only one epoch.\n\n\nWhen {\\bf fine-tuning} the models in Sec~\\ref{sec:exp:glue} and Sec~\\ref{sec:exp:introduction}, we train the models for $10$ epochs. We use grid search to select their learning rates and batch sizes from $\\{1e-5, 2e-5, 5e-5\\}$ and $\\{8, 16, 32,64\\}$, respectively. \n\n\\section{Statistics of Commonsense Descriptions}\nIn Table~\\ref{tab:stat:wsc} and Table~\\ref{tab:stat:glue}, we report statistics about down-stream tasks and their commonsense descriptions. Our report includes the size of the train/test splits for the downstream tasks, the proportion of samples that matched to at least one commonsense description ({\\it Matched proportion}) in each task, the average number of matched commonsense descriptions per sample ({\\it Average $|cs(x)|$}), and the average length of each matched commonsense description ({\\it Average length of $c$}).\n\nFrom the results, we found that more than half of the samples matched to at least one commonsense description in most of the datasets. This indicates that the OOD commonsense used in this paper is generalizable to different datasets. Also, the average length of the matched commonsense descriptions is short (about $17$), thus encoding them via Transformer is efficient.\n\\section{Conclusion}\n\nIn this paper, we study how to use commonsense to enhance the general text representation. We first analyzed the challenges brought by the domain discrepancy of commonsense. Then, we propose OK-Transformer to allow commonsense integration and enhancement. In the experiments, we verified the effectiveness of our proposed models in a variety of scenarios, including commonsense reasoning, general text classification, and low-resource commonsense. Our models consistently outperform the baselines.\nWe have also empirically analyzed other properties (e.g. interpretability) of the model.\n\n\\section{Experiments}\n\\label{sec:exp}\n\nWe evaluate the effectiveness of our proposed models in three scenarios: cloze-style commonsense reasoning, text classification, and low-resource commonsense settings. All the experiments run over a computer with 4\nNvidia Tesla V100 GPUs.\n\n\\iffalse\n\\begin{table}[]\n\\centering\n\\small\n\\caption{Statistical Result on WSC Datasets}\n\\begin{tabular}{lccccc}\n\\toprule[1pt]\nDataset               & WSC    & WSCR   & PDP    & WinoGender & WinoGrande \\\\ \\hline\nDataset Size          & 273    & 1185   & 60     & 720        & 40938/1267 \\\\\nmatched proportion    & 0.67   & 0.63   & 0.83   & 0.65       & 0.71       \\\\\nAverage number of $c$ & 129.71 & 115.42 & 189.68 & 80.63      & 140.56     \\\\\nAverage length of $c$ & 17.88  & 17.75  & 17.91  & 16.83      & 17.91   \\\\\n\\bottomrule[1pt]\n\\end{tabular}\n\\end{table}\n\n\\begin{table}[]\n\\centering\n\\small\n\\caption{Statistical results on GLUE}\n\\begin{tabular}{lcccccc}\n\\toprule[1pt]\nDataset               & MRPC     & CoLA      & RTE      & QNLI        & STS-B     & SST-2     \\\\ \\hline\nDataset Size          & 3668/408 & 8551/1043 & 2490/277 & 104743/5463 & 5749/1500 & 67349/872 \\\\\nmatched proportion    & 0.59     & 0.40      & 0.72     & 0.52        & 0.56      & 0.25      \\\\\nAverage number of $c$ & 80.71    & 84.85     & 122.60   & 81.35       & 117.00    & 83.07     \\\\\nAverage length of $c$ & 17.47    & 17.60     & 17.71    & 17.59       & 17.34     & 17.59    \\\\\n\\bottomrule[1pt]\n\\end{tabular}\n\\end{table}\n\\fi\n\n{\\bf Models} We consider adapting OK-Transformer to BERT and RoBERTa, which are denoted as OK-BERT and OK-RoBERTa, respectively. We use the BERT-base and RoBERTa-large from the HuggingFace Transformer library~\\cite{wolf2020transformers}. \n\n{\\bf Implementation details for candidate knowledge retrieval} For a given text $x$, we retrieve candidate commonsense from ATOMIC2020. We use the if-then descriptions in ATOMIC2020 (e.g. Fig.~\\ref{fig:commonsense_list}). Since these descriptions cover 173k different verb phrases -- one of the fundamental elements of language -- the retrieval is applicable to a broad range of downstream text understanding tasks.\n\nWe use a simple retrieval method. We simply consider word segments with window size 5 of the input text $x$. All the commonsense descriptions matching one of these text segments will be regarded as the candidate commonsense descriptions $c_i \\in cs(x)$.\n\n\n\n\\subsection{Commonsense Reasoning}\n\n\\subsubsection{Setup}\n\n{\\bf Datasets} We consider the following commonsense reasoning benchmarks: WSC273~\\cite{levesque2012winograd}, PDP~\\cite{morgenstern2016planning}, Winogender~\\cite{rudinger2018gender}, WinoGrande~\\cite{sakaguchi2019winogrande}, CommonsenseQA~\\cite{talmor2019commonsenseqa} and PhysicalQA~\\cite{bisk2020piqa}.\n\n\n{\\bf Model details}\nDue to the different implementations between \\cite{kocijan19acl} and~\\cite{sakaguchi2019winogrande}, in this paper, we also follow their settings to compare with them, respectively. For~\\cite{kocijan19acl}, we conduct disambiguation tasks directly through masked language modeling in OK-BERT. For the latter one, we convert cloze-style problems to multiple-choice classification problems in OK-RoBERTa. In particular, we replace the target pronoun of one query sentence with each candidate reference, then put the new sentences into the language model. We use a single linear layer and a softmax layer over the encoding of its $[CLS]$ token to compute the probability of each new sentence, and select the one with the highest probability as the pronoun disambiguation result.\n\n\n{\\bf Hyperparameters of pre-training} We follow~\\cite{kocijan19acl,sakaguchi2019winogrande} to first pre-train models for 30 and 3 epochs over WSCR~\\cite{kocijan19acl} or WinoGrande~\\cite{sakaguchi2019winogrande}, respectively. Then we fine-tune models over specific tasks. We use AdamW as the optimizer with learning rate 5e-6, which is selected from $\\{2e-5, 1e-5,5e-6\\}$. We set the batch size to 8.\n\n\\begin{table}[!htb]\n\\small\n\\setlength{\\tabcolsep}{2pt}\n\\centering\n\\begin{tabular}{l c c}\n\\toprule\nModel & WSC & PDP \\\\ \\hline\nKEE\\cite{liu2016commonsense} & 52.8 & 58.3 \\\\\nWKH~\\cite{emami2018generalized} & 57.1 & -  \\\\\nMAS~\\cite{klein2019attention}  & 60.3  & 68.3  \\\\\nDSSM~\\cite{wang2019unsupervised} & 63.0 & 75.0  \\\\\nLM\\cite{trinh2018simple} & 63.8 & 70.0   \\\\\nCSS~\\cite{klein2020contrastive} & 69.6 & 90.0   \\\\\nGPT2~\\cite{radford2019language} & 70.7 & -    \\\\\nBERT-large+WSCR~\\cite{kocijan19acl} & 71.4 & 79.2 \\\\\nHNN~\\cite{he2019hybrid} & 75.1 & 90.0  \\\\ \nHuman~\\cite{sakaguchi2019winogrande}  & 96.5 & 92.5\\\\ \\hline\nBERT+WSCR  & 66.3 & 85.0 \\\\\n{\\bf OK-BERT+WSCR }         & {\\bf 67.4} & {\\bf 86.7 }  \\\\ \\hline\nRoB.+WinoGrande  & 90.1 & 87.5 \\\\\n{\\bf OK-RoB.+WinoGrande } & {\\bf 91.6} & {\\bf 91.7} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Results on WSC and PDP. RoB. denotes RoBERTa.}\n\\label{tab:main1}\n\\end{table}\n\n\\begin{table}[!htb]\n\\centering\n\\small\n\\setlength{\\tabcolsep}{2pt}\n\\setlength{\\tabcolsep}{1.3pt}\n\\begin{tabular}{lcc}\n    \\toprule\n    Model & WinoGen. & WinoGran. \\\\ \\hline\n    WikiCREM~\\cite{kocijan2019wikicrem}                & 82.1 & -   \\\\\n    WinoGrande~\\cite{sakaguchi2019winogrande} & 94.6 & 79.3 \\\\ \\hline\n    BERT+WSCR                      & 68.2 & 51.4 \\\\\n    {\\bf OK-BERT+WSCR}                   & {\\bf 72.4} & {\\bf 53.4} \\\\ \\hline\n    RoB.+WinoGrande            & 94.6 & 79.3 \\\\\n    {\\bf OK-RoB.+WinoGrande }        & {\\bf 96.2} & {\\bf 79.6} \\\\\n    \\bottomrule\n\\end{tabular}\n\\caption{Results on WinoGender and WinoGrande.}\n\\label{tab:main2}\n\\end{table}\n\n\\begin{table}[!htb]\n\\centering\n\\setlength{\\tabcolsep}{2.2pt}\n\\small\n\\begin{tabular}{  l | c c}\n\\toprule\nModel & CommonsenseQA & PhysicalQA \\\\ \\hline\nBERT               & 55.86 & 68.71   \\\\\n{\\bf OK-BERT}                   & {\\bf 56.27} & {\\bf 69.09} \\\\ \\hline\nRoBERTa           & 73.55 & 79.76 \\\\\n{\\bf OK-RoBERTa }        & {\\bf 75.92} & {\\bf 80.09} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Results on CommonsenseQA and PhysicalQA.}\n\\label{tab:main3}\n\\end{table}\n\n\\subsubsection{Results}\nWe compare our models with state-of-the-art commonsense reasoning models in Table~\\ref{tab:main1},~\\ref{tab:main2}, and~\\ref{tab:main3}. It can be seen that our models outperform other models in most settings. This verifies the effectiveness of our proposed models for commonsense reasoning.\n\n{\\bf Ablations} In Table~\\ref{tab:main1},~\\ref{tab:main2}, and~\\ref{tab:main3} we also compare OK-BERT with BERT. We found that OK-BERT with OK-Transformers effectively improved the accuracy of BERT with Transformers. Similar results can be found between OK-RoBERTa and RoBERTa. This shows that the proposed OK-Transformer improves pre-trained language models by adapting to them for free, i.e. without retraining on large-scale unsupervised corpora.\n\n\\subsection{General Text Classification}\n\\label{sec:exp:glue}\nWe use MRPC, CoLA, RTE, STS-B, SST-2, and QNLI in the GLUE dataset~\\cite{wang2018glue} to verify the effectiveness of the proposed models on general text classification tasks. We did not evaluate over MNLI, because our model needs to represent the corresponding $n$ commonsense for each sentence, which is too costly for MNLI. We believe that this efficiency problem can be solved by further applying model compression~\\cite{iandola2020squeezebert}, but this is beyond the scope of this paper. It can be seen from Table~\\ref{tab:glue} that OK-BERT and OK-RoBERTa outperform their baselines.\n\n\n\\begin{table*}[!htb]\n\\centering\n{\n    \\begin{tabular}{lcccccc}\n    \\toprule\n    GLUE Task       & MRPC & CoLA & RTE & QNLI & STS-B & SST-2 \\\\ \\hline\n    BERT       & 86.27/90.21 & \\textbf{59.50} & 71.43 & 91.20 & 89.35/88.93 & 91.97 \\\\\n    OK-BERT         & \\textbf{87.25/90.84} & 58.29 & \\textbf{73.65} & \\textbf{91.58} & \\textbf{89.82/89.46} & \\textbf{93.69} \\\\ \\hline\n    RoBERTa   & 90.44/93.15 & 66.57 & 84.11 & 94.00 & 91.83/91.95 & 95.70 \\\\\n    OK-RoBERTa      & \\textbf{91.91/94.24} & \\textbf{66.89} & \\textbf{86.28} & \\textbf{94.41} & \\textbf{92.41/92.20} & \\textbf{96.10} \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\caption{Results on text classification tasks. Models are evaluated by the dev split from GLUE. }\n\\label{tab:glue}\n\\end{table*}\n\n\\subsection{Commonsense Introduction Methods}\n\\label{sec:exp:introduction}\n{\\bf Continue pre-train} In the introduction section, we mentioned that a typical method of introducing textual knowledge is continuing pre-training~\\cite{gururangan2020don,sun2019finetune}. However, due to the domain discrepancy of commonsense, this method will cause catastrophic forgetting. To verify this intuition, in this subsection we compare with the continuing pre-trained model. We first continue pre-training the language model over ATOMIC2020, then fine-tune it over the target task.\n\n\n\n{\\bf ExpBERT}~\\cite{murty2020expbert}\nWe also compare our OK-Transformer with ExpBERT, another model that is able to introduce textual knowledge. In Sec~\\ref{sec:intro}, we mentioned that ExpBERT is not applicable to large-scale commonsense knowledge bases for its disability to select related commonsense and ignore unrelated commonsense. To verify this, we use the retrieved candidate commonsense descriptions from ATOMIC2020 as the additional explanations for ExpBERT. ExpBERT concatenates all the embedding of a fixed number of commonsense, which is inflexible for ATOMIC2020. For this reason, we fix the number of commonsense to 48. If there are more than 48 candidate commonsense descriptions for one sample, we will randomly select 48 of them. Otherwise, we will pad null commonsense to it. In our experiments, we also apply ExpBERT to RoBERTa~\\cite{liu2019roberta} (i.e. ExpRoBERTa).\n\n\\begin{table*}[!htb]\n\\centering\n\\begin{tabular}{lccccccc}\n\\toprule\n                 & MRPC        & CoLA  & RTE   & QNLI  & STS-B       & SST-2 & WSC273 \\\\ \\hline\nBERT        & 86.27/90.21 & \\textbf{59.50} & 71.43 & 91.20 & 89.35/88.93 & 91.97    & 66.30  \\\\\nBERT-continue & 83.58/88.81         & 54.70  & 62.09 & 90.24 & 87.41/87.46 & 91.74 & 63.00  \\\\\nExpBERT         & 85.78/89.79 & 58.29 & 62.82 & 87.06 & 84.78/84.67 & 91.51 & --\\\\ \n{\\bf OK-BERT}    & \\textbf{87.25/90.84} & 58.29 & \\textbf{73.65} & \\textbf{91.58} & \\textbf{89.82/89.46} & \\textbf{93.69}  & \\textbf{67.40} \\\\\n\\hline\nRoBERTa    & 90.44/93.15 & 66.57 & 84.11 & 94.00 & 91.83/91.95 & 95.70  & 90.10  \\\\\nRoBERTa-continue  & 87.01/90.38 & 61.74 & 74.01 & 93.61 & 89.57/89.66 & 95.99 & 87.91 \\\\\nExpRoBERTa      & 89.46/92.22 & \\textbf{66.90}  & 83.39 & 93.78 & 89.81/89.94 & 95.99 & -- \\\\\n{\\bf OK-RoBERTa} & \\textbf{91.91/94.24} & 66.89 & \\textbf{86.28} & \\textbf{94.41} & \\textbf{92.41/92.20} & \\textbf{96.10} & \\textbf{91.58} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Comparison of different commonsense introduction approaches. Continuing pre-training even injures the effectiveness. On the other hand, using OK-Transformers to introduce external knowledge achieves better results than using Transformer.}\n\\label{tab:continue}\n\\end{table*}\n\nWe show the results in Table~\\ref{tab:continue}. We do not report the results of ExpBERT on WSC273, as ExpBERT cannot solve the cloze-style problems. It can be seen that the performance of language models was suffered when we simply continue pre-training the models on the commonsense knowledge base. This verifies that the continuing pre-training on the out-of-domain commonsense will cause catastrophic forgetting and injure the effectiveness. On the other hand, using OK-Transformer to introduce commonsense as the extra input significantly improves the accuracy. The results also suggest that ExpBERT is not applicable to large-scale commonsense knowledge bases.\n\n\n\\subsection{Why is OK-Transformer effective?}\n\nWe now analyze why OK-Transformer can effectively introduce out-of-domain commonsense without pre-training. We are inspired by an observation of language model fine-tuning LMs~\\cite{radiya2020fine}, i.e., the parameters after fine-tuning are close to those before fine-tuning.\nTherefore, we argue that the key to effective introduction is whether the parameters of the meta LM is good initialization for the commonsense-enhanced LM, that the parameters do not change much before and after fine-tuning.\n\nTo verify this, we compare the parameter changes of different knowledge integration methods. These methods include (1) OK-Transformer, (2) KnowBERT~\\cite{peters2019knowledge}, (3) using the original $[CLS]$ token instead of the proposed knowledge token, and (4) abandoning the knowledge token and instead calculating the $cs_{emb}$ of each verb phrase of the target sentence separately, and adding them to these verb phrases' hidden states in $\\rm{\\bf H}_{i-1}$. We follow~\\cite{radiya2020fine} to use the $L1$ as the distance metric. \\cite{radiya2020fine} found that the main change in parameters occurs on the $W_I$ matrix of the Transformer. Our experimental results also follow this phenomenon. Therefore, for greater clarity, we only show the distances of the $W_I$ matrices after fine-tune. We show the distances of different methods in Fig.~\\ref{fig:heatmap}, and their training losses in Fig.~\\ref{fig:loss}.\n\n\n\\begin{figure}[t]\n \\centering\n  \\includegraphics[scale=.35]{figs/heatmap.pdf}\n\\caption{$L_1$ distances in parameter space between pre-trained and fine-tuned meta LMs. We show the metrics of $W_I$ across the 12 Transformer layers.}\n\\label{fig:heatmap}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n\\begin{minipage}[t]{0.225\\textwidth}\n\t\\centering\n  \\strut\\vspace*{-\\baselineskip}\\newline\\includegraphics[scale=.42]{figs/loss.pdf}\n \n\\caption{Losses of different knowledge integration methods in SST-2. The [CLS] token method does not converge.}\n\\label{fig:loss}\n\\end{minipage}\n\\hspace{0.2cm}\n\\begin{minipage}[t]{0.225\\textwidth}\n\t\\centering\n\n\\strut\\vspace*{-\\baselineskip}\\newline\\includegraphics[scale=.4]{figs/few-shot.pdf}\n\t\t\t\t\\vspace{0.2cm}\n\\caption{Effect in low-resource commonsense settings with different $k$s over SST-2. }\n\t\\label{fig:low_resource}\n\\end{minipage}\n\\end{figure}\n\nIt can be seen that the distances of OK-Transformer are much smaller than other methods, except the [CLS] token method, which does not converge as shown in Fig.~\\ref{fig:loss}. \nThis fits our intuition of reducing the parameter variations to introduce external knowledge more effectively.\n\n\n\n\\subsection{Effect in Low-Resource Commonsense Settings}\nSince there is a large number of commonsense descriptions in ATOMIC2020, a large portion of descriptions only occur a few times in the training set. In this subsection, we want to verify for these rare descriptions, can the model still benefit from it? If so, we think it means that the model uses the contextual information of the commonsense to improve the understanding of the commonsense.\n\n\n\t\n\nTo do this, we proposed a low-resource commonsense setting. We evaluate the effect of the model if the training dataset only contains $k=8/16/32/64$ samples. Therefore the frequency of the appeared commonsense descriptions is low. \nIn order to exclude the influence of other samples, we only use test samples whose candidate commonsense descriptions have already occurred in the $k$ training samples. For example, when $k=8$, we randomly select $8$ samples from the training set for training, and use all samples in the test set which contains the commonsense of the $8$ training samples for evaluation. We show the results over the SST-2 dataset in Fig.~\\ref{fig:low_resource}. It can be seen that our models still benefit from low-frequency commonsense.\n\n\n\n\n\n\n\n\\iffalse\n\\subsection{Comparison of Different Network Structures for Commonsense Enhancement}\nHow to design an effective network structure to incorporate commonsense into text representation is the key problem in this paper. In this subsection, in addition to the layer-by-layer connection and knowledge update mechanism of the knowledge enhancement module in Sec~\\ref{sec:knowledge:input}, we compare with other network structures, including: (1) replacing layer-by-layer connection with connecting all $\\mathrm{Transformer^{(1)}}$s in BERT1 to the final $\\mathrm{Transformer^{(2)}}$ layer in BERT2 ({\\it one-to-all}); (2) discard the extra knowledge token $[k]$ and use $[CLS]$ token as the knowledge token instead ({\\it remove $[k]$}).\n\n\\begin{table}[!htb]\n\\setlength{\\tabcolsep}{2.2pt}\n\\small\n\\centering\n\\begin{tabular}{@{}lccccc@{}}\n\\toprule\n& WSC273 & MRPC  & CoLA  & RTE & Average \\\\ \\midrule\nOK-BERT     & 66.70   & {\\bf 87.75} & 59.24 & {\\bf 72.20} & {\\bf 67.86} \\\\\none-to-all & 65.57  & 87.75 & 58.63 & 70.04 & 67.09 \\\\\nremove $[k]$ & {\\bf 67.03}  & 85.04 & {\\bf 60.06} & 71.48 & 67.69 \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Effect of different network structures for commonsense enhancement.}\n\\label{tab:enhance}\n\\end{table}\n\nWe show the comparisons of these structures in Table~\\ref{tab:enhance}. We found using an additional knowledge token $[k]$ may have competitive results in some datasets, while layer-by-layer connections perform better overall.\n\\fi\n\n\n\\subsection{Does OK-Transformer Provide Interpretability?}\n\nIn this subsection, we try to answer if the integration of candidate commonsense descriptions by OK-Transformer is interpretable. To answer this question, we calculate the influence of different commonsense descriptions on the model's predictions. We follow~\\cite{wu2020perturbed} to quantify the influence of a commonsense description $c_i$ as: If $c_i$ is removed from $cs(x)$, how much will the prediction change? This change is measured by the Euclidean distance between the prediction by $cs(x)-c_i$ and by $cs(x)$. The greater the change in the prediction, the greater the influence of this commonsense.\n\n\\begin{table}[!htb]\n\\setlength{\\tabcolsep}{2.2pt}\n\\small\n\\centering\n\\begin{tabular}{@{}llll@{}}\n\\toprule\n\\multicolumn{4}{l}{John \\underline{promised} Bill to leave, so an hour later {[}John{]} left.}               \\\\ \\midrule\n\\multicolumn{4}{l}{PersonX promises PersonY.}   \\\\\n\\multicolumn{4}{l}{1. $\\cdots$ As a result, PersonX wants to fulfill his promise.} \\\\ \n\\multicolumn{4}{l}{2. $\\cdots$ PersonX is seen as truthful}                        \\\\\n\\multicolumn{4}{l}{3. $\\cdots$ PersonX is seen as trustworthy.}                    \\\\\n\\multicolumn{4}{l}{4. $\\cdots$ Before, PersonX needed to talk to PersonY.}         \\\\\n\\multicolumn{4}{l}{5. $\\cdots$ Before, PersonX needed to go to PersonY's house.}   \\\\ \\bottomrule\n\\end{tabular}\n\\caption{A case study of top 5 commonsense descriptions.}\n\\label{tab:interpret}\n\\end{table}\n\nThrough the case studies of the samples in WSC273, we found that although commonsense with higher influence is somewhat interpretable for people, the interpretability is not significant. We show some examples in Table~\\ref{tab:interpret}. We believe that this is because some commonsense for people has been learned in pre-training. Therefore, the out-of-domain commonsense that these pre-trained language models need to incorporate for downstream tasks is inconsistent with human understanding.\n\n\n\\section{Introduction}\n\\label{sec:intro}\nAlthough unsupervised language models have achieved big success on many tasks~\\cite{devlin2018bert}, they are incapable of learning low-frequency knowledge. For example, in the masked language model task in Fig.~\\ref{fig:commonsense_list}, even if we replace ``Kevin was'' (left) with ``Jim was'' (right), BERT~\\cite{devlin2018bert} still predicts the masked word as sick, crying, dying, etc. This is because similar texts in its training corpus rarely describe the subject of ``comforted''.\nTo improve the model's ability to generalize and understand low-frequency knowledge, we propose to incorporate commonsense into language models. In Fig.~\\ref{fig:commonsense_list}, to make correct predictions, we need to enhance the language model with the commonsense $c_1$.\n\n\\begin{figure*}[!htb]\n\t\\centering\n\t\t\\includegraphics[scale=.5]{figs/example.pdf}\n\\caption{The prediction of [MASK] by BERT. BERT cannot distinguish between {\\it Jim} and {\\it Kevin} in {\\it Jim comforted Kevin because}. }\n\\label{fig:commonsense_list}\n\\vspace{-0.3cm}\n\\end{figure*}\n\n\nHowever, commonsense has the nature of {\\it domain discrepancy}. The downstream task and the commonsense knowledge have distribution discrepancies. Taking the commonsense knowledge base we use (i.e. ATOMIC2020~\\cite{hwang2020comet}) as an example, the distribution discrepancy is specifically manifested in (1) their data formats. The format of a commonsense description usually belongs to some specific patterns (e.g. \u201c... As a result ...\u201d, \u201c... Because ...\u201d), while the downstream tasks can have arbitrary patterns. (2) The commonsense belongs to the domain of event causality, while the downstream tasks may belong to arbitrary domains.\n\n\n\n\n\n\nHere we highlight the challenges caused by the domain discrepancy. To introduce external textual knowledge to a pre-trained language model, a common practice is to continue pre-training the language model on the corpus of the external knowledge~\\cite{gururangan2020don,sun2019finetune}. However, the study~\\cite{gururangan2020don} also found that continuing pre-training requires external knowledge and downstream tasks to have similar domains. Due to its domain discrepancy, introducing commonsense through continuing pre-training will cause catastrophic forgetting to downstream tasks, thereby injuring the effectiveness. We have verified this empirically in Sec~\\ref{sec:exp:introduction}. Therefore, the domain discrepancy prevents us from introducing commonsense by continuing pre-training.\n\nTo enhance the representation of the target text with external commonsense, we propose to directly use its candidate commonsense as an extra input. Our setup is different from a typical natural language understanding setup since the latter one only takes the target text as the input~\\cite{devlin2018bert}. We argue that our setup -- where the commonsense is introduced explicitly as input -- is a more practicable setup to introduce out-of-domain commonsense that cannot be learned through pre-training. As far as we know, ExpBERT~\\cite{murty2020expbert} is the closest setup to us. It also uses external knowledge (manually constructed templates) as the input.\n\n\n\nAnother challenge is the {\\bf scale} of the commonsense. Although ExpBERT also allows extra textual commonsense as input, it only captures small-scale commonsense with a fixed size. In addition, when we introduce commonsense from a large-scale knowledge base for general purpose (i.e. ATOMIC2020), unrelated commonsense (e.g. $c_2$ and $c_3$ in Fig.~\\ref{fig:commonsense_list}) will certainly occur. However, ExpBERT lacks the ability to distinguish related and unrelated commonsense. Therefore, the power of large-scale commonsense knowledge was restricted in ExpBERT. We will verify this empirically in Sec~\\ref{sec:exp:introduction}.\n\nIn order to incorporate the large-scale out-of-domain commonsense, we propose the OK-Transformer (\\underline{O}ut-of-domain \\underline{K}nowledge enhanced \\underline{Transformer}) on the basis of Transformer~\\cite{vaswani2017attention}. OK-Transformer has two modules. The knowledge enhancement module is used to encode the target text with commonsense, and the knowledge integration module is used to encode and integrate all candidate commonsense. OK-Transformer has two advantages. First, it fully represents the contextual information of the textual commonsense. Second, it can be adapted to existing pre-trained language models (e.g. BERT and RoBERTa) for free.\nThat is, we are able to adapt OK-Transformer to the pre-trained language models, without pre-training OK-Transformer over large-scale unsupervised corpora from scratch.\n\nSome other methods are related to our work, such as introducing {\\it structured} knowledge~\\cite{peters2019knowledge,zhang2019ernie,guan2020knowledge,zhou2018commonsense} and {\\it plain text} knowledge~\\cite{guu2020realm} in language models. These methods do not represent the specific inductive bias of commonsense knowledge and therefore are not suitable to introduce commonsense. We will compare these studies with more details in Sec~\\ref{sec:related}.\n\n\n\n\\section{OK-Transformer}\n\nIn this section, we propose OK-Transformer based on Transformer to introduce extra commonsense descriptions. We first show OK-Transformer on an abstract level in Sec~\\ref{sec:framework}. Then we elaborate two modules within it, i.e. knowledge enhancement and knowledge integration, in Sec~\\ref{sec:knowledge:input} and Sec~\\ref{sec:method:output}, respectively.\n\n\\subsection{Framework}\n\\label{sec:framework}\nIn this subsection, we show how our OK-Transformer works at an abstract level. For the target sentence $x$, OK-Transformer takes both $x$ and $cs(x)$ as inputs. To incorporate all the information of $x$ and $cs(x)$, the OK-Transformer contains three vanilla Transformers, denoted by $\\mathrm{Transformer^{(1)(2)(3)}}$. The knowledge enhancement module uses $\\mathrm{Transformer^{(1)}}$ to encode the target text. Compared with the vanilla Transformer, $\\mathrm{Transformer^{(1)}}$ leverages a new knowledge token to represent the commonsense that interacts with other words. The knowledge integration module encodes each individual commonsense description by $\\mathrm{Transformer^{(2)}}$, and then integrates all candidate commonsense descriptions by $\\mathrm{Transformer^{(3)}}$. This is shown in Fig.~\\ref{fig:ok_transformer}.\n\n\\begin{figure}[!tb]\n\t\\centering\n\t\t\\includegraphics[scale=.45]{figs/ok_transformer_new.pdf}\n\\caption{OK-Transformer. $\\mathrm{Transformer^{(1)}}$ encodes the target text $x$ with enhanced commonsense $k_i$. $\\mathrm{Transformer^{(2)}}$ encodes each individual commonsense description. $\\mathrm{Transformer^{(3)}}$ integrates all candidate commonsense descriptions and transfers knowledge to $\\mathrm{Transformer^{(1)}}$.}\n\\label{fig:ok_transformer}\n\\end{figure}\n\n\\iffalse\n\\begin{figure}[!htb]\n\\begin{subfigure}[t]{0.45\\textwidth}\n\t\\centering\n\t\t\\includegraphics[scale=.43]{figs/ok_transformer.pdf}\n\\caption{OK-Transformer. $\\mathrm{Transformer^{(1)}}$ represents the target text $x$ with enhanced commonsense $k_i$. $\\mathrm{Transformer^{(2)}}$ represents each individual commonsense description. $\\mathrm{Transformer^{(2)}}$ integrates all candidate commonsense descriptions and transfers knowledge to $\\mathrm{Transformer^{(1)}}$.}\n\\label{fig:ok_transformer}\n\\end{subfigure}\n\\hspace{0.3cm}\n\\begin{subfigure}[t]{0.45\\textwidth}\n\t\\centering\n\t\t\\raisebox{3cm}{\\includegraphics[scale=.43]{figs/ok_bert.pdf}}\n\\caption{OK-BERT: Adapt OK-Transformer to BERT. We only draw edges which connect to the $i$-th layer.}\n\\label{fig:ok_bert}\n\\end{subfigure}\n\\caption{Architectures.}\n\t\\label{fig:effect_adaptive_gamma}\n\\end{figure}\n\\fi\n\n\\subsection{Knowledge Enhancement Module}\n\\label{sec:knowledge:input}\nThe knowledge enhancement module allows commonsense knowledge to enhance the representation of the target text.\n\n{\\bf Interaction between words and commonsense.} We use $\\mathrm{Transformer^{(1)}}$ to represent the interaction between words of the target text $x$. In addition, we introduce a special token $[k]$ to represent the commonsense knowledge. We denote it as the knowledge token. $\\mathrm{Transformer^{(1)}}$ encodes all words and the knowledge token together via multi-head attention. Formally, given word sequence $\\mathrm{x=w_1, \\cdots, w_n}$, $\\mathrm{Transformer^{(1)}}$ accepts a sequence of $n+1$ word-piece tokens: $\\mathrm{[k], \\; w_1, \\cdots w_n}$. We denote the knowledge embedding and word embeddings produced by the $i$-th layer of $\\mathrm{Transformer^{(1)}}$ as $k_i \\in \\mathbb{R}^{d}$ and $\\bf{H}_i \\in \\mathbb{R}^{n \\times d}$, respectively. The $\\mathrm{Transformer^{(1)}}$ block first uses a multi-head self-attention layer followed by a residual connection and a layer normalization to model their interactions:\n\\begin{equation}\n\\small\n\\label{eqn:kh}\n\\begin{aligned}\n    & \\mathrm{k_i',{\\bf H}_i'= LayerNorm( [k_{i-1},{\\bf H}_{i-1}] +} \\\\ \n    & \\mathrm{MultiHeadAttn([k_{i-1},{\\bf H}_{i-1}],[k_{i-1},{\\bf H}_{i-1}],[k_{i-1},{\\bf H}_{i-1}]))} \\\\\n\\end{aligned}\n\\end{equation}\nwhere $\\mathrm{[k_{i-1},{\\bf H}_{i-1}] \\in \\mathbb{R}^{(n+1) \\times d}}$ means appending $k_{i-1}$ at the front of ${\\bf H}_{i-1}$. $\\mathrm{[k_{i-1},{\\bf H}_{i-1}]}$ is used as the query, key, and value in the multi-head attention.\n\n{\\bf Knowledge update} The vanilla Transformer projects $\\mathrm{k_i', \\;{\\bf H}_i'}$ in Eq.~\\eqref{eqn:kh} to the output space with a multi-layer perceptron neural network (MLP). Compared to the vanilla Transformer, we use an extra update operation to update the knowledge token by the integrated commonsense knowledge after the MLP. As in the vanilla Transformer, the update layer is followed by a residual connection and a layer normalization. This can be formulated by:\n\\begin{equation}\n\\small\n\\begin{aligned}\n&\\mathrm{k_{i}=LayerNorm(k_i'+MLP(k_i')+ cs_{emb})} \\\\\n&\\mathrm{{\\bf H}_{i}=LayerNorm({\\bf H}_{i}'+MLP({\\bf H}_i'))}\n\\end{aligned}\n\\end{equation}\nwhere $cs_{emb}$ is the embedding of the commonsense computed by the knowledge integration module in Sec~\\ref{sec:method:output}. \n\n\\subsection{Knowledge Integration Module}\n\\label{sec:method:output}\nThe knowledge integration module encodes all candidate commonsense descriptions and integrates them. We first use $\\mathrm{Transformer^{(2)}}$ to represent each candidate commonsense description. Then, we use $\\mathrm{Transformer^{(3)}}$ to integrate all candidate commonsense, and transfer the integrated knowledge to the knowledge enhancement module.\n\n\n\n{\\bf Representing single commonsense} We use a vanilla Transformer as $\\mathrm{Transformer^{(2)}}$ to model each candidate commonsense description. For all the retrieved commonsense $cs(x)=\\{c_1,\\cdots,c_n\\}$, we compute the embedding $emb_j$ of each commonsense description $c_j$ by:\n\\begin{equation}\n\\small\n\\mathrm{emb_j = Transformer^{(2)}(c_j)}\n\\end{equation}\n\n{\\bf Knowledge integration} We integrate all candidate commonsense by $\\mathrm{Transformer^{(3)}}$. Since not all the candidate commonsense leads to high confidence prediction as we have discussed in Sec~\\ref{sec:intro}, we need to select relevant commonsense and ignore irrelevant commonsense. Transformer is adequate to conduct this selection. Specifically, in the query-key-value mechanism in Transformer, we use the embedding of the knowledge token in $\\mathrm{Transformer^{(1)}}$ as the query of $\\mathrm{Transformer^{(3)}}$. \nand the commonsense embeddings by $\\mathrm{Transformer^{(2)}}$ as keys and values of $\\mathrm{Transformer^{(3)}}$. Then, we integrate representations of all different commonsense descriptions based on their similarities with the knowledge token.\n\n$\\mathrm{Transformer^{(3)}}$ also uses multi-head attention to allow the knowledge token to interact with the candidate commonsense in multiple ways. The output of multi-head self-attention is followed by a residual connection and a layer normalization.\n\\begin{equation}\n\\small\n\\begin{aligned}\n\\mathrm{cs_{emb}}= & \\mathrm{LayerNorm(k_{i-1} }\\\\ \n& + \\mathrm{MultiHeadAttn(k_{i-1},{\\bf emb},{\\bf emb}))}\n\\end{aligned}\n\\end{equation}\nwhere $\\mathrm{{\\bf emb}=[emb_1,\\cdots,emb_n]}$ denotes the sequence of embeddings of all candidate commonsense descriptions. We then apply a residual connection and a layer normalization to it.\n\n{\\bf Null Commonsense} Some target texts may not have valid commonsense from ATOMIC2020 to enhance their representations. Therefore, we refer to the settings of REALM~\\cite{guu2020realm} to add a null commonsense into the candidate commonsense of all target texts. We denote the null commonsense as $c_0$. Matching to the null commonsense indicates that the commonsense knowledge base cannot help enhance the target text.\n\n\\section{Adaptation to Pre-trained Language Models}\nIn this section, we take BERT as an example to illustrate how we adapt OK-Transfomer to existing pre-trained language models. We denote the adapted model as OK-BERT. An important manifestation of the effectiveness of the  Transformer structure is its applications in large-scale pre-trained models (e.g. BERT, RoBERTa). In order to introduce external knowledge, many other studies conduct training over large-scale unsupervised corpus~\\cite{peters2019knowledge,xiong2019pretrained}. However, OK-Transformer is able to directly adapt to the existing pre-trained language models for free. In other words, when adapting OK-Transformer to OK-BERT, we directly use the parameters of each Transformer layer of BERT to initialize the OK-Transformer layers of OK-BERT. This property greatly improves the applicability of OK-BERT. In the rest of this section, we will describe how $\\mathrm{Transformer^{(1)}}$, $\\mathrm{Transformer^{(2)}}$, and $\\mathrm{Transformer^{(3)}}$ are adapted respectively in Sec~\\ref{sec:layer_adapt}, and how to fine-tune OK-BERT in Sec~\\ref{sec:train}.\n\n\\subsection{Layer-by-Layer Adaptation}\n\\label{sec:layer_adapt}\nThe OK-BERT we designed uses two original BERTs to serve as $\\mathrm{Transformer^{(1)}}$ and $\\mathrm{Transformer^{(2)}}$, respectively. We denote them as BERT1 and BERT2.\nWe connect the $\\mathrm{Transformer^{(1)}}$ and $\\mathrm{Transformer^{(2)}}$ in the corresponding layer of each BERT by $\\mathrm{Transformer^{(3)}}$. Therefore, OK-BERT makes full use of the multi-layer structure of BERT, while allowing commonsense in the knowledge token to fully interact with the target text in each layer. The architecture is shown in Fig.~\\ref{fig:ok_bert}.\n\n\\begin{figure}[!htb]\n\t\\centering\n\t\t\\includegraphics[scale=.45]{figs/ok_bert2.pdf}\n\\caption{The architecture of OK-BERT. We only draw edges that connect to the $i$-th layer.}\n\\label{fig:ok_bert}\n\\end{figure}\n\n$\\mathrm{\\bf Transformer^{(1)}}$ We adapt the Transformer of BERT1 to $\\mathrm{Transformer^{(1)}}$ in the knowledge enhancement module of OK-Transformer. Note that the original BERT's tokens are $\\mathrm{[CLS]\\; w_1 \\cdots w_L\\; [SEP]}$ (for a single sentence) or $\\mathrm{[CLS] \\; w_1 \\cdots w_{m}\\; [SEP]\\; w_{m+1} \\cdots w_{L}\\; [SEP]}$ (for a sentence pair). We follow~\\cite{wang2020cross} and use a special token $[k]$ as the knowledge token. \nWhen tokenizing sentences, we insert the $[k]$ token after the $[CLS]$ token for each given text. In this way, the input tokens become $\\mathrm{[CLS]\\; [k]\\; w_1 \\;\\cdots w_L\\; [SEP]}$ or $\\mathrm{[CLS]\\; [k]\\; w_1 \\cdots w_m \\; [SEP]\\; w_{m+1} \\cdots w_{L}\\; [SEP]}$\n, respectively. This simple modification allows us to use $[k]$ as the knowledge token in the knowledge enhancement module.\n\n\n$\\mathrm{\\bf Transformer^{(2)}}$ We adapt each Transformer layer of BERT2 to the $\\mathrm{Transformer^{(2)}}$ layer. The adaptation is straightforward since $\\mathrm{Transformer^{(2)}}$ uses the vanilla Transformer structure. We use the encoding of the $[CLS]$ token in each corresponding layer as the commonsense representation $emb_j$ to enhance the representation of the corresponding layer in BERT1.\n\n$\\mathrm{\\bf Transformer^{(3)}}$ For each pair of corresponding $\\mathrm{Transformer^{(1)}}$ and $\\mathrm{Transformer^{(2)}}$ from the same layer, we use one $\\mathrm{Transformer^{(3)}}$ to connect them to transfer the information from BERT2 to BERT1.\n\nIn summary, when adapting to BERT-base with 12 Transformer layers, OK-BERT contains 12 $\\mathrm{Transformer^{(1)}}$ layers for BERT1, 12 $\\mathrm{Transformer^{(2)}}$ layers for BERT2, and 12 $\\mathrm{Transformer^{(3)}}$ layers for layer-wise knowledge integration.\n\n\n\\subsection{Parameter Initialization and Model Training}\n\\label{sec:train}\nIn our implementation, BERT1 and BERT2 have independent parameters. We use the parameters of BERT to initialize both BERT1 and BERT2. The parameters of $\\mathrm{Transformer^{(3)}}$ layers are randomly initialized. For downstream tasks, we then fine-tune all the parameters in the fashion of end2end.\n\n\\section{How does OOD Commonsense Help: Explanation from OOD predictions}\n\n\\section{Related work}\n\\label{sec:related}\nIn this section, we compare different ways to introduce knowledge into language models. We divide the knowledge introduction methods into (1) continuing pre-training method~\\cite{gururangan2020don,sun2019finetune} and (2) explicit introduction in the downstream task~\\cite{guu2020realm,murty2020expbert}. \n\n{\\bf Continuing pre-training} the language model is effective when the external knowledge is similar to the downstream task~\\cite{gururangan2020don,sun2019finetune}. However, commonsense and downstream tasks have domain discrepancies, so continuing pre-training is unsuitable for introducing commonsense. We have empirically verified this in Sec~\\ref{sec:exp:introduction}.\n\n{\\bf Introducing explicit knowledge in downstream tasks} We classify the knowledge into structured knowledge, plain text, and semi-structured knowledge, depending on its form. The entries of {\\bf structured knowledge} are represented as individual embeddings~\\cite{peters2019knowledge,zhang2019ernie,guan2020knowledge,zhou2018commonsense}, while commonsense descriptions in this paper can be represented more accurately by the contextual information of their word sequences. \n\n\n\n\n\\section{Problem Setup: Commonsense as the Extra Input}\n\nWe consider a text classification task where the text $x$ and its label $y$ are provided for training. Assuming that the candidate commonsense descriptions for enhancing $x$ come from a large-scale commonsense knowledge base (i.e. ATOMIC2020), we retrieve candidate commonsense for $x$ as the extra input. We denote the commonsense descriptions for $x$ as $cs(x)=\\{c_1 \\cdots c_n\\}$, where each $c_i$ is a commonsense description. The retrieval process will be shown in Sec~\\ref{sec:exp}. The model takes both $x$ and $cs(x)$ as the input. Since ATOMIC2020 contains if-then knowledge for general purposes, the problem setup can be expanded to a broad range of text understanding tasks. The goal of training is to find parameter $\\theta$ that minimizes the loss of training examples given the texts and candidate commonsense descriptions:\n\\begin{equation}\n\\small\n\\label{eqn:goal}\n\\mathrm{{\\arg\\min}_{\\theta} \\mathbb{E}_{(x,y)\\in train} \\loss(f(x,cs(x)|\\theta),y)}\n\\end{equation}\nwhere $\\mathrm{f(\\cdot|\\theta)}$ is the model taking $x$ and $cs(x)$ as inputs, $\\loss$ is the loss function.\n\n\n\\nop{\n\\subsection{Explanation: Converting OOD Predictions to In-Distribution}\n\\label{sec:rational}\n\nTo illustrate how we use the commonsense to enhance the representation of $x$, we consider the example in Fig.~\\ref{fig:commonsense_list}. We want to infer who ``he'' is ($y=$Kevin or Jim) from $x$, that is, predicting $P(y|x)$. Here $x$ and $y$ are both natural language descriptions.\nThis inference requires the joint distribution of $x$ and $y$. We show the corresponding frequency distribution in the training data in Fig.~\\ref{fig:sphere}. In our example, this inference is hard if $(x,y)$ is OOD in the training dataset.\n\n\nTo avoid directly predicting the OOD data $(x,y)$, we introduce the commonsense $c$. Instead of directly inferencing $P(y|x)$, we use $z$ as the intermediate variable to reduce the difficulty of prediction. We have\n\\begin{equation}\n\\small\n\\label{eqn:yx}\n\\mathrm{P(y|x)=\\sum_z P(y|x,c) P(c|x)}\n\\end{equation}\n\n\nIn Eq.~\\eqref{eqn:yx}, $P(c|x)$ can be derived from the commonsense knowledge base. Predicting $P(y|x,c)$ rather than $P(y|x)$ is easier. Since {\\it (crying, keep calm)} has high frequency in the training data, we infer that ``he'' denotes Kevin. Intuitively, the commonsense description $c_1$ bridge the gap between $\\mathrm{x:comforted}$ and $\\mathrm{y:crying}$ via the OOD commonsense media $\\mathrm{c_1:comforted \\rightarrow keep \\; calm}$.\n\n\n\\begin{figure}[!htb]\n\\begin{subfigure}[b]{0.46\\textwidth}\n\t\\centering\n\t\t\\includegraphics[scale=.5]{figs/intuition2.pdf}\n\\caption{The reference of ``he'' can be inferred if we know the OOD commonsense $c_1$.}\n\\label{fig:commonsense_list}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.46\\textwidth}\n\t\\centering\n\t\t\\includegraphics[scale=.5]{figs/intuition_draw2.pdf}\n\\caption{Distribution of the training dataset. Shallow blue means OOD. Deep blue means ID. With $c_2$, we convert the OOD prediction $(x,y)$ to the ID prediction $P((x,c_1),y)$}\n    \\label{fig:sphere}\n\\end{subfigure}\n\t\\caption{An example of using OOD commonsense to convert OOD predictions to in-distribution.}\n\\end{figure}\n\n\nNote that for some candidate commonsense (e.g. $c_2$, $c_3$), the corresponding $((x,c),y)$ is still OOD. Therefore the model should distinguish different candidate commonsense. This is achieved by the knowledge integration module in Sec~\\ref{sec:method:output}.\n}\n\\section{Related Work}\n\n\n{\\bf Commonsense reasoning} is considered to be a challenging task in natural language processing~\\cite{levesque2012winograd}. With the development of unsupervised language models in recent years~\\cite{devlin2018bert,peters2018deep,radford2019language}, people realized that commonsense can be derived from them. For example, \\cite{petroni2019language} found that we can extract structured factual knowledge (e.g. ConceptNet~\\cite{liu2004conceptnet}) from unsupervised language models. \\cite{weir2020existence} found that the unsupervised language model implicitly represents concepts and its associated properties. In some follow-up work, researchers found that language models can be used to solve the task of commonsense-based reference disambiguation. This is also the main task that this paper focuses on.\n\nZero-shot's commonsense reasoning of unsupervised language models aims to study how to design algorithms to better extract commonsense from unsupervised language models. For example, \\cite{trinh2018simple} found that we can use the joint probability of the sentence provided by the language model to determine whether a sentence conforms to commonsense. The author believes that sentences with high joint probability are more in line with commonsense. Therefore, the author completes the disambiguation by replacing the original pronoun with the candidate reference, and using the joint probability as the score of the sentence. Several subsequent works have used similar ideas. \\cite{klein2019attention} uses the attention-guided method to calculate the score. \\cite{tamborrino2020pre} uses a larger language model Roberta~\\cite{liu2019roberta}, and calculates the scores of candidate sentences based on the masked word probability. \\cite{shwartz2020unsupervised} provides additional clues for using generative language models (i.e. GPT-2~\\cite{radford2019language}) for the target task. These works have achieved good results on some commonsense reasoning tasks.\n\nHowever, \\cite{zellers2019hellaswag} found that the language model is actually looking for sentences that are more in line with lexical cues, rather than sentences that are more in line with commonsense. We agree with the viewpoint of \\cite{zellers2019hellaswag}. Since the language model learns and represents the association of the text and its context, this is an inevitable result of a lot of commonsense not being written directly (\\cite{gordon2013reporting}). Since the representation learning of the language model is based on data association, it only models ID data, but cannot effectively represent OOD data. We believe that introducing commonsense into the language model is the only way for the language model to understand OOD data. This is also the main research direction of this article.\n\nIn contrast to the commonsense reasoning of the zero-shot unsupervised language model, some works believe that the current limitation of commonsense reasoning lies in the lack of relevant training data. Therefore, some data sets similar to common commonsense reasoning were constructed. The earliest data set similar to WSC is WSCR~\\cite{rahman2012resolving}. \\cite{kocijan19acl} found that if the unsupervised language model is fine-tuning on similar reference disambiguation data sets, the effect of the language model on commonsense-based reference disambiguation can be improved. Therefore, the author constructs MaskedWiki~\\cite{kocijan19acl} and WikiCREM~\\cite{kocijan2019wikicrem} through masking of Wikipedia data. \\cite{sakaguchi2019winogrande} constructed the WinoGrande data set, which has improved WSC in terms of scale and difficulty. Some language models themselves also add such data sets to pre-training corpus. For example, Roberta's pre-training corpus STORIES~\\cite{trinh2018simple} is a corpus similar to WSC. These datasets have indeed improved the effectiveness of specific commonsense reasoning tasks.\n\nHowever, it is clear that the construction of these commonsense data sets is for specific tasks. Its essence is to expand the scope of the model's ID data. But the corpus is always limited. We believe that the introduction of larger data sets cannot solve the OOD problem of more general tasks. We believe that the introduction of task-independent commonsense data is the general way to solve the OOD problem.\n\n{\\bf Incorporating External Knowledge into Language Representation Learning} \n\n\\section*{Acknowledgments and Disclosure of Funding}\nWe thank Wenting Ba for her valuable plotting assistance. This paper was supported by National Natural Science Foundation of China (No. 61906116), by Shanghai Sailing Program (No. 19YF1414700).\n\n\n\n\n", "meta": {"timestamp": "2022-03-15T01:24:28", "yymm": "2109", "arxiv_id": "2109.02572", "language": "en", "url": "https://arxiv.org/abs/2109.02572"}}
{"text": "\\section{Introduction}\n\nGalaxies that form close to a matter over-density are affected by the tide induced by the quadrupole of the surrounding gravitational field, and the distribution of stars will adjust accordingly. This process, which starts during the initial stages of galaxy formation \\citep{Catelan2001}, can persist over their entire lifetime, as galaxies have continuous gravitational interactions with the surrounding matter \\citep[e.g.][]{Bhowmick2019}, and leads to the intrinsic alignment (IA) of galaxies.\n\nThis tendency of neighbouring galaxy pairs to have a similar orientation of their intrinsic shapes is an important contaminant for weak gravitational lensing measurements \\citep[e.g.][]{Joachimi2015review}. The matter distribution along the line-of-sight distorts the images of background galaxies, resulting in apparent correlations in their shapes. Intrinsic alignment contributes to the observed correlations, complicating the interpretation. To infer unbiased cosmological parameter estimates it is therefore crucial to account for the IA contribution. This is particularly important in the light of future surveys, such as \\textit{Euclid}\\footnote{\\url{https://www.euclid-ec.org}} \\citep{Laureijs2011} and the Large Synoptic Survey Telescope (LSST)\\footnote{\\url{https://www.lsst.org}} at the Vera C. Rubin Observatory \\citep{Abell2009lsst}, which aim to constrain the cosmological parameters with sub-percent accuracy \\citep[for a forecast of the IA impact on current and upcoming surveys see][among others]{Kirk2010, Krause2016}. Some recent results on current weak lensing studies are available in, for example, \\citet[][]{Aihara2018HSC,Asgari2021, Abbott2021DESY3}.\n\nTo provide informative priors to lensing studies, it is essential to learn as much as possible from direct observations of IA. It is, however, also important that such results can be related to the properties of galaxies that give rise to the alignment signal in cosmic shear surveys \\citep{fortuna2020halo}. Intrinsic alignment studies are typically limited to relatively bright galaxies, which often sit at the centre of their own group or cluster, and it is thus possible to connect their alignment to the underlying dark matter halo alignment via analytic models \\citep{Hirata2004}. The picture becomes more complicated when considering samples that contain a significant fraction of satellite galaxies: The alignment of satellites arises as a result of the continuous torque exercised by the intra-halo tidal fields while the satellite orbits inside the halo \\citep{Pereira2008, Pereira2010}. This leads to a radial alignment, which also depends on the galaxy distance from the centre of the halo \\citep{Georgiou2019b}. At the same time, satellites fall into halos through the filaments of the large-scale structure, and this persists as an anisotropic distribution within the halo, which has been detected both in simulations \\citep{Knebe2004, Zentner2005} and observations \\citep{West2000VirgoCluster, Bailin2008, Huang2016, Johnston2019, Georgiou2019b}. The combination of these two effects complicates the picture. At small scales, where the satellite contribution is expected to be important, their signal may be described using a halo model formalism \\citep{SchneiderBridle2010, fortuna2020halo}, but their contribution to IA on large scales remains poorly constrained \\citep{Johnston2019}; although it is expected that they are not aligned, they do affect the inferred amplitude because they contribute to the overall mix of galaxies. This prevents a straightforward interpretation of any secondary sample dependence of the IA signal sourced by the central galaxy population, such as the dependence on luminosity or colour, in mixed samples where the fraction of satellites is relevant.\n\nObservational studies have found discordant results regarding the presence of a luminosity dependence of the IA signal, with the bright end being well described by a steep power law with index $\\sim 1.2$ \\citep{Hirata2007, Joachimi2011b, Singh2015}, while less luminous galaxies do not show any significant dependence of the IA signal with luminosity \\citep{Johnston2019}. A recent investigation using hydrodynamic simulations by \\cite{Samuroff2020} supports a flatter slope, in agreement with \\citet{Johnston2019} and \\citet[][]{fortuna2020halo} at low luminosities but in tension with previous studies that probe more luminous galaxies. The interpretation of these results is also affected by the presence of satellites, whose fraction varies with luminosity and depends on the specific selection function of the data. At low redshift, a cosmic shear survey is dominated by faint galaxies, and improving our understanding of the  IA signal at low luminosities is one of the most urgent questions for IA studies.\n\nAnother relevant aspect that is often neglected is the dependence of IA on the shape measurement method \\citep{Singh2016}. The tendency to align in the direction of the surrounding tidal field is a function of galaxy scale \\citep{Georgiou2019b}, with the outermost parts -- which are more weakly gravitationally locked to the galaxy -- showing a more severe twist. It increases the IA signal associated with shapes measured via algorithms that assign more importance to the galaxy outskirts. In contrast, lensing studies typically prefer shape methods that give more weight to the inner part of a galaxy. Accounting for this discrepancy is potentially relevant for future cosmic shear studies.\n\nIn this work we focus on investigating the luminosity dependence of the IA signal in the least constrained regime, $M_r\\gtrsim-22$. We employ two different samples, which differ in mean luminosity and number density. We limit the analysis to the large-scale alignment, for which a theoretical framework is already available and where the luminosity dependence is known to play a crucial role \\citep{fortuna2020halo}. We also provide estimates of the satellite fractions present in our samples in order to guide future work on the modelling of satellite alignment at large scales.\nWe also explore the dependence of our signal on the shape measurement algorithm used to create the shape catalogue. We compare the signal as measured by two complementary algorithms: \\textsc{DEIMOS} \\citep[DEconvolution In MOment Space; ][]{Melchior2011}, which has been widely used in IA studies \\citep{Georgiou2019, Johnston2019, Georgiou2019b}, and \\textit{lens}fit\\ \\citep{Miller2007, Miller2013} which has been used for the cosmological analysis of the Canada-France-Hawaii Telescope Lensing Survey \\citep[CFHTLenS;][]{Heymans2013CFHTLens} and the Kilo-Degree Survey \\citep[KiDS; see][and references therein]{Asgari2021}.\n\nOne of the main limitations for measuring IA is the necessity of simultaneously relying on high-quality images and precise redshifts to properly identify physically close pairs of galaxies that share the same gravitational tidal shear. Wide field image surveys provide high-quality images, but the uncertainty in the photometric redshifts is too large for\nuseful IA measurements. Fortunately, using a specific selection in colours, it is possible to obtain a sub-sample of galaxies with more precise photometric redshifts: the luminous red galaxies (LRGs). At any given redshift, LRGs populate a well-defined region in the colour-magnitude diagram, known as the red-sequence ridgeline. Using this unique property, it is possible to design a specific algorithm to select LRGs in photometric surveys, which results in both precise and accurate redshifts \\citep{Rozo2016, Vakili2019, Vakili2020}. Luminous red galaxies have also been shown to be strongly affected by the surrounding tidal fields, making them an extremely suitable sample for exploring the behaviour of IA at different redshifts and as a function of secondary galaxy properties, such as luminosity and type (central or satellites).\n\n\\citet{Joachimi2011b} first studied the IA signal of an LRG sample with photometric redshifts. In this paper we follow their main approach but use a catalogue of LRGs selected by \\citet[]{Vakili2020} using the KiDS fourth public data release \\citep[KiDS-1000][]{Kuijken2019DR4}.\n\nThe paper is structured as follows. In Sect. \\ref{sec:kids} we describe our data and the characteristics of our two main samples. In Sect.~\\ref{sec:shape_measurements} we introduce the two shape measurement methods employed in the analysis and present the strategy adopted to calibrate the bias in the measured shapes. Section ~\\ref{sec:correlation_function_measurements} presents the estimators we use to extract the signal from the data, while Sect.~\\ref{sec:theoretical_framework} illustrates the theoretical framework we rely on when modelling the signal: the way the model accounts for the use of photometric redshifts as well as the way we account for astrophysical contaminants. Finally, we present our main results in Sect.~\\ref{sec:results} and conclude in Sect.~\\ref{sec:conclusions}.\n\nThroughout the paper, we assume a flat $\\Lambda$ cold dark matter cosmology with $h=0.7, \\Omega_{\\rm m} = 0.25, \\Omega_{\\rm b} = 0.044, \\sigma_8 = 0.8$, and $n_{\\rm s} = 0.96$. \n\n\\section{KiDS}\n\\label{sec:kids}\n\nThe Kilo-Degree Survey is a multi-band imaging survey designed for weak lensing studies, currently at its fourth data release \\citep[KiDS-1000;][]{Kuijken2019DR4}. The data are obtained with the OmegaCAM instrument \\citep{Kuijken2011} on the VLT Survey Telescope \\citep[VST;][]{Capaccioli2012}. This combination of telescope and camera was designed specifically to produce high-quality images in the $ugri$ filters, with best seeing-conditions in the $r-$band, and a mean magnitude limit of $\\sim 25$ ($5\\sigma$ in a $2''$ aperture). These measurements are combined with results from the VISTA Kilo-degree INfrared Galaxy survey\n\\citep[VIKING;][]{Edge2013}, which surveyed the same area in five infrared bands ($ZYJHK_\\mathrm{s}$). This resulted in high-quality photometry in nine bands across approximately $1000 {\\rm \\ deg}^2$ imaged by the fourth data release\\footnote{The survey was recently completed, imaging a final total of 1350 deg$^2$.}. The VIKING data are important for the LRG selection at high redshift \\citep{Vakili2020}: the $Z$ band is included in the red-sequence template and improves the constraints on the redshift of the high-redshift galaxies, while the $K_\\mathrm{s}$ band allows for a clean separation between galaxies and stars in the $(r-K_\\mathrm{s})-(r-z)$ colour-colour space.\n\n\\subsection{The LRG sample}\n\\label{sec:lrg_sample}\n\nRed-sequence galaxies are characterised by a tight colour-redshift relation, so that at any given redshift they follow a narrow ridgeline in the colour-magnitude space. This relation can be exploited to select red galaxies from photometric data and obtain precise photometric redshifts. Here we use the catalogue of LRGs presented in \\cite{Vakili2020}. It uses a variation of the \\textsc{redMagiC} algorithm \\citep{Rykoff2014} to select LRGs from the KiDS-1000 data. As detailed in \\citet[][]{Vakili2019} and \\citet[][]{Vakili2020}, the red-sequence template is calibrated using the regions of KiDS that overlap with a number of spectroscopic surveys: SDSS DR13 \\citep{Albareti2017}, 2dFLenS \\citep{Blake2016}, GAMA \\citep{Driver2011}, together with the GAMA G10 region, which overlaps with COSMOS \\citep{Davies2015}.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{plots/lrg_lum_and_dense_DEIMOS_vs_lensfit_z_counts.pdf}\n\\caption{Photometric redshift distributions for our density (all) and shape catalogues (\\textit{lens}fit\\ and \\textsc{DEIMOS}; see text for details). The orange histograms show the distribution for the \\texttt{dense}\\ samples, which is limited to $z_{\\rm phot}<0.6$, whereas the \\texttt{luminous}\\ sample (green) is restricted to $z_{\\rm phot}<0.8$.}\n\\label{fig:z_histo_shapes}\n\\end{figure}\n\nThe algorithm is designed to return a sample of LRGs with a constant comoving number density. It achieves this by imposing a redshift-dependent magnitude cut that depends on $m_{r}^\\mathrm{pivot}(z)$, the characteristic $r$-band magnitude of the \\cite{Schechter1976} function, assuming a faint-end slope $\\alpha=1$ \\citep[for more details, see][sect. 3.1]{Vakili2019}. We use this to define two samples that differ from each other in terms of their minimum luminosity relative to the luminosity $L_\\mathrm{pivot}(z)$. We refer to them as our \\texttt{luminous}\\ sample (high luminosity, low number density, $L_{\\rm min}/L_\\mathrm{pivot}(z)=1$) and \\texttt{dense}\\ sample (lower luminosity, higher number density, $L_{\\rm min}/L_\\mathrm{pivot}(z)=0.5$). To ensure that the two samples are separate, we remove the galaxies in the \\texttt{dense}\\ sample that also belong to the \\texttt{luminous}\\ one. However, this does not mean they do not overlap in their \nphysical properties. In particular, they overlap partially in luminosity, a feature that we will exploit later in the paper.\n\n\\begin{figure*}\n\\centering\n\\includegraphics[width=\\textwidth]{plots/mags_histograms.pdf}\n\\caption{The magnitude distributions of the samples used in the analysis. {\\it Left panel:} Histograms of the apparent magnitude, {\\texttt{MAG\\_AUTO}} in the $r$-band for the galaxies in the \\texttt{dense}\\ (orange lines) and \\texttt{luminous}\\ (green lines) samples with shapes measured by \\textit{lens}fit\\ (darker colours) and \\textsc{DEIMOS} (lighter colours). {\\it Right panel:} Histograms of the absolute magnitudes in the $r$-band ($K+e$ corrected) for the same samples.} \\label{fig:mag_histo_dense_and_lum}\n\\end{figure*}\n\nAs shown in Fig.~\\ref{fig:z_histo_shapes}, the two samples also span different redshift ranges. The \\texttt{luminous}\\ sample extends from $z=0.2$ to $z=0.8$. After applying a conservative mask to select only objects with a high probability to be red-sequence galaxies (corresponding to objects with a clear separation from the star sequence in the colour-colour diagram), we are left with $117\\,001$ galaxies, which comprise our density sample. By density sample---not to be confused with the \\texttt{dense}\\ sample described above---we refer to the sample used to trace galaxy positions, as opposed to the shape sample, which is the sample used for the measurement of galaxy orientations and is composed by the galaxies of the corresponding density sample for which a given shape measurement algorithm is able to measure the galaxy shape. The density and shape samples used in this analysis are visible in Fig.~\\ref{fig:z_histo_shapes}, where the density samples of the \\texttt{luminous}\\ and \\texttt{dense}\\ samples are referred to as `all' galaxies.\nThe \\texttt{dense}\\ sample is obtained with the same strategy, but we further impose $z<0.6$ to ensure the completeness and purity of the sample (see Fig. 4 in \\citet[][]{Vakili2020}). This leads to a final sample of $173\\,445$ galaxies.\nAs shown in \\cite{Vakili2020}, the redshift errors are well described by a Student's $t-$distribution. The width of the distribution increases slightly with redshift, with typical values around $\\sigma_z \\sim 0.014- 0.019$. For further details on the sample selection and redshift estimation, we refer the interested reader to \\citet{Vakili2020}. \n\nWe infer galaxy absolute magnitudes using \\textsc{Lephare}\\footnote{\\url{https://www.cfht.hawaii.edu/~arnouts/LEPHARE/lephare.html}} \\citep{Arnouts2011Lephare}, assuming the dust extinction law from \\citep{Calzetti1994} and the stellar population synthesis model from \\citet{BruzualCharlot2003}. We correct our magnitudes to $z=0$; the K-correction is provided by \\textsc{Lephare} and the correction for the evolution of the stellar populations ($e-$correction) is computed with the python package \\textsc{EzGal}\\footnote{\\url{http://www.baryons.org/ezgal}} \\citep{Mancone2012EzGal}, assuming Salpeter initial mass function \\citep{Chabrier2003} and a single star formation\nburst at $z = 3$. These corrections are based on the magnitudes used to define the colours (\\texttt{MAG$\\_$GAAP}), which are measured using Gaussian apertures \\citep{Kuijken2019DR4}. Although ideal for colour estimates, these underestimate the flux and should not be used to compute the luminosity. For that purpose we correct\\footnote{The total flux in the $x$ filter can be computed using $m_x=\\texttt{MAG\\_AUTO}_r+\\texttt{(MAG\\_GAAP}_x-\\texttt{MAG\\_GAAP}_r)$, which implicitly assumes that colour gradients are negligible.} them using the Kron-like \\texttt{MAG$\\_$AUTO} measured from the $r$-band images by \\textsc{SExtractor} \\citep{Bertin1996SExtractor}. \n\nThe left panel of Fig.~\\ref{fig:mag_histo_dense_and_lum} shows the distribution in apparent magnitude \\texttt{MAG}$\\_$\\texttt{AUTO} for galaxies in the \\texttt{dense}\\ and \\texttt{luminous}\\ samples for which shapes were determined by \\textit{lens}fit\\ or \\textsc{DEIMOS}. In Sect.~\\ref{sec:shape_measurements} we describe the two shape measurement methods and explain the difference in their number counts. We note that the LRGs are much brighter than the limiting magnitude of KiDS in the $r$-band. The corresponding distributions in absolute magnitude in the rest-frame $r$ filter, K+$e$ corrected to $z=0$, are presented in the right panel of Fig.~\\ref{fig:mag_histo_dense_and_lum}. This shows that the \\texttt{dense}\\ sample overlaps somewhat with the \\texttt{luminous}\\ sample in terms of luminosity, as a consequence of the photometric redshift uncertainty\\footnote{The selection through the redshift-dependent apparent magnitude cut results in an overlap in apparent magnitudes of the \\texttt{dense}\\ and \\texttt{luminous}\\ samples. Because the cut is redshift-dependent, this implies a threshold in luminosity: In the case of perfect redshifts, this would result in a disjoint sample, because we removed the galaxies from the \\texttt{dense}\\ sample that overlap with the \\texttt{luminous}\\ one. The photometric redshift uncertainty, however, assigns to galaxies with the same apparent magnitude different luminosities, and thus a portion of the \\texttt{dense}\\ sample extends above the luminosity threshold of the \\texttt{luminous}\\ sample.}.\n\n\\subsection{Satellite galaxy fraction estimation} \\label{subsec:satellite_fraction}\n\nObservations suggest that satellite galaxies are only weakly aligned \\citep[see e.g.][for recent constraints]{Georgiou2019b} and thus suppress the IA signal at large scales. We do not take this into account in our analysis but provide here an estimate of the fraction of satellites we expect in our samples. Such information will be useful for future modelling studies.\n\nWe use the publicly available \\texttt{G3GGal} and \\texttt{G3GFoFGroup} catalogues \\citep[][]{Robotham2011GAMAFoF} from the GAMA survey \\citep[][]{Driver2009, Driver2011, Driver2015}. Since KiDS overlaps with GAMA, these catalogues provide group information for a subset of our galaxies, obtained with a Friends-of-Friends algorithm. We cross-match our LRG samples with the \\texttt{G3GGal} catalogue and select galaxies with $z<0.21$ ($z<0.32$), which provide a roughly volume-complete match to the \\texttt{dense}\\ (\\texttt{luminous}) sample. With the information in both group catalogues,  we identify both the brightest group galaxies and ungrouped galaxies as centrals, and the rest as satellites. With this strategy, we obtain $f_\\mathrm{sat}=0.34$ for our \\texttt{dense}$\\times$GAMA sample and $f_\\mathrm{sat}=0.23$ for the \\texttt{luminous}$\\times$GAMA\\footnote{These estimates refer to the full samples, but should be representative for the shape samples as well.}. Since our samples are selected to resemble the same galaxy populations at different redshifts, these estimates should be fairly representative beyond the redshift range probed by our direct comparison. \n\n\n\n\\section{Shape measurements}\\label{sec:shape_measurements}\n\nIn addition to precise redshifts, a successful IA measurement requires accurate shape measurements. In this work, we compare two different algorithms, \\textsc{DEIMOS} and \\textit{lens}fit\\,  both in terms of their ability to recover reliable ellipticity measurements and the resulting IA signal. Exploring the dependence of the IA signal on the shape measurement algorithm is important if one aims to provide informative priors to lensing studies \\citep{Singh2016}. Both algorithms have been used to analyse KiDS data: \\textsc{DEIMOS} to provide the shape catalogue \\citep{Georgiou2019} for a number of IA studies,\nwhile \\textit{lens}fit\\ was used for cosmic shear analyses \\citep[see][for the most recent shape measurements]{Giblin2021}.\n\n\\subsection{DEIMOS}\\label{subsec:DEIMOS}\n\n\\textsc{DEIMOS} \\citep{Melchior2011} is a moment-based shape measurement algorithm designed to measure the\nmoments of the surface brightness distribution from an image, which are subsequently used to estimate the ellipticity. The main features of \\textsc{DEIMOS} are its rigorous treatment of the PSF moments to arbitrary order, the lack of model assumptions and the flexibility in changing the size of the weight function so that it is possible to assign more importance to different parts of a galaxy while performing the shape measurement (bulge or outskirts). \n\nThe unweighted moments of the surface brightness $G(\\vec{x})$ are defined as\n\\begin{equation}\n\t\\tens{Q}_{ij} \\equiv \\{ G\\}_{ij} = \\int G(\\vec{x})\\, x^{i} y^{j} \\, {\\rm{d}}x\\, {\\rm{d}}y \\ ,\n\t\\label{eq:unweighted_moments}\n\\end{equation}\nwhere $(x,y)$ are the Cartesian coordinates with origin at the galaxy's centroid. The complex ellipticity is then defined in terms of the second-order moments as\n\\begin{equation}\n\t\\label{eq:ellipticity}\n\t\\epsilon \\equiv \\epsilon_{1} + \\rm{i}\\epsilon_{2} = \\frac{ \\tens{Q}_{20} - \\tens{Q}_{02} + 2\\rm{i}\\,\\tens{Q}_{11} }{ \\tens{Q}_{20} + \\tens{Q}_{02} + 2\\, \\sqrt{\\tens{Q}_{20}\\, \\tens{Q}_{02} - \\tens{Q}_{11}^{2}} } \\ .\n\\end{equation}\n\nIn practice, unweighted moments cannot be used because of noise in the images, and weighted moments have to be employed instead. We will return to this issue later. Moreover, the galaxy images are smeared and distorted by the atmospheric blurring and the telescope optics, so that the observed image, $G^{\\vec{*}}$, is convolved with the PSF kernel $P(\\vec{x})$,\n\\begin{equation}\n\tG^{\\vec{*}}(\\vec{x}) = \\int G(\\vec{x}')\\, P(\\vec{x} - \\vec{x}')\\, {\\rm{d}}\\vec{x}' \\ .\n\\end{equation}\n\nThe \\textsc{DEIMOS} algorithm estimates the unweighted moments by correcting the \nobserved weighted moments of the galaxy surface brightness for the convolution by the PSF. The underlying mathematical framework is a deconvolution in moment space. In order to measure the moments in Eq. (\\ref{eq:unweighted_moments}) we then need to deconvolve them. This can easily be achieved in Fourier space, where the convolution becomes a product. Using the Cauchy product, we can write \\citep{Melchior2011}: \n\\begin{equation} \\label{eq:G*_ij}\n\t\\{ G^{*} \\}_{ij} = \\sum^{i}_{k} \\sum^{j}_{l} \\, \\begin{pmatrix} \\, i\\, \\\\ k \\end{pmatrix} \\begin{pmatrix} j \\\\ \\, l\\, \\end{pmatrix} \\, \\{ G \\}_{kl} \\{P\\}_{i-k,j-l} \\ ,\n\\end{equation}\nwhich shows that the ($i+j$)-order convolved moments are determined by the same- or lower-order moments of the galaxy and the PSF kernel. The deconvolution procedure to estimate the galaxy moments is to invert the above hierarchical system of equations, starting from the zeroth order.\n\nAs mentioned above, it is necessary to introduce a weight function to avoid noise dominating the second-order moments outside the galaxy light profile. In this work, we adopt an elliptical Gaussian weight function with size $r_\\mathrm{wf} = r_\\mathrm{iso}$, where $r_\\mathrm{iso}$ is the isophotal radius, defined as $r_\\mathrm{iso} = \\sqrt{A_\\mathrm{iso}/\\pi}$, following \\citet[][]{Georgiou2019}. \nThe area $A_{\\rm{iso}}$ of the galaxy's isophote is computed using the \\texttt{ISOAREA$\\_$IMAGE} by \\textsc{SExtractor} \\citep{Bertin1996SExtractor}.\nThe shape measurement procedure is the same as described in \\citet{Georgiou2019} and we point the interested reader to their Section 2 for a detailed description of the algorithm. In Appendix~\\ref{A:mbias_calibration} we report our analysis of the measured shape bias for different setups, which led to our final choice reported above. \n\nUsing \\textsc{DEIMOS}, we successfully measure the shapes of 96\\,863 galaxies from the \\texttt{luminous}\\ sample, $\\sim 83 \\%$ of the corresponding density sample, and 152\\,832 shapes from the \\texttt{dense}\\ sample, roughly $\\sim 88 \\%$ of its density sample. The shape measurements mainly fail\\footnote{We only considered shapes with \\texttt{flag$\\_$\\textsc{DEIMOS}==0000}, corresponding to measurements that do not raise any flag \\citep[see ][]{Georgiou2019}.} for the faintest galaxies in the sample.\n\n\\subsection{\\textit{lens}fit}\\label{subsec:lensfit}\n\nThe second shape catalogue is obtained using the self-calibrating version of \\textit{lens}fit\\ \\citep{Miller2013}, described in more detail in \\cite{FenechConti2017}. It is a likelihood-based model-fitting method that fits a PSF-convolved two-component bulge and disk galaxy model. This is applied simultaneously to the multiple exposures in the KiDS-1000 $r$-band imaging, to get an ellipticity estimate for each galaxy. \n\n\\textit{lens}fit\\ provides shapes for 84\\,785 galaxies from the \\texttt{luminous}\\ sample ($72\\%$ of the density sample), and for 121\\,500 galaxies from the \\texttt{dense}\\ sample ($70\\%$ of the density sample). The lower completeness with respect to \\textsc{DEIMOS} is largely explained by the fact that \\textit{lens}fit\\ has been optimised for cosmic shear studies, where the signal is maximised for high-redshift galaxies, which are typically small and faint.  Whilst \\textit{lens}fit\\ could determine ellipticity measurements for the large bright galaxies with $\\texttt{MAG\\_AUTO}<20$, this model-fitting algorithm becomes prohibitively slow given the large number of pixels that these bright galaxies span. Therefore, the \\textit{lens}fit\\ catalogue only contains galaxies fainter than $\\texttt{MAG\\_AUTO}>20$  (hence the sharp cut-off in apparent magnitude in Fig.~\\ref{fig:mag_histo_dense_and_lum}). It performs better than \\textsc{DEIMOS} for relatively faint and low S/N galaxies. As these are preferentially found at higher redshifts, this also explains the different redshift distributions, as illustrated in Fig.~\\ref{fig:z_histo_shapes}.\n\n\\subsection{Image simulations}\\label{subsec:image_simulations}\n\n\\begin{figure*}\n\\centering\n\\subfloat[]{{\n\\includegraphics[width=0.9\\columnwidth]{plots/mbias_e1e2_R2_proxy_30bins_DEIMOS_vs_lensfit.pdf}\n}}\n\\qquad\n\\subfloat[]{{\n\\includegraphics[width=0.9\\columnwidth]{plots/mbias_e1e2_SNR_30bins_DEIMOS_vs_lensfit.pdf}}}\n\\caption{Average multiplicative bias, $m = (m_{\\epsilon_1}+m_{\\epsilon_2})/2$, as a function of (a) the galaxy resolution, $R$, and (b) the signal-to-noise ratio, S/N. Each point is measured on the same number of simulated galaxies and the error bars are estimated using bootstraps. For a comparison we also display in the background the weighted distribution of the two definitions of $R$ and the S/N in the real data for the \\texttt{dense}\\ shape samples (pink: $lens$fit; blue: \\textsc{DEIMOS}). The solid lines show the polynomial fit to $m(R)$ and $m(\\mathrm{S/N})$, which guided the construction of the two-dimensional bias surface.\n}\n\\label{fig:mbias_gals_par}\n\\end{figure*}\n\nWe want to measure the shapes of galaxies from images that are corrupted by noise and blurred by the atmosphere and telescope optics. These bias the inferred shapes and thus need to be carefully corrected for. Although both\n\\textsc{DEIMOS} and \\textit{lens}fit\\ are designed to do so, residual biases remain. These can be expressed as \\citep{Heymans2006}\n\\begin{equation}\n    \\epsilon_i^\\mathrm{obs} = (1+m_i) \\epsilon_i^\\mathrm{true} + c_i \\ ,\n\\end{equation}\nwith $i\\in\\{1,2\\}$ the ellipticity components introduced in \\ref{eq:ellipticity}. Here $\\epsilon_i^\\mathrm{true}$ is the true ellipticity, while $\\epsilon_i^\\mathrm{obs}$ is the output of the shape measurement algorithm; $m_i$ is the multiplicative bias and $c_i$ is the additive bias. Differently from what is done in lensing studies \\citep[e.g.][]{Kannawadi2019}, here we calibrate the ellipticity rather than the shear.\nOur aim is to determine the biases in our shape measurements using realistic image simulations, with a precision that is better than the statistical error on our IA signal. \n\nWe stress that although it is important to start with an algorithm that does not lead to a large bias in the first place, what matters the most is to calibrate the residual bias on realistic image simulations in order to properly account for galaxy blending and the different observing conditions \\citep{Hoekstra2017, Kannawadi2019, Samuroff2018IM3SHAPE, MacCrann2020}. We use dedicated image simulations generated with the COllege pipeline \\citep[COSMOS-like lensing emulation of ground experiments; ][]{Kannawadi2019}. These simulations reproduce the observations from the Cosmic Evolution Survey \\citep[COSMOS,][]{Scoville2007}, for which we have both KiDS imaging (KiDS-COSMOS) and deeper images from the \\textit{Hubble} Space Telescope (HST). We use the HST observations to generate our input catalogue and simulate the KiDS observations by varying the observation conditions. Under the assumption that COSMOS is representative of our galaxy sample (in practice we only require that it covers the signal-to-noise (S/N) and size parameter space, while we do not need the galaxy distributions to match) we study the $m-$bias properties of the LRGs in our KiDS-COSMOS field and use the bias model obtained from this set of galaxies to calibrate our full sample.\n\nThe image simulations used in this work differ slightly from those presented in \\citet[]{Kannawadi2019} because we require a larger number of simulated LRGs for our calibration. To achieve this, we adopt the ZEST catalogue \\citep[Zurich Estimator of Structural Type;][]{Scarlata2007, Sargent2007} for the input galaxy parameters. We generated 52 KiDS-like images by varying the observing conditions and rotating the galaxies. We used 13 different PSF sets and four rotations per each image. Since our underlying galaxy selection is identical for both the \\textit{lens}fit\\ and \\textsc{DEIMOS} shape catalogues, we employed the same suite of simulations for both calibrations.\n\nThe shape measurement bias depends on the size, S/N, radial surface brightness profile and ellipticity of the galaxy, as well as the observing conditions. Of these, the size and S/N are the most relevant, and we use these to capture the dependence of the bias for our set of simulated galaxies. Rather than the intrinsic size of the galaxy, we use a proxy for how well it is resolved: $R$ quantifies the relative size of the PSF compared to the size of the galaxy. Here, we adopt two slightly different definitions, depending on the shape algorithm employed. For \\textsc{DEIMOS} we use\n\\begin{equation}\n    R^\\mathrm{DEIMOS} = 1 - \\frac{T^\\mathrm{PSF}}{T^\\mathrm{gal}} \\ ,\n\\end{equation}\nwhere $T^\\mathrm{PSF} = \\tens{Q}^\\mathrm{PSF}_{20} + \\tens{Q}^\\mathrm{PSF}_{02}$ and $T^\\mathrm{gal} = \\tens{Q}^{\\vec{*} \n\\mathrm{gal}}_{20} + \\tens{Q}^{\\vec{*} \\mathrm{gal}}_{02}$, where $\\tens{Q}_{ij}^{\\vec{*}\\mathrm{gal}}$ are the unweighted moments of the PSF-convolved surface brightness profile (see Eqs. \\ref{eq:G*_ij} and \\ref{eq:unweighted_moments}). \nIn the case of $lens$fit we use\n\\begin{equation}\n    R^{lens\\mathrm{fit}} = 1 -  \\frac{r^2_\\mathrm{PSF}}{\\left( r^2_\\mathrm{ab} + r^2_\\mathrm{PSF} \\right)} \\ ,\n\\end{equation}\nwhere $r^2_\\mathrm{PSF} = \\sqrt{P_{11} P_{22} - P_{12}^2}$ and $r_\\mathrm{ab} = r_{\\rm e} \\sqrt{q}$. Here, $P_{ij}$ are the $lens$fit PSF weighted quadrupole moments \\citep[see Eq. (2) in ][]{Giblin2021}, measured with a circular Gaussian function of size $2.5$ pixels; $r_{\\rm e}$ is the half-light radius measured along the major axis of the best-fit elliptical profile by $lens$fit, which is an estimate of the true galaxy size before PSF-convolution, while $q$ is the axis ratio, such that $r_\\mathrm{ab}$ is the azimuthally averaged size of the galaxy. As we can see, $R$ can in practice only assume values between 0 and 1, where 1 corresponds to galaxies with sizes that are much larger than the PSF.\n\nWe evaluate the multiplicative bias $m$ in bins of S/N and $R$ that contain an equal number of galaxies and the error bars are computed using 500 bootstrap realisations. The resulting biases are presented in Fig. \\ref{fig:mbias_gals_par} for both \\textit{lens}fit\\ and \\textsc{DEIMOS}. \nWe find that the two components $\\epsilon_{1,2}$ show similar dependencies, and we, therefore, calibrate the bias for the two components jointly. The additive bias for both components is consistent with zero, and thus we do not consider it further in our calibration. \n\nFor both $m(\\mathrm{S/N})$ and $m(R)$, we find that \\textit{lens}fit\\ has a small bias and thus also our correction is small; in general, it performs better than \\textsc{DEIMOS} for poorly resolved galaxies and low S/N. It is, however, prohibitively slow when measuring shapes for large galaxies, limiting the lensfit sample to galaxies with $m_r > 20$. In contrast, \\textsc{DEIMOS} shows a large bias for low values of $R$: the galaxy size correlates with its ellipticity, and we find that removing the highly elliptical galaxies significantly reduces the bias. However, once we calibrate the shapes of those galaxies, we recover a very similar signal for the full shape sample and the one cut in ellipticity. Similarly, we have also tested that adding inverse-variance weights to account for these noisy galaxies does not significantly improve our signal. This motivates our choice to keep all galaxies in our sample and not to introduce additional weighting; we assume that the measurements are dominated by shape noise only.\n\nWe can see that $m(R)$ for both \\textsc{DEIMOS} and \\textit{lens}fit\\ is well described by a polynomial curve, which we truncate at degree 3 and 4, respectively, while $m(\\mathrm{S/N})$ is well described by the expansion: $d(\\mathrm{S/N}) = d_1/\\sqrt{\\mathrm{S/N}} + d_2/(\\mathrm{S/N})$. We combine the two individual bias dependencies into a single bias surface as detailed in Appendix~\\ref{A:mbias_calibration}. The specific functional forms for the two shape methods differ to better adapt the surface to our observed bias. We use these empirical relations to infer the $m$-bias associated with each galaxy, given its S/N and $R$. \n\nTo ensure that our empirical correction performs well on our sample, we select sets of galaxies from the image simulations that resemble our LRG samples by reproducing the observed distributions in S/N and $R$. We measure the residual biases for these samples, defined as the difference in the estimated $m$-bias (inferred using our model for the bias) and the bias\nmeasured directly from the simulations for the given set of galaxies. For the \\textsc{DEIMOS} shape method, we find an average residual of $-0.002 \\pm 0.007$\nfor the \\texttt{dense}-like sample, while this is $-0.002 \\pm 0.008$ for the \\texttt{luminous}-like sample. Similarly, in the case of \\textit{lens}fit\\, the residuals for the \\texttt{luminous}-like and \\texttt{dense}-like galaxies are, respectively, $-0.0014 \\pm 0.0013$ and $-0.0019 \\pm 0.0020$. As we will see later, this is much smaller than the uncertainty in the IA measurements: the average bias introduced by the shape measurement process is subdominant and does not affect our best estimate of the IA amplitude.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{plots/e1_DEIMOS_vs_lensfit_tstudent_and_norm.pdf}\n\\caption{Histogram of the difference of the $\\epsilon_1$ component of the ellipticity measured by the two shape measurement algorithms, \\textit{lens}fit\\ and \\textsc{DEIMOS}, on a common sub-sample of galaxies, after applying the $m-$bias correction as described in the text. The $\\epsilon_2$ component shows the same behaviour. The distribution is more peaked than a Gaussian (red dashed line) and is best described by a Student's $t-$distribution with $\\nu=4.3$, and a width $\\sigma=0.08$ with zero mean (black solid line).}\n\\label{fig:e1_lensfit_vs_DEIMOS}\n\\end{figure}\n\nThe LRGs are relatively bright and we thus expect the shape measurements to be shape noise-dominated. This also implies that the \\textsc{DEIMOS} and \\textit{lens}fit\\ measurements are correlated. To quantify this, we show the distribution of the difference between the $m-$corrected ellipticities measured by the two algorithms in Fig. \\ref{fig:e1_lensfit_vs_DEIMOS}. The distribution is more peaked than a Gaussian, and well described by a Student's $t-$distribution centred on zero, with $\\nu=4.30$ (degrees of freedom) and with scale parameter $\\sigma=0.08$. This is to be compared to the intrinsic ellipticity of galaxies, which is about $\\epsilon_\\mathrm{rms} = 0.12$ based on \\textsc{DEIMOS} measurements for galaxies with apparent magnitude $m_r<20$. It is interesting to note that our sample is considerably rounder than a typical cosmic shear sample, as expected for an LRG sample \\citep[see for example][]{vanUitert2012}; this implies that it might be affected differently by a weighting scheme in a lensing analysis. The differences between the \\textsc{DEIMOS} and \\textit{lens}fit\\ measurements are caused by differences in how each method deals with noise in the images.\n\n\\section{Correlation function measurements} \\label{sec:correlation_function_measurements}\n\nWe measure the IA signal using the two-points statistic $w_{\\rm{g+}}$, defined as the projection along the line-of-sight of the cross-correlation between galaxy positions and galaxy shapes. It measures the tendency of galaxies to point in the direction of another galaxy as a function of their comoving transverse separation, $r_{\\rm p}$, and comoving line-of-sight separation, $\\Pi$. To quantify the alignment signal in our data, we employ the estimator presented in \\citet{Mandelbaum2006a}\\footnote{Instead of normalising by $R_{\\rm S} R_{\\rm D}$, we actually normalise by the density - randoms vs. shapes pair count, $R_{\\rm D} D_{\\rm S}$. This significantly speeds up the computation and has been tested to have negligible impact \\citep{Johnston2019}.},\n\\begin{equation}\n    \\hat{\\xi}_{g+} (r_{\\rm p},\\Pi) = \\frac{S_+D-S_+R_{\\rm D}}{R_{\\rm S} R_{\\rm D}},\n\\end{equation}\nwhere $R_{\\rm D}$ and $R_{\\rm S}$ are catalogues of random points designed to reproduce the galaxy distribution of the density and shape samples, respectively. We indicate with $D$ the density sample that provides the galaxy positions, while $S_+$ is the shape sample, such that the quantity\n\\begin{equation}\n    S_+D = \\sum_{i \\neq j} \\gamma_+(i|j),\n\\end{equation}\ngives us the tangential shear component of the galaxy pair $(i,j)$, $\\gamma_+(i|j)$, where $i$ is extracted from the shape sample and $j$ from the density sample. $\\gamma_+$, in turn, is defined as\n\\begin{equation}\n    \\gamma_{+} (i|j) = \\frac{1}{\\mathcal{R}} \\Re \\left[\\epsilon_i \\exp(-2i\\phi_{ij} )\\right],\n\\end{equation}\nwhere $\\Re$ denotes the real part; $\\epsilon_i$ is the complex ellipticity associated with the galaxy $i$, $\\epsilon_i = \\epsilon_{1,i} + i \\epsilon_{2,i}$, whose components 1,2 are measured by the shape measurement algorithms presented in Sect. \\ref{sec:shape_measurements}; $\\phi_{ij}$ is the polar angle of the vector that connects the galaxy pair; $\\mathcal{R}=\\partial\\epsilon/\\partial\\gamma$ is the shear responsivity and it quantifies by how much the ellipticity changes when a shear is applied: for an ensemble of sources,  $\\mathcal{R} = 1 - \\epsilon^2_\\mathrm{rms}$. \n\n\nThe galaxy clustering signal is computed with the standard estimator \\citep{Landy&Szalay1993},\n\\begin{equation}\n    \\hat{\\xi}_\\mathrm{gg} (r_{\\rm p}, \\Pi) = \\frac{DD - 2 DR_D - R_D R_D}{R_D R_D} \\ .\n\\end{equation}{}\n\nTo measure our clustering and IA signals, we use uniform random samples that reproduce the KiDS footprint, accounting for the masked regions; to these we assign redshifts randomly extracted from the galaxy unconditional photometric redshift distributions. For each sample, we construct the random sample to match their redshift distribution.\n\nTo account for the spatial variation in the survey systematics, we apply weights to the galaxies when computing the signal, as discussed in \\citet[][]{Vakili2020}. These weights are designed to remove the systematic-induced variation in the galaxy number density across the survey footprint. For a detailed discussion of how the weights are generated and tested, we refer to Sect.~4 in \\citet[][]{Vakili2020}. To capture the variation in the survey systematics along the line-of-sight, we split each sample into three redshift bins and assign the weights to those sub-samples. We tested that this procedure does not induce a correlation between the galaxy weights and the redshifts themselves. We have also verified that the impact of the weights is very small and can be neglected when considering the split in luminosity of the samples (see Sect.~\\ref{subsec:luminosity_dependence}). We apply such weights to both the density and shape samples.\n\nIn this work, we measure the clustering and IA signals using an updated version of the pipeline presented in \\citet[]{Johnston2019}, which makes use of the publicly available software \\textsc{Treecorr} \\citep[][]{Jarvis2004treecorr}\\footnote{\\url{https://github.com/rmjarvis/TreeCorr}} for clustering correlations. $\\xi_{\\rm{g+}}$  and $\\xi_{\\rm{gg}}$  are then projected by integrating over the line-of-sight component of the comoving separation, $\\Pi$,\n\\begin{equation} \\label{eq:w_estimator}\n    \\hat{w}_{\\mathrm{g} i} (r_{\\rm p}) = \\int_{- \\Pi_\\mathrm{max}}^{\\Pi_\\mathrm{max}} \\mathrm{d} \\Pi \\ \\hat{\\xi}_{\\mathrm{g} i}(r_{\\rm p}, \\Pi) \\quad i=\\{+, g \\} \\ .\n\\end{equation}\n\nThe largest scales probed in this analysis are limited by the effective survey area ($\\sim777$ deg$^2$). We set a maximum transverse separation of $60 \\,h^{-1}{\\rm{Mpc}}$ and measure the signal in 10 logarithmically spaced bins, from $r_{\\rm p, min}=0.2 \\,h^{-1}{\\rm{Mpc}}$. \n\nWe perform the measurements for three different setups: we adopt $\\Pi_\\mathrm{max}=120 \\,h^{-1}{\\rm{Mpc}}$ as the fiducial case, but repeat the analysis for $\\Pi_\\mathrm{max}= 90 \\,h^{-1}{\\rm{Mpc}}$ and $\\Pi_\\mathrm{max}= 180 \\,h^{-1}{\\rm{Mpc}}$ (see Appendix~\\ref{A:systematic_tests}). We always bin our galaxies in equally spaced bins with $\\Delta \\Pi = 10 \\,h^{-1}{\\rm{Mpc}}$. We observe an extended signal to $\\Pi>180 \\,h^{-1}{\\rm{Mpc}}$, but the signal is comparable to the noise at those distances.\n\nOur choice of $\\Pi_\\mathrm{max}$ is conservative since the uncertainties in the photometric redshifts are $\\sigma_z<0.02(1+z)$ for both the \\texttt{dense} and \\texttt{luminous}\\ samples \\citep{Vakili2020}, and if we choose $\\Pi_\\mathrm{max}$ based on the $1\\sigma$ uncertainty in the photometric redshifts \\citep{Joachimi2011b}, we could potentially reduce $\\Pi_\\mathrm{max}$  to $70 \\,h^{-1}{\\rm{Mpc}}$. However, this might be too optimistic given that the error on $\\sigma_z$ increases with redshift.\nThe choice of $\\Pi_\\mathrm{max}$ is motivated by two opposite necessities: to maximise the S/N, we want to minimise the amount of signal that we discard, whilst we also want to avoid adding uncorrelated pairs that would increase the noise. To find the best balance, we calculate the S/N of our signal as a function of $(r_{\\rm p}, \\Pi)$ by dividing the measured $w_\\mathrm{gg}(r_{\\rm p},\\Pi)$  by the root-diagonal of the jackknife covariance. \nWe truncate at $\\Pi_\\mathrm{max}$ based on the 10 $\\sigma$ detection, which roughly corresponds to $\\Pi_\\mathrm{max} = 120 \\,h^{-1}{\\rm{Mpc}}$. In addition to these considerations, there is a further motivation to limit the integral to modest line-of-sight separations: as discussed in Appendix~\\ref{A:lensing_contamination}, the contamination from galaxy-galaxy lensing has a shallower dependence on the line-of-sight separation; as we move along the $\\Pi$ direction, we see an increase in the contamination with a mild increase in the IA signal, until lensing dominates.\n\nThe error bars are computed via a delete-one jackknife re-sampling of the observed volume. The covariance matrix is constructed as \n\\begin{equation}\n    \\mathrm{Cov}_\\mathrm{jack.} = \\frac{N-1}{N} \\sum_{\\alpha = 1}^{N} (w^{\\alpha} - \\bar{w}) (w^{\\alpha} - \\bar{w})^\\top ,  \n\\end{equation}\nwhere $w^{\\alpha}$ is the signal measured from jackknife sample $\\alpha$, while $\\bar{w}$ is the average over $N$ samples; $\\top$ denotes the transpose of the vector.\n\nThe number of regions $N$ is ultimately set by the size of the survey and the scales we aim to probe. A maximum value of $r_{\\rm p}=60 \\,h^{-1}{\\rm{Mpc}}$ corresponds to an angular separation of $\\sim 8$ degrees (\\texttt{dense}\\ sample) and $\\sim 6 $ degrees (\\texttt{luminous}\\ sample) at the lowest redshifts probed in the analysis. However, to increase the number of jackknife regions, we decide to set the minimum angular scale to 5 degrees, which strictly satisfies our requirement only for $z\\gtrsim 0.2$. This is motivated by the fact that the majority of our galaxies are at high redshift and hence only $\\lesssim 5\\%$ of our galaxies have unreliable error estimates in the last $r_{\\rm p}-$bin. The total number of jackknife regions that we are able to obtain for our samples is $N = 37$. \nWe correct our inverse covariance matrices, which enter into our likelihood estimations,  as recommended in \\citet[][]{Hartlap2007}: because of the presence of noise, the inverse of a covariance matrix obtained from a finite number of jackknife (or bootstrap) realisations is a biased estimator of the true inverse covariance matrix. \n\n\n\n\n\\section{Modelling}\n\\label{sec:theoretical_framework}\n\nThe linear alignment model \\citep{Catelan2001, Hirata2004} predicts a linear relation between the contribution to the shear induced by IA and the quadrupole of the gravitational field responsible of the tidal effect. This can be expressed as\n\\begin{equation}\\label{eq:intrinsic_shear_equation}\n    \\gamma^{\\rm I} = (\\gamma^{\\rm I}_+, \\gamma^{\\rm I}_\\times) = -\\frac{C_1}{4 \\pi G} (\\partial_x^2 + \\partial_y^2, \\partial_x \\partial_y) \\Phi_{\\rm p} \\ , \n\\end{equation}\nwhere the partial derivatives are with respect to comoving coordinates and provide the tangential and cross components of the shear with respect to the $x$-axis; $\\Phi_{\\rm p}$ is the gravitational potential at the moment of galaxy formation, assumed to take place during the matter-dominated era \\citep{Catelan2001}; $C_1$ is a normalisation constant and $G$ is the gravitational constant. \n\nUsing Eq. (\\ref{eq:intrinsic_shear_equation}), by correlating the intrinsic shear with itself or with the matter density field $\\delta$, we can construct the relevant equations for the IA correlation functions \\citep{Hirata2004}. In Fourier space, the matter density-shear power spectrum $(\\delta {\\rm I})$ becomes\n\\begin{equation}\\label{eq:LA_deltaI}\n    P^{\\mathrm{LA}}_{\\delta \\mathrm{I}}(k,z) = A_\\mathrm{IA} C_1 \\rho_{\\rm c} \\frac{\\Omega_m}{D(z)} P_{\\delta \\delta}^{\\mathrm{lin}}(k,z) \\ .\n\\end{equation}\nHere, $D(z)$ is the linear growth factor, normalised to unity at $z=0$, $\\rho_{\\rm c}$ is the critical density of the Universe today, and $P_{\\delta \\delta}^{\\mathrm{lin}}$ is the linear matter power spectrum. We set $C_1=5\\times10^{-14} h^{-2} M_\\odot^{-1} \\mathrm{Mpc}^3$  based on the IA amplitude measured at low redshifts using SuperCOSMOS \\citep{Brown2002}, which is the standard normalisation for IA power spectra. \n\nGalaxies are biased tracers of the matter density field, and at large scales this relation is linear, $\\delta_{\\rm g} \\sim b_{\\rm g} \\delta$. We can thus relate the galaxy position--intrinsic shear power spectrum to the matter density--intrinsic shear power spectrum via the galaxy bias $b_{\\rm g}$:\n\\begin{equation}\\label{eq:LA_g+}\n    P^{\\mathrm{LA}}_{\\mathrm{g I}}(k,z) = b_{\\rm g}  P^{\\mathrm{LA}}_{\\delta \\mathrm{I}}(k,z) \\ ,\n\\end{equation}\nwhich is the power spectrum of interest for our analysis.\n\nA successful modification of the LA model replaces the linear matter power spectrum in Eq.~\\ref{eq:LA_deltaI} with the non-linear one, to account for the non-linearities arising at intermediate scales \\citep{BridleKing2007}. This so-called NLA model was succesfully employed in a number of studies \\citep[e.g.][]{Blazek2011, Joachimi2011b} and here we follow the same approach to model our signal. More sophisticated treatments of the IA signal, which include the modelling of the mildly or fully non-linear scales, have been developed in the last decade \\citep[][]{SchneiderBridle2010, Blazek2019TATT, fortuna2020halo}, but given the scales probed in our analysis (see Sect.~\\ref{subsec:likelihoods}) and the homogeneous characteristics of the galaxy population studied, the NLA model provides a sufficient description for this work. Unless stated otherwise, in the following we always assume the NLA model as our reference choice. To generate the linear matter power spectrum we use \\textsc{CAMB}\\footnote{https://camb.info} \\citep{Lewis2000, Lewis:2002ah}, while the non-linear modifications are computed using \\textsc{Halofit} \\citep{Smith2002} with the implementation presented in \\citet{ Takahashi2012halofit}. In the rest of the paper, we simply refer to the non-linear matter power spectrum as $P_{\\delta \\delta}(k,z)$.\n\n\\subsection{Incorporating the photometric redshift uncertainty into the model}\n\nThe use of photometric redshifts results in an uncertainty in the estimated distance of the galaxies, which has to be included in the model. In particular, if we express the correlation function $\\xi_\\mathrm{gI}$ in terms of the two components of the galaxy separation vector $\\mathbf{r}$, $(r_{\\rm p}, \\Pi)$, we can map the redshift probability distribution into the probability that the true values of $r_{\\rm p}$ and $\\Pi$ correspond to their photometric estimates. Here, we follow the approach derived in \\citet[]{Joachimi2011b} and use their approximated expression,\n\\begin{equation} \\label{eq:xi_gI}\n    \\xi^\\mathrm{ph}_\\mathrm{gI}(\\bar{r}_{\\rm p}, \\bar{\\Pi}, \\bar{z}_{\\rm m}) = \\int \\frac{\\mathrm{d} \\ell \\ell }{2 \\pi} J_2 \\left(\\ell \\theta (\\bar{r}_{\\rm p}, \\bar{z}_{\\rm m}) \\right)C_\\mathrm{gI} \\left( \\ell; \\bar{z}_1 (\\bar{z}_{\\rm m}, \\bar{\\Pi}), \\bar{z}_2(\\bar{z}_{\\rm m}, \\bar{\\Pi}) \\right) \\ .\n\\end{equation}{}\n\nThe observables are: $\\bar{z}_1$ and $\\bar{z}_2$, the photometric redshift estimates of the pair of galaxies for which we are measuring the correlation, and their angular separation $\\theta$. These can be related to $(\\bar{r}_{\\rm p}, \\bar{\\Pi}, \\bar{z}_{\\rm m})$, through the approximate relations\n\\begin{align} \\label{eq:coordinate_transformation}\n    z_{\\rm m} & = \\frac{1}{2} (z_1 + z_2) \\ , \\\\\n    r_{\\rm p} & \\approx \\theta \\chi (z_{\\rm m}) \\ , \\\\\n    \\Pi & \\approx \\frac{c}{H(z_{\\rm m})} (z_2 - z_1) \\ , \n\\end{align}{}\nwhere $\\chi(z_{\\rm m})$ and $H(z_{\\rm m})$ are, respectively, the comoving distance and the Hubble parameter at redshift $z_m$, and $c$ is the speed of light.\n\nThe conditional redshift probability distributions are incorporated into the angular power spectrum $C_\\mathrm{gI}$, which can be expressed in terms of the three-dimensional power spectrum $P_\\mathrm{gI}(k,z)$,\n\\begin{multline}\\label{eq:C_gI}\n    C_\\mathrm{gI}(\\ell, \\bar{z}_1, \\bar{z}_2) = \\int_0^{\\chi_\\mathrm{hor}} \\mathrm{d} \\chi' \\frac{p_n(\\chi'|\\chi(\\bar{z}_1)) p_{\\epsilon}(\\chi'|\\chi(\\bar{z}_2))}{\\chi'^2} \\\\  \\times P_\\mathrm{gI} \\left( \\frac{\\ell + 1/2}{\\chi'}, z (\\chi') \\right)\n\\end{multline}\nwhere we have implicitly assumed the flat-sky and Limber approximations, and $n$ and  $\\epsilon$ indicate the density and shape sample respectively. $p(\\chi'|\\chi)$ are the conditional comoving distance probability distributions, which are related to the redshift distributions via $p(\\chi'|\\chi) \\mathrm{d} \\chi = p(z|\\bar{z}) \\mathrm{d} z$. When computing our predictions, we bin our photometric data and compute the corresponding $p(z|\\bar{z}) \\equiv p(z_\\mathrm{spec}|z_\\mathrm{phot})$ per each bin; $z_1$ and $z_2$ in Eq. (\\ref{eq:coordinate_transformation}) corresponds to the mean values of the  probability distribution with $z_1$ being the mean of the i-th bin and $z_2$ of the j-th bin. In Appendix \\ref{A:redshift_distributions} we show the redshift distributions entering our analysis. We refer the interested reader to appendices A.2 and A.3 in \\citet[]{Joachimi2011b} for the full derivation of equation \\ref{eq:C_gI}. The exact same formalism can then be applied to the clustering signal, where $C_\\mathrm{gI} \\to C_\\mathrm{gg}$, $J_2 \\to J_0$ and the redshift distributions are those corresponding to the density sample.\n\n\nThe projected correlation functions $w_{\\rm{g+}}$ and $w_{gg}$ can then be obtained as:\n\\begin{equation}\n    w_{\\mathrm{g+}}(r_{\\rm p}) = \\int \\mathrm{d} \\bar{\\Pi} \\ \\int \\mathrm{d} z_\\mathrm{m} \\mathcal{W}(\\bar{z}_\\mathrm{m})  \\xi^\\mathrm{ph}_\\mathrm{gI} (\\bar{r}_{\\rm p}, \\bar{\\Pi}, \\bar{z}_\\mathrm{m}) \n\\end{equation}\nand\n\\begin{equation}\n    w_\\mathrm{gg}(r_{\\rm p}) = \\int \\mathrm{d} \\bar{\\Pi} \\ \\int \\mathrm{d} z_\\mathrm{m} \\mathcal{W}(\\bar{z}_\\mathrm{m})  \\xi^\\mathrm{ph}_\\mathrm{gg} (\\bar{r}_{\\rm p}, \\bar{\\Pi}, \\bar{z}_\\mathrm{m}) \\ ,\n\\end{equation}\nwhere the redshift window function $\\mathcal{W}(z)$ is defined as \\citep{Mandelbaum2011WiggleZ}:\n\\begin{equation}\n    \\mathcal{W}(z) = \\frac{p_i(z) p_j(z)}{\\chi^2(z) \\mathrm{d} \\chi / \\mathrm{d} z} \\left[ \\int \\mathrm{d} z \\frac{p_i(z) p_j(z)}{\\chi^2(z) \\mathrm{d} \\chi / \\mathrm{d} z} \\right]^{-1} \\ ,\n\\end{equation}\nwhere $p_{i,j}(z)$ with ${i,j} \\in {S,D}$ are now the unconditional redshift distributions for the shape and density samples, and $\\chi(z)$ is the comoving distance to redshift $z$. \n\n\n\\subsection{Contamination to the signal} \\label{sec:contaminations}\n\nAll possible two-point correlations between galaxy shapes and positions contribute to the estimator in Eq. (\\ref{eq:w_estimator}). Following the notation in \\cite{JoachimiBridle2010J}, here we consider: the correlation between the intrinsic shear and the galaxy position (g+), which is the quantity we aim to constrain; but also the correlation between gravitational shear and galaxy position, sourced by the galaxy lensing of a background galaxy by a foreground galaxy (gG); and the apparent modification of the galaxy number counts due to the effect of lensing magnification, which affects both the correlations with the intrinsic shear and the gravitational shear (mI and mG).\n\nAmong these effects, galaxy-galaxy lensing is the main  contaminant to our signal. While IA requires physically close galaxies, galaxy-galaxy lensing occurs between galaxies at different redshifts. This implies that the level of contamination depends on our ability to select close pairs of galaxies, which ultimately depends on the photometric redshift precision.\nFor this reason, the width and the tails of the redshift distributions play an important role in the amount of contamination. Since our $p(z^\\mathrm{spec}|z^\\mathrm{phot})$ are quite narrow (see Appendix \\ref{A:redshift_distributions}) we do not expect this to be a major effect in our data. Nevertheless, we fully model both lensing and magnification effects, and account for them when interpreting the signal. We note that the sign of the gI and gG terms are opposite, such that adding the lensing to the model allows us to remove its suppressing contribution and capture the true IA signal.\n\n\nIt is convenient to write the various correlations in terms of the projected angular power spectra: indicating with $n$ the density sample (that provides the galaxy positions) and with $\\epsilon$ the shape sample, we have\n\\begin{equation} \\label{eq:C_n_eps}\n    C^{(ij)}_{n \\epsilon}(\\ell) =  C^{(ij)}_\\mathrm{gI}(\\ell) +  C^{(ij)}_\\mathrm{gG}(\\ell) +  C^{(ij)}_\\mathrm{mI}(\\ell) +  C^{(ij)}_\\mathrm{mG}(\\ell) \\ ,\n\\end{equation}\nwhere, in a flat cosmology, these read\n\\begin{equation}\n    C_\\mathrm{gG}^{(ij)}(\\ell) = b_g \\int_0^{\\chi_\\mathrm{hor}} \\mathrm{d} \\chi \\frac{p_n^{(i)}(\\chi) q_\\epsilon^{(j)}(\\chi)}{\\chi^2} P_{\\delta \\delta} \\left( \\frac{\\ell + 1/2}{\\chi}, \\chi \\right)\\ ,\n\\end{equation}\n\\begin{equation} \\label{eq:C_mI}\n    C^{(ij)}_\\mathrm{mI}(\\ell) = 2 (\\alpha^{(i)} - 1) C^{(ij)}_\\mathrm{IG}(\\ell),\n\\end{equation}\nand\n\\begin{equation} \\label{eq:C_mG}\n    C^{(ij)}_\\mathrm{mG}(\\ell) = 2 (\\alpha^{(i)} - 1) C^{(ij)}_\\mathrm{GG}(\\ell) \\ .\n\\end{equation}\nHere $\\alpha^{(i)}$ is the slope of the faint-end logarithmic luminosity function\\footnote{Formally, the magnification of the \\textit{lens}fit\\ sample is also affected by the slope of the luminosity function at the bright end of $m_r=20$. We ignore such complexity: we find magnification to be a subdominant effect for the faint distant galaxies, thus the contribution of low-redshift galaxies is expected to be negligible for our analysis.}.\nThe lensing weight function, $q_X$, $X \\in \\{n, \\epsilon\\}$ is defined as\n\\begin{equation}\n    q_X(\\chi) = \\frac{3 H_0^2 \\Omega_m}{2 c^2} \\frac{\\chi}{a(\\chi)} \\int_0^{\\chi_\\mathrm{hor}} \\mathrm{d} \\chi' p_X(\\chi') \\frac{\\chi' - \\chi}{\\chi'} \\ .\n\\end{equation}\n$C^{(ij)}_\\mathrm{IG}$ is the intrinsic-shear power spectrum. It models the correlation between the shearing of source galaxies by a foreground matter overdensity and the simultaneous IA of galaxies located near that overdensity:\n\\begin{equation} \\label{eq:C_IG}\n    C_\\mathrm{IG}^{(ij)}(\\ell) = \\int_0^{\\chi_\\mathrm{hor}} \\mathrm{d} \\chi \\frac{p_n^{(i)}(\\chi) q_\\epsilon^{(j)}(\\chi)}{\\chi^2} P_{\\delta {\\rm I}} \\left( \\frac{\\ell + 1/2}{\\chi}, \\chi \\right)\\ ;\n\\end{equation}\n$C^{(ij)}_\\mathrm{GG}$ is instead defined as:\n\\begin{equation} \\label{eq:C_GG}\n    C_\\mathrm{GG}^{(ij)}(\\ell) = \\int_0^{\\chi_\\mathrm{hor}} \\mathrm{d} \\chi \\frac{q_n^{(i)}(\\chi) q_\\epsilon^{(j)}(\\chi)}{\\chi^2} P_{\\delta \\delta} \\left( \\frac{\\ell + 1/2}{\\chi}, \\chi \\right)\\ .\n\\end{equation}\nWe note that with respect to the usual shear power spectrum, we require here that one of the samples refers to the density sample, $n$.\n\nTo account for these sources of contamination in the fit, we replace $\\xi_\\mathrm{gI}$ with $\\xi_\\mathrm{n \\epsilon}$, which can be obtained from Eq. (\\ref{eq:C_n_eps}). The prediction for $\\xi^\\mathrm{obs}$ is then used to constrain the measured signal $\\hat{w}_\\mathrm{g+}$. In Appendix~\\ref{A:lensing_contamination} we expand further on the impact of lensing on our measurements, while in Appendix~\\ref{A:magnification_contamination} we describe our strategy to measure the values of $\\alpha^{(i)}$ in our data.\n\n\n\n\n\n\\subsection{Likelihoods} \n\\label{subsec:likelihoods}\n\nWe perform the fits to the data using a Markov Chain Monte Carlo (MCMC)\nthat samples the multi-dimensional parameter posterior distributions and finds the set of parameters that maximise the likelihood. We assume a Gaussian likelihood of the form $\\mathcal{L} \\propto \\exp(-\\chi^2/2)$, where\n\\begin{equation}\n    \\chi^2 = \\chi^2_{w_{\\rm gg}} + \\chi^2_{w_{\\rm{g+}}}\n\\end{equation}\nand we simultaneously fit for the galaxy bias, $b_{\\rm g}$ and the IA amplitude, $A_{\\rm IA}$.\n\nTo correct for the effects of a partial-sky survey window, we also introduce an integral constraint, IC, when modelling the clustering, signal,\n\\begin{equation}\n    w_\\mathrm{gg} \\to w_\\mathrm{gg} + \\mathrm{IC} \\ .\n\\end{equation}\nThis term, which becomes important only on large scales, has the function of capturing the bias that arises from a mis-estimation of the global mean density \\citep{Roche&Eales1999}. We treat this term as a nuisance parameter, such that our parameter vector reads\n\\begin{equation}\n    \\lambda = \\{ b_g, A_\\mathrm{IA}; \\mathrm{IC} \\} \\ .\n\\end{equation}\n\nWe limit our fits to the quasi-linear regime, $r_{\\rm p}>6 \\,h^{-1}{\\rm{Mpc}}$, to ensure that the linear bias approximation is satisfied and the IA signal is well described by the NLA model. To perform our fits, we make use of the \\textsc{Emcee} \\citep{Foreman-Mackey2013emcee} package as implemented in the cosmology software \\textsc{CosmoSIS}\\footnote{\\url{http://bitbucket.org/joezuntz/cosmosis/wiki/Home}} \\citep{Zuntz2015CosmoSIS}. When analysing the chains, we exclude the first 30$\\%$ of samples for a burn-in phase.\n\n\n\n\\section{Results}\n\\label{sec:results}\n\n\\begin{figure*}\n\\centering\n\\includegraphics[width=0.9\\columnwidth]{plots/DEIMOS_vs_lensfit_wgp_lum_and_dense_sample_Pimax120Mpch_bestfit.pdf}\n\\includegraphics[width=0.9\\columnwidth]{plots/wgg_lum_and_denses_ample_Pimax120Mpch_bestfit.pdf}\n\\caption{Projected correlation functions (IA and clustering signal) measured in this work and the best-fit curve predicted by our model. \\textit{Left:} Projected position-shape correlation function, $w_{\\rm{g+}}$, measured for our \\texttt{luminous}\\ (top panel) and \\texttt{dense}\\ (bottom panel) samples. We show results for shapes measured with \\textsc{DEIMOS} (light squares) and \\textit{lens}fit\\ (dark triangles). The best-fit models to the data with $r_{\\rm p}>6 \\,h^{-1}{\\rm{Mpc}}$ (indicated by the vertical dashed line), are shown as well, with the same colour scheme (\\textsc{DEIMOS}: dash-dotted lines, \\textit{lens}fit: dashed lines). For clarity, the \\textit{lens}fit\\ results have been slightly offset horizontally. \\textit{Right:} Projected clustering signal, $w_\\mathrm{gg}$, of the \\texttt{dense}\\ and \\texttt{luminous}\\ samples. The dot-dashed lines corresponds to the best-fit models.  As we do not include a scale-dependent bias in our model, the mismatch between data and prediction at small scales is expected.}\n\\label{fig:wgp_wgg_DEIMOS_lensifit_lum_dense}\n\\end{figure*}\n\n\\begin{table*}\n\t\\caption{Properties of the individual galaxy samples used in our analysis and the corresponding best-fit galaxy bias ($b_g$) and IA amplitude ($A_\\mathrm{IA}$) as constrained by our model.}\n\t\\label{tab:galinfo}\n\t\\begin{center}\n\t\\begin{tabular}{lcccccccr}\n\t\t\\hline\n\t\t\\hline\n\t\tSamples & $\\langle z \\rangle$ & $N_{\\mathrm{D}}$ & $N_{\\mathrm{S}}$ & $[L_\\mathrm{min}, L_\\mathrm{max}]$ & $ \\langle L \\rangle/L_0$ & $b_\\mathrm{g}$ & $A_\\mathrm{IA}$ & $\\chi^2_\\mathrm{red}$\n\t\t\\\\\n\t\t\\hline\n\t\t\\textsc{DEIMOS} & & & & & & & & \\\\\n\t\t\\hline \n        \\texttt{dense}\\  & 0.44 & 173 445 & 152 832 & & 0.38 & $1.59^{+0.04}_{-0.04}$\n        & $3.69^{+0.66}_{-0.65}$\n        & 0.78\n        \\\\\n        \\texttt{luminous}\\  & 0.54 & 117 001 & 96 863 & & 0.64 & $2.06^{+0.04}_{-0.04}$ \n        & $4.03^{+0.81}_{-0.79}$\n        & 1.19 \n        \\\\\n\t\t\\hline \n        \\texttt{D1} & 0.41 & 173 445 & 39 108 & $[0.09, 1.13]$ & 0.21 & $1.60^{+0.04}_{-0.04}$ \n        & $3.02^{+1.53}_{-1.48}$\n        & 1.00\n        \\\\\n        \\texttt{D2} & 0.42 & 173 445 & 39 322 & [1.13, 1.43] & 0.27 & $1.60^{+0.04}_{-0.04}$\n        & $1.21^{+1.63}_{-1.64}$\n        & 0.91\n        \\\\\n        \\texttt{D3} & 0.43 & 173 445 & 39 229 & [1.43, 1.92] & 0.35 & $1.59^{+0.04}_{-0.04}$\n        & $4.11^{+1.48}_{-1.48}$\n        & 1.05\n        \\\\\n        \\texttt{D4} & 0.45 & 173 445 & 19 333 & [1.92, 2.81] & 0.49 & $1.59^{+0.04}_{-0.04}$\n        & $3.02^{+2.37}_{-2.33}$\n        & 1.52\n        \\\\\n        \\texttt{D5} & 0.45 & 173 445 & 19 235 & $\\geq 2.81$ & 0.89 & $1.59^{+0.04}_{-0.04}$ \n        & $8.39^{+1.04}_{-1.30}$ \n        & 0.47\n        \\\\\n        \\texttt{L1} & 0.53 & 117 001 & 48 588 & $[0.29, 2.66] $ & 0.46 & $2.06^{+0.04}_{-0.04}$\n        & $1.80^{+0.96}_{-0.95}$\n        & 1.17\n        \\\\\n        \\texttt{L2} & 0.55 & 117 001 & 24 208 & [2.66, 3.51] & 0.65 & $2.06^{+0.04}_{-0.04}$\n        & $4.95^{+1.24}_{-1.21}$\n        & 1.19\n        \\\\\n        \\texttt{L3} & 0.56 & 117 001 & 24 067 & $\\geq 3.51$ & 1.00 & $2.06^{+0.04}_{-0.04}$\n        & $5.71^{+1.57}_{-1.60}$ \n        & 2.03\n        \\\\\n\t\t\\hline\n\t\t\\textit{lens}fit\\ & & & & & & & \\\\ \n\t\t\\hline\n\t\t\\texttt{dense}\\  & 0.49 & 173 445 & 121 500 & & 0.33 & $1.60^{+0.04}_{-0.04}$\n\t\t& $4.94^{+1.24}_{-1.22}$ \n\t\t& 1.52\n\t\t\\\\\n        \\texttt{luminous}\\  & 0.63 & 117 001 & 84 785 & & 0.59 & $2.06^{+0.04}_{-0.04}$\n        & $2.95^{+1.49}_{-1.42}$\n        & 1.54\n        \\\\\n\t\t\\hline\n\t\t\\textsc{DEIMOS} + \\textit{lens}fit\\ & & & & & & & \\\\ \n\t\t\\hline\n        Z1 $(z \\leq 0.585)$ & 0.44 & 56 754 & 56 754 & & 0.63 & $2.01^{+0.06}_{-0.06}$\n        & $3.84^{+1.10}_{-1.06}$\n        & 0.22 \n        \\\\\n        Z2 $(z>0.585)$ & 0.70 & 57 613 & 57 613 & & 0.61 & $2.39^{+0.08}_{-0.08}$ \n        & $3.97^{+2.02}_{-2.04}$ \n        & 2.43 \n        \\\\\n        \\hline\n        \\end{tabular}\n  \\small\n    \\tablefoot{The galaxy properties are summarised by: the mean redshift, $\\langle z \\rangle$; the number of galaxies in the density (shape) sample, $N_\\mathrm{D}$ ($N_\\mathrm{S}$); the mean luminosity in terms of a pivot luminosity $L_0=4.6\\times10^{10} h^{-2} L_{\\odot}$; the bias, $b_\\mathrm{g}$. To compute the ratio $\\langle L \\rangle / L_0$, we only consider the galaxies in the corresponding shape sample. For our $L-$cuts sub-samples, we also provide the range in luminosity they probe, $[L_\\mathrm{min}, L_\\mathrm{max}]$, in units of $10^{10} h^{-2} L_{\\odot}$. Similarly, we provide in brackets the cut adopted to split our sample in two redshift bins. When cross-correlating different samples, $N_D$ refers to the density sample used in the correlation and the bias is the best-fit bias of the density tracer as obtained for that given measurement. All measurements are performed assuming $\\Pi_\\mathrm{max}=120 \\,h^{-1}{\\rm{Mpc}}$. \n    Since the best-fit parameters and the medians of the marginal posterior distributions are in agreement, we quote the marginal values, while the $\\chi^2$ refers to the maximum likelihood. In all cases, the degrees of freedom are 5; the $p-$values are all above 0.03, with the majority of them being in the range 0.3-0.7.}\n    \\end{center}\n\\end{table*}\n\nThe left panels in Fig.~\\ref{fig:wgp_wgg_DEIMOS_lensifit_lum_dense} show the measurements of the projected position-shape correlation function $w_{\\rm{g+}}$ for the \\texttt{luminous}\\ (top panel) and \\texttt{dense}\\ (bottom panel) samples. We present results for both the \\textit{lens}fit\\ (dark green triangles) and \\textsc{DEIMOS} (light green squares) shape catalogues. As described in Sect.~\\ref{subsec:likelihoods}, we simultaneously fit the IA and the clustering signals. We show the resulting best-fit models to measurements with $r_{\\rm p}>6 \\,h^{-1}{\\rm{Mpc}}$ of $w_{\\rm{g+}}$ and $w_{\\rm gg}$ as solid lines in the figures. The estimates from the two shape measurement algorithms are fit independently, but given that the corresponding clustering signal is the same, here we only show the best-fit curve for the \\textsc{DEIMOS} fit. The clustering measurements use the full density samples, and thus do not rely on a successful shape measurement.\n\nWe observe similar signals for the \\textsc{DEIMOS} and \\textit{lens}fit\\ samples, with the \\textit{lens}fit\\ measurements having a lower S/N, because of the lack of shape measurements for galaxies with $m_r<20$. We note that we do not necessarily expect to observe the same signal, because \n\\textsc{DEIMOS} contains more bright, low-redshift galaxies, whereas the\n\\textit{lens}fit\\ sample includes fainter, distant galaxies (see Figs.~\\ref{fig:z_histo_shapes} and~\\ref{fig:mag_histo_dense_and_lum}). If the alignment signal depends on luminosity or redshift, the two shape samples would give different signals. In Appendix~\\ref{A:DEIMOS_lensfit_comparison} we restrict the comparison to the sample of galaxies with shape measurements from both methods, and find that the average difference \n$\\langle r_{\\rm p}\\Deltaw_{\\rm{g+}}\\rangle=0.003\\pm0.13$ is negligible, especially compared to the amplitude of the IA signal quantified as\n$\\langle r_{\\rm p}w_{\\rm{g+}}\\rangle=0.90\\pm0.17$ (\\textsc{DEIMOS} shapes; see Appendix~\\ref{A:DEIMOS_lensfit_comparison} for details).\n\nWe also show the models that provide the best-fit to the combined \n$w_\\mathrm{gg}$ and $w_{\\rm{g+}}$ measurements in Fig.~\\ref{fig:wgp_wgg_DEIMOS_lensifit_lum_dense}, and report the \nvalues for the bias $b_{\\rm g}$ and IA amplitude $A_{\\rm IA}$ in\nTable~\\ref{tab:galinfo}. The results for \\textsc{DEIMOS} and \\textit{lens}fit\\  are consistent.\n\nOur constraints on the galaxy bias of the \\texttt{dense}\\ and \\texttt{luminous}\\ samples are in broad agreement with the values presented in \\citet[][]{Vakili2020}: We find a larger bias for the \\texttt{luminous}\\ sample than for the \\texttt{dense}\\ one, as expected by its higher luminosity and the higher redshift baseline. \n\n\\subsection{Luminosity dependence} \\label{subsec:luminosity_dependence}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{plots/wgp_D1_D2_D3_D4_D5_bestfit.pdf}\n\\caption{Projected correlation function, $w_{\\rm{g+}}$, measured for the different cuts in luminosity of the \\textsc{DEIMOS} \\texttt{dense}\\ sample. The best-fit curves are plotted on top of the data points, and the fits are performed for $r_{\\rm p}> 6 \\,h^{-1}{\\rm{Mpc}}$. All but the yellow points have been slightly offset horizontally; to better visualise the goodness of fit, the corresponding best-fit curves have been displaced accordingly.}\n\\label{fig:wgp_Lcuts}\n\\end{figure}\n\nPrevious studies of LRGs \\citep{Joachimi2011b, Singh2015} have found a significant dependence of their IA signal with luminosity, with more luminous galaxies showing stronger alignments. On average our LRG sample probes somewhat lower luminosities than those earlier studies, but the overlap with these earlier works also enables a direct comparison. Thanks to the large range in luminosity it covers, the \\texttt{dense}\\ sample is particularly suited to explore the dependence  with luminosity. To do so, we use the \\textsc{DEIMOS} shape catalogue\\footnote{The internal cut at $m_r<20$ in \\textit{lens}fit\\ makes it less suitable for this analysis, as we have fewer galaxies at high luminosities.} and split the \\texttt{dense}\\ LRG galaxies in five sub-samples: \\texttt{D1}, \\texttt{D2}, and \\texttt{D3}, correspond to the lowest three quartiles in luminosity; the remaining two, \\texttt{D4} and \\texttt{D5}, are obtained by splitting the highest luminosity quartile into two equally sized samples. The motivation to split the quartile with the highest luminosities is that it encompasses a very large range in luminosity, which complicates the interpretation if the signal depends on luminosity (see below). Relevant details for the sub-samples are listed in Table~\\ref{tab:galinfo}. We keep the \\texttt{dense}\\ and \\texttt{luminous}\\ samples separate, in order to better isolate the effect of the luminosity dependence from any redshift evolution of the sample itself. For instance, as listed in Table \\ref{tab:galinfo}, the mean redshift of the sub-samples increases somewhat from \\texttt{D1} to \\texttt{D5}. \n\nWe cross-correlate the \\textsc{DEIMOS} shape catalogues for the individual sub-samples with the positions of galaxies in the full \\texttt{dense}\\ sample. In this way, we can disentangle the luminosity dependence of the IA signal from the luminosity dependence of the density tracer (brighter galaxies are typically found in denser environments). The measurements and the best-fit models are presented in Fig.~\\ref{fig:wgp_Lcuts}. In Table~\\ref{tab:galinfo} we list the best-fit values for the galaxy bias $b_{\\rm g}$ and IA amplitude $A_{\\rm IA}$, as well as the reduced $\\chi^2$, as before, using the measurements for $r_{\\rm p}>6 \\,h^{-1}{\\rm{Mpc}}$. We also show the measurements in Fig.~\\ref{fig:A_lum} as orange stars as a function of $L/L_0$, where $L_0=4.6 \\times 10^{10}h^{-2} L_{\\odot}$. \n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{plots/A_L_log_scaling_all_LRGs.pdf}\n\\caption{Luminosity dependence of the IA amplitude as measured by different observational studies \\citep[]{Joachimi2011b, Singh2015, Johnston2019, fortuna2020halo}; our new measurements on the LRG samples are shown as star markers. We provide horizontal error bars to indicate that the measurement is performed on a bin in luminosity, here plotted as the weighted standard deviation of the luminosity distribution of each sample, with the marker placed at the weighted mean. The solid (dashed) black line shows the median of the distribution of the MCMC sample associated with the double (single) power law; the shaded area corresponds to the $68\\%$ confidence region.\n}\n\\label{fig:A_lum}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{plots/corner_hod_pars_dodgerblue_vline.png}\n\\caption{Constraints on the double power law parameters described in equation ~\\ref{eq:double_powerlaw} by jointly fitting all the measurements in Fig.~\\ref{fig:A_lum}. The red crosses indicate the value of the parameters that maximise the likelihood, while the blue squares correspond to the medians.}\n\\label{fig:a_lum_mcmc}\n\\end{figure}\n\n\\begin{table*}\n\t\\caption{Best-fit parameters of the single and double power law fit on the measurements in Fig.~\\ref{fig:A_lum}.}\n\t\\label{tab:all_lum_fit_pars}\n\t\\begin{center}\n\t\\begin{tabular}{lcccccr}\n\t\t\\hline\n\t\t\\hline\n\t\tModel & $A_{\\beta}$ & $\\beta_1$ & $\\beta_2$ & $L_{\\rm break}$ & $\\chi^2$/dof & dof\\\\\n\t\t\\hline\n\t\tDouble power law & $3.28^{+2.41}_{-1.17}$ (2.01) & $0.26^{+0.42}_{-0.77}$ (-0.75) & $1.17^{+0.21}_{-0.17}$ (1.11) & $0.64^{+0.45}_{-0.24} L_0$ ($0.39 L_0$) & 1.36 (1.33) & 22\\\\\n        Single power law & $5.98^{+0.27}_{-0.27}$ (6.0) & $0.93^{+0.11}_{-0.10}$ (0.92) & - & $L_0$ & 1.61 (1.61) & 24\\\\\n        \\hline\n    \\end{tabular}\n    \\small\n    \\tablefoot{The listed values correspond to the medians of the marginal posterior distributions, and the associated errors correspond to the 16th and 84th percentiles, while in brackets we report the parameters that maximise the likelihood. The same scheme is adopted for the corresponding reduced $\\chi^2$. $L_{\\rm break}$ is the pivot luminosity that enters in the denominator of the power law argument. For convenience, the slope of the single power law model is here reported as $\\beta_1$.}\n    \\end{center}\n\\end{table*}\n\nWe repeat the same analysis for the \\texttt{luminous}\\ sample, which we divide in three bins, with a similar bin refining approach as for the \\texttt{dense}\\ sample (in this case \\texttt{L1} contains half of the \\texttt{luminous}\\ galaxies, while \\texttt{L2} and \\texttt{L3} the remaining quarters). The best-fit amplitudes for these samples are reported in Table~\\ref{tab:galinfo}, and presented as green stars in Fig.~\\ref{fig:A_lum}. In the luminosity range where the \\texttt{luminous}\\ and \\texttt{dense}\\ samples overlap, we find the results between the two samples to be compatible. The \\texttt{luminous}\\ sample seems to show a more pronounced luminosity dependence compared to the \\texttt{dense}\\ sample, which can either be an effect of being brighter overall (from \\texttt{L1} to \\texttt{L3}, $L/L_0 = 0.46, 0.64, 1.01$) or \ndue to the satellite fraction being lower (see Sect. \\ref{subsec:satellite_fraction}), or a combination of the two. We note that the measurements of the \\texttt{L3} sample appear to scatter more than the covariance predicts, which results in higher $\\chi^2$. \nA similar issue is present in the \\texttt{D4} sample and it is visible in Fig.~\\ref{fig:wgp_Lcuts}.\n\nThe horizontal error bars in Fig.~\\ref{fig:A_lum} indicate the weighted standard deviation of the luminosity distribution within the bin for each sample, with the measurement placed at the luminosity-weighted mean of the bin. If the range is too large, and the IA signal varies within the bin, the resulting amplitude is difficult to interpret, and may even appear discrepant. For instance, when we combine the \\texttt{D4} and \\texttt{D5} samples we obtain $A_\\mathrm{IA} = 6.70^{+1.15}_{-1.14}$. We note, however, that the luminosity range probed by this combined bin is particularly extended, and the high signal measured is mainly driven by the galaxies in the high luminosity tail of the bin (\\texttt{D5}, $A_\\mathrm{IA} = 8.39^{+1.04}_{-1.30}$). The other half of the bin has a relatively low signal with very large uncertainties (\\texttt{D4}, $A_\\mathrm{IA}=3.02^{+2.37}_{-2.33}$). This is relevant because it suggests that the alignment of galaxies with luminosities below $L/L_0 \\sim 0.60-0.70$ hardly depends on luminosity, and thus with a similar amplitude to \\texttt{D1} and \\texttt{D3}, the smaller sample is less constraining. As soon as we exceed this approximate threshold, the signal increases significantly, suggesting a luminosity dependence. This overall picture is enhanced when we also consider previous results for LRGs \\citep[][]{Joachimi2011b, Singh2015, Johnston2019, fortuna2020halo}\\footnote{The GAMA points \\citep{Johnston2019} have been adjusted to homogenise the units convention, as discussed in \\citet[][]{fortuna2020halo}.}.\nThese are also shown in Fig.~\\ref{fig:A_lum}. We investigate how well the current measurements support the picture of a single or double power law by fitting the data points in Fig.~\\ref{fig:A_lum}, assuming them to be uncorrelated. For each data point, we only use the quoted $L/L_0$ as we do not have the underlying luminosity distribution for most of the measurements. We propose a double power law with knee at $L_{\\rm break}$, amplitude $A_{\\beta}$ and slopes $\\beta_{1,2}$:\n\\begin{equation}\\label{eq:double_powerlaw}\n    A(L) = A_{\\beta} \\left( \\frac{L}{L_{\\rm break}} \\right)^{\\beta} \\rm {with} \\ \\begin{cases} \\beta = \\beta_1 & {\\rm for} \\ L< L_{\\rm break}\\\\\n    \\beta = \\beta_2 & {\\rm for} \\ L> L_{\\rm break}\n    \\end{cases}\n\\end{equation}\nand fit for \n\\begin{equation}\n    \\lambda = \\left\\{ A_{\\beta}, \\beta_1, \\beta_2, L_{\\rm scale} \\right\\} \\ ,\n\\end{equation}\nwhere $L_{\\rm scale} = L_{\\rm break}/L_0$. We explored the parameter space using a MCMC and assuming a Gaussian likelihood. Figure~\\ref{fig:a_lum_mcmc} shows our parameter constraints, while the model prediction is shown in Fig~\\ref{fig:A_lum} as a solid black line. Our best-fit parameters are reported in Table ~\\ref{tab:all_lum_fit_pars}\\footnote{We note that the parameters that maximise the likelihood differ from the medians of the posterior distributions as a consequence of the degeneracies between the parameters. This is particularly evident for $\\beta_1$, which has negative slope, $\\beta_1 = -0.75$.}. We repeated the same analysis assuming a single power law, as parametrised in \\citet{Joachimi2011b}. The best-fit parameters are also reported in Table~\\ref{tab:all_lum_fit_pars}.\nThe larger $\\chi^2/$dof of the single power law compared to the double power law suggests that the latter is a better description of our current data, although the scatter between the points at low $L$ is still too large to draw definitive conclusions and the data are also mildly inconsistent in that regime. The degeneracy between the parameters, and in particular between $A_{\\beta}$ and $L_{\\rm scale}$, shows that the data can weakly constrain the model. Nevertheless, the emerging picture seems to support more the broken power law scenario presented in \\citet[][]{fortuna2020halo}, but with a transition luminosity around $0.4-0.6 L_0$, also in line with the results from simulations by \\citet{Samuroff2020}. The double power law is also supported by the fact that the alignment of redMaPPer clusters \\citep[][]{vanUitert2017,Piras2018}, not included in this analysis, forms a smooth extension towards higher mass of the alignment observed for the high luminosity LRGs. This result is hard to reconcile with a single shallow power law, but finds a natural framework in the double power law scenario, where the slope of the relation at high luminosities recovers the trend in \\citet[][]{Joachimi2011b, Singh2015}. \n\nWe caution that this analysis does not aim to be fully comprehensive, but rather to provide a sense of the current trends. A proper analysis should jointly fit all of the measurements incorporating the full luminosity distributions of each sample, as well as accounting for the presence of satellites, which might suppress the signal at low luminosities. \n\n\\subsection{Redshift dependence} \\label{subsec:redshift_dependence}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{plots/wgp_z1_x_z2_lum_sample_10bins.pdf}\n\\caption{Projected correlation function, $w_{\\rm{g+}}$, measured on our different cuts in redshift of the \\texttt{luminous}\\ sample. The best-fit curves are plotted on top of the data points, and the fits are performed for $r_{\\rm p}> 6 \\,h^{-1}{\\rm{Mpc}}$. The red points are slightly displaced for clarity and the corresponding best-fit curve has been displaced accordingly.}\n\\label{fig:wgp_zcuts}\n\\end{figure}\n\nHaving assessed that the two shape measurements produce compatible IA signals and that their calibrations are robust, we merge\nthe two shape catalogues to span the largest possible range in redshift. This allows us to extend the sample from the low-$z$, high S/N galaxies, where only \\textsc{DEIMOS} provides shapes, to the high-$z$, low S/N galaxies, where we preferentially measure the shapes via \\textit{lens}fit. In the case of overlap between \\textsc{DEIMOS} and \\textit{lens}fit, we select the \\textsc{DEIMOS} shapes. We only focus on the \\texttt{luminous}\\ sample as we are interested in a long redshift baseline with the same luminosity cut. In this way, we can probe the redshift evolution of the sample, without confusing the results with any luminosity dependence.\n\nOur final catalogue contains 115\\,322 galaxies that we split at $z=0.585$, which roughly provides two equally populated bins. We call these two samples Z1 and Z2. The measurements for \n$w_{\\rm{g+}}$ are presented in Fig.~\\ref{fig:wgp_zcuts}. The best-fit values for the two redshift bins are listed in Table~\\ref{tab:galinfo} and agree within their error bars, despite their mean redshift being $\\langle z \\rangle = 0.44$ and $\\langle z \\rangle = 0.70$, respectively. \n\nWe note that the $\\chi^2$ of our Z2 sample is quite high: This is driven by the poor fit of the clustering signal. We attribute this to our photo$-z$, which at high redshift are less reliable. We note, however, that the uncertainty in the IA amplitude is large enough to absorb the inaccuracies in $p(z_{\\rm spec}|z_{\\rm phot})$, such that modifying the redshift distributions has little impact on the recovered IA amplitude.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{plots/A_z_all.pdf}\n\\caption{Intrinsic alignment amplitude, $A_\\mathrm{IA}$, as a function of redshift and luminosity for different best-fit values in the literature. Different markers refer to different studies and are colour-coded based on their luminosity: MegaZ \\citep[][]{Joachimi2011b} is shown as triangles, LOWZ \\citep{Singh2015} as circles, GAMA \\citep[][]{Johnston2019} as squares and the LRG \\texttt{luminous}\\ sample Z1 and Z2 as stars.}\n\\label{fig:A_z_all}\n\\end{figure}\n\nFigure~\\ref{fig:A_z_all} compares our results with the best-fit amplitudes at various redshifts found by previous studies \\citep{Joachimi2011b,Singh2015,Johnston2019}. The colour of the data points reflects the luminosity of the sample used to measure the signal\\footnote{The colour of the marker corresponds to the bin centre, which may not be sufficient if the range in luminosity is large, as it\nis typically the case for these samples. The information provided by the colour has therefore only qualitative meaning and should be considered as such.}. As previously discussed, galaxies with different luminosities may manifest different levels of IA, and hence even with a lack of  redshift dependence, we should still expect points at different amplitudes: the bottom part of the plot should be mainly populated by darker points and the upper part by brighter points. Figure ~\\ref{fig:A_z_all} confirms this scenario: overall, the points exhibit a similar alignment and the scatter between the different points is consistent with the extra luminosity dependence. We can conclude that there is little evidence for a strong redshift dependence of the IA signal.\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nWe have constrained the IA signal of a sample of LRGs selected by \n\\cite{Vakili2020} from KiDS-1000, which images $\\sim 1000$ deg$^2$.\nThese data allowed us to investigate the luminosity dependence and the redshift evolution of the signal. To do so, we measured the shapes of the LRGs with two different algorithms, \\textsc{DEIMOS} and \\textit{lens}fit. We used custom\nimage simulations to calibrate and correct the residual biases that arise from measurements of noisy images. \n\nWe used the calibrated ellipticities to compute the projected position-shape correlation function $w_{\\rm{g+}}$ and analyse the signals obtained by the two different algorithms independently, thus exploring the dependence of IA on the specific shape method employed. We found \\textit{lens}fit\\ measurements to be overall noisier than the \\textsc{DEIMOS} ones and we attributed this to the prevalence of faint galaxies in the sample, due to the internal magnitude cut in the \\textit{lens}fit\\ algorithm. Because bright galaxies typically carry more alignment signal, this cut, which removes galaxies with $m_r<20$, can potentially reduce the IA contamination in KiDS cosmic shear analyses, which employ \\textit{lens}fit\\ as the shape method. For a sub-sample of galaxies, where both shape methods return successful measurements of the shapes, we find a remarkable agreement in the measured $w_{\\rm{g+}}$, with a difference in the signal of $0.003 \\pm 0.13$ (amplitude of a fitted power law).\n\nWe explored the luminosity dependence and the redshift evolution independently, selecting our galaxies in such a way that ensures the two do not mix.  Within the luminosity range probed by the measurements our results agree with previous studies \\citep[][]{Joachimi2011b, Singh2015, Johnston2019}. However, a single power law fit, as was used in \\cite{Joachimi2011b} and \\cite{Singh2015} does not describe the measurements well. Instead, our results suggest a more complex dependence with luminosity: for $L_r\\lesssim 2.9\\times 10^{10}h^{-2}L_{r,\\odot}$\nthe IA amplitude does not vary significantly, whereas the signal rises rapidly at higher luminosity. This also has implications for the width of the luminosity binning, as the use of broad bins may complicate the interpretation of the measurements. Analyses that aim to combine these measurements to model the  luminosity dependence should incorporate the underlying luminosity distributions to properly link the signal to the galaxy luminosity. Nevertheless, we provide a preliminary fit on the current measurements available in the literature and found that the data are best described by a broken power law. This result can already be used by cosmic shear analyses to improved their modelling of the IA carried by the red galaxy population. We remind the reader that this sample is not representative of the galaxy population. Different galaxy samples carry different alignment signals and should thus be individually modelled as described in \\citet{fortuna2020halo}.\n\nTo probe the redshift dependence of the IA signal with the largest baseline to date, we merge the \\textsc{DEIMOS} and \\textit{lens}fit\\ catalogues. We find no evidence for redshift evolution of the IA signal. This result is in line with previous studies of LRG samples \\citep[][]{Joachimi2011b, Singh2015}, and it is consistent with the current paradigm that IA is set at the moment of galaxy formation. However, it is also possible that galaxy mergers counteract the evolution of the tidal alignment, such that the net signal does not change. Further improvements in the measurements are needed to distinguish between scenarios.\n\n\\section*{Acknowledgements}\n\nWe thank Sandra Unruh for providing useful comments to the manuscript. MCF, AK, MV and HH acknowledge support from Vici grant 639.043.512, financed by the Netherlands Organisation for Scientific Research (NWO). HH also acknowledges funding from the EU Horizon 2020 research and innovation programme under grant agreement 776247. HJ acknowledges support from the Delta ITP consortium, a program of the Netherlands Organisation for Scientific Research (NWO) that is funded by the Dutch Ministry of Education, Culture and Science (OCW). We also acknowledge support from: the European Research Council under grant agreement No. 770935 (AHW, HHi) and No. 647112 (CH and MA); the Polish Ministry of Science and Higher Education through grant DIR/WK/2018/12 and the Polish National Science Center through grants no. 2018/30/E/ST9/00698 and 2018/31/G/ST9/03388 (MB);  the Max Planck Society and the Alexander von Humboldt Foundation in the framework of the Max Planck-Humboldt Research Award endowed by the Federal Ministry of Education and Research (CH); the Heisenberg grant of the Deutsche Forschungsgemeinschaft Hi 1495/5-1 (Hi); the Royal Society and Imperial College (KK) and from the Science and Technology Facilities Council (MvWK).\n\nThe MICE simulations have been developed at the MareNostrum supercomputer (BSC-CNS) thanks to grants AECT-2006-2-0011 through AECT-2015-1-0013. Data products have been stored at the Port d'Informaci$\\acute{\\mathrm{o}}$ Cient$\\acute{\\mathrm{i}}$fica (PIC), and distributed through the CosmoHub webportal (cosmohub.pic.es). Funding for this project was partially provided by the Spanish Ministerio de Ciencia e Innovacion (MICINN), projects 200850I176, AYA2009-13936, AYA2012-39620, AYA2013-44327, ESP2013-48274, ESP2014-58384 , Consolider-Ingenio CSD2007- 00060, research project 2009-SGR-1398 from Generalitat de Catalunya, and the Ramon y Cajal MICINN program.\n\nBased on data products from observations made with ESO Telescopes at the La Silla Paranal Observatory under programme IDs 177.A- 3016, 177.A-3017 and 177.A-3018, and on data products produced by Tar- get/OmegaCEN, INAF-OACN, INAF-OAPD and the KiDS production team, on behalf of the KiDS consortium. OmegaCEN and the KiDS production team acknowledge support by NOVA and NWO-M grants. Members of INAF-OAPD and INAF-OACN also acknowledge the support from the Department of Physics $\\&$ Astronomy of the University of Padova, and of the Department of Physics of Univ. Federico II (Naples). \n\n\\textit{Author contributions:} All authors contributed to the development and writing of this paper. The authorship list is given in three groups: the lead authors (MCF, HH, HJ, MV, AK, CG) followed by two alphabetical groups. The first alphabetical group includes those who are key contributors to both the scientific analysis and the data products. The second group covers those who have either made a significant contribution to the data products, or to the scientific analysis.\n\n\n\n\\bibliographystyle{aa}\n", "meta": {"timestamp": "2021-09-07T02:37:44", "yymm": "2109", "arxiv_id": "2109.02556", "language": "en", "url": "https://arxiv.org/abs/2109.02556"}}
{"text": "\\section{Proofs}\\label{sec:proofs}\n\n\\begin{proposition*}{\\textbf{\\normalfont (Training fairness in the supernet.)}}\\label{prop:fair}\nLet $T$ and $T_{{\\mathcal{A}}}$ denote the number of steps applied to train the supernet and candidate architecture ${\\mathcal{A}}$ in the search space of size $N$, by uniformly randomly sampling a single architecture from this search space for the model training in each step, we have\n\\begin{equation*}\n    \\begin{gathered}\n        \\normalfont\\text{Pr}(\\lim_{T\\rightarrow\\infty} T_{{\\mathcal{A}}_i} / T = \\lim_{T\\rightarrow\\infty} T_{{\\mathcal{A}}_j} / T) = 1 \\quad \\forall i,j \\in \\{1, \\cdots, N\\} \\ .\n    \\end{gathered}\n    \\end{equation*}\n\\end{proposition*}\n\\begin{proof}\nLet random variable $X_i^t \\in \\{0,1\\}$ denote the selection of candidate architecture ${\\mathcal{A}}_i$ at training step $t$ under our sampling scheme in the proposition above. For any $t>0$ and $i,j \\in [N]$, random variable $X_i^t - X_j^t$ can achieve following possible assignments and probabilities (denoted by $p$):\n\\begin{equation}\n\\begin{gathered}\n    X_i^t - X_j^t = \n    \\left\\{\\begin{array}{rcl}\n             +1, & & p = 1/N \\\\\n             0, & & p = (N-2)/N \\\\\n             -1, & & p = 1/N\n    \\end{array}\\right. \\ .\n\\end{gathered}\n\\end{equation}\nConsequently, $\\mathbb{E}[X_i^t - X_j^t] = 0$. According to the strong law of large numbers, we further have\n\\begin{equation}\n\\begin{gathered}\n    \\text{Pr}(\\lim_{T\\rightarrow\\infty} T^{-1}\\sum_{t=1}^T X_i^t - X_j^t = 0) = 1 \\ .\n\\end{gathered}\n\\end{equation}\nNote that \n\\begin{equation}\n\\begin{gathered}\n    T^{-1}(T_{{\\mathcal{A}}_i} - T_{{\\mathcal{A}}_j}) = T^{-1}\\sum_{t=1}^T X_i^t - X_j^t \\ .\n\\end{gathered}\n\\end{equation}\nWe thus can complete this proof by\n\\begin{equation}\n\\begin{aligned}\n    &\\text{Pr}(\\lim_{T\\rightarrow\\infty} T^{-1}(T_{{\\mathcal{A}}_i} - T_{{\\mathcal{A}}_j}) = 0) = 1 \\ .\n\\end{aligned}\n\\end{equation}\n\\end{proof}\n\n\\paragraph{Proof of Proposition \\ref{prop:svgdrd-update}.}\nAs particles $\\{{\\bm{x}}_i\\}_{i=1}^n$ of size $n$ are applied to approximate the density $q$ in our SVGD-RD, the second term (i.e., the controllable diversity term) in our \\eqref{eq:min-kl-max-div} can then be approximated using these particles as\n\\begin{equation}\n\\begin{aligned}\n    n\\delta \\mathbb{E}_{{\\bm{x}},{\\bm{x}}' \\sim q} \\left[k({\\bm{x}}, {\\bm{x}}')\\right] &\\approx  \\delta/n \\sum_{i=1}^n\\sum_{j=1}^n k({\\bm{x}}_i, {\\bm{x}}_j) \\triangleq \\sum_{i=1}^n h({\\bm{x}}_i) \\ ,\n\\end{aligned}\n\\end{equation}\nwhere $h({\\bm{x}})\\triangleq\\delta/n\\sum_{j=1}^n k ({\\bm{x}}, {\\bm{x}}_j)$. We take ${\\bm{x}}_j$ in $k({\\bm{x}}, {\\bm{x}}_j)$ as a constant for the approximation above. Consequently, we have\n\\begin{equation}\\label{eq:equal-grad}\n\\begin{aligned}\n    \\nabla_{{\\bm{x}}_k}\\sum_{i=1}^n h({\\bm{x}}_i) = \\nabla_{{\\bm{x}}_k}h({\\bm{x}}_k) \\ .\n\\end{aligned}\n\\end{equation}\n\nLet ${\\bm{x}}_i^{+} \\triangleq {\\bm{x}}_i + \\epsilon{\\bm{\\phi}}^*({\\bm{x}}_i)$ ($\\forall i\\in\\{1,\\cdots,n\\}$) denote the functional gradient decent in the RKHS ${\\mathcal{H}}$ to minimize the KL divergence term in our \\eqref{eq:min-kl-max-div}. Based on \\eqref{eq:equal-grad} above, given proximal operator $\\text{prox}_h({\\bm{x}}^{+})=\\argmin_{{\\bm{y}}} h({\\bm{y}})+1/2\\|{\\bm{y}}-{\\bm{x}}^{+}\\|_2^2$, by using proximal gradient method \\cite{proximal}, our \\eqref{eq:min-kl-max-div} can then be optimized via the following update to each particle ${\\bm{x}}_i$:\n\\begin{equation}\n    {\\bm{x}}_i \\leftarrow \\text{prox}_h({\\bm{x}}_i^{+})=\\argmin_{{\\bm{y}}} h({\\bm{y}})+1/2\\|{\\bm{y}}-{\\bm{x}}_i^{+}\\|_2^2 \\ .\n\\end{equation}\n\nAccording to the \\textit{Karush-Kuhn-Tucker} (KKT) conditions, the local optimum ${\\bm{y}}^*$ of this proximal operator satisfies\n\\begin{equation}\n\\begin{aligned}\n    \\text{prox}_h({\\bm{x}}_i^{+}) = {\\bm{y}}^* = {\\bm{x}}_i^{+} - \\nabla_{{\\bm{y}}^*} h({\\bm{y}}^*)\\ .\n\\end{aligned}\n\\end{equation} \nWhen $h(\\cdot)$ is convex, this local optimum is also a global optimum. As (16) is intractable to solve given a complex $h(\\cdot)$, we approximate $h({\\bm{y}}^*)$ with its first-order Taylor expansion, i.e., $h({\\bm{y}}^*) \\approx h({\\bm{x}}_i) + \\nabla_{{\\bm{x}}_i} h({\\bm{x}}_i)({\\bm{y}}^* - {\\bm{x}}_i)$ and achieve following approximation:\n\\begin{equation}\n\\begin{aligned}\n    \\text{prox}_h({\\bm{x}}_i^{+}) &\\approx {\\bm{x}}_i^{+} - \\nabla_{{\\bm{x}}_i} h({\\bm{x}}_) \\\\\n    &\\approx {\\bm{x}}_i + \\epsilon{\\bm{\\phi}}^*({\\bm{x}}_i) - \\nabla_{{\\bm{x}}_i} h({\\bm{x}}_i) \\\\\n    &\\approx {\\bm{x}}_i + \\epsilon{\\bm{\\phi}}^*({\\bm{x}}_i) - \\delta/n \\textstyle\\sum_j^n \\nabla_{{\\bm{x}}_i} k({\\bm{x}}_i, {\\bm{x}}_j) \\ .\n\\end{aligned}\n\\end{equation}\n\nGiven the approximation ${\\bm{\\phi}}^*({\\bm{x}}_i) \\approx \\widehat{{\\bm{\\phi}}}^*({\\bm{x}}_i)$ and the definition of $\\widehat{{\\bm{\\phi}}}^*({\\bm{x}}_i)$ in \\eqref{eq:svgd-approx}, we complete our proof by\n\\begin{equation}\n\\begin{aligned}\n    {\\bm{x}}_i \\leftarrow {\\bm{x}}_i &+ 1/n \\textstyle\\sum_{j=1}^n k({\\bm{x}}_j, {\\bm{x}}_i)\\nabla_{{\\bm{x}}_j}\\log p({\\bm{x}}_j) \\\\\n    & + \\nabla_{{\\bm{x}}_j}k({\\bm{x}}_j, {\\bm{x}}_i) - \\delta \\nabla_{{\\bm{x}}_i} k({\\bm{x}}_j, {\\bm{x}}_i) \\ .\n\\end{aligned}\n\\end{equation}\n\n\\paragraph{Proof of Proposition \\ref{prop:exploitation}.}\nNotably, since $k({\\bm{x}},{\\bm{x}}')=c$ when ${\\bm{x}}={\\bm{x}}'$, we will achieve a constant $k({\\bm{x}}, {\\bm{x}})$ for any particle ${\\bm{x}}$ in the case of $n=1$, which can be ignored in our SVGD-RD for any $\\delta \\in \\mathbb{R}$. In light of this, our SVGD-RD in the case of $n=1$ degenerates into standard SVGD. Consequently, to prove Proposition \\ref{prop:exploitation}, we only need to consider SVGD in the case of $n=1$.\n\nConsidering SVGD in the case of $n=1$, we can frame the density $q$ represented by a single particle ${\\bm{x}}'$ as \n\\begin{equation}\n\\begin{aligned}\n    q({\\bm{x}}) = \n    \\left\\{\\begin{array}{rcl}\n             1 & & {\\bm{x}} = {\\bm{x}}' \\\\\n             0 & & {\\bm{x}} \\neq {\\bm{x}}'\n    \\end{array}\\right. \\ .\n\\end{aligned}\n\\end{equation}\nThe KL divergence between $q({\\bm{x}})$ and the target density $p({\\bm{x}})$ can then be simplified as\n\\begin{equation}\n\\begin{aligned}\n    \\text{KL}(q \\| p) &= \\mathbb{E}_{q({\\bm{x}})}[\\log(q({\\bm{x}})/p({\\bm{x}}))] = -\\log p({\\bm{x}}') \\ .\n\\end{aligned}\n\\end{equation}\nFinally, standard SVGD in the case of $n=1$ obtain its optimal particle by optimizing the following problem:\n\\begin{equation}\n\\begin{aligned}\n    q^* &= \\argmin_q \\text{KL}(q \\| p) \\\\\n    & = \\argmin_{{\\bm{x}}'} \\{-\\log p({\\bm{x}}')\\} \\\\\n    & = \\argmax_{{\\bm{x}}'} p({\\bm{x}}') \\ ,\n\\end{aligned}\n\\end{equation}\nwhich finally concludes the proof.\n\n\\begin{remark}\n\\emph{\nIn practice, this $k({\\bm{x}},{\\bm{x}})=c$ can be well satisfied, such as the radial basis function (RBF) kernel that we have applied in our experiments.\n}\n\\end{remark}\n\n\\section{Experimental Settings}\\label{sec:setting-exp}\n\\subsection{The DARTS Search Space}\\label{sec:setting-search-space}\nIn the DARTS \\citep{darts} search space, each candidate architecture consists of a stack of $L$ cells, which can be represented as a directed acyclic graph (DAG) of $N$ nodes denoted by $\\{z_0, z_1, \\dots, z_{N-1}\\}$. Among these $N$ nodes in a cell, $z_0$ and $z_1$ denote the input nodes produced by two preceding cells, and $z_N$ denotes the output of a cell, which is the concatenation of all intermediate nodes, i.e., from $z_2$ to $z_{N-1}$. As in the work of \\citet{darts}, to select the best-performing architectures, we need to select their corresponding cells, including the normal and reduction cell. We refer to the DARTS paper for more details.\nIn practice, this search space is conventionally represented as a supernet stacked by 8 cells (6 normal cells and 2 reduction cells) with initial channels of 16.\n\n\\subsection{Model Training of Supernet}\\label{sec:setting-one-shot}\nFollowing \\citep{pc-darts}, we apply a partial channel connection with $K=2$ in the model training of the supernet, which allows us to accelerate and reduce the GPU memory consumption during this model training. We split the standard training dataset of CIFAR-10 into two piles in our ensemble search: 70\\% randomly sampled data is used in the model training of the supernet, and the rest is used to obtain the posterior distribution of neural architectures in Sec. \\ref{sec:posterior} and also the final selected ensembles in Sec. \\ref{sec:sampling}. To achieve not only a fair but also a sufficient model training for every candidate architecture, we apply \\textit{stochastic gradient descent} (SGD) with epoch 50, learning rate cosine scheduled from 0.1 to 0, momentum 0.9, weight decay $3\\times10^{-4}$ and batch size 128 in the model training of the supernet, where only a single candidate architecture is uniformly randomly sampled from this supernet in every training step.\n\n\\subsection{Posterior Distribution}\\label{sec:setting-posterior}\n\\paragraph{Variational posterior distribution.}\nFollowing \\citep{snas}, the variational posterior distribution of architectures is represented as $p_{{\\bm{\\alpha}}}({\\mathcal{A}})$ parameterized by ${\\bm{\\alpha}}$. Specifically, within the search space demonstrated in our Appendix \\ref{sec:setting-search-space}, each intermediate nodes $z_i$ is the output of one selected operation $o \\sim p_{{\\bm{\\alpha}}_i}(o)$ using the inputs from its proceeding nodes or cells, where ${\\mathcal{O}}$ is a predefined operation set for our search. Specifically, given ${\\bm{\\alpha}}_i=(\\alpha_i^{o_1} \\cdots \\alpha_i^{o_{|{\\mathcal{O}}|}})$, $p_{{\\bm{\\alpha}}_i}(o)$ can be represented as \n\\begin{equation}\\label{eq:discrete-dist}\n    p_{{\\bm{\\alpha}}_i}(o) = \\frac{\\exp(\\alpha_i^o/\\tau)}{\\sum_{o \\in {\\mathcal{O}}}\\exp(\\alpha_i^o/\\tau)} \\ ,\n\\end{equation}\nwhere $\\tau$ denotes the softmax temperature, which is usually set to be 1 in practice. Based on this defined probability for each intermediate node $z_i$, our variational posterior distribution can be framed as\n\\begin{equation}\n    p_{{\\bm{\\alpha}}}(A) = \\prod_{i=2}^{N-2}p_{{\\bm{\\alpha}}_i}(o) \\ .\n\\end{equation}\nMore precisely, this representation is applied for single-path architecture with identical cells. We use it to ease our representation. For double-path architectures consisting of two different cells (i.e., normal and reduction cell), e.g., the candidate architecture in the DARTS search space, a similar representation can be obtained.\n\n\\paragraph{Optimization details.}\nTo optimize \\eqref{eq:elbo}, we firstly relax our variational posterior distribution to be differentiable using the Straight-Through (ST) Gumbel-Softmax \\citep{concrete, gumbel-softmax} with the reparameterization trick. More precisely, we propose a variant of ST Gumbel-Softmax outputting the double-path architectures in the DARTS search space. Then, we use stochastic gradient-based algorithms to optimize \\eqref{eq:elbo} efficiently. In each optimization step, we sample one neural architecture from the distribution $p_{{\\bm{\\alpha}}}({\\mathcal{A}})$ to estimate $\\mathbb{E}_{{\\mathcal{A}} \\sim p_{{\\bm{\\alpha}}}({\\mathcal{A}})}\\left[\\log p({\\mathcal{D}}|{\\mathcal{A}})\\right]$ (i.e., the commonly used Cross-Entropy loss). In practice, we use Adam \\citep{adam} with learning rate 0.01, $\\beta_1=0.9$, $\\beta_2=0.999$ and weight decay $3\\times10^{-4}$ to update our variational posterior distribution $p_{{\\bm{\\alpha}}}({\\mathcal{A}})$ for 20 epochs. \n\n\\subsection{SVGD of Regularized Diversity}\\label{sec:setting-svgd}\n\\paragraph{Continuous relaxation of variational posterior distribution.}\nNotably, SVGD \\citep{svgd} and also our SVGD-RD is applied for continuous distribution. Unfortunately, the variational posterior distribution $p_{{\\bm{\\alpha}}}({\\mathcal{A}})$ is discrete due to a discrete search space. To apply SVGD-RD, we firstly relax this discrete posterior into its continuous counterpart using a mixture of Gaussian distribution. Specifically, we represent each operation $o\\in{\\mathcal{O}}$ in \\eqref{eq:discrete-dist} into a one-hot vector ${\\bm{h}}_o$. By introducing the random variable ${\\bm{o}}_i\\in\\mathbb{R}^{|{\\mathcal{O}}|}$ and multi-variate normal distribution ${\\mathcal{N}}({\\bm{o}}_i | {\\bm{h}}_o, \\Sigma)$ into our relaxation, our relaxed posterior distribution of neural architectures can be framed as\n\\begin{equation}\\label{eq:relax-dist}\n\\begin{gathered}\n    \\widehat{p}_{{\\bm{\\alpha}}}({\\mathcal{A}}) = \\prod_{i=2}^{N-2} 1/Z_i\\sum_{o \\in {\\mathcal{O}}} p_{{\\bm{\\alpha}}_i}(o) {\\mathcal{N}}({\\bm{o}}_i | {\\bm{h}}_o, \\Sigma) \\ ,\n\\end{gathered}\n\\end{equation}\nwhere $Z_i$ denotes the normalization constant. Given the sampled particle ${\\bm{x}}^*=(\\cdots {\\bm{o}}_i^* \\cdots)$ in SVGD-RD, the final selected architecture can then be derived using the determination of each selected operation $o_i^*$, i.e., \n\\begin{equation}\n    o_i^* = \\argmin_{o \\in {\\mathcal{O}}} \\|{\\bm{o}}_i^* - {\\bm{h}}_o\\|_2 \\ .\n\\end{equation}\n\n\\paragraph{Optimization details.} \nSince \\citet{svgd} have demonstrated that SVGD is able to handle unnormalized target distributions, the normalization constant in \\eqref{eq:relax-dist} can then be ignored in our SVGD-RD algorithm. In practice, the covariance matrix $\\Sigma$ in \\eqref{eq:relax-dist} is set to an identity matrix scaled by $|{\\mathcal{O}}|$. Besides, the parameter $\\delta$ is optimized as a hyper-parameter via grid search or Bayesian Optimization \\citep{bo} within the range of $[-2, 1]$ in practice. To obtain well-performing particles in our SVGD-RD algorithms efficiently, we apply SGD using the gradient provided in Sec. \\ref{sec:svgd-rd} with a \\textit{radial basis function} (RBF) kernel on randomly initialized particles for $L{=}1000$ iterations under a learning rate of 0.1 and a momentum of 0.9.\n\n\\subsection{Evaluation on Benchmark Datasets}\\label{sec:setting-training}\n\\paragraph{Evaluation on CIFAR-10/100.} \nWe apply the same constructions in DARTS \\citep{darts} for our final performance evaluation on CIFAR-10/100: The final selected architectures consist of 20 cells, and 18 of them are identical normal cells, with the rest being the identical reduction cell. An auxiliary tower with a weight of 4 is located at the 13-th cell of the final selected architectures. The final selected architecture is then trained using stochastic gradient descent (SGD) for 600 epochs with a learning rate cosine scheduled from 0.025 to 0, momentum 0.9, weight decay $3\\times10^{-4}$, batch size 96 and initial channels 36. Cutout \\citep{cutout}, and a scheduled DropPath, i.e., linearly decayed from 0.2 to 0,  are employed to achieve SOTA generalization performance.\n\n\\paragraph{Evaluation on ImageNet.}\nFollowing \\citep{darts}, the architectures evaluated on ImageNet consist of 14 cells (12 identical normal cells and 2 identical reduction cells). To meet the requirement of evaluation under the mobile setting (less than 600M multiply-add operations), the number of initial channels for final selected architectures are conventionally set to 44. We adopt the training enhancements in \\cite{darts,p-darts,sdarts}, including an auxiliary tower of weight 0.4 and label smoothing. Following P-DARTS \\cite{p-darts} and SDARTS-ADV \\cite{sdarts}, we train the selected architectures from scratch for 250 epochs using a batch size of 1024 on 8 GPUs, SGD optimizer with a momentum of 0.9 and a weight decay of $3\\times10^{-5}$. The learning rate applied in this training is warmed up to 0.5 for the first 5 epochs and then decreased to zero linearly.\n\n\\subsection{Adversarial Defense}\\label{sec:setting-adversarial}\nAdversarial attack intends to find a small change for each input such that this input with its corresponding small change will be misclassified by a model. As ensemble is known to be a possible defense against such adversarial attacks \\citep{ensemble-for-defense}, we also examine the effectiveness of our NESBS algorithm by comparing the model robustNESBS achieved by our algorithms to other ensemble and ensemble search algorithms under various benchmark adversarial attacks. To the best of our knowledge, we are the first to examine the advantages of ensemble search algorithms in defending against adversarial attacks.\n\nIn this experiment, two processes are required, i.e., \\emph{attack} and \\emph{defense}. The \\emph{attack} process is a typical white-box attack scenario: Only a single model (randomly sampled from an ensemble) is attacked by an attacker, and this process will be repeated for $n$ rounds given an ensemble of size $n$ in order to accurately measure the improvement of model robustNESBS induced by an ensemble. In each round, a different model from this ensemble is selected to be attacked. \nThe \\emph{defense} process is then applied using neural network ensembles, i.e., neural network ensembles will make predictions based on those perturbed images produced by the aforementioned attacker. Corresponding to the \\emph{attack} process, we also need to repeat this defense process for $n$ rounds. In fact, such an adversarial defense setting is reasonably practical when only a single model from an ensemble is required to be publicly available for model producers.\n\nWe apply the following attacks in our experiment: The \\emph{Fast Gradient Signed Method} (FGSM) attack \\cite{fgsm}, the \\emph{Projected Gradient Descent} (PGD) attack \\cite{pgd}, the \\emph{Carlini Wagner} (CW) attack \\cite{cw} and the AutoAttack~\\citep{autoattack}. In both the FGSM attack and the PGD attack, we impose a $L_\\infty$ norm constrain of $0.01$. The step size and the number of iterations in the PGD attack are set to $0.008$ and $40$, respectively. We adopt the same configurations of the CW attack under a $L_2$ norm constrain in \\citep{cw}: We set the confidence constant, the range of constant $c$, the number of binary search steps, and the maximum number of optimization steps to $0$, $[0.001,10]$, $3$, and $50$, respectively; we then adopt Adam \\citep{adam} optimizer with learning rate $0.01$ and $\\beta_1=0.9$, $\\beta_2=0.999$ in its search process. Besides, we adopt the same configuration of AutoAttack from \\citep{autoattack}.\n\n\\section{Complementary Results}\n\n\\subsection{Ensemble Performance Estimation}\\label{sec:exp-oneshot}\n\\begin{table}\n\\renewcommand\\multirowsetup{\\centering}\n\\centering\n\\begin{tabular}{lcccc}\n\\toprule\n\\textbf{Metric} & \n\\textbf{$n=1$} & \n\\textbf{$n=3$} & \n\\textbf{$n=5$} & \n\\textbf{$n=7$} \\\\\n\\midrule \nSpearman & 0.65 & 0.33 & 0.40 & $-$0.12 \\\\\nPearson & 0.82 & 0.45 & 0.45 & $-$0.16 \\\\\nAgreement-30\\% & 33\\% & 20\\% & 31\\% & 25\\% \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The correlation between the estimated and true performances of candidate architectures and their ensembles in the DARTS search space on CIFAR-10.}\n\\label{tab:rank}\n\\end{table}\n\n\nAs shown in Sec. \\ref{sec:oneshot}, we apply the model parameters inherited from a trained supernet to estimate the performance of candidate architectures as well as their ensembles in our NESBS algorithm. We therefore use the following three metrics to measure the effectiveness of such estimation in the DARTS search space: the Spearman's rank order coefficient between the estimated and true performances, the Pearson correlation coefficient between the estimated and true performances, and the percentage of architectures achieving both Top-$k$ estimated performance and Top-$k$ true performances (named the Agreement-$k$). Since the evaluation of the true performances is prohibitively costly, we randomly sample 10 architectures of diverse estimated performances from the DARTS search space for this experiment. Notably, based on these 10 architectures, there are hundreds of possible ensembles under the ensemble size of 3, 5, 7, which we believe is sufficiently large to validate the effectiveness of our performance estimations. To obtain the true performance of candidate architectures as well as their ensembles, we train these architectures independently for 100 epochs following the settings in Appendix \\ref{sec:setting-training}. \n\nTable \\ref{tab:rank} summarizes the results. Notably, the estimated and true performances are shown to be positively correlated in the case of $n{=}1,3,5$ by achieving relatively high Spearman and Pearson coefficients as well as a  high agreement in these cases. Although the coefficients are low when the ensemble size is larger (i.e., $n{=}7$), the estimated and true performances are still capable of achieving a reasonably good agreement in this case. \nBased on these results, we argue that our estimated ensemble performance is informative and effective for our ensemble search. This effectiveness can also be supported by the competitive search results achieved by our NESBS in Sec. \\ref{sec:darts}.\n\n\\begin{figure}[t]\n\\centering\n\\resizebox{0.75\\columnwidth}{!}{\n\\begin{tabular}{cc}\n    \\includegraphics[width=0.36\\textwidth]{figs/discrepancy.pdf}\n\\end{tabular}\n}\n\\caption{The comparison of performance discrepancy with the post-training and best-response posterior distribution on CIFAR-10. This performance discrepancy is measured by the gap of test error between the best-performing architecture (i.e., the architecture with the smallest test error) and the maximal-probability architecture (i.e., the architecture with the largest probability in the corresponding posterior distribution) in the DARTS search space.}\n\\label{fig:best_vs_post}\n\\end{figure}\n\n\\subsection{Post-training vs.~Best-response Posterior Distribution}\\label{sec:exp-best_vs_post}\nTo examine the advantages of our post-training posterior distribution, we compare it with its best-response counterpart applied in \\citep{gdas, snas}. While our post-training posterior distribution is obtained \\emph{after} the model training of the supernet, the best-response posterior distribution is updated \\emph{during} the model training of the supernet. We refer to \\citep{gdas, snas} for more details about this best-response posterior distribution. We follow the optimization details in Appendix \\ref{sec:setting-one-shot} and \\ref{sec:setting-posterior} to obtain these two posterior distributions.\n\n\\paragraph{More accurate characterization of single-model performances using post-training posterior distribution.}\nWe firstly compare the characterization of single-mode performance using these two posterior distributions by examining the performance discrepancy between their best-performing architecture (i.e., the architecture achieving the smallest test error) and maximal-probability architecture (i.e.,  the architecture achieving the largest probability in the corresponding posterior distribution) in the search space. In this experiment, the performance discrepancy is measured by the gap of test error achieved by the best-performing architecture and the maximal-probability architecture using the model parameters inherited from the supernet.\n\nFigure \\ref{fig:best_vs_post} illustrates the comparison. The results show that our post-training posterior distribution enjoys a smaller performance discrepancy, suggesting that our post-training posterior distribution is able to provide a more accurate characterization of the single-model performances. Interestingly, the best-response counterpart contributes to the best-performing architecture with a lower test error than our post-training posterior distribution, which should result from the Matthew Effect as justified in \\citep{dropnas}. Specifically, well-performing architectures contribute to the frequent selections of these architectures for their model training during the optimization of the best-response posterior distribution. This will finally result in unfair model training in the search space and therefore the inaccurate characterization of single-model performances. Notably, we need a more accurate characterization of single-mode performance in this paper, as shown in Sec. \\ref{sec:posterior}. Therefore, our post-training posterior distribution should be more suitable than its best-response counterpart in our ensemble search.\n\n\\paragraph{Improved performance of selected ensembles using post-training posterior distribution.}\nWe then compare the final ensemble test performance achieved by our NESBS algorithm using the post-training posterior distribution and its best-response counterpart on CIFAR-10 with the ensemble size of $n=3$. To obtain the final ensemble performance, we train each architecture in an ensemble for 100 epochs following the settings in Appendix \\ref{sec:setting-training}. Table \\ref{tab:best_vs_post} summarizes the results. Notably, our post-training posterior distribution is shown to be capable of contributing to an improved ensemble performance than its best-response counterpart, which further demonstrates the advantages of applying the post-training posterior distribution in our ensemble search.\n\n\n\\begin{table}[t]\n\\renewcommand\\multirowsetup{\\centering}\n\\centering\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{lcc}\n\\toprule\n\\textbf{Method} & \n\\textbf{Best-response} & \n\\textbf{Post-training} \\\\\n\\midrule \nNESBS (MC Sampling) & 4.74 & \\textbf{4.54}$_{\\Delta{=}0.20}$ \\\\\nNESBS (SVGD-RD) & 4.81 & \\textbf{4.48}$_{\\Delta{=}0.33}$\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{The comparison of true ensemble test error (\\%) on CIFAR-10 achieved by our NESBS algorithm using the post-training posterior distribution and its best-response counterpart with an ensemble size of $n{=}3$. We use $\\Delta$ to denote the improved generalization performance achieved by our post-training posterior distribution.}\n\\label{tab:best_vs_post}\n\\end{table}\n\n\\subsection{Effectiveness and Efficiency}\\label{sec:efficient-and-effective}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\columnwidth]{figs/sample_efficiency.pdf}\n\\caption{The comparison of search effectiveness (test error of ensembles in the $y$-axis) and efficiency (evaluation budget in the $x$-axis) for different ensemble search algorithms under varying ensemble size $n$. The single best baseline refers to the single best architecture achieving the lowest test error in the search space. The $y$-axis is shown in log-scale to ease visualization. Note that the test error for each algorithm is reported with the mean and standard error of five independent trials.\n}\n\\label{fig:search-efficiency}\n\\end{figure}\n\nAs justified in Sec \\ref{sec:sampling}, both our MC Sampling and SVGD-RD algorithms can sample neural architectures with competitive single-model performances and diverse model predictions, which are known to be the criteria for well-performing ensembles \\citep{zhou-ensemble}. \nTo further demonstrate that our algorithms are capable of selecting well-performing ensembles effectively and efficiently based on this sampling property,\nwe compare our NESBS algorithm, including NESBS (MC Sampling) and NESBS (SVGD-RD), with the following ensemble search baselines on CIFAR-10 \\citep{cifar} in the DARTS \\citep{darts} search space: (a) Uniform random sampling which we refer to as URS, and (b) NES-RS \\citep{nes}. That is, we only replace the Bayesian sampling in our NESBS algorithm with these two different sampling/selection algorithms in this experiment and we keep using the model parameters inherited from a supernet to estimate the single-model and ensemble performances of architectures (including the test errors).\nThe detailed experimental settings are in Appendix \\ref{sec:setting-exp}.\n\nFigure \\ref{fig:search-efficiency} illustrates the search results. Note that both NES-RS and our NESBS are able to achieve lower test errors than the single best-performing architecture in the search space. These results therefore demonstrate that these two ensemble search algorithms are indeed capable of achieving improved performance over conventional NAS algorithms that select only one single architecture from the search space. More importantly, given the same evaluation budgets, our NESBS algorithm consistently achieves lower test errors than URS and NES-RS, indicating the superior search effectiveness achieved by our NESBS algorithm. \nMeanwhile, our NESBS algorithm requires fewer evaluation budgets than URS and NES-RS to achieve comparable test errors, which also suggests that our algorithm is more efficient than URS and NES-RS.\nInterestingly, compared with MC Sampling, SVGD-RD can consistently produce improved search effectiveness and efficiency, which likely results from its controllable trade-off between the single-model performances and the diverse model predictions as justified in Sec.~\\ref{sec:sampling}.\nOverall, these results have well justified the effectiveness and efficiency of our NESBS algorithm.\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\columnwidth]{figs/sample_efficiency_tau.pdf}\n\\caption{\nThe comparison of search effectiveness (test error of ensembles in the $y$-axis) and efficiency (evaluation budget in the $x$-axis) between our NESBS (MC Sampling) and NESBS (SVGD-RD) algorithm under varying softmax temperature $\\tau$. The single best baseline refers to the single best architecture achieving the lowest test error in the search space. Each test error is reported with the mean and standard error of five independent trials.}\n\\label{fig:robust-tau}\n\\end{figure}\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\columnwidth]{figs/diversity_test.pdf}\n\\caption{The comparison of ensemble test error achieved by our NESBS (SVGD-RD) algorithm with varying $\\delta$ under different softmax temperature $\\tau$ given an ensemble size of $n=5$. We use $\\delta^*$ to denote the optimal $\\delta$ we obtained in our SVGD-RD algorithm under different temperature $\\tau$. The test error for each $\\delta$ is reported with the mean and standard error of five independent trials.}\n\\label{fig:optimal-diversity}\n\\end{figure}\n\n\\subsection{The Advantages of Controllable Diversity in SVGD-RD}\\label{sec:exp-optimal-diversity}\n\nTo examine the advantages of controllable diversity in our SVGD-RD, we firstly compare the search effectiveness and efficiency achieved by our NESBS (MC Sampling) and NESBS (SVGD-RD) algorithm with varying softmax temperature $\\tau$ (appeared in \\eqref{eq:discrete-dist}). A larger temperature $\\tau$ will lead to a flatter posterior distribution and hence degenerate its capability of characterizing single-model performances of neural architectures as indicated in \\eqref{eq:discrete-dist}. We use these posterior distributions with varying temperature $\\tau$ to simulate the possible posterior distributions we may obtain in practice. Figure \\ref{fig:robust-tau} illustrates the comparison on CIFAR-10 in the DARTS search space with an ensemble size of $n=5$. Notably, our NESBS (SVGD-RD) with controllable diversity can consistently achieve improved search effectiveness and efficiency than our NESBS (MC Sampling). Interestingly, this improvement becomes larger in the case of $\\tau=0.1,10.0$, which should be the consequences of a bad exploration and exploitation achieved by our NESBS (MC Sampling), respectively. These results therefore suggest that the controllable diversity in our SVGD-RD generally can lead to improved search effectiveness and efficiency than our NESBS (MC Sampling).\n\n\nWe further provide the comparison of ensemble test error achieved by our SVGD-RD with varying $\\delta$ under different softmax temperature $\\tau$ in Figure \\ref{fig:optimal-diversity}. Notably, when the posterior distribution tends to be flatter (i.e., $\\tau=10$), a smaller $\\delta$ is preferred by our SVGD-RD in order to sample architectures with better single-model performances while maintaining the compelling diverse model predictions. Meanwhile, when this posterior distribution tends to be sharper (i.e., $\\tau=0.1$), a larger $\\delta$ is preferred by our SVGD-RD in order to sample architectures with a larger diverse model predictions while preserving the competitive single-model performances. Based on this controllable diversity and hence the controllable trade-off between the single-model performances and the diverse model predictions, our SVGD-RD is thus capable of achieving comparable performances under varying $\\tau$, which usually improve over our NESBS (MC Sampling) by comparing them with the results in Figure~\\ref{fig:robust-tau}. These results further validate the advantages of the controllable diversity in our SVGD-RD.\n\\end{appendices}\n\\subsubsection*{References}}\n\\usepackage{booktabs}\n\\usepackage{tikz}\n\n\n\\usepackage{xr}\n\\newcommand{\\swap}[3][-]{#3#1#2}\n\\usepackage{amsfonts}   \n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{mathtools}\n\\usepackage{amsthm}\n\\usepackage{thmtools}\n\\usepackage{threeparttable}\n\\usepackage{multirow}\n\\usepackage{algorithm}\n\\usepackage{algorithmic}\n\n\\usepackage{caption}\n\\usepackage{subcaption}\n\\usepackage{appendix}\n\n\n\\usepackage{amsmath,amsfonts,bm}\n\n\\newcommand{{\\em (Left)}}{{\\em (Left)}}\n\\newcommand{{\\em (Center)}}{{\\em (Center)}}\n\\newcommand{{\\em (Right)}}{{\\em (Right)}}\n\\newcommand{{\\em (Top)}}{{\\em (Top)}}\n\\newcommand{{\\em (Bottom)}}{{\\em (Bottom)}}\n\\newcommand{{\\em (a)}}{{\\em (a)}}\n\\newcommand{{\\em (b)}}{{\\em (b)}}\n\\newcommand{{\\em (c)}}{{\\em (c)}}\n\\newcommand{{\\em (d)}}{{\\em (d)}}\n\n\\newcommand{\\newterm}[1]{{\\bf #1}}\n\n\n\\def\\figref#1{fig.~\\ref{#1}}\n\\def\\Figref#1{Fig.~\\ref{#1}}\n\\def\\twofigref#1#2{figures \\ref{#1} and \\ref{#2}}\n\\def\\quadfigref#1#2#3#4{figures \\ref{#1}, \\ref{#2}, \\ref{#3} and \\ref{#4}}\n\\def\\secref#1{section~\\ref{#1}}\n\\def\\Secref#1{Section~\\ref{#1}}\n\\def\\twosecrefs#1#2{sections \\ref{#1} and \\ref{#2}}\n\\def\\secrefs#1#2#3{sections \\ref{#1}, \\ref{#2} and \\ref{#3}}\n\\def\\Eqref#1{Equation~\\ref{#1}}\n\\def\\plaineqref#1{\\ref{#1}}\n\\def\\chapref#1{chapter~\\ref{#1}}\n\\def\\Chapref#1{Chapter~\\ref{#1}}\n\\def\\rangechapref#1#2{chapters\\ref{#1}--\\ref{#2}}\n\\def\\algref#1{algorithm~\\ref{#1}}\n\\def\\Algref#1{Algorithm~\\ref{#1}}\n\\def\\twoalgref#1#2{algorithms \\ref{#1} and \\ref{#2}}\n\\def\\Twoalgref#1#2{Algorithms \\ref{#1} and \\ref{#2}}\n\\def\\partref#1{part~\\ref{#1}}\n\\def\\Partref#1{Part~\\ref{#1}}\n\\def\\twopartref#1#2{parts \\ref{#1} and \\ref{#2}}\n\n\\def\\ceil#1{\\lceil #1 \\rceil}\n\\def\\floor#1{\\lfloor #1 \\rfloor}\n\\def\\bm{1}{\\bm{1}}\n\\newcommand{\\mathcal{D}}{\\mathcal{D}}\n\\newcommand{\\mathcal{D_{\\mathrm{valid}}}}{\\mathcal{D_{\\mathrm{valid}}}}\n\\newcommand{\\mathcal{D_{\\mathrm{test}}}}{\\mathcal{D_{\\mathrm{test}}}}\n\n\\def{\\epsilon}{{\\epsilon}}\n\n\n\\def{\\textnormal{$\\eta$}}{{\\textnormal{$\\eta$}}}\n\\def{\\textnormal{a}}{{\\textnormal{a}}}\n\\def{\\textnormal{b}}{{\\textnormal{b}}}\n\\def{\\textnormal{c}}{{\\textnormal{c}}}\n\\def{\\textnormal{d}}{{\\textnormal{d}}}\n\\def{\\textnormal{e}}{{\\textnormal{e}}}\n\\def{\\textnormal{f}}{{\\textnormal{f}}}\n\\def{\\textnormal{g}}{{\\textnormal{g}}}\n\\def{\\textnormal{h}}{{\\textnormal{h}}}\n\\def{\\textnormal{i}}{{\\textnormal{i}}}\n\\def{\\textnormal{j}}{{\\textnormal{j}}}\n\\def{\\textnormal{k}}{{\\textnormal{k}}}\n\\def{\\textnormal{l}}{{\\textnormal{l}}}\n\\def{\\textnormal{n}}{{\\textnormal{n}}}\n\\def{\\textnormal{o}}{{\\textnormal{o}}}\n\\def{\\textnormal{p}}{{\\textnormal{p}}}\n\\def{\\textnormal{q}}{{\\textnormal{q}}}\n\\def{\\textnormal{r}}{{\\textnormal{r}}}\n\\def{\\textnormal{s}}{{\\textnormal{s}}}\n\\def{\\textnormal{t}}{{\\textnormal{t}}}\n\\def{\\textnormal{u}}{{\\textnormal{u}}}\n\\def{\\textnormal{v}}{{\\textnormal{v}}}\n\\def{\\textnormal{w}}{{\\textnormal{w}}}\n\\def{\\textnormal{x}}{{\\textnormal{x}}}\n\\def{\\textnormal{y}}{{\\textnormal{y}}}\n\\def{\\textnormal{z}}{{\\textnormal{z}}}\n\n\\def{\\mathbf{\\epsilon}}{{\\mathbf{\\epsilon}}}\n\\def{\\mathbf{\\theta}}{{\\mathbf{\\theta}}}\n\\def{\\mathbf{a}}{{\\mathbf{a}}}\n\\def{\\mathbf{b}}{{\\mathbf{b}}}\n\\def{\\mathbf{c}}{{\\mathbf{c}}}\n\\def{\\mathbf{d}}{{\\mathbf{d}}}\n\\def{\\mathbf{e}}{{\\mathbf{e}}}\n\\def{\\mathbf{f}}{{\\mathbf{f}}}\n\\def{\\mathbf{g}}{{\\mathbf{g}}}\n\\def{\\mathbf{h}}{{\\mathbf{h}}}\n\\def{\\mathbf{u}}{{\\mathbf{i}}}\n\\def{\\mathbf{j}}{{\\mathbf{j}}}\n\\def{\\mathbf{k}}{{\\mathbf{k}}}\n\\def{\\mathbf{l}}{{\\mathbf{l}}}\n\\def{\\mathbf{m}}{{\\mathbf{m}}}\n\\def{\\mathbf{n}}{{\\mathbf{n}}}\n\\def{\\mathbf{o}}{{\\mathbf{o}}}\n\\def{\\mathbf{p}}{{\\mathbf{p}}}\n\\def{\\mathbf{q}}{{\\mathbf{q}}}\n\\def{\\mathbf{r}}{{\\mathbf{r}}}\n\\def{\\mathbf{s}}{{\\mathbf{s}}}\n\\def{\\mathbf{t}}{{\\mathbf{t}}}\n\\def{\\mathbf{u}}{{\\mathbf{u}}}\n\\def{\\mathbf{v}}{{\\mathbf{v}}}\n\\def{\\mathbf{w}}{{\\mathbf{w}}}\n\\def{\\mathbf{x}}{{\\mathbf{x}}}\n\\def{\\mathbf{y}}{{\\mathbf{y}}}\n\\def{\\mathbf{z}}{{\\mathbf{z}}}\n\n\\def{\\textnormal{a}}{{\\textnormal{a}}}\n\\def{\\textnormal{b}}{{\\textnormal{b}}}\n\\def{\\textnormal{c}}{{\\textnormal{c}}}\n\\def{\\textnormal{d}}{{\\textnormal{d}}}\n\\def{\\textnormal{e}}{{\\textnormal{e}}}\n\\def{\\textnormal{f}}{{\\textnormal{f}}}\n\\def{\\textnormal{g}}{{\\textnormal{g}}}\n\\def{\\textnormal{h}}{{\\textnormal{h}}}\n\\def{\\textnormal{i}}{{\\textnormal{i}}}\n\\def{\\textnormal{j}}{{\\textnormal{j}}}\n\\def{\\textnormal{k}}{{\\textnormal{k}}}\n\\def{\\textnormal{l}}{{\\textnormal{l}}}\n\\def{\\textnormal{m}}{{\\textnormal{m}}}\n\\def{\\textnormal{n}}{{\\textnormal{n}}}\n\\def{\\textnormal{o}}{{\\textnormal{o}}}\n\\def{\\textnormal{p}}{{\\textnormal{p}}}\n\\def{\\textnormal{q}}{{\\textnormal{q}}}\n\\def{\\textnormal{r}}{{\\textnormal{r}}}\n\\def{\\textnormal{s}}{{\\textnormal{s}}}\n\\def{\\textnormal{t}}{{\\textnormal{t}}}\n\\def{\\textnormal{u}}{{\\textnormal{u}}}\n\\def{\\textnormal{v}}{{\\textnormal{v}}}\n\\def{\\textnormal{w}}{{\\textnormal{w}}}\n\\def{\\textnormal{x}}{{\\textnormal{x}}}\n\\def{\\textnormal{y}}{{\\textnormal{y}}}\n\\def{\\textnormal{z}}{{\\textnormal{z}}}\n\n\\def{\\mathbf{A}}{{\\mathbf{A}}}\n\\def{\\mathbf{B}}{{\\mathbf{B}}}\n\\def{\\mathbf{C}}{{\\mathbf{C}}}\n\\def{\\mathbf{D}}{{\\mathbf{D}}}\n\\def{\\mathbf{E}}{{\\mathbf{E}}}\n\\def{\\mathbf{F}}{{\\mathbf{F}}}\n\\def{\\mathbf{G}}{{\\mathbf{G}}}\n\\def{\\mathbf{H}}{{\\mathbf{H}}}\n\\def{\\mathbf{I}}{{\\mathbf{I}}}\n\\def{\\mathbf{J}}{{\\mathbf{J}}}\n\\def{\\mathbf{K}}{{\\mathbf{K}}}\n\\def{\\mathbf{L}}{{\\mathbf{L}}}\n\\def{\\mathbf{M}}{{\\mathbf{M}}}\n\\def{\\mathbf{N}}{{\\mathbf{N}}}\n\\def{\\mathbf{O}}{{\\mathbf{O}}}\n\\def{\\mathbf{P}}{{\\mathbf{P}}}\n\\def{\\mathbf{Q}}{{\\mathbf{Q}}}\n\\def{\\mathbf{R}}{{\\mathbf{R}}}\n\\def{\\mathbf{S}}{{\\mathbf{S}}}\n\\def{\\mathbf{T}}{{\\mathbf{T}}}\n\\def{\\mathbf{U}}{{\\mathbf{U}}}\n\\def{\\mathbf{V}}{{\\mathbf{V}}}\n\\def{\\mathbf{W}}{{\\mathbf{W}}}\n\\def{\\mathbf{X}}{{\\mathbf{X}}}\n\\def{\\mathbf{Y}}{{\\mathbf{Y}}}\n\\def{\\mathbf{Z}}{{\\mathbf{Z}}}\n\n\\def{\\textnormal{A}}{{\\textnormal{A}}}\n\\def{\\textnormal{B}}{{\\textnormal{B}}}\n\\def{\\textnormal{C}}{{\\textnormal{C}}}\n\\def{\\textnormal{D}}{{\\textnormal{D}}}\n\\def{\\textnormal{E}}{{\\textnormal{E}}}\n\\def{\\textnormal{F}}{{\\textnormal{F}}}\n\\def{\\textnormal{G}}{{\\textnormal{G}}}\n\\def{\\textnormal{H}}{{\\textnormal{H}}}\n\\def{\\textnormal{I}}{{\\textnormal{I}}}\n\\def{\\textnormal{J}}{{\\textnormal{J}}}\n\\def{\\textnormal{K}}{{\\textnormal{K}}}\n\\def{\\textnormal{L}}{{\\textnormal{L}}}\n\\def{\\textnormal{M}}{{\\textnormal{M}}}\n\\def{\\textnormal{N}}{{\\textnormal{N}}}\n\\def{\\textnormal{O}}{{\\textnormal{O}}}\n\\def{\\textnormal{P}}{{\\textnormal{P}}}\n\\def{\\textnormal{Q}}{{\\textnormal{Q}}}\n\\def{\\textnormal{R}}{{\\textnormal{R}}}\n\\def{\\textnormal{S}}{{\\textnormal{S}}}\n\\def{\\textnormal{T}}{{\\textnormal{T}}}\n\\def{\\textnormal{U}}{{\\textnormal{U}}}\n\\def{\\textnormal{V}}{{\\textnormal{V}}}\n\\def{\\textnormal{W}}{{\\textnormal{W}}}\n\\def{\\textnormal{X}}{{\\textnormal{X}}}\n\\def{\\textnormal{Y}}{{\\textnormal{Y}}}\n\\def{\\textnormal{Z}}{{\\textnormal{Z}}}\n\n\\def{\\bm{0}}{{\\bm{0}}}\n\\def{\\bm{1}}{{\\bm{1}}}\n\\def{\\bm{\\mu}}{{\\bm{\\mu}}}\n\\def{\\bm{\\theta}}{{\\bm{\\theta}}}\n\\def{\\bm{\\Theta}}{{\\bm{\\Theta}}}\n\\def{\\bm{\\alpha}}{{\\bm{\\alpha}}}\n\\def{\\bm{\\phi}}{{\\bm{\\phi}}}\n\\def{\\bm{\\beta}}{{\\bm{\\beta}}}\n\\def{\\bm{\\Sigma}}{{\\bm{\\Sigma}}}\n\\def{\\bm{a}}{{\\bm{a}}}\n\\def{\\bm{b}}{{\\bm{b}}}\n\\def{\\bm{c}}{{\\bm{c}}}\n\\def{\\bm{d}}{{\\bm{d}}}\n\\def{\\bm{e}}{{\\bm{e}}}\n\\def{\\bm{f}}{{\\bm{f}}}\n\\def{\\bm{g}}{{\\bm{g}}}\n\\def{\\bm{h}}{{\\bm{h}}}\n\\def{\\bm{i}}{{\\bm{i}}}\n\\def{\\bm{j}}{{\\bm{j}}}\n\\def{\\bm{k}}{{\\bm{k}}}\n\\def{\\bm{l}}{{\\bm{l}}}\n\\def{\\bm{m}}{{\\bm{m}}}\n\\def{\\bm{n}}{{\\bm{n}}}\n\\def{\\bm{o}}{{\\bm{o}}}\n\\def{\\bm{p}}{{\\bm{p}}}\n\\def{\\bm{q}}{{\\bm{q}}}\n\\def{\\bm{r}}{{\\bm{r}}}\n\\def{\\bm{s}}{{\\bm{s}}}\n\\def{\\bm{t}}{{\\bm{t}}}\n\\def{\\bm{u}}{{\\bm{u}}}\n\\def{\\bm{v}}{{\\bm{v}}}\n\\def{\\bm{w}}{{\\bm{w}}}\n\\def{\\bm{x}}{{\\bm{x}}}\n\\def{\\bm{y}}{{\\bm{y}}}\n\\def{\\bm{z}}{{\\bm{z}}}\n\n\\def{\\alpha}{{\\alpha}}\n\\def{\\beta}{{\\beta}}\n\\def{\\epsilon}{{\\epsilon}}\n\\def{\\lambda}{{\\lambda}}\n\\def{\\omega}{{\\omega}}\n\\def{\\mu}{{\\mu}}\n\\def{\\psi}{{\\psi}}\n\\def{\\sigma}{{\\sigma}}\n\\def{\\theta}{{\\theta}}\n\\def{a}{{a}}\n\\def{b}{{b}}\n\\def{c}{{c}}\n\\def{d}{{d}}\n\\def{e}{{e}}\n\\def{f}{{f}}\n\\def{g}{{g}}\n\\def{h}{{h}}\n\\def{i}{{i}}\n\\def{j}{{j}}\n\\def{k}{{k}}\n\\def{l}{{l}}\n\\def{m}{{m}}\n\\def{n}{{n}}\n\\def{o}{{o}}\n\\def{p}{{p}}\n\\def{q}{{q}}\n\\def{r}{{r}}\n\\def{s}{{s}}\n\\def{t}{{t}}\n\\def{u}{{u}}\n\\def{v}{{v}}\n\\def{w}{{w}}\n\\def{x}{{x}}\n\\def{y}{{y}}\n\\def{z}{{z}}\n\n\\def{\\bm{A}}{{\\bm{A}}}\n\\def{\\bm{B}}{{\\bm{B}}}\n\\def{\\bm{C}}{{\\bm{C}}}\n\\def{\\bm{D}}{{\\bm{D}}}\n\\def{\\bm{E}}{{\\bm{E}}}\n\\def{\\bm{F}}{{\\bm{F}}}\n\\def{\\bm{G}}{{\\bm{G}}}\n\\def{\\bm{H}}{{\\bm{H}}}\n\\def{\\bm{I}}{{\\bm{I}}}\n\\def{\\bm{J}}{{\\bm{J}}}\n\\def{\\bm{K}}{{\\bm{K}}}\n\\def{\\bm{L}}{{\\bm{L}}}\n\\def{\\bm{M}}{{\\bm{M}}}\n\\def{\\bm{N}}{{\\bm{N}}}\n\\def{\\bm{O}}{{\\bm{O}}}\n\\def{\\bm{P}}{{\\bm{P}}}\n\\def{\\bm{Q}}{{\\bm{Q}}}\n\\def{\\bm{R}}{{\\bm{R}}}\n\\def{\\bm{S}}{{\\bm{S}}}\n\\def{\\bm{T}}{{\\bm{T}}}\n\\def{\\bm{U}}{{\\bm{U}}}\n\\def{\\bm{V}}{{\\bm{V}}}\n\\def{\\bm{W}}{{\\bm{W}}}\n\\def{\\bm{X}}{{\\bm{X}}}\n\\def{\\bm{Y}}{{\\bm{Y}}}\n\\def{\\bm{Z}}{{\\bm{Z}}}\n\\def{\\bm{\\beta}}{{\\bm{\\beta}}}\n\\def{\\bm{\\Phi}}{{\\bm{\\Phi}}}\n\\def{\\bm{\\Lambda}}{{\\bm{\\Lambda}}}\n\\def{\\bm{\\Sigma}}{{\\bm{\\Sigma}}}\n\n\\DeclareMathAlphabet{\\mathsfit}{\\encodingdefault}{\\sfdefault}{m}{sl}\n\\SetMathAlphabet{\\mathsfit}{bold}{\\encodingdefault}{\\sfdefault}{bx}{n}\n\\newcommand{\\tens}[1]{\\bm{\\mathsfit{#1}}}\n\\def{\\tens{A}}{{\\tens{A}}}\n\\def{\\tens{B}}{{\\tens{B}}}\n\\def{\\tens{C}}{{\\tens{C}}}\n\\def{\\tens{D}}{{\\tens{D}}}\n\\def{\\tens{E}}{{\\tens{E}}}\n\\def{\\tens{F}}{{\\tens{F}}}\n\\def{\\tens{G}}{{\\tens{G}}}\n\\def{\\tens{H}}{{\\tens{H}}}\n\\def{\\tens{I}}{{\\tens{I}}}\n\\def{\\tens{J}}{{\\tens{J}}}\n\\def{\\tens{K}}{{\\tens{K}}}\n\\def{\\tens{L}}{{\\tens{L}}}\n\\def{\\tens{M}}{{\\tens{M}}}\n\\def{\\tens{N}}{{\\tens{N}}}\n\\def{\\tens{O}}{{\\tens{O}}}\n\\def{\\tens{P}}{{\\tens{P}}}\n\\def{\\tens{Q}}{{\\tens{Q}}}\n\\def{\\tens{R}}{{\\tens{R}}}\n\\def{\\tens{S}}{{\\tens{S}}}\n\\def{\\tens{T}}{{\\tens{T}}}\n\\def{\\tens{U}}{{\\tens{U}}}\n\\def{\\tens{V}}{{\\tens{V}}}\n\\def{\\tens{W}}{{\\tens{W}}}\n\\def{\\tens{X}}{{\\tens{X}}}\n\\def{\\tens{Y}}{{\\tens{Y}}}\n\\def{\\tens{Z}}{{\\tens{Z}}}\n\n\n\\def{\\mathcal{A}}{{\\mathcal{A}}}\n\\def{\\mathcal{B}}{{\\mathcal{B}}}\n\\def{\\mathcal{C}}{{\\mathcal{C}}}\n\\def{\\mathcal{D}}{{\\mathcal{D}}}\n\\def{\\mathcal{E}}{{\\mathcal{E}}}\n\\def{\\mathcal{F}}{{\\mathcal{F}}}\n\\def{\\mathcal{G}}{{\\mathcal{G}}}\n\\def{\\mathcal{H}}{{\\mathcal{H}}}\n\\def{\\mathcal{I}}{{\\mathcal{I}}}\n\\def{\\mathcal{J}}{{\\mathcal{J}}}\n\\def{\\mathcal{K}}{{\\mathcal{K}}}\n\\def{\\mathcal{L}}{{\\mathcal{L}}}\n\\def{\\mathcal{M}}{{\\mathcal{M}}}\n\\def{\\mathcal{N}}{{\\mathcal{N}}}\n\\def{\\mathcal{O}}{{\\mathcal{O}}}\n\\def{\\mathcal{P}}{{\\mathcal{P}}}\n\\def{\\mathcal{Q}}{{\\mathcal{Q}}}\n\\def{\\mathcal{R}}{{\\mathcal{R}}}\n\\def{\\mathcal{S}}{{\\mathcal{S}}}\n\\def{\\mathcal{T}}{{\\mathcal{T}}}\n\\def{\\mathcal{U}}{{\\mathcal{U}}}\n\\def{\\mathcal{V}}{{\\mathcal{V}}}\n\\def{\\mathcal{W}}{{\\mathcal{W}}}\n\\def{\\mathcal{X}}{{\\mathcal{X}}}\n\\def{\\mathcal{Y}}{{\\mathcal{Y}}}\n\\def{\\mathcal{Z}}{{\\mathcal{Z}}}\n\n\\def{\\mathbb{A}}{{\\mathbb{A}}}\n\\def{\\mathbb{B}}{{\\mathbb{B}}}\n\\def{\\mathbb{C}}{{\\mathbb{C}}}\n\\def{\\mathbb{D}}{{\\mathbb{D}}}\n\\def{\\mathbb{F}}{{\\mathbb{F}}}\n\\def{\\mathbb{G}}{{\\mathbb{G}}}\n\\def{\\mathbb{H}}{{\\mathbb{H}}}\n\\def{\\mathbb{I}}{{\\mathbb{I}}}\n\\def{\\mathbb{J}}{{\\mathbb{J}}}\n\\def{\\mathbb{K}}{{\\mathbb{K}}}\n\\def{\\mathbb{L}}{{\\mathbb{L}}}\n\\def{\\mathbb{M}}{{\\mathbb{M}}}\n\\def{\\mathbb{N}}{{\\mathbb{N}}}\n\\def{\\mathbb{O}}{{\\mathbb{O}}}\n\\def{\\mathbb{P}}{{\\mathbb{P}}}\n\\def{\\mathbb{Q}}{{\\mathbb{Q}}}\n\\def{\\mathbb{R}}{{\\mathbb{R}}}\n\\def{\\mathbb{S}}{{\\mathbb{S}}}\n\\def{\\mathbb{T}}{{\\mathbb{T}}}\n\\def{\\mathbb{U}}{{\\mathbb{U}}}\n\\def{\\mathbb{V}}{{\\mathbb{V}}}\n\\def{\\mathbb{W}}{{\\mathbb{W}}}\n\\def{\\mathbb{X}}{{\\mathbb{X}}}\n\\def{\\mathbb{Y}}{{\\mathbb{Y}}}\n\\def{\\mathbb{Z}}{{\\mathbb{Z}}}\n\n\\def{\\Lambda}{{\\Lambda}}\n\\def{A}{{A}}\n\\def{B}{{B}}\n\\def{C}{{C}}\n\\def{D}{{D}}\n\\def{E}{{E}}\n\\def{F}{{F}}\n\\def{G}{{G}}\n\\def{H}{{H}}\n\\def{I}{{I}}\n\\def{J}{{J}}\n\\def{K}{{K}}\n\\def{L}{{L}}\n\\def{M}{{M}}\n\\def{N}{{N}}\n\\def{O}{{O}}\n\\def{P}{{P}}\n\\def{Q}{{Q}}\n\\def{R}{{R}}\n\\def{S}{{S}}\n\\def{T}{{T}}\n\\def{U}{{U}}\n\\def{V}{{V}}\n\\def{W}{{W}}\n\\def{X}{{X}}\n\\def{Y}{{Y}}\n\\def{Z}{{Z}}\n\\def{\\Sigma}{{\\Sigma}}\n\n\\newcommand{\\etens}[1]{\\mathsfit{#1}}\n\\def{\\etens{\\Lambda}}{{\\etens{\\Lambda}}}\n\\def{\\etens{A}}{{\\etens{A}}}\n\\def{\\etens{B}}{{\\etens{B}}}\n\\def{\\etens{C}}{{\\etens{C}}}\n\\def{\\etens{D}}{{\\etens{D}}}\n\\def{\\etens{E}}{{\\etens{E}}}\n\\def{\\etens{F}}{{\\etens{F}}}\n\\def{\\etens{G}}{{\\etens{G}}}\n\\def{\\etens{H}}{{\\etens{H}}}\n\\def{\\etens{I}}{{\\etens{I}}}\n\\def{\\etens{J}}{{\\etens{J}}}\n\\def{\\etens{K}}{{\\etens{K}}}\n\\def{\\etens{L}}{{\\etens{L}}}\n\\def{\\etens{M}}{{\\etens{M}}}\n\\def{\\etens{N}}{{\\etens{N}}}\n\\def{\\etens{O}}{{\\etens{O}}}\n\\def{\\etens{P}}{{\\etens{P}}}\n\\def{\\etens{Q}}{{\\etens{Q}}}\n\\def{\\etens{R}}{{\\etens{R}}}\n\\def{\\etens{S}}{{\\etens{S}}}\n\\def{\\etens{T}}{{\\etens{T}}}\n\\def{\\etens{U}}{{\\etens{U}}}\n\\def{\\etens{V}}{{\\etens{V}}}\n\\def{\\etens{W}}{{\\etens{W}}}\n\\def{\\etens{X}}{{\\etens{X}}}\n\\def{\\etens{Y}}{{\\etens{Y}}}\n\\def{\\etens{Z}}{{\\etens{Z}}}\n\n\\newcommand{p_{\\rm{data}}}{p_{\\rm{data}}}\n\\newcommand{\\hat{p}_{\\rm{data}}}{\\hat{p}_{\\rm{data}}}\n\\newcommand{\\hat{P}_{\\rm{data}}}{\\hat{P}_{\\rm{data}}}\n\\newcommand{p_{\\rm{model}}}{p_{\\rm{model}}}\n\\newcommand{P_{\\rm{model}}}{P_{\\rm{model}}}\n\\newcommand{\\tilde{p}_{\\rm{model}}}{\\tilde{p}_{\\rm{model}}}\n\\newcommand{p_{\\rm{encoder}}}{p_{\\rm{encoder}}}\n\\newcommand{p_{\\rm{decoder}}}{p_{\\rm{decoder}}}\n\\newcommand{p_{\\rm{reconstruct}}}{p_{\\rm{reconstruct}}}\n\n\\newcommand{\\laplace}{\\mathrm{Laplace}}\n\n\\newcommand{\\mathbb{E}}{\\mathbb{E}}\n\\newcommand{\\mathcal{L}}{\\mathcal{L}}\n\\newcommand{\\mathbb{R}}{\\mathbb{R}}\n\\newcommand{\\tilde{p}}{\\tilde{p}}\n\\newcommand{\\alpha}{\\alpha}\n\\newcommand{\\lambda}{\\lambda}\n\\newcommand{\\mathrm{rectifier}}{\\mathrm{rectifier}}\n\\newcommand{\\mathrm{softmax}}{\\mathrm{softmax}}\n\\newcommand{\\sigma}{\\sigma}\n\\newcommand{\\zeta}{\\zeta}\n\\newcommand{D_{\\mathrm{KL}}}{D_{\\mathrm{KL}}}\n\\newcommand{\\mathrm{Var}}{\\mathrm{Var}}\n\\newcommand{\\mathrm{SE}}{\\mathrm{SE}}\n\\newcommand{\\mathrm{Cov}}{\\mathrm{Cov}}\n\\newcommand{L^0}{L^0}\n\\newcommand{L^1}{L^1}\n\\newcommand{L^2}{L^2}\n\\newcommand{L^p}{L^p}\n\\newcommand{L^\\infty}{L^\\infty}\n\n\\newcommand{\\parents}{Pa}\n\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\n\\DeclareMathOperator{\\sign}{sign}\n\\DeclareMathOperator{\\Tr}{Tr}\n\\DeclareMathOperator{\\diag}{diag}\n\\let\\ab\\allowbreak\n\n\n\\newtheorem{proposition}{Proposition}\n\\newtheorem*{proposition*}{Proposition}\n\\newtheorem*{remark}{Remark}\n\n\\newcommand{\\rom}[1]{%\n  \\textup{\\uppercase\\expandafter{\\romannumeral#1}}%\n}\n\n\n\\title{Neural Ensemble Search via Bayesian Sampling}\n\n\\author[1]{Yao Shu}\n\\author[1]{Yizhou Chen}\n\\author[1]{Zhongxiang Dai}\n\\author[1]{Bryan Kian Hsiang Low}\n\\affil[1]{%\n    Department of Computer Science\\\\\n    National University of Singapore\\\\\n    Singapore\n}\n  \n\\begin{document}\n\\maketitle\n\n\\begin{abstract}\nRecently, \\emph{neural architecture search} (NAS) has been applied to automate the design of neural networks in real-world applications. A large number of algorithms have been developed to improve the search cost or the performance of the final selected architectures in NAS. Unfortunately, these NAS algorithms aim to select only \\emph{one single} well-performing architecture from their search spaces and thus have overlooked the capability of \\emph{neural network ensemble} (i.e., an ensemble of neural networks with diverse architectures) in achieving improved performance over a single final selected architecture.\nTo this end, we introduce a novel neural ensemble search algorithm, called \\emph{neural ensemble search via Bayesian sampling} (NESBS), to effectively and efficiently select well-performing neural network ensembles from a NAS search space. In our extensive experiments, NESBS algorithm is shown to be able to achieve improved performance over state-of-the-art NAS algorithms while incurring a comparable search cost, thus indicating the superior performance of our NESBS algorithm over these NAS algorithms in practice.\n\\end{abstract}\n\n\\section{Introduction}\n\nRecent years have witnessed a surging interest in designing well-performing architectures for different tasks. These architectures are typically manually designed by human experts, which requires numerous trials and errors during this manual design process and therefore is prohibitively costly. Consequently, the increasing demand for developing well-performing architectures in different tasks makes this manual design infeasible. To avoid such human efforts, \\citet{nas} have introduced \\emph{neural architecture search} (NAS) to help automate the design of architectures. Since then, a number of NAS algorithms \\citep{enas, darts, p-darts} have been developed to improve the search efficiency (i.e., search cost) or the search effectiveness (i.e., generalization performance of their final selected architectures) in NAS.\n\n\nHowever, conventional NAS algorithms aim to select only \\emph{one single architecture} from their search spaces and have thus overlooked the capability of other candidate architectures from the same search spaces in helping improve the performance achieved by their final selected single architecture. \nThat is, \\emph{neural network ensembles} are widely known to be capable of achieving an improved performance compared with a single neural network in practice \\citep{adanet, mc-dropout, deepens}.\nThis naturally begs the question: \\emph{How to select best-performing neural network ensembles with diverse architectures from a NAS search space in order to improve the performances achieved by existing NAS algorithms?}\nTo the best of our knowledge, only limited efforts (e.g., \\citep{nes}) have been devoted to this problem in the NAS literature. Unfortunately, the \\emph{neural ensemble search} (NES) algorithm based on random search or evolutionary algorithm in \\citep{nes} requires excessive search costs to select their final neural network ensembles, which will not be affordable in resource-constrained scenarios.\n\n\nTo this end, this paper introduces a novel algorithm, namely \\emph{neural ensemble search via Bayesian sampling} (NESBS), to effectively and efficiently select the well-performing neural network ensemble with diverse architectures from a search space.\nWe firstly represent the search space as a supernet following conventional one-shot NAS algorithms and then use the model parameters inherited from this supernet after its model training to estimate the single-model performances and also the ensemble performance of independently trained architectures (Sec. \\ref{sec:oneshot}). Next, since both single-model performances and diverse model predictions affect the final ensemble performance according to \\citep{zhou-ensemble}, we propose to use a variational posterior distribution of architectures based on a trained supernet to characterize these two factors, i.e., single-model performances and diverse model predictions (Sec. \\ref{sec:posterior}). We then introduce two novel Bayesian sampling algorithms based on the posterior distribution of architectures, i.e., \\emph{Monte Carlo sampling} (MC Sampling) and \\emph{Stein Variational Gradient Descent with regularized diversity} (SVGD-RD), to effectively and efficiently select ensembles with both competitive single-model performances and compelling diverse model predictions (Sec. \\ref{sec:sampling}), which is also guaranteed to be able to achieve impressive ensemble performances \\citep{zhou-ensemble}. Lastly, we use extensive experiments to show that our NESBS algorithm is indeed able to select well-performing neural network ensembles effectively and efficiently in practice (Sec.~\\ref{sec:exps}).\n\n\n\\section{Related Works \\& Background}\n\n\\subsection{Neural Architecture Search}\nIn the literature, many NAS algorithms \\citep{amoebanet, nas, nasnet} have been developed to automate the design of well-performing neural architectures. However, these NAS algorithms are inefficient in practice due to their requirement of the independent model training for each candidate architecture in the search space. To reduce such training costs, a supernet has been introduced to represent the search space and also share model parameters among the candidate architectures in the search space \\citep{enas}. As a result, only the model training of this supernet is required, which can significantly improve the search efficiency of conventional NAS algorithms. After that, a number of one-shot NAS algorithms based on model parameter sharing \\citep{p-darts, sdarts, darts-, darts, snas} have been developed. \nUnfortunately, these algorithms aim to select \\emph{only one single architecture} from their search spaces. Thus, the capability of other candidate architectures from the same search spaces in helping improve the performance of their final selected single architecture have been overlooked. \n\n\n\\subsection{Neural Network Ensembles}\nMeanwhile, neural network ensembles have been widely applied to improve the performance of a single neural network in different applications \\citep{eml}. Over the years, a number of methods have been proposed to construct such neural network ensembles. For example, \\citet{mc-dropout} have proposed to use Monte Carlo Dropout to obtain neural network ensembles at test time. Meanwhile, \\emph{deep ensembles} (DeepEns) \\citep{deepens} adopt neural networks trained with different random initializations to construct ensembles and has achieved impressive performances on various tasks. \nAnother line of ensemble works uses the checkpoints obtained during model training to build neural network ensembles \\citep{snapshot}. More recently, \\citet{nes} have introduced \\emph{neural ensemble search} (NES) into NAS area to build well-performing neural network ensembles by selecting diverse architectures from the NAS search space, which has achieved competitive performance even compared with other ensemble methods. Unfortunately, the algorithm presented in \\citep{nes} is shown to be prohibitively costly, which will not be affordable in resource-constrained scenarios. To this end, this paper presents a novel NESBS algorithm to advance this line of works (e.g., NES) by achieving state-of-the-art performances for neural network ensembles with diverse architectures while incurring a reduced search cost.\n\n\n\\subsection{Stein Variational Gradient Descent}\\label{sec:bg-svgd}\n\\emph{Stein Variational Gradient Descent} (SVGD) \\citep{svgd} is a variational inference algorithm that approximates a target distribution $p({\\bm{x}})$ with a simpler density $q^*({\\bm{x}})$ in a predefined set ${\\mathcal{Q}}$ by minimizing the \\emph{Kullback-Leibler} (KL) divergence between these two densities:\n\\begin{equation}\\label{eq:min-kl}\n    q^* = \\argmin_{q \\in {\\mathcal{Q}}} \\{\\text{KL}(q || p)\\triangleq\\mathbb{E}_q\\left[\\log\\left(q({\\bm{x}}) / p({\\bm{x}})\\right)\\right]\\} \\ .\n\\end{equation}\nSpecifically, SVGD represents $q^*({\\bm{x}})$ with a set of particles $\\{{\\bm{x}}_i\\}_{i=1}^{n}$ which are firstly randomly initialized and then iteratively updated with updates ${\\bm{\\phi}}^*({\\bm{x}}_i)$ and a step size $\\epsilon$:\n\\begin{equation}\\label{eq:svgd-iter}\n    {\\bm{x}}_i \\leftarrow {\\bm{x}}_i + \\epsilon{\\bm{\\phi}}^*({\\bm{x}}_i) \\quad \\text{for}\\ i = 1,\\ldots,n \\ .\n\\end{equation}\nLet $q_{[\\epsilon{\\bm{\\phi}}]}$ denote the distribution of updated particles ${\\bm{x}}' = {\\bm{x}} + \\epsilon{\\bm{\\phi}}({\\bm{x}})$. Let $\\mathbb{F}$ denote the unit ball of a vector-valued \\emph{reproducing kernel Hilbert space} (RKHS) ${\\mathcal{H}}\\triangleq{\\mathcal{H}}_0 \\times \\ldots \\times{\\mathcal{H}}_0$ where ${\\mathcal{H}}_0$ is an RKHS formed by scalar-valued functions associated with a positive definite kernel $k({\\bm{x}}, {\\bm{x}}')$. The work of \\citet{svgd} has shown that \\eqref{eq:svgd-iter} can be viewed as functional gradient descent in the RKHS ${\\mathcal{H}}$ and the optimal ${\\bm{\\phi}}^*$ in \\eqref{eq:svgd-iter} can be obtained by solving the following problem:\n\\begin{equation}\n    {\\bm{\\phi}}^* = \\argmax_{{\\bm{\\phi}} \\in \\mathbb{F}}\\left\\{-\\frac{d}{d\\epsilon}\\text{KL}(q_{[\\epsilon{\\bm{\\phi}}]} || p)\\Bigr|_{\\epsilon=0}\\right\\} \\ ,\n\\end{equation}\nwhich yields a closed-form solution: \n\\begin{equation}\n    {\\bm{\\phi}}^*(\\cdot) = \\mathbb{E}_{{\\bm{x}}\\sim q}[k({\\bm{x}}, \\cdot)\\nabla_{{\\bm{x}}}\\log p({\\bm{x}}) + \\nabla_{{\\bm{x}}}k({\\bm{x}}, \\cdot)] \\ .\n\\end{equation}\nIn practice, \\citet{svgd} have approximated the expectation in this closed-form solution with the empirical mean of particles: ${\\bm{\\phi}}^*({\\bm{x}}_i) \\approx \\widehat{{\\bm{\\phi}}}^*({\\bm{x}}_i)$ where $\\widehat{{\\bm{\\phi}}}^*({\\bm{x}}_i)$ is defined as\n\\begin{equation}\\label{eq:svgd-approx}\n    \\widehat{{\\bm{\\phi}}}^*({\\bm{x}}_i) \\triangleq \\\\\n    \\frac{1}{n}\\sum_{j=1}^n k({\\bm{x}}_j, {\\bm{x}}_i)\\nabla_{{\\bm{x}}_j}\\log p({\\bm{x}}_j) + \\nabla_{{\\bm{x}}_j} k ({\\bm{x}}_j, {\\bm{x}}_i) \\ .\n\\end{equation}\nAs revealed in \\citep{svgd}, the two terms in the aforementioned closed-form solution take different effects: The first term with $\\nabla_{{\\bm{x}}}\\log p({\\bm{x}})$ favors particles with higher probability density, while the second term pushes the particles away from each other to encourage diversity.\n\n\n\\section{Neural Ensemble Search via Bayesian Sampling}\nContrary to the selection of one single architecture in conventional NAS algorithm, this paper focuses on the problem of selecting a well-performing neural network ensemble with diverse architectures from the NAS search space, i.e., \\emph{neural ensemble search} (NES) \\citep{nes}. Let ${\\bm{f}}_{{\\mathcal{A}}}({\\bm{x}}, {\\bm{\\theta}}_{{\\mathcal{A}}})$ denote the output of an architecture ${\\mathcal{A}}$ with input data ${\\bm{x}}$ and model parameter ${\\bm{\\theta}}_{{\\mathcal{A}}}$,  $S$ be a set of architectures, ${\\bm{\\Theta}}_S$ be a set of the corresponding model parameters of these architectures, and $\\mathcal{L}_{\\text{train}}$ and $\\mathcal{L}_{\\text{val}}$ denote the training and validation losses, respectively. Given the ensemble scheme ${\\mathcal{F}}_S({\\bm{x}}, {\\bm{\\Theta}}_S)\\triangleq n^{-1}\\textstyle\\sum_{{\\mathcal{A}} \\in S}{\\bm{f}}_{{\\mathcal{A}}}({\\bm{x}}, {\\bm{\\theta}}_{{\\mathcal{A}}})$ with an ensemble size of $|S|=n$,\\footnote{We apply this ensemble scheme for simplicity. Other ensemble schemes can also be used in the algorithm of this paper.} NES can be formally framed as\n\\begin{equation}\\label{eq:nes}\n\\begin{gathered}\n    \\min_{S} \\mathcal{L}_{\\text{val}}({\\mathcal{F}}_S({\\bm{x}}, {\\bm{\\Theta}}_S^*)) \\\\\n    \\text{s.t.} \\; \\forall  {\\bm{\\theta}}^*_{{\\mathcal{A}}} \\in {\\bm{\\Theta}}^*_S \\quad {\\bm{\\theta}}^*_{{\\mathcal{A}}} = \\argmin_{{\\bm{\\theta}}_{{\\mathcal{A}}}} \\mathcal{L}_{\\text{train}}({\\bm{f}}_{{\\mathcal{A}}}({\\bm{x}}, {\\bm{\\theta}}_{{\\mathcal{A}}})) \\ .\n\\end{gathered}\n\\end{equation}\n\n\nUnfortunately, \\eqref{eq:nes} is challenging to solve mainly due to the following two reasons: (\\rom{1}) The enormous number of candidate architectures in the NAS search space  (e.g., ${\\sim}10^{25}$ in the DARTS search space \\citep{darts}) makes the independent model training of every candidate architecture (i.e., lower-level optimization in \\eqref{eq:nes}) unaffordable. (\\rom{2}) The ensemble search space is exponentially increasing in the ensemble size $n$: For example, there are ${\\sim}m^n$ different ensembles given $m$ diverse architectures. The combinatorial optimization problem (i.e., upper-level optimization in \\eqref{eq:nes}) is thus intractable to solve within this huge ensemble search space. Recently, \\citet{nes} have attempted to avoid these two problems by sampling a small pool of architectures from the search space for their final ensemble search. Thus, they fail to explore the whole search space and may achieve poor ensemble performances in practice. Moreover, their search cost is still unaffordable due to the independent model training of every architecture in the pool.\n\nTo this end, we novelly present the \\emph{neural ensemble search via Bayesian sampling} (NESBS) algorithm to solve \\eqref{eq:nes} effectively and efficiently. We firstly employ the model parameters inherited from a supernet (i.e., a representation of the NAS search space) after its model training to estimate the single-model performances and also the ensemble performance of independently trained architectures (Sec. \\ref{sec:oneshot}). This only requires the model training of the supernet and thus allows us to overcome the aforementioned challenge \\rom{1}. We then derive a posterior distribution of architectures to characterize both the single-model performances and the diverse model predictions of candidate architectures in the search space (Sec. \\ref{sec:posterior}). Finally, based on this posterior distribution and also the aforementioned ensemble performance estimation, we introduce \\emph{Monte Carlo Sampling} (MC Sampling) and \\emph{Stein Variational Gradient Descent with regularized diversity} (SVGD-RD) to explore the ensembles in the whole~search space~effectively and efficiently (Sec. \\ref{sec:sampling}), which thus allows us to overcome the aforementioned challenge \\rom{2}. An overview of our NESBS is in Algorithm~\\ref{alg:nes}.\n\n\n\\subsection{Model Training of Supernet}\\label{sec:oneshot}\n\n\nSimilar to one-shot NAS algorithms \\citep{darts, enas}, we represent NAS search space as a supernet. This then allows us to use the model parameters inherited from this trained supernet to estimate not only the single-model performances but also the ensemble performance of independently trained candidate architectures in the search space. However, in order to realize an accurate and fair estimation of these performances, we need to further ensure that every candidate architecture in the search space is trained for a comparable number of steps, namely, the training fairness among candidate architectures \\citep{fairnas}.\nTo achieve this, in every training step of this supernet, we uniformly randomly sample one single candidate architecture from this supernet for model training (see Fig.~\\ref{fig:train}). The training fairness of such a training scheme can then be theoretically guaranteed, as demonstrated in Appendix \\ref{sec:proofs}. Moreover, we provide empirical results in Appendix \\ref{sec:exp-oneshot} to validate the effectiveness of such performance estimations.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\columnwidth]{figs/supernet-training.pdf}\n\\caption{An illustration of the model training of~supernet. The supernet here consists of three candidate architectures with $r_i$ indicating the selection of one architecture and ${\\bm{\\theta}}_i^{t}$ denoting its model parameters at step $t$. In every training step, only one architecture is uniformly sampled to update its parameters and all other architectures will be ignored.}\n\\label{fig:train}\n\\end{figure}\n\n\\subsection{Distribution of Architectures}\\label{sec:posterior}\nIt has been demonstrated that both competitive single-model performances and diverse model predictions are required to achieve compelling ensemble performances \\citep{zhou-ensemble}. That is, NES algorithms should be capable of selecting architectures with both competitive single-model performances and diverse model predictions to achieve competitive ensemble performances. To realize this, we introduce a posterior distribution of architectures to firstly characterize these two factors. Let ${\\mathcal{D}}$ denote the validation dataset, and $p({\\mathcal{A}})$ and $p({\\mathcal{A}} | {\\mathcal{D}})$ denote, respectively, the prior and posterior distributions of a candidate architecture after its model training where $p({\\mathcal{A}})$ follows from a categorical uniform distribution, as required in Sec. \\ref{sec:oneshot}. According to the Bayes' theorem, since $p({\\mathcal{A}})$ is uniform and $p({\\mathcal{D}})$ is constant,\n\\begin{equation}\\label{eq:bayes}\n\\begin{gathered}\n    p({\\mathcal{A}}|{\\mathcal{D}}) = p({\\mathcal{D}}|{\\mathcal{A}})p({\\mathcal{A}})/p({\\mathcal{D}}) \\propto p({\\mathcal{D}}|{\\mathcal{A}})\n\\end{gathered}\n\\end{equation}\nwhere $p({\\mathcal{D}}|{\\mathcal{A}})$ (i.e., likelihood) is widely used to represent the single-model performance (i.e., loss) in practice.\nSo, \\eqref{eq:bayes} implies that the posterior distribution $p({\\mathcal{A}}|{\\mathcal{D}})$ can also characterize the single-model performances of architectures.\n\nMeanwhile, given a $\\gamma$-Lipschitz continuous loss function $\\mathcal{L}({\\bm{f}})$, \nthe diversity of model predictions (i.e., $\\|{\\bm{f}}_{{\\mathcal{A}}_1}{-}{\\bm{f}}_{{\\mathcal{A}}_2}\\|_2$) can then be lower bounded  based on the Lipschitz continuity of $\\mathcal{L}(\\cdot)$:\n\\begin{equation}\\label{eq:approx}\n    \\begin{gathered}\n        \\|{\\bm{f}}_{{\\mathcal{A}}_1} - {\\bm{f}}_{{\\mathcal{A}}_2}\\|_2 \\geq \\gamma^{-1}|\\mathcal{L}({\\bm{f}}_{{\\mathcal{A}}_1}) - \\mathcal{L}({\\bm{f}}_{{\\mathcal{A}}_2})| \\ .\n    \\end{gathered}\n    \\end{equation}\nTherefore, \\eqref{eq:approx} suggests that in addition to being able to characterize the single-model performances of architectures (i.e., $\\mathcal{L}({\\bm{f}})$), the posterior distribution $p({\\mathcal{A}}|{\\mathcal{D}})$ can estimate the diversity of model predictions for different architectures (e.g., ${\\mathcal{A}}_1$ and ${\\mathcal{A}}_2$) using $|p({\\mathcal{A}}_1|{\\mathcal{D}}) - p({\\mathcal{A}}_2|{\\mathcal{D}})|$.\n\nHowever, it is intractable to obtain exact posterior distribution $p({\\mathcal{A}}|{\\mathcal{D}})$ in the NAS search space. So, we approximate it with a variational distribution $p_{{\\bm{\\alpha}}}({\\mathcal{A}})$ (parameterized by a low-dimensional ${\\bm{\\alpha}}$) that can be optimized via variational inference, i.e., by minimizing the KL divergence between $p_{{\\bm{\\alpha}}}({\\mathcal{A}})$ and $p({\\mathcal{A}}|{\\mathcal{D}})$. Equivalently, we only need to maximize a lower bound of the log-marginal likelihood (i.e., the \\emph{evidence lower bound} (ELBO) \\citep{vae}) to get an optimal variational distribution $p_{{\\bm{\\alpha}}^*}({\\mathcal{A}})$:\n\\begin{equation}\\label{eq:elbo}\n\\begin{gathered}\n    \\max_{{\\bm{\\alpha}}} \\mathbb{E}_{{\\mathcal{A}} \\sim p_{{\\bm{\\alpha}}}({\\mathcal{A}})}\\left[\\log p({\\mathcal{D}}|{\\mathcal{A}})\\right] - \\text{KL}[p_{{\\bm{\\alpha}}}({\\mathcal{A}}) || p({\\mathcal{A}})] \\ .\n\\end{gathered}\n\\end{equation}\nSimilar to \\citep{vae}, a gradient-based optimization algorithm with the reparameterization trick is employed to solve \\eqref{eq:elbo} efficiently (see Appendix \\ref{sec:setting-posterior}). While \\citet{snas} have adopted a similar form to \\eqref{eq:elbo} (without the KL term) \\emph{during} the model training of the supernet (namely, the \\emph{best-response} posterior distribution), our \\emph{post-training} posterior distribution is able to not only provide a more accurate characterization of the single-model performances but also contribute to an improved ensemble search performance, as demonstrated in Appendix \\ref{sec:exp-best_vs_post}. \n\n\\subsection{Bayesian Sampling}\\label{sec:sampling}\nTo solve \\eqref{eq:nes} effectively and efficiently, we finally introduce two novel Bayesian sampling algorithms based on the posterior distribution of architectures in Sec.~\\ref{sec:posterior}, i.e., \\emph{Monte Carlo sampling} (MC Sampling) and \\emph{Stein Variational Gradient Descent with regularized diversity} (SVGD-RD), to sample ensembles with both competitive single-model performances and compelling diversity of model predictions, as required by well-performing ensembles~\\citep{zhou-ensemble}.\n\n\\begin{figure}[t]\n\\begin{minipage}{\\columnwidth}\n\\begin{algorithm}[H]\n  \\caption{NES via Bayesian Sampling (NESBS)}\n  \\label{alg:nes}\n\\begin{algorithmic}[1]\n  \\STATE {\\bfseries Input:} Iterations $T$, ensemble size $n$, a supernet\n  \\STATE Train the supernet to get its tuned parameters ${\\bm{\\theta}}^*$\n  \\STATE Obtain the posterior distribution $p_{{\\bm{\\alpha}}^*}({\\mathcal{A}})$ with \\eqref{eq:elbo}\n  \\FOR{iteration $t=1, \\ldots, T$}\n  \\STATE Sample $S_t$ of size $n$ via Algorithm \\ref{alg:mc} or \\ref{alg:svgd-rd} \n  \\STATE Evaluate estimated $\\mathcal{L}_{\\text{val}}({\\mathcal{F}}_{S_t}({\\bm{x}}, {\\bm{\\Theta}}_{S_t}^*))$ given ${\\bm{\\theta}}^*$\n  \\ENDFOR\n  \\STATE Select optimum $S^* = \\argmin_{S_t} \\mathcal{L}_{\\text{val}}({\\mathcal{F}}_{S_t}({\\bm{x}}, {\\bm{\\Theta}}_{S_t}^*))$\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\hfill\n\\begin{minipage}{\\columnwidth}\n\\begin{algorithm}[H]\n  \\caption{MC Sampling}\n  \\label{alg:mc}\n\\begin{algorithmic}[1]\n  \\STATE {\\bfseries Input:} Ensemble size $n$, set $S=\\emptyset$, posterior $p_{{\\bm{\\alpha}}^*}({\\mathcal{A}})$\n  \\FOR{iteration $i=1, \\ldots, n$}\n  \\STATE Sample ${\\mathcal{A}}_i \\sim p_{{\\bm{\\alpha}}^*}({\\mathcal{A}})$\n  \\STATE $S \\leftarrow S \\cup \\{{\\mathcal{A}}_i\\}$\n  \\ENDFOR\n  \\STATE {\\bfseries Output:} $S$\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\n\\begin{minipage}{\\columnwidth}\n\\begin{algorithm}[H]\n  \\caption{SVGD-RD}\n  \\label{alg:svgd-rd}\n\\begin{algorithmic}[1]\n  \\STATE {\\bfseries Input:} Diversity coefficient $\\delta$, ensemble size $n$, iterations $L$, initial particles $\\{{\\bm{x}}_i^{(0)}\\}_{i=1}^n$, posterior $p_{{\\bm{\\alpha}}^*}({\\mathcal{A}})$, kernel $k({\\bm{x}}, {\\bm{x}}')$, step size $\\{\\epsilon_l\\}_{l=1}^L$\n  \\FOR{iteration $l=0, \\ldots, L-1$}\n  \\STATE Evaluate updates $\\widehat{{\\bm{\\phi}}}_l^*({\\bm{x}}) =\\displaystyle\\frac{1}{n}\\sum_{j=1}^n\\nabla_{{\\bm{x}}_j^{(l)}}k({\\bm{x}}_j^{(l)}, {\\bm{x}})- \\delta \\nabla_{{\\bm{x}}}k({\\bm{x}}_j^{(l)}, {\\bm{x}}) + k({\\bm{x}}_j^{(l)},{\\bm{x}})\\nabla_{{\\bm{x}}_j^{(l)}}\\log p_{{\\bm{\\alpha}}^*}$\n  \\STATE Update particles ${\\bm{x}}_i^{(l+1)} \\leftarrow {\\bm{x}}_i^{(l)} + \\epsilon_l\\ \\widehat{{\\bm{\\phi}}}_l^*({\\bm{x}}_i^{(l)})$\n  \\ENDFOR\n  \n  \\STATE {\\bfseries Output:} $S=\\{{\\mathcal{A}}_i\\}_{i=1}^n$ derived based on $\\{{\\bm{x}}_i^{(L)}\\}_{i=1}^n$\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\end{figure}\n\n\n\\subsubsection{Monte Carlo Sampling (MC Sampling)}\\label{sec:mc}\nGiven the posterior distribution of architectures in Sec. \\ref{sec:posterior}, we firstly propose to use \\emph{Monte Carlo sampling} (MC Sampling) to sample a set of architectures from this posterior distribution (Algorithm \\ref{alg:mc}). Note that MC Sampling guarantees that architectures with better single-model performances will be sampled (i.e., exploited) with higher probabilities, while architectures with diverse model predictions\ncan also be sampled (i.e., explored) due to the inherent randomness in the sampling process.\nCompared with conventional NAS algorithms that select only one single well-performing architecture from the search space \\citep{gdas, snas}, our MC sampling algorithm extends these algorithms by exploring the capability of diverse architectures while preserving its exploitation of architectures with compelling single-model performances.\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\columnwidth]{figs/svgd_rd.pdf}\n\\caption{Impact of $\\delta$ in SVGD-RD. We use contours and dots to denote the density of target distribution and sampled particles, respectively. The target distribution is chosen to be $p({\\bm{x}}){=}(1/Z)\\left[{\\mathcal{N}}({\\bm{x}} | {\\bm{u}}_1, \\Sigma_1) + {\\mathcal{N}}({\\bm{x}} | {\\bm{u}}_2, \\Sigma_2)\\right]$ where ${\\bm{u}}_1{=}(-1, 0)$, ${\\bm{u}}_2{=}(0, 1)$, $\\Sigma_1{=}\\Sigma_2{=}\\diag((0.25, 0.5))$, and $Z$ denotes the normalization constant. Those sampled particles are obtained from Algorithm \\ref{alg:svgd-rd} using $L{=}1000$, $n{=}15$, $\\epsilon_l{=}0.1$, and a \\emph{radial basis function} (RBF) kernel. Notably, SVGD-RD tends to sample particles with more diverse probability densities as $\\delta$ is increased, hence indicating a controllable (via $\\delta$) diversity in our SVGD-RD algorithm. Meanwhile, SVGD-RD can consistently sample particles with high probability densities under varying $\\delta$.}\n\\label{fig:diversity}\n\\end{figure}\n\n\n\\subsubsection{SVGD with Regularized Diversity (SVGD-RD)}\\label{sec:svgd-rd}\nHowever, the diversity of sampled architectures using the MC Sampling algorithm above cannot be controlled and hence may lead to poor ensemble search results.\nSo, in order to achieve a controllable diversity, we resort to \\emph{Stein Variational Gradient Descent} (SVGD). Theoretically, SVGD is capable of sampling particles with both large probability density and good diversity where the diversity is explicitly encouraged (i.e., by the second term in \\eqref{eq:svgd-approx}). Nonetheless, in practice, the particles sampled by SVGD may still fail to represent the target distribution well owing to the lack of diversity among those sampled particles, as observed in \\citep{mp-svgd}. Besides, the diversity of sampled particles in standard SVGD  still cannot be controlled by human experts. \n\nWe hence develop an \\emph{SVGD with regularized diversity} (SVGD-RD) sampling algorithm that can achieve a controllable diversity among those sampled particles. We follow the notations from Sec. \\ref{sec:bg-svgd}.\nIn particular, when optimizing the distribution $q^*$ (represented by the $n$ particles $\\{{\\bm{x}}^*_i\\}_{i=1}^{n}$), we modify the objective in \\eqref{eq:min-kl} by adding a term representing the (controllable) diversity among the particles measured by the kernel function $k({\\bm{x}}, {\\bm{x}}')$:\n\\begin{equation}\\label{eq:min-kl-max-div}\n    q^* = \\argmin_{q \\in {\\mathcal{Q}}}\\text{KL}(q||p) + n\\delta\\  \\mathbb{E}_{{\\bm{x}},{\\bm{x}}' \\sim q} \\left[k({\\bm{x}}, {\\bm{x}}')\\right] \n\\end{equation}\nwhere $\\delta$ is the parameter explicitly controlling the diversity, and $p$ in \\eqref{eq:min-kl-max-div} denotes the posterior distribution $p_{{\\bm{\\alpha}}^*}({\\mathcal{A}})$ derived in Sec. \\ref{sec:posterior} which we intend to sample from.\nFollowing the work of SVGD, $q^*$ in \\eqref{eq:min-kl-max-div} is represented by $\\{{\\bm{x}}^*_i\\}_{i=1}^{n}$ denoting our final selected neural network ensemble that can achieve both competitive single-model performances (i.e., large probability density) and also diverse model predictions. Proposition \\ref{prop:svgdrd-update} below provides one possible update rule for the particles $\\{{\\bm{x}}_i\\}_{i=1}^{n}$ to optimize \\eqref{eq:min-kl-max-div} (see its proof in Appendix \\ref{sec:proofs}).\nFinally, Algorithm \\ref{alg:svgd-rd} summarizes the details of our SVGD-RD algorithm and Appendix \\ref{sec:setting-svgd} provides its optimization details in practice. After obtaining those optimal particles $\\{{\\bm{x}}^*_i\\}_{i=1}^{n}$ in our SVGD-RD algorithm, we then apply these particles to derive the architectures in our final selected ensembles (see details in Appendix \\ref{sec:setting-svgd}).\n\n\\begin{proposition}\\label{prop:svgdrd-update}\n    Given the proximal operator $\\normalfont \\text{prox}_h({\\bm{y}})=\\argmin_{{\\bm{z}}}h({\\bm{z}})+1/2\\|{\\bm{z}}-{\\bm{y}}\\|_2^2$, by applying proximal gradient method \\citep{proximal} and proper approximation, \\eqref{eq:min-kl-max-div} can be optimized via the following updates of the particles $\\{{\\bm{x}}_i\\}_{i=1}^{n}$:\n    \\begin{equation*}\n    \\begin{array}{l}\n        \\displaystyle{\\bm{x}}_i \\leftarrow {\\bm{x}}_i + \\frac{1}{n} \\sum_{j=1}^n k({\\bm{x}}_j, {\\bm{x}}_i)\\nabla_{{\\bm{x}}_j}\\log p({\\bm{x}}_j) \\\\\n        \\displaystyle\\qquad\\qquad\\qquad\\quad\\ + \\nabla_{{\\bm{x}}_j} k ({\\bm{x}}_j, {\\bm{x}}_i) - \\delta\\nabla_{{\\bm{x}}_i} k ({\\bm{x}}_j, {\\bm{x}}_i)\\ .\n    \\end{array}\n    \\end{equation*}\n\\end{proposition}\n\n\nCompared with MC Sampling, our SVGD-RD algorithm provides a controllable trade-off between the single-model performances and the diverse model predictions.\nOn the one hand, the minimization of the KL divergence term in \\eqref{eq:min-kl-max-div} encourages the selection of architectures with competitive single-model performances by favoring particles with high probability densities, as shown by Proposition \\ref{prop:exploitation} below (its proof is in Appendix \\ref{sec:proofs}).\\footnote{Although Proposition \\ref{prop:exploitation} is only applicable in the case of $n=1$, our SVGD-RD is still capable of sampling particles with high probability densities when $n>1$, as validated in Fig.~\\ref{fig:diversity}.}\nOn the other hand, the maximization of the scaled distance $-n\\delta\\ \\mathbb{E}_{{\\bm{x}},{\\bm{x}}' \\sim q} \\left[k({\\bm{x}}, {\\bm{x}}')\\right]$ among the sampled particles leads to a controllable diversity (via $\\delta$) among these sampled particles and also a controllable diversity of the probability densities among these particles (see Fig.~\\ref{fig:diversity}), which also implies a controllable diversity of the model predictions, as suggested in Sec.~\\ref{sec:posterior}.\n\n\\begin{proposition}\\label{prop:exploitation}\nLet $p$ be a target density and $k({\\bm{x}},{\\bm{x}}')=c$ for every ${\\bm{x}}={\\bm{x}}'$ where $c$ is a constant. For any $\\delta \\in \\mathbb{R}$, our SVGD-RD algorithm is equivalent to the maximization of the density $p$ w.r.t.~${\\bm{x}}$ in the case of $n=1$.\n\\end{proposition}\n\n\n\n\\section{Experiments}\\label{sec:exps}\n\\subsection{Search in NAS-Bench-201}\\label{sec:nasbench}\n\n\\begin{table*}[t]\n\\caption{Comparison of architectures selected by different NAS and ensemble (search) algorithms in NAS-Bench-201 with ensemble size $n=3$. Test errors are reported with the mean and standard error of three independent trials and our search costs are evaluated on a single Nvidia $1080$Ti GPU.  Results marked by $\\dagger$ are reported by \\citet{nasbench201}.}\n\\centering\n\\resizebox{0.87\\textwidth}{!}{\n\\begin{threeparttable}\n\\begin{tabular}{lcccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Architecture(s)}} & \\multicolumn{3}{c}{\\textbf{Test Error} (\\%)} &\n\\multirow{2}{2cm}{\\textbf{ Search Cost} (GPU Hours)} \\\\\n\\cmidrule(l){2-4}\n& \\textbf{CIFAR-10} & \\textbf{CIFAR-100} & \\textbf{ImageNet-16-200} &  \\\\\n\\midrule\n& \\multicolumn{4}{c}{\\textbf{Manual design}} \\\\\nResNet$^{\\dagger}$ \\citep{resnet} & 6.03 & 29.14 & 56.37 & - \\\\\n\\midrule\n& \\multicolumn{4}{c}{\\textbf{NAS algorithms}} \\\\\nENAS$^{\\dagger}$ \\citep{enas} & 45.70$\\pm$0.00 & 84.39$\\pm$0.00 & 83.68$\\pm$0.00 & 3.7 \\\\\nDARTS$^{\\dagger}$ (2nd) \\citep{darts} & 45.70$\\pm$0.00 & 84.39$\\pm$0.00 & 83.68$\\pm$0.00 & 8.3 \\\\\nGDAS$^{\\dagger}$ \\citep{gdas} & 6.49$\\pm$0.13 & 29.39$\\pm$0.26 & 58.16$\\pm$0.90 & 8.0 \\\\\nSETN$^{\\dagger}$ \\citep{setn} & 13.81$\\pm$4.63 & 43.13$\\pm$7.77 & 68.10 $\\pm$4.07 & 8.6 \\\\\nRSPS$^{\\dagger}$ \\citep{rsps} & 12.34$\\pm$1.69 & 41.67$\\pm$4.34 & 68.86$\\pm$3.88 & 2.1 \\\\\n\\midrule\n& \\multicolumn{4}{c}{\\textbf{Ensemble (search) algorithms}} \\\\\nDeepEns \\citep{deepens} & 5.75 & 25.27 & 54.70 & - \\\\\nNES-RS \\citep{nes} & 5.83$\\pm$0.33 & 25.58$\\pm$0.84 & 54.34$\\pm$1.67 & 5.1 \\\\\n\\midrule\n& \\multicolumn{4}{c}{\\textbf{Our ensemble search algorithm}} \\\\\nNESBS (MC Sampling) & 5.76$\\pm$0.25 & 25.39$\\pm$0.69 & \\textbf{53.47}$\\pm$1.75 & \\textbf{1.1} \\\\\nNESBS (SVGD-RD) & 5.92$\\pm$0.07 & \\textbf{25.00}$\\pm$0.17 & \\textbf{52.68}$\\pm$0.35 & \\textbf{1.2} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{threeparttable}\n}\n\\label{tab:nasbench201}\n\\end{table*}\n\n\n\nTo verify the effectiveness and efficiency of our NESBS algorithm, we firstly compare it with other well-known NAS and ensemble (search) algorithms in NAS-Bench-201 \\citep{nasbench201}. Table~\\ref{tab:nasbench201} summarizes the results. Table~\\ref{tab:nasbench201} shows that ensemble (search) algorithms, including our NESBS, consistently achieve improved generalization performance over conventional NAS algorithms. This is because ensemble (search) algorithms will select neural network ensembles whereas NAS algorithms will select only one single architecture. Moreover, it has been widely verified that model ensembles generally outperform a single machine learning model in practice \\citep{zhou-ensemble}. In addition, our NESBS algorithm outperforms other ensemble (search) baseline (i.e., DeepEns and NES-RS), especially on large-scale datasets (i.e., CIFAR-100 \\citep{cifar} and ImageNet-16-200 \\citep{imagenet-16-120}) while incurring less search costs than NES-RS, which thus implies the superior performance of our NESBS over these ensemble (search) baselines. Even on a small-scale dataset (i.e., CIFAR-10), our NESBS can also achieve comparable search results to DeepEns and NES-RS.  Interestingly, our NESBS algorithm is even able to incur reduced search costs than conventional NAS algorithms. This is likely because more training epochs have been used in these NAS algorithms, whereas a small number of training epochs can already contribute to well-performing results for our NESBS algorithm.\n\n\\subsection{Search in The DARTS Search Space}\\label{sec:darts}\nWe further demonstrate the superior search effectiveness and efficiency of our NESBS by comparing it with other NAS and ensemble (search) baselines in a larger search space (i.e.,  DARTS \\citep{darts} search space) using both classification and adversarial defense tasks on CIFAR-10/100 or ImageNet~\\citep{imagenet}. We follow Appendix \\ref{sec:setting-training} to evaluate the final neural network ensembles selected by our NESBS algorithm with ensemble size $n=3$, $T=5$, and optimization details in Appendix~\\ref{sec:setting-exp}.\n\n\\begin{table*}[t]\n\\caption{Comparison of different image classifiers on CIFAR-10/100. Results of MC DropPath are from a drop~probability of $0.01$ and our search costs are evaluated on Nvidia $1080$Ti.}\n\\centering\n\\resizebox{0.87\\textwidth}{!}{\n\\begin{threeparttable}\n\\begin{tabular}{lcccccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Architecture(s)}} & \\multicolumn{2}{c}{\\textbf{Test Error} (\\%)} &\n\\multicolumn{2}{c}{\\textbf{Params} (M)} &\n\\multirow{2}{1.8cm}{\\textbf{Search Cost} (GPU Days)} &\n\\multirow{2}{*}{\\textbf{Search Method}} \\\\\n\\cmidrule(l){2-3} \\cmidrule(l){4-5} \n& \\textbf{C10} & \\textbf{C100} & \\textbf{C10} & \\textbf{C100} & \\\\\n\\midrule \n& \\multicolumn{6}{c}{\\textbf{NAS algorithms}} \\\\\n\nNASNet-A \\citep{nasnet} & 2.65 & - & 3.3 & - & 2000 & RL\\\\\nAmoebaNet-A \\citep{amoebanet} & 3.34 & 18.93 & 3.2 & 3.1 & 3150 & evolution\\\\\nPNAS \\citep{pnas} & 3.41 & 19.53 & 3.2 & 3.2 & 225 & SMBO\\\\\nENAS \\citep{enas} & 2.89 & 19.43 & 4.6 & 4.6 & 0.5 & RL\\\\\nDARTS \\citep{darts} & 2.76 & 17.54 & 3.3 & 3.4 & 1 & gradient\\\\\nGDAS \\citep{gdas} & 2.93 & 18.38 & 3.4 & 3.4 & 0.3 & gradient \\\\\n\nP-DARTS \\citep{p-darts} & 2.50 & - & 3.4 & - & 0.3 & gradient \\\\\nDARTS- (avg) \\citep{darts-} & 2.59 & 17.51 & 3.5 & 3.3 & 0.4 & gradient \\\\\nSDARTS-ADV \\citep{sdarts} & 2.61 & - & 3.3 & - & 1.3 & gradient \\\\\n\\midrule\n& \\multicolumn{6}{c}{\\textbf{Ensemble (search) algorithms}} \\\\\nMC DropPath (ENAS) & 2.88 & 16.83 & 3.8$^\\ddagger$ & 3.9$^\\ddagger$ & - & - \\\\\n\nDeepEns (ENAS) & 2.49 & 15.04 & 3.8$^\\ddagger$ & 3.9$^\\ddagger$ & - & - \\\\\nDeepEns (DARTS) & 2.42 & 14.56 & 3.3$^\\ddagger$ & 3.4$^\\ddagger$ & - & - \\\\\n\nNES-RS$^{\\sharp}$ \\citep{nes} & 2.50 & 15.24 & 3.0$^\\ddagger$ & 3.1$^\\ddagger$ & 0.7 & greedy \\\\\n\n\\midrule\n& \\multicolumn{6}{c}{\\textbf{Our ensemble search algorithm}} \\\\\nNESBS (MC Sampling) & \\textbf{2.41} & 14.70 & 3.8$^\\ddagger$ & 3.9$^\\ddagger$ & \\textbf{0.2} & sampling \\\\\nNESBS (SVGD-RD) & \\textbf{2.36} & \\textbf{14.55} & 3.7$^\\ddagger$ & 3.8$^\\ddagger$ & \\textbf{0.2} & sampling \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tablenotes}\\footnotesize\n    \\item[$\\ddagger$] Reported as the averaged parameter size of the architectures in a neural network ensemble.\n    \\item[$\\sharp$] Obtained from a pool of size $50$, in which every architecture is uniformly randomly sampled from the DARTS search spaces and then trained independently for $50$ epochs following the evaluation settings in Appendix~\\ref{sec:setting-training}.\n\\end{tablenotes}\n\\end{threeparttable}\n}\n\\label{tab:accuracy-cifar}\n\\end{table*}\n\n\n\\begin{table}[!t]\n\\caption{Comparison of image classifiers on ImageNet. The ensemble size is set to $n=3$ for NES-RS and NESBS.}\n\\centering\n\\resizebox{\\columnwidth}{!}{\n\\begin{threeparttable}\n\\begin{tabular}{lcccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Architecture(s)}} & \\multicolumn{2}{c}{\\textbf{Test Error} (\\%)} &\n\\multirow{2}{1.0cm}{\\textbf{Params}} &\n\\multirow{2}{0.6cm}{\\textbf{$+\\times$}} \\\\\n\\cmidrule(l){2-3}\n& \\textbf{Top-1} & \\textbf{Top-5} & (M) & (M) \\\\\n\n\\midrule\n\\multicolumn{5}{c}{\\textbf{NAS algorithms}} \\\\\nNASNet-A  & 26.0 & 8.4 & 5.3 & 564 \\\\\nAmoebaNet-A & 25.5 & 8.0 & 5.1 & 555 \\\\\nPNAS & 25.8 & 8.1 & 5.1 & 588 \\\\\nDARTS & 26.7 & 8.7 & 4.7 & 574 \\\\\nGDAS & 26.0 & 8.5 & 5.3 & 581 \\\\\nP-DARTS & 24.4 & 7.4 & 4.9 & 557 \\\\\nSDARTS-ADV & 25.2 & 7.8 & 5.4 & 594 \\\\\n\\midrule\n\\multicolumn{5}{c}{\\textbf{Ensemble (search) algorithm}} \\\\\nNES-RS & 23.4 & 6.8 & 3.9 & 432 \\\\\n\\midrule\n\\multicolumn{5}{c}{\\textbf{Our ensemble search algorithm}} \\\\\nNESBS (MC Sampling) & \\textbf{22.3} & \\textbf{6.2} & 4.6 & 522 \\\\\nNESBS (SVGD-RD) & \\textbf{22.3} & \\textbf{6.1} & 4.9 & 562 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{threeparttable}\n}\n\\label{tab:accuracy-imagenet}\n\\end{table}\n\n\\paragraph{Ensemble for classification.}\nTable \\ref{tab:accuracy-cifar} summarizes the comparison of classification performances on CIFAR-10/100. Similar to the results in Sec. \\ref{sec:nasbench}, ensemble (search) algorithms, including our NESBS, are generally able to achieve improved generalization performances over conventional NAS algorithms, which thus justifies the essence of ensemble (search) algorithms for improved performance.\nNotably, even compared with other ensemble baselines such as MC DropPath (i.e., developed following Monte Carlo Dropout \\citep{mc-dropout}) and DeepEns, our NESBS is still able to achieve improved performances. Since these ensemble baselines are orthogonal to our NESBS, they can be integrated into our NESBS for further performance improvement in real-world applications. More importantly, our algorithm outperforms NES-RS by achieving both improved search effectiveness (lowest test errors) and efficiency (lowest search costs). Furthermore, our NESBS even incurs comparable search costs compared with the most efficient NAS algorithms (e.g., GDAS, P-DARTS), which also highlights the efficiency of our NESBS. Similar results on ImageNet can be achieved by our NESBS as shown in Table~\\ref{tab:accuracy-imagenet}. \\footnote{Following the convention of NAS and ensemble search algorithms in Table~\\ref{tab:accuracy-imagenet}, the ensembles selected by our NESBS are also searched on CIFAR-10 and then transferred to ImageNet.}\n\n\\paragraph{Ensemble for adversarial defense.}\\label{sec:exp-adversarial}\nEnsemble methods have already been shown to be an essential and effective defense mechanism against adversarial attacks~\\citep{ensemble-for-defense}.\nSpecifically, an adversarial attacker can only use \\emph{a single model} randomly sampled from an ensemble to generate the adversarial examples, whereas the ensemble method defends against adversarial attacks (i.e., makes its predictions) using \\emph{all models} in this ensemble. Ensemble methods can defend against the adversarial attacks in such a setting because the generated adversarial examples using only one single model are unlikely to fool all models in an ensemble.\nMore details are provided in Appendix \\ref{sec:setting-adversarial}.\nTable \\ref{tab:adversarial} summarizes the comparison of adversarial defense among ensemble (search) algorithms on CIFAR-10/100 under different\nwhite-box adversarial attacks, including the \\emph{Fast Gradient Signed Method} (FGSM) attack \\cite{fgsm}, the \\emph{Projected Gradient Descent} (PGD) attack \\cite{pgd}, the \\emph{Carlini Wagner} (CW) attack \\cite{cw}, and the AutoAttack~\\citep{autoattack}.\nTable \\ref{tab:adversarial} shows that ensemble (search) algorithms are indeed able to significantly improve the performance of adversarial defense, i.e., the test accuracies in the \\emph{Defense} column are consistently higher than the ones in \\emph{Attack} column. More importantly, even under different white-box adversarial attacks, our NESBS algorithm can generally achieve improved defense performances (i.e., higher test accuracy in the \\emph{Defense} columns) than other baselines including DeepEns and NES-RS. These results thus further support the effectiveness of our NESBS over existing ensemble (search) algorithms.\nBesides, even regarding the adversarial robustness of the single models in an ensemble, the architectures selected by our NESBS are also more advanced (i.e., by achieving higher test accuracy in the \\emph{Attack} columns) than well-known architectures such as RobNet~\\citep{robnet} and DARTS.\n\n\\begin{table*}[t]\n\\caption{Comparison of adversarial defense among different ensemble (search) algorithms on CIFAR-10/100 under white-box adversarial attacks.\nThe \\emph{Attack} and \\emph{Defense} columns denote the test \\emph{accuracy} under the attack using a single model randomly sampled from an ensemble and the defense using the whole ensemble, respectively.\nEach result reports the mean and standard deviation of test accuracies for $3$ rounds of the attack-defense process with an ensemble size of $n=3$.}\n\\newcommand{\\phantom{-}}{\\phantom{-}}\n\\renewcommand\\multirowsetup{\\centering}\n\\centering\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l*{8}{c}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Method}} & \n\\multicolumn{2}{c}{\\textbf{FGSM}} &  \n\\multicolumn{2}{c}{\\textbf{PGD-40}} &\n\\multicolumn{2}{c}{\\textbf{CW}} &\n\\multicolumn{2}{c}{\\textbf{AutoAttack}}\n\\\\\n\\cmidrule(l){2-3} \\cmidrule(l){4-5} \\cmidrule(l){6-7} \\cmidrule(l){8-9} \n&\nAttack (\\%) & Defense (\\%) &\nAttack (\\%) & Defense (\\%) & \nAttack (\\%) & Defense (\\%) & \nAttack (\\%) & Defense (\\%)  \\\\\n\\midrule \n& \\multicolumn{8}{c}{\\textbf{On CIFAR-10 Dataset}} \\\\\nDeepEns & - & - & - & - & - & - & - & - \\\\\n$\\quad \\hookrightarrow$ RobNet-free & 66.62$\\pm$0.32 & 85.25$\\pm$0.39 & 41.81$\\pm$0.80 & 77.48$\\pm$0.67 & $\\;\\;$5.74$\\pm$1.41 & 86.53$\\pm$0.50 & 21.35$\\pm$0.33 & 45.51$\\pm$0.15 \\\\\n$\\quad \\hookrightarrow$ ENAS & 77.85$\\pm$0.58 & 87.94$\\pm$0.21 & 59.51$\\pm$1.13 & 86.57$\\pm$0.15 & 31.36$\\pm$1.20 & 85.20$\\pm$0.77 & 31.71$\\pm$0.72 & 50.96$\\pm$0.07 \\\\\n$\\quad \\hookrightarrow$ DARTS & 76.79$\\pm$0.80 & 88.21$\\pm$0.14 & 57.71$\\pm$1.65 & 82.02$\\pm$0.10 & 26.90$\\pm$1.37 & 82.46$\\pm$0.35 & 29.97$\\pm$1.17 & 49.67$\\pm$0.14 \\\\\nNES-RS & 79.19$\\pm$1.39 & 89.32$\\pm0.27$ & 65.59$\\pm$2.11 & 85.22$\\pm$0.41 & 37.20$\\pm$4.62 & 86.75$\\pm$0.88 & 35.00$\\pm$1.15 & 53.80$\\pm$0.14 \\\\\n\\cmidrule(l){1-9}\nNESBS (MC Sampling) & 78.75$\\pm$1.29 & 89.15$\\pm$0.08 & 63.60$\\pm$1.87 & 85.35$\\pm$0.31 & \\textbf{37.71}$\\pm$1.97 & \\textbf{86.86}$\\pm$0.66 & \\textbf{36.02}$\\pm$0.64 & \\textbf{56.90}$\\pm$0.17 \\\\\nNESBS (SVGD-RD) & 79.12$\\pm$0.61 & \\textbf{89.86}$\\pm$0.33 & 65.53$\\pm$1.56 & 85.37$\\pm$0.38 & \\textbf{38.27}$\\pm$1.27 & 86.00$\\pm$1.10 & \\textbf{37.55}$\\pm$0.68 & \\textbf{57.15}$\\pm$0.20\\\\\n\\midrule\n& \\multicolumn{8}{c}{\\textbf{On CIFAR-100 Dataset}} \\\\\nDeepEns & - & - & - & - & - & - & - & - \\\\\n$\\quad \\hookrightarrow$ RobNet-free & 36.47$\\pm$0.25 & 61.39$\\pm$0.30 & 18.18$\\pm$0.47 & 52.61$\\pm$0.13 & 2.36$\\pm$0.13 & 69.44$\\pm$0.04 & $\\;\\;$7.31$\\pm$0.35 & 24.56$\\pm$0.33 \\\\\n$\\quad \\hookrightarrow$ ENAS & 46.40$\\pm$0.37 & 64.94$\\pm$0.27 & 28.87$\\pm$0.27 & 56.79$\\pm$0.25 & 9.60$\\pm$0.30 & 69.43$\\pm$0.44 & 11.53$\\pm$0.47 & 27.01$\\pm$0.27 \\\\\n$\\quad \\hookrightarrow$ DARTS & 46.98$\\pm$0.57 & 65.38$\\pm$0.23 & 28.78$\\pm$0.74 & 57.10$\\pm$0.04 & 9.73$\\pm$0.43 & 70.15$\\pm$0.29 & 11.20$\\pm$0.40 & 26.86$\\pm$0.36 \\\\\nNES-RS & 47.10$\\pm$1.46 & 65.33$\\pm0.36$ & 30.68$\\pm$1.66 & 58.80$\\pm$0.80 & 9.96$\\pm$1.45 & 70.24$\\pm$0.33 & 12.01$\\pm$0.93 & 27.49$\\pm$0.34 \\\\\n\\cmidrule(l){1-9}\nNESBS (MC Sampling) & \\textbf{50.69}$\\pm$1.58 & \\textbf{67.63}$\\pm$0.05 & \\textbf{33.37}$\\pm$0.42 & \\textbf{60.36}$\\pm$0.62 & \\textbf{15.64}$\\pm$2.83 & \\textbf{71.25}$\\pm$1.27 & \\textbf{13.11}$\\pm$1.16 & \\textbf{29.87}$\\pm$1.17 \\\\\nNESBS (SVGD-RD) & \\textbf{51.47}$\\pm$0.40 & \\textbf{66.66}$\\pm$0.13 & \\textbf{35.02}$\\pm$0.37 & \\textbf{59.96}$\\pm$0.18 & \\textbf{16.72}$\\pm$0.61 & \\textbf{69.88}$\\pm$0.16 & \\textbf{14.62}$\\pm$0.55 & \\textbf{31.07}$\\pm$0.33 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:adversarial}\n\\end{table*}\n \n\n\\subsection{Single-model Performances and Diverse Model Predictions}\\label{sec:trade-off}\nWe demonstrate that the effectiveness of our NESBS results from its ability to achieve a good trade-off between the single-model performances and the diversity of model predictions. \nWe firstly quantitatively compare the single-model performances (measured by the \\emph{averaged test error} (ATE) of the models in an ensemble) and the diversity of model predictions (measured by the \\emph{pairwise predictive disagreement} (PPD) of an ensemble \\citep{disagreement}) achieved by different ensemble (search) algorithms on CIFAR-10/100. \nWe further qualitatively visualize their single-model performances and diverse model predictions using a histogram of the ATE of the models in their ensembles and a t-SNE \\citep{t-sne} plot of their model predictions, respectively.\n\n\\begin{table}[t]\n\\caption{Quantitative comparison of the single-model performances (measured by ATE (\\%), smaller is better) and the diversity of model predictions (measured by PPD (\\%), larger is better) achieved by different ensemble (search) algorithms with an ensemble size of $3$ on CIFAR-10/100.}\n\\renewcommand\\multirowsetup{\\centering}\n\\centering\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{l *{4}{c}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Method}} & \n\\multicolumn{2}{c}{\\textbf{C10}} &\n\\multicolumn{2}{c}{\\textbf{C100}} \\\\\n\\cmidrule(l){2-3} \\cmidrule(l){4-5}\n& ATE & PPD & ATE & PPD \\\\\n\\midrule \nMC DropPath (DARTS) & 2.71 & 0.39 & 16.68 & 2.63 \\\\\nDeepEns (DARTS) & \\textbf{2.69} & 2.08 & \\textbf{16.18} & 12.45  \\\\\nNES-RS  & 2.87 & 2.29 & 17.20 & \\textbf{14.14} \\\\\n\\midrule\nNESBS (MC Sampling)  & 2.80 & \\textbf{2.57}  & 16.70 & 13.84\\\\\nNESBS (SVGD-RD) & 2.78 & 2.27 & 16.50 & 13.16 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:perf_vs_diver}\n\\end{table}\n\nTable \\ref{tab:perf_vs_diver} and Fig.~\\ref{fig:perf_vs_diver} present the results of our quantitative and qualitative comparisons, respectively. Compared with the ensemble baselines of MC DropPath and DeepEns, our NESBS is capable of enjoying a larger diversity of model predictions while preserving competitive single-model performances. Meanwhile, compared with the ensemble search baselines of NES-RS, our algorithm can achieve improved single-model performances while maintaining comparably diverse model predictions. These results suggest that our NESBS is able to select ensembles achieving a better trade-off between the single-model performances and the diversity of model predictions among these baselines, which is known to be an important criterion for well-performing ensembles \\citep{zhou-ensemble}. Thus, Table \\ref{tab:perf_vs_diver} and Fig.~\\ref{fig:perf_vs_diver} provide empirical justifications for the improved effectiveness of NESBS.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\begin{tabular}{cc}\n    \\hspace{-6mm}\\includegraphics[width=0.525\\columnwidth]{figs/avg_test_error_cifar10.pdf} & \\hspace{-6mm}\\includegraphics[width=0.485\\columnwidth]{figs/tsne.pdf} \\\\\n    {\\hspace{-3mm} (a) Single-model performances} & \\hspace{-6mm} {(b) Diverse predictions}\n\\end{tabular}\n\\caption{Qualitative comparison of (a) the single-model performances and (b) the diverse model predictions achieved by different ensemble (search) algorithms with an ensemble size of $n=3$ on CIFAR-10. Each architecture in (b) is independently evaluated for ten times to visualize their model predictions, which follows from DeepEns.}\n\\label{fig:perf_vs_diver}\n\\end{figure}\n\n\n\\section{Conclusion}\nThis paper presents a novel neural ensemble search algorithms, called NESBS, that can effectively and efficiently select well-performing neural network ensembles with diverse architectures from a NAS search space. Our extensive experiments have shown that NESBS is able to achieve improved performances while preserving a comparable search cost compared with conventional NAS algorithms. Moreover, even compared with other ensemble (search) baselines (e.g., DeepEns and NES-RS), our NESBS is also capable of enjoying boosted search effectiveness and efficiency, which further suggests the superior performance of our NESBS in practice.\n\n\\begin{acknowledgements}\n                        \n                        \nThis research/project is supported by A*STAR under its RIE$2020$ Advanced Manufacturing and Engineering (AME) Programmatic Funds (Award A$20$H$6$b$0151$).\n\n\\end{acknowledgements}\n\n", "meta": {"timestamp": "2022-06-20T02:08:16", "yymm": "2109", "arxiv_id": "2109.02533", "language": "en", "url": "https://arxiv.org/abs/2109.02533"}}
{"text": "\\section{Introduction}\\label{sec:introduction}\n\\input{sections/introduction}\n\n\n\\section{Knowledge Graph Construction}\\label{chap:datapreperation}\n\\input{sections/datapreperation}\n\n\\section{KG Completion using Link Prediction}\\label{chap:RQ1}\n\\input{sections/RQ1}\n\n\\section{Career Pathfinding using Shortest Path Algorithms}\\label{chap:RQ2}\n\\input{sections/RQ2}\n\n\\section{Most Relevant Skills per Occupation Group}\\label{chap:RQ3}\n\\input{sections/RQ3}\n\n\\section{Conclusion}\\label{chap:conclusion}\n\\input{sections/conclusion}\n\n\\section{Discussion \\& Future research}\\label{chap:discussion}\n\\input{sections/discussion}\n\n\n\\begin{acks}\nA special thanks to the thesis supervisors for the project Niels van Weeren and Prof. Aske Plaat as well as everybody at Randstad involved in this project.\n\\end{acks}\n\n\\bibliographystyle{ACM-Reference-Format}\n\n\\subsection{Experimental setup}\nWe employ link prediction to estimate the relatedness between skills and occupation nodes. \nTo evaluate and reliably compare different methods, we first split our KG into train, test, and validation sets. \nMore specifically, we sample 55\\% of all edges for training the link prediction algorithms (where applicable), leaving leave 30\\% for testing, and 15\\% for validation. \nFor each existing pair of occupation and skills node --- which we consider a positive sample in our train, test and validation sets --- we randomly generate a negative sample (i.e., a pair of skills and occupation nodes that do not exist in our KG). \nAn overview of the number of edges in each set is shown in Table~\\ref{tab:train_val_test}.\n\n\\begin{table}[!h]\n\\centering\n\\begin{tabular}{@{}lrr@{}}\n\\toprule\n                 & Positive & Negative \\\\ \\midrule\nTraining edges   & 2151     & 2151     \\\\\nValidation edges & 586      & 586      \\\\\nTest edges       & 1173     & 1173     \\\\ \\midrule\nTotal            & 3910     & 3910     \\\\ \\bottomrule\n\\end{tabular}\n\\caption[Number of positive and negative edges]{Number of positive and negative edges with a training (55\\%), validation (15\\%), test (30\\%) split }\n\\label{tab:train_val_test}\n\\end{table}\n\n\n\n\n\\subsection{Link Prediction Methods}\n\\subsubsection{Method 1: Preferential Attachment (PA)}\n\\label{sec:method_pa}\nThe first link prediction method is preferential attachment~\\cite{liben2007link}. \nThis method takes a set of nodes, i.e. node $v$ and node $u$, and calculates a closeness ($C$) between two nodes:\n\n\\begin{equation}\nC(u,v) = |\\Gamma(u)| \\times |\\Gamma(v)|,\n\\end{equation}\nwhere $\\Gamma(u)$ denotes the neighbors of $u$.\n\nA higher score here corresponds to a larger probability the nodes are connected. \nThe intuition behind this is that if both nodes have a high amount of neighbors the nodes might function as a hub. \nMost graphs have the property that hubs have a higher chance to be connected. \n\nTo compute all scores, we represent our KG as a matrix, where each node is represented as a row and a column.\nNote that this matrix is symmetric since the value for row $u$ and column $v$ is equal to the value at row $v$ and column $u$. \nAt the intersecting cell of two nodes, we store the preferential attachment. \nWe normalize this matrix by dividing each score by the maximum Closeness score, to ensure that each value is between 0 and 1. \nWe consider the resulting normalized Closeness score as the probability the corresponding nodes are related. \n\n\\subsubsection{Method 2: Node2Vec (N2V)}\n\\label{sec:method_n2v}\nThe second link prediction method we use is the Node2Vec algorithm~\\cite{grover2016node2vec}. \nThis algorithm can have a number of configurations. \nFor this paper we use the following parameters:\n\n\\begin{itemize}\n\\item dimensions = 1024\n\\item walk length = 4\n\\item number of walks = 2500\n\\item $p$ (return parameter) = 1 \n\\item $q$ (in-out parameter) = 1\n\\end{itemize}\n\nThese parameters were selected after a grid search on a large number of possible combinations of parameters. \n\n\\subsection{Results}\nTable \\ref{tab:F1-results} shows the performance of both Preferential Attachment (PA) and Node2Vec (N2V).\n\n\\begin{table}[!h]\n\\centering\n\\begin{tabular}{@{}llllll@{}}\n\\toprule\n                                         & class    & precision & recall & f1-score \\\\ \\midrule\n\\multirow{2}{*}{PA} & 0.0      & \\textbf{0.83}      & 0.64   & 0.72    \\\\\n                                         & 1.0      & 0.71      & \\textbf{0.87}   & \\textbf{0.78}    \\\\ \\midrule\n\\multirow{2}{*}{N2V}                & 0.0      & 0.66      & \\textbf{0.90}   & \\textbf{0.76}    \\\\\n                                         & 1.0      & \\textbf{0.84}      & 0.53   & 0.65    \\\\ \\bottomrule\n\\end{tabular}\n\\caption[Precision, recall and F1-scores of multiple link prediction algorithms]{Precision, recall and F1-scores of multiple link prediction algorithms with an equal number of positive and negative edges used for training}\n\\label{tab:F1-results}\n\\end{table}\n\nWhen the number of positive and negative edges in the test set is equal, PA outperforms the more complex N2V method, with an f1-score for the positive class of $0.78$ against $0.65$. \nIn most realistic situations however, we may want to explore how a node can be linked to any other node, making the number of comparisons, or edges to predict 1-to-(N-1), i.e., for each node we compare each other node (excluding self). \nTo approximate this real world performance the ratio of negative to positive edges should reflect these more realistic proportions. \nTo do so we compute F1-score at increasing ratios of positive-to-negative edges, ranging from 1 (as shown in Table~\\ref{tab:F1-results}) to 7. \nResults are shown in Figure~\\ref{fig:f1-score}.\nThe figure shows that up to ratio of 3:1, N2V is on par with PA, but as ratios increase, N2V outperforms PA, suggesting N2V is better suited for most real world situations. \n\n\n\\begin{figure}[!h]\n\\centering\n\\includegraphics[width=\\columnwidth]{figs/f1-score_v2}\n\\caption[Comparison of Node2Vec and Preferential Attachment]{Comparison of Node2Vec and Preferential Attachment for different ratio's negative edges / positive edges}\n\\label{fig:f1-score}\n\\end{figure}\n\n\n\\subsection{Analysis}\nNow that it has been established that N2V is more suitable for our task, we aim to employ this algorithm to predict the relationships between occupations and skills. \nWhen doing so we need to realize that the graph which we use as input is imperfect in terms of correctness and completeness~\\cite{paulheim2017knowledge}. \n\nLooking at the false positives of the algorithm, skills that are --- according to our dataset --- incorrectly linked to occupations can be identified. \nFor KG completion, we aim to identify those skills that are not linked to occupations, but should be. \nTable~\\ref{tab:example_node2vec} shows a random sample of False Positives: it reinforces our intuition that link prediction can be employed for KG completion, as some of the predicted edges make sense, e.g., the skill: ``preparing materials for dental procedures'' is shown as a relevant skill for the occupation: ``dentist.''\nBy consulting domain experts, skills can be efficiently added to enrich the current graph. \n\n\\begin{table*}[t]\n\\centering\n\\begin{tabular}{@{}lll@{}}\n\\toprule\nISCO-Code & Occupation & Predicted Skill \\\\ \\midrule\n1341  & Child care services managers  & children's physical development \\\\\n2261  & Dentists  & prepare materials for dental procedures \\\\\n3251  & Dental assistants and therapists  & dentistry science \\\\\n4110  & General office clerks  & demonstrate professional attitude to clients \\\\\n5411  & Fire fighters  & safety engineering \\\\\n6121  & Livestock and dairy producers  & promote animal welfare \\\\\n7132  & Spray painters and varnishers  & spray pesticides \\\\\n8344  & Lifting truck operators  & hazardous materials transportation \\\\\n9111  & Domestic cleaners and helpers  & provide lawn care \\\\ \\bottomrule\n\\end{tabular}\n\\caption{False positives: edges predicted by N2V that do not exist in our KG}\n\\label{tab:example_node2vec}\n\\end{table*}\n\nTo further explore these intuitions, in Figure~\\ref{fig:tb} we show the edges to skill nodes predicted by N2V, for the node representing ISCO code 2611: ``Lawyers.'' \nThe y-axis shows skills edges, and the x-axes show the link prediction probabilities, for all predictions with a probability$>$0.5 (i.e., positive predictions by the method). \nThe green bars denote True Positives (i.e., correctly predicted edges between the skill and occupation), and blue bars depict False Positives (skills that are predicted to have an edge with the occupation, but do not exist in our KG). %\nThe figure shows ``education law'' and ``investigation research methods'' as newly identified skills for lawyers, not found in the original ESCO taxonomy nor in co-occurrences in job postings. \n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width = \\columnwidth]{figs/lawyer}\n\\caption[Predictions of the Node2Vec algorithm]{Predictions of the Node2Vec algorithm for ISCO group $2611$ (Lawyers)}\n\\label{fig:tb}\n\\end{figure}\n\n\\subsection{Skills-based Occupation Similarity}\nTo determine the feasibility of an occupation transfer, we propose to model the distance between occupations with Jaccard distance. \nWe compute Jaccard distances between occupations by representing each occupation as the set of its required skills (which we extract from our KG), and computing the overlap between two sets of skills. \nSee Figure for an illustration~\\ref{fig:jaccard}.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width = 0.6\\columnwidth]{figs/jac_graph}\n\\caption[Jaccard distance in a graph]{Jaccard distance in a graph where nodes \\{A, B\\} are occupations and nodes \\{C, D, E, F\\} are skills. Solid lines denote direct connections, dashed lines denote Jaccard distance.}\n\\label{fig:jaccard}\n\\end{figure}\n\nIn our KG a total of $120,952$ links can be made between pairs of skills and pairs of occupations. \nFrom these pairs $89.3\\%$ is between skills and $10.7\\%$ between occupations. \nTo gain insight in the overall similarity of skills and occupations, we study the distribution of jaccard distances in Figure~\\ref{fig:jaccard_dist}.\n\n\\begin{figure}[!h]\n\\centering\n\\includegraphics[width = \\columnwidth]{figs/jaccard_2}\n\\caption[Distribution of the jaccard distance ]{Distribution of the jaccard distance where the orange color represents the skills and the blue color represent the occupations}\n\\label{fig:jaccard_dist}\n\\end{figure}\n\nLooking at the distribution of Jaccard distance one can see that on average, skills are more similar to one another than occupations. \nThis becomes apparent when looking at the mean value of both distributions: for occupations the mean is $0.96$, and for skills around $0.88$. \nOver 99\\% of occupations have a Jaccard distance between 0.8 and 1, meaning that occupations require distinct skillsets. \nBoth distributions are skewed to the left, meaning that the mean (average of the observations) is left of the mode (most observed value). \n\nIn the distribution we see a number of spikes, which can be explained by the prevalence of some fractions over others, e.g., if half of the neighbors are shared, the Jaccard distance will be $\\frac{1}{2}$, which can be achieved in a number of different ways. \nOther spikes occur at additional common fractions such as $\\frac{2}{3}$ and $\\frac{3}{4}$. \n\n\n\\begin{table}[!h]\n\\begin{tabular}{@{}lrrr@{}}\n\\toprule\n      & Skill  & Occupation & Total  \\\\ \\midrule\ncount & 107959 & 12993      & 120952 \\\\\nmean  & 0.825  & 0.938      & 0.837  \\\\\nstd   & 0.163  & 0.070      & 0.160  \\\\\nmin   & 0.000  & 0.000      & 0.000  \\\\\n25\\%  & 0.800  & 0.928      & 0.800  \\\\\n50\\%  & 0.875  & 0.960      & 0.888  \\\\\n75\\%  & 0.923  & 0.977      & 0.933  \\\\\nmax   & 0.985  & 0.993      & 0.993  \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Statistics of the jaccard distribution}\n\\label{tab:jacard_distibution}\n\\end{table}\n\nIn Table \\ref{tab:jacard_distibution} we show a description of the distance distributions. \nFor both skills and occupations the minimum distance is 0, meaning that a skill is shared by every occupation where the skill is connected to or that two occupations share every skill. \nAn example is \\textit{``Food service counter attendants''} and \\textit{``Hotel receptionists,''} both share the same skillset and thus have a Jaccard distance of 0. \nSkills with a distance of 0 are for example \\textit{``Lop trees''} and \\textit{``Pruning techniques.''} \nThe highest distance found in the dataset is $0.993$, this corresponds with the occupations \\textit{``Electronics engineers''} and \\textit{``Policy administration professionals.''} \nThey share at least one skill but are --- next to the shared skill --- completely different. \nThe common skill in this example is \\textit{``perform project management.''} \n\n\\subsection{Career Pathfinding using Dijkstra's algorithm}\nWith the distances between each occupation and between skills, we can proceed to identify the most efficient transition between every pair of occupations. \nThis is done by assigning the Jaccard distance scores as edge weights between nodes in our graph, to enable computational methods for finding the most efficient path between a start node (the current occupation) and an end node (the desired occupation). \nWe show an example of such a transition in Figure~\\ref{fig:path_example}: here we set a threshold for the maximum possible distance at $0.8$. This threshold was determined to be optimal based on eye-balling and comparing a different cutoff points.\nIf two occupations are further apart than $0.8$ we consider the step too large. \n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.5 \\columnwidth]{figs/transition.pdf}\n\\caption[Example of distance between occupations]{Distance between the occupations \\{$W,X, Y, Z$\\}. Black lines denote distances lower than $0.8$. Red lines denote distances higher than $0.8$.}\n\\label{fig:path_example}\n\\end{figure}\n\nIn this example we start at node $W$ and want to go to node $Z$. \nWe are not able to directly transition between $W$ and $Z$ because the occupations are not similar enough ($0.9>0.8$). \n\n\\subsubsection{Method}\nFinding the most efficient path in an undirected weighted graph can be done by applying shortest path algorithms. \nFor this paper we turn to Dijkstra's algorithm~\\cite{dijkstra1959note}, because of its proven speed and widespread availability of implementations. \nAccording to Dijkstra's algorithm, the shortest allowed path between $W$ and $Z$ in Figure~\\ref{fig:path_example} is via node $X$. \n\nWe show a real world example in Figure~\\ref{fig:covid_example}. \nDue to the COVID-19 pandemic a lot of people find themselves out of a job, especially individuals that work in restaurants. \nUsing the described model we can calculate which occupation has the smallest distance to the occupation: ``cook.'' \nDijkstra's algorithm yields ``bakers, pastry-cooks and confectionery makers'' as most feasible transition. \n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=1\\columnwidth]{figs/Cooks.PNG}\n\\caption[The shortest path between occupations]{The shortest path between the occupation ``Cook'' and the closest connected occupation, in this case ``Bakers, pastry-cooks and confectionery makers.''}\n\\label{fig:covid_example}\n\\end{figure}\n\n\n\n\\subsection{Problem Statement}\\label{sec:PS}\n\nTo facilitate candidate to job posting matching, it is important to know which skills are relevant, in demand, and in supply. \nHere, the need for a flexible data representation for skills arises. \nThis representation should facilitate various tasks, such as a skills similarity metric to be able to quantify likeliness between skills, skills-to-occupation similarity metrics, to help people navigate the labor market and find new occupations, and understanding which skills relate to which occupations to inform which skills are needed for desired occupations. \nAnd since relations between skills and occupations are not static and need robust and accurate updating methods to ensure the information does not get outdated. \n\n\nIn this paper we address the task of skills and occupation graph construction which we describe in Section~\\ref{chap:datapreperation}, and apply this data representation to the following set of use-cases: \nlink prediction for identifying novel skills-occupation relations in Section~\\ref{chap:RQ1}, \nskills-based occupational similarity for career pathfinding in Section~\\ref{chap:RQ2}, and \nidentifying distinctive skills per occupational group for learning \\& development in Section~\\ref{chap:RQ3}.\n\n\\subsection{Method}\nThe two criteria described above fit naturally to the Term Frequency\u2013Inverse Document Frequency (TF-IDF) weighting scheme for terms~\\cite{schutze2008introduction}. \nThis statistic is chosen as it directly models the desired criteria described in the previous section, more specifically, TF-IDF is used to assign weights to words in a corpus of documents, where a word is deemed more important if it (i) is observed frequently within the document but (ii) not frequently across different documents in the corpus. \n\n\\begin{equation}\n\\displaystyle TF-IDF(t,d) = tf_{t,d} \\times \\log\\Big(\\frac{N}{df_{t} + 1}\\Big),\n\\end{equation}\nwhere $tf_{t,d}$ denotes the Term Frequency of $t$ in $d$, $df_t$ denotes the number of documents containing $t$, and $N$ denotes the total number of documents in the corpus.\n\nWe ``transplant'' this TF-IDF weighting scheme from terms in documents to skills associated to occupations. \nTF-IDF consists of two parts: \nTerm Frequency (TF) is the frequency of a word (skill) used in a given document (observed with an occupation), \nInverse Document Frequency (IDF) is a way to discount highly common terms, i.e., it is high when a word (skill) appears in a smaller number of documents (observed with a low number of occupations). \nCommon terms (skills) will thus yield a lower IDF score. \n\nFor our TF-IDF-based model, we consider skills identified in job postings terms, and documents can be modeled as a collection of job postings belonging to an ISCO group. \nThe counts of skills, which model term frequency, correspond to the number of times a skill is found in a job posting associated to a certain ISCO code. \n\n\n\\subsection{Results and analysis}\n\\subsubsection{Level 1 ISCO groups}\nThe resulting score provides us with skills that are common for a given occupation (group) but uncommon in all other occupation(s) (groups). \nTable~\\ref{tab:skill_relevance} shows the top 5 skills for the level 1 ISCO groups. \n\nIn this table \\textit{Microsoft Office} appears both in the \\textit{Managers} and \\textit{Clerical support workers} groups.\nFor this skill to score high in multiple contexts (occupation groups) the frequencies need to be substantial in both, to be able to compensate for the IDF component of the metric. \nIn the Managers group, \\textit{Microsoft Office} has a TF of 9\\% and in \\textit{Clerical support workers} a TF of 5\\%. \n\n\\subsubsection{Multiple ISCO levels}\nISCO level 1 helps us to understand which skills are relevant for the least granular level; to deepen our understanding we look at the development of multiple layers of ISCO group 2 in Figure~\\ref{fig:skill_relevance}.\nHere, we show the 3 most relevant skills for several ISCO levels of the ``Professionals'' ISCO group.\n\nWe notice the following:\nFirst, communication-related skills appear in multiple forms across occupation groups. \nThe terms \\textit{communication}, \\textit{communication sciences}, \\textit{communication studies}, \\textit{ICT communication protocols}, \\textit{manage online communications} and \\textit{communication disorders} seem to be closely related. \nBecause these skills are defined as distinct skills, each skill receives its own ranking. This concept can appear multiple times. \n\nNext, ``Nursing professionals'' and ``Nursing and midwifery professionals'' share the same set of relevant skills, which are highly similar to those of their parent group ``Health professionals''. Skills that appear in those groups are the most frequent skills in the parent group. \n\n\nFinally, the further down the figure we go, the more specialized the skills appear to be, and more specialized skills, such as ``dental studies,'' are more commonly observed in level 4 ISCO groups. A possible explanation for this is that specialized skills do only appear at specialized occupations. \n\n\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{figs/skill_relevance}\n\\caption{Three most relevant skills for multiple levels in major ISCO group 2}\n\\label{fig:skill_relevance}\n\\end{figure}\n\n\n\\begin{table*}[]\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}ll|l|l@{}}\n\\toprule\n  & Managers & Professionals & Technicians and associate professionals \\\\ \\midrule\n1 & Microsoft Office & Network Marketing & Marker Making \\\\\n2 & Service-oriented Modelling & Manage Online Communications & Electronic Communication \\\\\n3 & Communication Principles & Communication & Service-oriented Modelling \\\\\n4 & Electronic Communication & Explain Accounting Records & Education Administration \\\\\n5 & Coordinate Patrols & Accounting & Manage Standard ERP System \\\\ \\midrule\n  & Clerical support workers & Service and sales workers & Skilled agricultural, forestry and fishery workers \\\\ \\midrule\n1 & Execute Administration & Security Panels & Leadership Principles \\\\\n2 & Perform Clerical Duties & Electronic Communication & Agricultural Information Systems and Databases \\\\\n3 & Microsoft Office & Create Solutions to Problems & Pruning Techniques \\\\\n4 & Education Administration & Execute Administration & Spray Pesticides \\\\\n5 & Human Resource Management & Recreation Activities & Lop Trees \\\\ \\midrule\n  & Craft and related trades workers & Plant and machine operators, and assemblers & Elementary occupations \\\\ \\midrule\n1 & Attend to Detail in Casting Processes & Mechatronics & Inventory Management Rules  \\\\\n2 & Attention to Detail & Mechanical Engineering & Have Computer Literacy \\\\\n3 & Adobe Illustrator & Electrical Engineering & Carpentry \\\\\n4 & Adobe Photoshop & Operate Soldering Equipment & Place Concrete Forms \\\\\n5 & ML (computer programming) & Act Reliably & Operate on-board Computer Systems \\\\ \\bottomrule\n\\end{tabular}}\n\\caption{Five most relevant skills per major ISCO group based in the TF-IDF matric}\n\\label{tab:skill_relevance}\n\\end{table*}\n\n\n\n\n\\subsection{Occupations (ISCO) and skills (ESCO)}\nThe first step involves constructing a shared Skills \\& Occupational Knowledge Graph, through combining the existing ISCO and ESCO taxonomies. \n\n\\subsubsection{ISCO (occupations)}\nThe International Standard Classification of Occupations (ISCO) is ordered as a taxonomy of occupational groups with four granularity levels across ten different major groups. \nAn occupation is defined as \\textit{``a set of jobs whose main tasks and duties are characterized by a high degree of similarity''}, where a job is defined as \\textit{``a set of tasks and duties performed, or meant to be performed, by one person, including for an employer or in self-employment.''~\\cite{ilo}}\nTake, for example: the occupation ``computer programmer,'' which is defined by the level 4 ISCO code: 2132. \nThe occupation then belongs to the the level 3 group ``computing professionals'' (ISCO-code 213), which in turn belongs the level 2 group ``computing, engineering and science professionals'' (ISCO-code 21),\nwhich, finally, falls in the level 1 group ``professionals'' (ISCO-code 2). \n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{@{}ll@{}}\n\\toprule\nGroup Number & Major Group Name                                   \\\\ \\midrule\n1            & Managers                                           \\\\\n2            & Professional                                       \\\\\n3            & Technicians and associate professionals            \\\\\n4            & Clerical support workers                           \\\\\n5            & Service and sales workers                          \\\\\n6            & Skilled agricultural, forestry and fishery workers \\\\\n7            & Craft and related trades workers                   \\\\\n8            & Plant and machine operators, and assemblers        \\\\\n9            & Elementary occupations                             \\\\\n10           & Armed forces occupations                           \\\\ \\bottomrule\n\\end{tabular}\n\\caption{The 10 major job groups of the ISCO-08}\n\\label{tab:isco}\n\\end{table}\n\n\n\n\\subsubsection{ESCO (skills)}\nWe define our initial high-level occupation groups by using the ISCO standard.\nFor skills, we turn to The European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy~\\cite{ESCO}. \nESCO defines a skill as follows:\n\n\\begin{description}\n\\item[Skill] \\textit{``the ability to apply knowledge and use know-how to complete tasks and solve problems''}\n\\end{description}\n\nThe ESCO covers 13,485 skills, connected to 2,942 occupations (in 27 languages). \n\nWe link our ISCO occupations to ESCO by using the direct links that are defined between ISCO level 4 groups (most fine-grained/lowest level of the taxonomy) and ESCO concepts, in the ESCO. \nThese links between ESCO and ISCO are not (necessarily) 1-to-1, as multiple ESCO occupations can be linked to a single (level 4) ISCO group. \n\nIn Figure~\\ref{fig:TheStructureOfTheOccupationsPillar} we illustrate this connection between ISCO and ESCO. \nESCO occupations are shown in blue, with ISCO occupation groups in purple. \nIn addition to the ESCO occupations shown in the image, ESCO also defines skills (not shown), e.g., the ESCO occupation ``Cattle breeder,'' has skills linked to them such as ``feed livestock'' and ``assist animal birth.'' \n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{figs/TheStructureOfTheOccupationsPillar.PNG}\n\\caption{The structure of the occupations pillar \\cite{ESCO}}\n\\label{fig:TheStructureOfTheOccupationsPillar}\n\\end{figure}\n\n\n\\subsection{KG enrichment through job posting data}\nNow that we have our high-level KG structure based on ISCO and ESCO, which defines occupations and skills as nodes, and edges as links between ESCO and ISCO objects, we turn to job posting data to account for the dynamic nature of associations between skills and occupations, as described in Section~\\ref{sec:introduction}. \nTo make sure our KG reflects the current status of the labor market, we use information from job postings to enrich the structure of our KG. \nMore specifically, we create additional edges by identifying and extracting ESCO skills for each job posting's ISCO occupation group, and assign weights to edges by relying on co-occurrence statistics of skills and occupations. \n\nThis second step of our process revolves around extracting skills from job postings. \nWe describe our job posting dataset in Section \\ref{Vacancy Parser}, our approach for skill extraction in Section \\ref{Vacancy Parser}, and how we match extracted skills to ESCO skills in~\\ref{Skill Matching}. \n\n\n\n\n\n\n\\subsubsection{Vacancy data}\\label{Vacancy data}\nOur vacancy dataset consists of sample of 600,000 Dutch vacancies collected by Jobdigger~\\cite{Jobdigger}, each job posting is labeled with a level 4 ISCO code. \nOur sample was chosen by selecting a uniform distribution of ISCO level 1 occupations, to make sure our set covers the entire breadth of the labor market. \nPrior to sampling our set at the ISCO level 1, the initial dataset was cleaned by discarding low quality and noisy job postings, such as postings that represented multiple occupations, or job postings that contained a low number of sentences. Here, we treat vacancy data as a proxy for the demand in the job market. By doing so, internal promotions and career paths and informal channels are not taken into account. \n\n\\subsubsection{Skill Extraction}\\label{Vacancy Parser}\nFor skill extraction we rely on the %\nindustry-standard Textkernel Extract~\\cite{TextKernel} parser. \nFor each vacancy text, Textkernel Extract returns a json object with corresponding skills, represented by the surface form identified in the job posting (skill mention), a unique identifier representing the skill (skill id), and finally, a confidence score that quantifies the likelihood of the extracted skill to be correct.\n\n\\subsubsection{Skill Matching}\\label{Skill Matching}\n\nGiven the skills extracted by Textkernel, we match them to the skill nodes in our KG, by relying on the surface forms of the skills (skill mentions).\nMore specifically, we leverage character $n$-grams Jaccard similarity between the normalized skill mention and the normalized ESCO skill names. \nWe set the similarity threshold to $0.66$, which was empirically determined to be optimal using a smaller set of our $39,758,827$ Textkernel skills to ESCO skill-mappings. \nThe high-level process is shown in Figure~\\ref{fig:skill_match_flow}. \n\\begin{figure}[t]\n\\includegraphics[width = 0.8\\columnwidth]{figs/skill_match_flow}\n\\caption[Overview of skill matching process]{Overview of skill matching process}\n\\label{fig:skill_match_flow}\n\\end{figure}\n\n\\subsection{Final Skills \\& Occupational Knowledge Graph}\nOur final KG, resulting from the process shown in Figure~\\ref{fig:flow} and described in the previous section, consists of 1,220 nodes, of which 983 represent (ESCO) skills, and 237 (ISCO) occupations. \nThese nodes are connected through $3,910$ edges, with an average node degree of $6.4$. \n\nThis KG is a subset of the full ESCO ($13.485$ skills), and ISCO ($436$ occupations) taxonomies. \nThere are several reasons why our KG is a subset and does not span the entirety of the ISCO and ESCO taxonomies.\n\nFirst, it is conceivable that not all ISCO occupations are in current demand, e.g., we found that there were no vacancies for ISCO occupation code 8111: ``mining-plant operators,'' which is not surprising with currently no mines in operation in The Netherlands. \nNext, it is likely we are dealing with coverage issues, from\n(i) the likely incomplete coverage of the TextKernel Extract method we use for skill extraction, and \n(ii) our skills matching methodology further reducing the number of identified skills. \nAs the focus of this paper is on downstream applications, we consider matching out of scope, and rely on our naive but solid character $n$-grams-based method. \n", "meta": {"timestamp": "2021-09-07T02:37:32", "yymm": "2109", "arxiv_id": "2109.02554", "language": "en", "url": "https://arxiv.org/abs/2109.02554"}}
{"text": "\\section{Introduction}\nAs light travels through a moving medium, it is dragged along the axis of the medium's motion.  This effect, known as optical drag, was first described theoretically by Augustin-Jean Fresnel in 1818~\\cite{1818Fresnel, 1972Schaffner}. In 1851, Hippolyte Fizeau demonstrated Fresnel's drag experimentally~\\cite{1851Fizeau}.  However, Fresnel and Fizeau ignored the effect of refractive index dispersion. This effect was incorporated by Hendrik Lorentz, who in 1904 predicted the influence of dispersion on the optical drag effect for a moving medium with a fixed boundary~\\cite{1904Lorentz}.  Four years later, Jakob Laub developed a theoretical treatment for optical drag in a dispersive moving medium with a moving boundary~\\cite{1908Laub}.\nMany experiments that measured the effect of dispersion on optical drag were then reported by Zeeman and coauthors~\\cite{1914Zeeman,1915Zeeman,1919Zeeman,1920Snethlage,1922Zeeman}.\nThey measured the wavelength dependence of optical drag in water~\\cite{1914Zeeman,1915Zeeman} and performed other experiments to measure optical drag in quartz and flint glass~\\cite{1919Zeeman,1920Snethlage,1922Zeeman}.\n\nThe transverse displacement of a light beam experiencing optical drag depends on the both the refractive and group index of the beam in the moving medium~\\cite{2003Carusotto}.  It is well known that materials can be highly dispersive under nearly-resonant excitation~\\cite{2020Boyd}.  This large dispersion leads to a large group index, to highly subluminal pulse propagation (sometimes referred to as slow light), and consequently to an enhancement of the optical drag effect~\\cite{2011Franke-Arnold,2016Safari}.  Since absorption is typically high under nearly-resonant excitation, most observations of slow light have relied upon electromagnetically-induced transparency~(EIT)~\\cite{2008Firstenberg,2009Firstenberg} or coherent population oscillation~\\cite{2003BigelowSC,2003BigelowPRL,2007Piredda}.  These effects have the added benefit of providing very narrow resonances, leading to high dispersion and large group indices.  As a result, EIT has been used to further enhance the optical drag effect \\cite{2020Solomons} and perform high precision velocimetry \\cite{2020Chen}.\n\nUnder conditions of anomalous dispersion, the group index can become negative, leading to the so-called ``fast light'' effect, where the peak of a pulse advances as it travels through a dispersive medium instead of accumulating a delay~\\cite{2009Boyd}.  Under certain circumstances, the advancement of the peak can be attributed to pulse reshaping caused by saturated absorption or gain~\\cite{1966BasovD,1966BasovJ}. \n\nAn experimental demonstration of optical drag under conditions of anomalous dispersion has not been previously reported. In this article, we report on the anomalous optical drag effect, where the light beam shifts in the direction opposite to the motion of the medium. We excite EIT in a moving cell of rubidium vapor and induce a transition from normal to anomalous optical drag by properly modifying the two-photon detuning.\n\\section{Theoretical Model}\nThe transverse Fresnel-Fizeau drag effect in a generic dispersive medium has been treated theoretically by starting from the linearized Lorentz transformation for the frequency and wave vector of a beam propagating inside a moving medium~\\cite{2003Carusotto}.\nLet us consider a nonmagnetic and isotropic medium (i.e., relative magnetic permeability $\\mu=1$ and dielectric tensor $\\epsilon_{ij}=\\epsilon\\delta_{ij}$), with $\\epsilon_r=\\Re[\\epsilon]$. \nAs shown in Fig.~\\ref{fig:drag}(a), if the medium is moving with velocity $\\vec{\\mathrm{v_0}}=\\mathrm{v_{0}\\hat{\\vec{x}}}$, where $\\mathrm{z}$ is the longitudinal and $\\mathrm{x,y}$ are the transverse coordinates, the probe light beam experiences a shift along $\\mathrm{x}$ of\n\\begin{equation}\n\\mathrm{\\Delta x}=\\mathrm{L}\\tan\\theta.\n\\label{eq:Delta}\n\\end{equation}\nIn Eq.~(\\ref{eq:Delta}), $\\mathrm{L}$ is the medium's longitudinal length, and $\\theta$ the light's walk-off angle inside the medium, with \n\\begin{equation}\n\\tan\\theta=\\frac{\\mathrm{v_{0}}}{\\mathrm{c}}\\left(\\frac{\\mathrm{c}}{\\mathrm{v_g}}-\\frac{\\mathrm{v_p}}{\\mathrm{c}}\\right),\n\\label{eq:dragangle}\n\\end{equation}\nwhere $\\mathrm{c}$ is the speed of light in vacuum~\\cite{2003Carusotto}.  The group velocity and phase velocity of light in the medium are given by\n\\begin{equation}\n\\mathrm{v_g}=\\frac{\\mathrm{\\mathrm{c}}}{\\sqrt{\\epsilon_\\mathrm{r}(\\omega_0)}+\\frac{\\omega_0}{2\\sqrt{\\epsilon_\\mathrm{r}(\\omega_0)}}\\left(\\frac{\\mathrm{d}\\epsilon_\\mathrm{r}}{\\mathrm{d}\\omega}\\right)_{\\omega_0}},\n\\label{eq:groupv}\n\\end{equation}\n\\begin{equation}\n\\mathrm{v_p}=\\frac{\\mathrm{c}}{\\sqrt{\\epsilon_\\mathrm{r}(\\omega_0)}},\n\\label{eq:phasev}\n\\end{equation}\nrespectively.\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{dragdiagram.eps}\n\\caption{Pictorial representation of the transverse Fresnel-Fizeau drag effect for (a) normal optical drag, and (b) anomalous optical drag.}\n\\label{fig:drag}\n\\end{figure}\n\nEquation~(\\ref{eq:dragangle}) can be expressed in terms of the medium's refractive index $\\mathrm{n}(\\omega)=\\sqrt{\\epsilon_\\mathrm{r}(\\omega)}$, or after fixing the value of the carrier frequency, as $\\mathrm{n_0}=\\sqrt{\\epsilon_\\mathrm{r}(\\omega_0)}$. Equations~(\\ref{eq:groupv},\\ref{eq:phasev}) can then be rewritten as $\\mathrm{v_g}=\\mathrm{c/{n_{g,0}}}$ and $\\mathrm{v_\\mathrm{p}= c/n_0}$, where $\\mathrm{n_{g,0}=n_0+\\left(\\omega\\frac{\\mathrm{d} n}{\\mathrm{d}\\omega}\\right)_{\\omega_0}}$ is the group refractive index.\nThe resulting expression, \n\\begin{equation}\n\\mathrm{\\tan\\theta=\\frac {v_{0}}{c}\\left(n_{g,0}-\\frac{1}{n_0}\\right)}, \n\\end{equation}\nreveals the existence of three different regimes:\\\\\n(i) $\\mathrm{n_{g,0}>n_0^{-1}}$ normal (or positive) optical drag [Fig.~\\ref{fig:drag}(a)];\\\\\n(ii) $\\mathrm{n_{g,0}=n_0^{-1}}$ absence of optical drag;\\\\\n(iii) $\\mathrm{n_{g,0}<n_0^{-1}}$ anomalous (or negative) optical drag [Fig.~\\ref{fig:drag}(b)]. For a beam experiencing normal dispersion, if $\\mathrm{v_{p} \\approx c}$ and $\\mathrm{v_{g} \\ll c}$, the drag can be approximated in terms of the group delay, $\\tau$: $\\mathrm{\\Delta x \\approx Lv_{0}/v_{\\mathrm{g}} = \\tau v_{0}}$ \\cite{2020Solomons}.  The conditions of anomalous dispersion, $\\mathrm{n_{g,0} < 0}$, satisfy regime (iii), hence the name \\textit{anomalous} drag.  In highly dispersive conditions, the relation $\\mathrm{\\Delta x \\approx \\tau v_{0}}$ is still valid because $\\mathrm{n_{g,0}} \\ll 0$. \n\nIn EIT, the transition from normal to anomalous drag occurs close to the transition from negative to positive two-photon detuning,\n$\\delta$. To model optical drag under EIT conditions, we begin from Eq.~(50) in \\cite{2008Firstenberg}, which describes the steady-state propagation of a probe beam under EIT conditions in a gaseous medium of diffusing atoms.  The medium's velocity, $\\vec{\\mathrm{v}}_0$, is accounted for by adding a nonzero mean to the Boltzmann distribution of atomic velocities with thermal velocity $\\mathrm{v_{\\mathrm{th}}=(k_\\mathrm{B} T/m)^{1/2}}$,\n\\begin{equation}\nF(\\vec{\\mathrm{v}})=\\frac{\\exp\\left(-\\frac{(\\vec{\\mathrm{v}}-\\vec{\\mathrm{v_0}})^2}{\\mathrm{2v_{\\mathrm{th}}^2}}\\right)}{\\mathrm{2\\pi v_{\\mathrm{th}}^2}}.\n\\label{eq:boltzmann}\n\\end{equation}\n$\\mathrm{k_\\mathrm{B}}$ is the Boltzmann constant, $\\mathrm{T}$ is the temperature, and $\\mathrm{m}$ is the atomic mass.\nAssuming negligible diffraction, the dynamics of the probe beam are described by\n\\begin{widetext}\n\\begin{equation}\n\\mathrm{\\left[-i\\delta+\\gamma+K\\left|\\tilde{\\Omega}_2(\\vec{\\mathrm{r}})\\right|^2-D\\left(\\partial_{\\vec{\\mathrm{r}}}-i\\vec{\\mathrm{\\Delta q}}\\right)^2+\\left(\\partial_{\\vec{\\mathrm{r}}}-i\\vec{\\mathrm{\\Delta q}}\\right)\\cdot\\vec{\\mathrm{v_0}}\\right]\\left[\\alpha\\tilde{\\Omega}_2(\\vec{\\mathrm{r}})\\right]^{-1}\\left(\\partial_z+\\alpha\\right)\\tilde{\\Omega}_1(\\vec{\\mathrm{r}})=K\\tilde{\\Omega}_2^*(\\vec{\\mathrm{r}})\\tilde{\\Omega}_1(\\vec{\\mathrm{r}})}.\n\\label{eq:rabi_real}\n\\end{equation}\n\\end{widetext}\nHere, $\\tilde{\\Omega}_{1,2}$ represent the Rabi frequency envelopes in the slowly varying envelope approximation for the probe and control beams, respectively, and $\\mathrm{K}$ is the one-photon complex spectrum with total linewidth $\\Gamma$ incorporating homogeneous dephasing and Doppler broadening of the optical transition. Moreover, $\\gamma$ is the decoherence rate of the ground-state transition, $\\mathrm{D}$ is the spatial diffusion coefficient, $\\vec{\\mathrm{\\Delta q}}=\\vec{\\mathrm{q}}_2-\\vec{\\mathrm{q}}_1$ is the probe-control wave vector mismatch (with absolute value $\\mathrm{\\Delta q}$ along $\\hat{\\vec{\\mathrm{x}}}$), and $\\alpha$ is the attenuation (per unit distance) without EIT.\n\n\\begin{figure*}[!ht]\n    \\centering\n    \\includegraphics[width = 0.8\\textwidth]{phdragexpsetup_update6.eps}\n    \\caption{Experimental setup for observing anomalous optical drag under conditions of EIT.  (left) The probe and control fields are derived from a diode laser ($795 \\un{nm}$) tuned to the $\\mathrm{D}_1$ transition in $^{87}\\mathrm{Rb}$.  A half-wave plate (HWP) controls the relative power of the orthogonally polarized probe and control fields before they are split by a polarizing beamsplitter.  The probe passes through an electro-optic modulator (EOM) and an acousto-optic modulator (AOM) to tune it to the $\\mathrm{F} = 1 \\rightarrow \\mathrm{F'} = 1,2$ transition and vary the two-photon detuning, $\\delta$, respectively.  The probe and control are then recombined at a PBS before being coupled into a single-mode polarization-maintaining fiber (PMF).  After exiting the PMF both fields pass through a $^{87}\\mathrm{Rb}$ cell that is shielded from stray magnetic fields by a double-layer magnetic shield ($\\mu\\text{-metal}$).  A solenoid creates a uniform magnetic field to lift the degeneracy of the Zeeman sublevels.  The probe and control fields interact through an EIT scheme shown in the upper right panel.  The dominant interaction occurs around the $\\mathrm{m_F} = 0 \\longrightarrow \\mathrm{m^{\\prime}_{F}} = \\pm 1$ transitions for the probe (yellow arrows) and control (brown arrows).  By detuning both the probe and control $-250 \\un{MHz}$ below the $\\mathrm{F}^{\\prime} = 2$ excited state, the two-photon lineshape is sufficiently asymmetric to produce anomalous dispersion (lower right panel).  The normalized transmission data were fit simultaneously to all other data sets using $\\Gamma$, $\\gamma$, and $\\mathrm{\\Delta q}$ as free parameters.  The entire Rb cell apparatus is mounted on a translation stage (TS) that travels transverse to the beam propagation.  A polarizer (P) after the cell filters out the control field, and the probe field is split into two paths by a beamsplitter (BS). One path terminates on a photodiode (PD) for measuring the group delay of the probe.  In the other, a lens (L) images the output facet of the cell onto a camera (CCD).}\n    \\label{fig:expsetup}\n\\end{figure*}\n\nIn Fourier space, stressing that the medium velocity is parallel to the $\\mathrm{x}$-axis, one obtains\n\\begin{equation}\n\\mathrm{\\partial_z\\Omega_1=i\\frac{k_z}{2}\\chi(k_x,k_z,\\delta)\\Omega_1}\n\\label{eq:rabi_FFT}\n\\end{equation}\nwith the susceptibility derived from Eq.~(\\ref{eq:rabi_real}),\n\\begin{widetext}\n\\begin{equation}\n\\mathrm{\\chi(k_x,k_z,\\delta)=i\\frac{2\\alpha}{k_z}\\left[1-\\frac{K|\\Omega_2|^2}\n{-i\\delta+\\gamma+K\\left|\\Omega_2\\right|^2+D\\left(k_x+\\Delta q\\right)^2-i\\left(k_x+\\Delta q\\right)v_{0}}\\right]}.\n\\label{eq:susceptibility}\n\\end{equation}\n\\end{widetext}\nThe probe susceptibility's dependence on $\\mathrm{k_z}$ is such that $\\mathrm{\\partial_{k_z}\\left[k_z\\chi(k_x,k_z,\\delta)\\right]=0}$.  It should be noted that Eq.~(\\ref{eq:rabi_FFT}) is exact only when the control field is an infinite plane wave, $\\Omega_2(\\vec{\\mathrm{r}}) = \\Omega_{2}$.  Nonetheless, quantitative agreement between Eq.~(\\ref{eq:rabi_FFT}) and experiment can be achieved using a control field of finite extent as long as its beam waist, $\\mathrm{w_0}$, is wide enough to satisfy $\\mathrm{w_0} \\gg \\mathrm{max\\{\\sqrt{D\\tau}, v_{0}\\tau\\}}$~\\cite{2020Solomons}.\n\nThe resulting solution of Eq.~(\\ref{eq:rabi_real}) is\n\\begin{equation}\n\\mathrm{\\tilde{\\Omega}_1(x,z)}=\\mathcal{F}^{-1}\\left\\{\\mathcal{F}\\left[\\mathrm{\\tilde{\\Omega}_1|_{z=0}}\\right]\\exp\\left[i\\mathrm{\\frac{\\chi(k_x,k_z,\\delta)}{2}k_z z}\\right]\\right\\},\n\\label{eq:rabi_probe}\n\\end{equation}\nwhere $\\mathcal{F}$ is the Fourier transform operator.\n\nThis picture furnishes an expression for the transverse shift in Eq.~(\\ref{eq:Delta}), where the dependence of the optical drag and group delay on the two-photon detuning appears explicitly~\\cite{2020Solomons}:\n\\begin{equation}\n\\mathrm{\\Delta x=\\partial_{k_x}\\Re\\left(\\frac{\\chi(k_x,k_z,\\delta)}{2}k_z L\\right)},\n\\label{eq:DeltaEIT}\n\\end{equation}\n\\begin{equation}\n\\mathrm{\\tau=\\partial_{\\delta}\\Re\\left(\\frac{\\chi(k_x,k_z,\\delta)}{2}k_z L\\right)}.\n\\label{eq:delayEIT}\n\\end{equation}\n\\section{Experimental Results}\nThe experimental arrangement for measuring anomalous optical drag is shown in Fig.~\\ref{fig:expsetup}.  The control and the probe beams are obtained from the same diode laser whose central frequency is set to $795\\un{nm}$ to excite the D1 transition of the $^{87}$Rb atoms. The probe is tuned to the $\\mathrm{F = 1 \\rightarrow F' = 1,2}$ transition, and the control is tuned to the $\\mathrm{F = 2 \\rightarrow F' = 1,2}$ transition.  Both fields are detuned by $-250 \\un{MHz}$ below the $\\mathrm{F'=2}$ level, which ensures a sufficiently asymmetric transmission lineshape around the two-photon resonance (see Fig.~\\ref{fig:expsetup}, lower right panel), resulting in negative group velocity~\\cite{2004Mikhailov}.  A normalized version of Eq.~(\\ref{eq:rabi_probe}) was fit to the transmission data simultaneously to all other data sets reported here.\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width = \\columnwidth]{dragvsdetuning7.eps}\n    \\caption{Dependence of optical drag on two-photon detuning $\\delta$. (top row) The measured transverse beam displacement $\\mathrm{\\Delta x}$ is in the same direction as the velocity of the cell on the  red-detuned side of the two-photon transition and in the opposite direction on the blue-detuned side.  This behavior is observed for both (a) negative and (b) positive cell velocities. (bottom row)  The drag expected from the measured group delay, $\\mathrm{\\Delta x \\approx \\tau v_{0}}$ is measured simultaneously with $\\mathrm{\\Delta x}$ and predicts substantially less anomalous optical drag.  The fit lines are obtained from maximum likelihood estimation using the models of Eqs.~(\\ref{eq:DeltaEIT},~\\ref{eq:delayEIT}) with $\\Gamma$, $\\gamma$, and $\\mathrm{\\Delta q}$ as free parameters. An offset of $425 \\un{Hz}$ is added to the fit lines to account for a systematic error in the detuning measurement.}\n    \\label{fig:dragvsdetune}\n\\end{figure}\nThe probe and control beams exit the same polarization-maintaining fiber with a radius $\\mathrm{w_p = w_c = 2 \\un{mm}}$. The total control power is $\\sim 40 \\un{\\mu W}$ and that of the probe is $\\sim 2 \\un{\\mu W}$. The $^{87}$Rb vapor is kept in a magnetically-shielded Pyrex cell of length $\\mathrm{L} = 7.5 \\un{cm}$. The cell contains buffer gases $\\mathrm{N_2}$ ($10\\un{Torr}$) and Ar ($90\\un{Torr}$) and is heated to $55\\un{^{\\circ}C}$, in which case the thermal motion of the atoms behaves diffusively with a diffusion coefficient of $\\mathrm{D} \\approx 166 \\un{mm^2/s}$. The cell assembly is mounted on a Thorlabs DDSM100 motorized stage, so it can move with a constant velocity while crossing the beams. \n\nA polarizer is placed after the cell to filter the probe from the control field.  The probe field is then split into two paths, one that forms an image of the probe at the output of the cell on a Princeton ProEM camera and another that terminates at a photodetector for temporal detection of the probe pulse.  The imaging path is used to measure $\\mathrm{\\Delta x}$, and the other is used to measure $\\tau$.  By comparing the transverse beam profile's center of mass when the cell is moving and not moving, the transverse drag is obtained.  Similarly, the mean temporal shift between pulses for various two-photon detunings gives the group delay of the pulse inside the cell.\n\nThe dependence of optical drag on the two-photon detuning is shown in Fig.~\\ref{fig:dragvsdetune}.  Both $\\mathrm{\\Delta x}$ and $\\mathrm{\\tau v_{0}}$ are measured at cell velocities of $200 \\un{mm/s}$ (a) and $-200 \\un{mm/s}$ (b), the maximum achievable velocities of our system.  The models of Eqs.~(\\ref{eq:DeltaEIT}, \\ref{eq:delayEIT}) are fit to the $\\mathrm{\\Delta x}$ and $\\mathrm{\\tau v_{0}}$ data respectively using maximum likelihood estimation (MLE) with three free parameters: $\\Gamma$, $\\gamma$, and $\\mathrm{\\Delta q}$. Furthermore, we assume that the data have Gaussian random errors.  In fact, all data sets shown were fit simultaneously using MLE, obtaining the following values of the three fit parameters: $\\Gamma = 266(2) \\un{MHz}$, $\\gamma = 145(2) \\un{Hz}$, and $\\mathrm{\\Delta q} = 1.2(3)*10^{-6} \\un{2\\pi/\\lambda}$. Uncertainties in the fit parameter values were obtained through Monte Carlo simulation.  The data show a strong dependence on the two-photon detuning $\\delta$; normal optical drag occurs for $\\delta < 425\\un{Hz}$, while anomalous optical drag occurs for $\\delta > 425\\un{Hz}$ and peaks around $\\delta = 900\\un{Hz}$.  Note that the $\\mathrm{\\tau v_{0}}$ data predict approximately half as much anomalous optical drag than is actually measured, $\\mathrm{\\Delta x}$.  The breakdown of this approximation may result from systematic error in the estimation of the pulse delay.\n\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width = \\columnwidth]{dragvsvelocity6.eps}\n    \\caption{Experimental observation of optical drag at various dragging velocities.  (a)  At a two-photon detuning of $\\delta = 0 \\un{Hz}$ the beam experiences normal optical drag.  The dependence of the drag on the velocity is approximately linear from $\\mathrm{v_{0}} = -200 \\un{mm/s}$ to $200 \\un{mm/s}$. (b) There is effectively no optical drag for any cell velocity when $\\delta = 425 \\un{Hz}$. (c) At $\\delta = 900 \\un{Hz}$ the beam undergoes anomalous optical drag for all velocities in the domain $\\mathrm{v_{0}} = -200 \\un{mm/s}$ to $200 \\un{mm/s}$. The fit lines were obtained by fitting Eq.~(\\ref{eq:DeltaEIT}) to all data sets simultaneously.}\n    \\label{fig:dragvsvelocity}\n\\end{figure}\n\nFigure~\\ref{fig:dragvsvelocity} shows the dependence of optical drag on dragging velocity.  Despite the nonlinear dependence of Eq.~(\\ref{eq:susceptibility}), and consequently Eq.~(\\ref{eq:DeltaEIT}), on the dragging velocity, the data are approximately linear in this experimental regime.  Interestingly, for a two-photon detuning of $\\delta = 425 \\un{Hz}$, almost no optical drag is present for any dragging velocity, as shown in Fig.~\\ref{fig:dragvsvelocity}(b).  The null optical drag is unique and corresponds to conditions of zero group delay in the medium (zero crossing in Fig.~\\ref{fig:dragvsdetune}).  Normal [Fig.~\\ref{fig:dragvsvelocity}(a)] and anomalous [Fig.~\\ref{fig:dragvsvelocity}(c)] drag are observed over a range of velocities with a high degree of symmetry between the normal and anomalous effects.\n\\section{Conclusions}\nWe demonstrated normal, anomalous, and null optical drag effects experimentally and modeled them theoretically.  By using electromagnetically-induced transparency in $^{87} \\mathrm{Rb}$ vapor we were able to maximize the dispersion and minimize the loss associated with near-resonant atomic excitation. Our measurements showed that the beam's displacement is proportional to the velocity of the medium through which it passes, with a direction depending on the sign of medium's dispersion.  For positive group delays, the beam experiences normal optical drag and is dragged in the direction of the material motion.  When the measured group delay is negative, the beam is dragged in a direction opposite to the material displacement. In a regime where the group index is exactly equal to the inverse of the refractive index and there is no measured group delay, there is no optical drag at any material velocity.  To our knowledge this is the first experimental demonstration of anomalous drag.\n\nIn the gaseous system we study, the average linear velocity of the atoms due to the transverse motion of the cell adds to their underlying thermal velocity distribution, resulting in a drift-diffusive atomic motion. The atomic diffusion can lead to paraxial diffusion of the light field~\\cite{FirstenbergRMP13, FirstenbergPRL10} and also, similarly to anomalous and null drag, to effective paraxial diffraction that eliminates and even reverses the free-space optical diffraction~\\cite{FirstenbergNatPhys09}.  While here we attenuate the effect of diffusion by using a dense buffer gas, it could be interesting to explore the interplay between (linear) drag and (quadratic) diffusion and diffraction in the anomalous and null regimes.\n\\begin{acknowledgements}\nThe authors would like to acknowledge M. Zahirul Alam for very enlightening discussions during the process of conducting this experiment and writing the manuscript.  This work was supported by the Pazy Foundation, the Israel Science Foundation, and the Consortium for quantum sensing of Israel Innovation Authority.\n\\end{acknowledgements}\nC.B. and Y.S. contributed equally to this work.\n\\input{AnomalousOpticalDrag.bbl}\n\\end{document}", "meta": {"timestamp": "2021-09-07T02:37:01", "yymm": "2109", "arxiv_id": "2109.02534", "language": "en", "url": "https://arxiv.org/abs/2109.02534"}}
{"text": "\\subsection{Shape differences in astragali of wild and domesticated sheep}\n\\label{sec_sheep}\n\n\\newcommand{\\coefi}[1]{\\boldsymbol{\\beta}_{\\mathrm{#1}_i}}\n\\newcommand{\\coefix}[1]{\\boldsymbol{\\beta}_{{#1}_i}}\n\\newcommand{\\psi}{\\psi}\n\\newcommand{\\boldsymbol{\\xi}}{\\boldsymbol{\\xi}}\n\n\\new{In a geometric morphometric study, }\\citet{Poellath2019sheepbones} \\old{in a paleoanatomical study }investigate shapes of sheep astragali (ankle bones) \\new{to understand the influence of different living conditions on the micromorphology of the skeleton}\\old{to understand morphological effects of domestication}. Based on a total of $n=163$ shapes recorded by \\citet{Poellath2019sheepbones\n, we model the astragalus shape in dependence on different \\new{variables}\\old{characteristics}, including domestication status (wild/feral/domesticated), sex (female/male/NA), age (juvenile/subadult/adult/NA), and mobility (confined/pastured/free) of the animals as categorical covariates. The sample comprises sheep of four different \\new{populations}\\old{breeds}: \n\\new{A}\\old{a}siatic wild sheep \\citep[Field Museum, Chicago;][]{lay1967wildsheep, zeder2006wildsheep}, feral Soay sheep\n\\citep[British Natural History Museum, London;][]{Clutton-Brock1990soay}, and domestic \\new{sheep of the Karakul and Marsch breed}\\old{Karakul sheep \nas well as Marsch sheep}\n\\citep[Museum of Livestock Sciences, Halle (Saale);][]{Schafberg2010KarakulMarsch}. Table \\ref{tab:data-summary} in Supplement \\ref{sec:bones_appendix} shows the distribution of available covariates within the \\new{population}\\old{breed}s.\nEach sheep astragalus shape, $i = 1,\\dots, n$, is represented by a configuration composed of 11 selected landmarks in a vector $\\mathbf{y}_{i}^{\\text{lm}}\\in\\mathds{C}^{11}$ and two vectors of \\new{sliding}  semi-landmarks $\\mathbf{y}_{i}^{\\text{c1}}\\in\\mathds{C}^{14}$ and $\\mathbf{y}_{i}^{\\text{c2}}\\in\\mathds{C}^{18}$ evaluated along two outline curve segments, marked on a 2D image of the bone (dorsal \\new{view}\\old{perspective}). \nSeveral example configurations are displayed in Supplement Figure \\ref{fig:sheepdataexamples}.\nIn general, we could separately specify\nsmooth function bases for the outline segments $y_i^{\\text{c1}}$ and $y_i^{\\text{c2}}$, respectively. \nDue to their systematic recording, we assume, however, that not only landmarks but also semi-landmarks are regularly observed on a fixed grid, and refrain from using smooth function bases for simplicity. \nAccordingly, shape configurations can directly be identified with their evaluation vectors $\\mathbf{y}_i = \\vec{\\mathbf{y}_i^{\\text{lm}\\top}, \\mathbf{y}_i^{\\text{c1}\\top}, \\mathbf{y}_i^{\\text{c2}\\top}} \\in \\mathds{C}^{43} = \\mathcal{Y}$, and the geometry of the response space $\\mathcal{Y}^*_{/\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}$ widely corresponds to the classic Kendall's shape space geometry, with the difference that, considering landmarks more descriptive than single semi-landmarks, we choose a weighted inner product $\\langle \\mathbf{y}_i, \\mathbf{y}_i'\\rangle = \\mathbf{y}_i^\\dagger \\mathbf{W} \\mathbf{y}_i'$ with diagonal weight matrix $\\mathbf{W}$ with diagonal $\\vec{\\boldsymbol{1}_{11}^\\top, \\frac{3}{14} \\boldsymbol{1}_{14}^\\top, \\frac{3}{18} \\boldsymbol{1}_{18}^\\top}$ assigning the weight of three landmarks to each outline segment. \nWe model the astragalus shapes $[\\mathbf{y}_i]\\in \\mathcal{Y}^*_{/\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}$ as \n\\begin{align*}\n\t[\\boldsymbol{\\mu}_i] &= \\operatorname{Exp}_{[\\mathbf{p}]}\\left(\\coefi{status} + \\coefi{\\new{pop}\\old{breed}} + \\coefi{age} + \\coefi{sex} + \\coefi{mobility}\\right)\n\\end{align*}\nwith the pole $[\\mathbf{p}]\\in \\mathcal{Y}^*_{/G}$ specified as overall mean and the conditional mean $[\\boldsymbol{\\mu}_i]\\in \\mathcal{Y}^*_{/\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}$ depending on the effect coded covariate effects $x_{ij} \\mapsto \\boldsymbol{\\beta}_{x_{ij}}\\in T_{[\\mathbf{p}]}\\mathcal{Y}^*_{/\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}$.\nFor identifiability, the \\new{population}\\old{breed} and mobility effects are centered around the status effect, as we only have data on different \\new{population}\\old{breed}s/mobility levels for domesticated sheep.\nAll base-learners are regularized to one degree of freedom by employing ridge penalties for the coefficients of the covariate bases $\\{b_j^{(l)}\\}_l$\nwhile the coefficients of the response basis (the standard basis for $\\mathds{C}^{43})$\nare left un-penalized. \nWith a step-length of $\\eta = 0.1$, 10-fold shape-wise cross-validation suggests early stopping after $89$ boosting iterations.\nDue to the regular design, we can make use of the functional linear array model \\citep{BrockhausGreven2015} for saving computation time and memory, which lead to 8 seconds of initial model fit followed by 47 seconds of cross-validation.\nTo interpret the categorical covariate effects, we rely on TP factorization (Figure \\ref{fig:bone_effects}). \nThe first component of the status effect explains about 2/3 of the variance of the status effect and over 50\\% of the cumulative effect variance in the model. In that main direction,  the effect of \\textit{feral} is not located between \\textit{wild} and \\textit{domestic}, as might be naively expected. By contrast, the second component of the effect seems to reflect the expected order and still explains a considerable amount of variance.\nSimilar to \\cite{Poellath2019sheepbones}, we find  little influence of age, sex and mobility on the astragalus shape. Yet, all covariates were selected by the boosting algorithm.\n\n\\begin{figure}[!h]\n\t\\centering \n\t\\includegraphics[width=1.5in]{figure/sheep_implot}\n\t\\includegraphics[width=3.5in]{figure/sheep_plots}\n\t\\caption{\n\t\t\\textit{Left}: Shares of different factorized covariate effects in the total predictor variance.\n\t\t\\textit{Right}: Factorized effect plots showing the two components of the status effect (\\textit{rows}): in the \\textit{right column}, the two first directions $\\boldsymbol{\\xi}^{(1)}_1, \\boldsymbol{\\xi}^{(2)}_1 \\in T_{[\\mathbf{p}]}\\mathcal{Y}^*_{/\\operatorname{Trl} + \\operatorname{Rot} + \\operatorname{Scl}}$ are visualized via line-segments originating at the overall mean shape (\\textit{empty circles}) and ending in the shape resulting from moving 1 unit into the target direction (\\textit{solid circles}; \\textit{large}: landmarks; \\textit{small}: semi-landmarks along the outline); in the  \\textit{left column}, the status effect in the respective direction is depicted. As illustrated in the \\textit{middle} plot, an effect of 1 would correspond to the full extend of the direction shown to the right.}\\label{fig:bone_effects}\n\\end{figure}\n\nVisually, differences in estimated mean shapes are rather small, which is, in our experience, quite usual for shape data. With differences in size, rotation and translation excluded by definition, only comparably small variance remains in the observed shapes.\nNonetheless, TP factorization provides accessible visualization of the effect directions and allows to partially order the effect levels in each direction.\n\n\\section{Component-wise Riemannian $L_2$-Boosting}\n\\label{chap_boosting}\n\n\n\nComponent-wise gradient boosting \\citep[e.g.][]{HothornBuehlmann2010} is a step-wise model fitting procedure accumulating predictors from smaller models, so called base-learners, to built an ensemble predictor aiming at minimizing a mean loss function. To this end, the base-learners are fit (via least squares) to the negative gradient of the loss function in each step and the best fitting base-learner is added to the current ensemble predictor. \nDue to its versatile applicability, inherent model selection, and slow over-fitting behavior, boosting has proven useful in various contexts \\citep{mayr2014evolution}.\nBoosting with respect to the least squares loss function $\\ell(y,\\mu) = \\frac{1}{2}(y-\\mu)^2$, $y,\\mu\\in\\mathds{R}$, is typically referred to as $L_2$-Boosting and simplifies to repeated re-fitting of residuals $\\varepsilon = y-\\mu = -\\nabla_\\mu \\ell(y, \\mu)$ corresponding to the negative gradient of the loss function. For $L_2$-Boosting with a single learner, \\cite{Buehlmann2003L2boosting} show how fast bias decay and slow variance increase over the boosting iterations suggest stopping the algorithm early before approaching the ordinary (penalized) least squares estimator.\n\\cite{LutzBuehlmann2006BoostingHighMultivariate} prove consistency of component-wise $L^2$-Boosting in a high-dimensional multivariate response linear regression setting and \\cite{Stoecker2019FResponseLSS} illustrate in extensive simulation studies how stopping the boosting algorithm early based on curve-wise cross-validation applies desired regularization when fitting (even highly autocorrelated) functional responses with parameter-intense additive model base-learners and, thus, leads to good estimates even in challenging scenarios.\\\\\nWhen generalizing to least squares on Riemannian manifolds with the loss $\\frac{1}{2}d^2([y],[\\mu])$ given by the squared geodesic distance, the negative gradient $-\\nabla_{[\\mu]}\\frac{1}{2}d^2([y], [\\mu])=\\operatorname{Log}_{[\\mu]}([y]) = \\varepsilon_{[\\mu]}$ \\citep[compare e.g.\\ ][]{Pennec2006BasicTools} corresponds to the local residuals $\\varepsilon_{[\\mu]}$ defined in Section \\ref{chap_model}. This analogy to $L_2$-Boosting motivates the presented generalization where local residuals are further transported to residuals $\\epsilon$ in a common linear space.\n\nConsider the pole $[p]$ known and fixed for now. Assuming its existence, we aim to minimize the population mean loss\n$$\n \\sigma^2(h) = \\mathds{E}\\left( d^2\\left([Y], \\operatorname{Exp}_ {[p]}\\left( h(\\mathbf{X}) \\right)\\right) \\right)\n$$\nwith the point-wise minimizer \n$h^\\star(\\mathbf{x}) = \\argmin[h:\\mathcal{X} \\rightarrow T_{[p]}\\mathcal{Y}_{/G}^*] \\mathds{E}\\left( d^2\\left([Y], \\operatorname{Exp}_ {[p]}\\left( h(\\mathbf{X}) \\right)\\right) \\mid \\mathbf{X} = \\mathbf{x} \\right)$\nminimizing the conditional expected squared distance. Fixing a covariate constellation $\\mathbf{x} \\in \\mathcal{X}$, the prediction $[\\mu] = \\operatorname{Exp}_ {[p]}\\left( h^\\star(\\mathbf{x}) \\right)$ corresponds to the Fr\u00e9chet mean \\citep{Karcher1977KarcherMean} of $[Y]$ conditional on $\\mathbf{X} = \\mathbf{x}$. \nIn a finite-dimensional context, \\citet{Pennec2006BasicTools} show that $\\mathds{E}\\left(\\varepsilon_{[\\mu]}\\right)=\\boldsymbol{0}$ for a Fr\u00e9chet mean $[\\mu]$ if residuals $\\varepsilon_{[\\mu]}$ are uniquely defined with probability one. This indicates the connection to our residual based model formulation in Section \\ref{chap_model}.\nWe fit the model by reducing the empirical mean loss\n$\n\\hat\\sigma^2(h) = \\frac{1}{n} \\sum_{i=1}^n d_i^2\\left([y_i], \\operatorname{Exp}_ {[p]}\\left(h(\\mathbf{x}_i)\\right)\\right),\n$\nwhere we replace the population mean by the sample mean and compute the geodesic distances $d_i$ with respect to the inner products $\\langle\\cdot, \\cdot\\rangle_i$ defined for the respective evaluations of $y_i$. \n\nA base-learner corresponds to a covariate effect $h_j(\\mathbf{x}) = \\sum_{r, l} \\theta_j^{(r,l)}\\, b_j^{(l)}(\\mathbf{x})\\, \\partial_r$, $\\boldsymbol{\\Theta}_j = \\{\\theta_j^{(r,l)}\\}_{r,l}$, which is repeatedly fit to the transported residuals $\\epsilon_1, \\dots, \\epsilon_n$ by penalized least-squares (PLS) minimizing $\\sum_{i=1}^n \\|\\epsilon_i - h_j(\\mathbf{x}_i)\\|_i^2 + \\lambda_j \\operatorname{tr}{\\left(\\boldsymbol{\\Theta}_j\\mathbf{P}_j\\boldsymbol{\\Theta}_j^\\top\\right)} + \\lambda_{} \\operatorname{tr}{\\left(\\boldsymbol{\\Theta}^\\top\\mathbf{P}_{}\\boldsymbol{\\Theta}\\right)}$. \nVia the penalty parameters $\\lambda_j, \\lambda_{} \\geq 0$ the effective degrees of freedom of the base-learners are controlled \\citep{HofnerSchmid2011} to achieve a balanced ``fair'' base-learner selection despite the typically large and varying number of coefficients involved in the TP effects. \nThe symmetric penalty matrices $\\mathbf{P}_j\\in\\mathds{R}^{m_j\\timesm_j}$ and $\\mathbf{P}_{}\\in\\mathds{R}^{m_{}\\timesm_{}}$ (imposing, e.g., a second-order difference penalty for B-splines in either direction) can equivalently be arranged as a $m_jm_{}\\timesm_jm_{}$ penalty matrix $\\mathbf{R}_j=\\lambda_j (\\mathbf{P}_j \\otimes \\mathbf{I}_{m_{}}) + \\lambda_{} (\\mathbf{I}_{m_j} \\otimes \\mathbf{P}_{})$ for the vectorized coefficients $\\operatorname{vec}{(\\boldsymbol{\\Theta}_j)}=(\\theta^{(1,1)}_j, \\dots, \\theta^{(m_{},1)}_j, \\dots, \\theta^{(m_{},m_j)})^\\top$, where $\\otimes$ denotes the Kronecker product. \nThe standard PLS estimator is then given by $\\operatorname{vec}{(\\widehat\\boldsymbol{\\Theta}_j)} = \\left(\\boldsymbol{\\Psi}_j + \\mathbf{R}_j\\right)^{-1} \\boldsymbol{\\psi}_j$ with\n$\\boldsymbol{\\Psi}_j = \\sum_{i=1}^n  \\left\\{ \\re{\\langle b^{(l)}_j(\\mathbf{x}_i) \\partial_r, b^{(l')}_j(\\mathbf{x}_i) \\partial_{r'}\\rangle_i} \\right\\}_{\\substack{(r,l)=(1,1), \\dots, (m_{},1), \\dots, (m_{},m_j)\\\\ (r',l')=(1,1), \\dots, (m_{},1), \\dots, (m_{},m_j) }} \\in \\mathds{R}^{m_{}\\,m_j \\times m_{}\\,m_j}$ and $\\boldsymbol{\\psi}_j = \\sum_{i=1}^n  \\left\\{ \\re{\\langle b^{(l)}_j(\\mathbf{x}_i) \\partial_r, \\epsilon_i\\rangle_i} \\right\\}_{(r,l)=(1,1), \\dots, (m_{}, 1), \\dots, (m_{},m_j)} \\in \\mathds{R}^{m_{}\\,m_j}$.\nIn a regular design, using the functional linear array model \\citep{BrockhausGreven2015} can  save memory and computation time by avoiding construction of the complete matrices.\nThe basis construction of $\\{\\partial_r\\}_r$ via a transformation matrix $\\mathbf{Z}_p$ (Section \\ref{sec_tensorproduct_effects}) is reflected in the penalty by setting $\\mathbf{P}_{} = \\mathbf{Z}_p^\\top (\\mathbf{I}_2 \\otimes \\mathbf{P}_0) \\mathbf{Z}_p$ with $\\mathbf{P}_0$ the penalty matrix for the un-transformed basis $\\{b_0^{(r)}\\}_r$. \n\nIn each iteration of the proposed Algorithm \\ref{riemmanianL2boosting}, the best-performing base-learner is added to the current ensemble additive predictor $h(\\mathbf{x})$ after multiplying it with a step-length parameter $\\eta\\in(0,1]$. Due to the additive model structure this corresponds to a coefficient update of the selected covariate effect. Accordingly, after repeated selection, the effective degrees of freedom of a covariate effect, in general, exceed the degrees specified for the base-learner. They are successively adjusted to the data.\nTo avoid over-fitting, the algorithm is typically stopped early before reaching a minimum of the empirical mean loss. The stopping iteration is determined, e.g., by re-sampling strategies such as bootstrapping or cross-validation on the level of shapes/forms. \n\n\\newcommand{\\eta}{\\eta}\n\n\\IncMargin{1em}\n\\begin{algorithm}\n\t\\SetKwFunction{matrix}{matrix}\t\n\t\\SetKwInOut{Input}{Hyper-parameters}\n\t\\SetKwInOut{Output}{Geometry}\n\t\\SetKwInOut{Initialize}{Base-learners}\n\t\\SetKwFor{For}{for}{iterations do}{end}\n\t\\SetKwFor{ForEach}{for}{do}{end}\n\t\\SetKwComment{rc}{\\#\\ }{}\n\t\\SetKwFunction{Solve}{Solve}\n\t\\DontPrintSemicolon\n\n\t\\rc*[h]{Initialization:}\\\\\n\t\\Output{specify geometry (shape/form) and pole representative $p$}\n\t\\Input{Step-length $\\eta \\in (0,1]$, number of boosting iterations}\n\t\\Initialize{ $h_j(\\mathbf{x})$ with penalty matrix $\\mathbf{R}_j$ and\\\\ initial coefficient matrix $\\boldsymbol{\\Theta}_j=\\boldsymbol{0}$  }\n\t\\ForEach(\\hfill\\rc*[h]{Prepare penalized least-squares (PLS)}){$j = 1$ \\KwTo $J$}{\n\t\t\\rc*[h]{set up $m_{}\\,m_j \\times m_{}\\,m_j$ matrix:}\n\t\t$\\boldsymbol{\\Psi}_j \\leftarrow \\sum_{i=1}^n  \\left\\{ \\re{\\langle b^{(l)}_j(\\mathbf{x}_i) \\partial_r, b^{(l')}_j(\\mathbf{x}_i) \\partial_{r'}\\rangle_i} \\right\\}_{\\substack{(r,l)=(1,1), \\dots, (m_{},1), \\dots, (m_{},m_j)\\\\ (r',l')=(1,1), \\dots, (m_{},1), \\dots, (m_{},m_j) }}$\\; \n\t\n\t}\n\t\\Repeat(\\qquad\\rc*[h]{boosting steps}){Stopping criterion (e.g. minimal cross-validation error)}{\n\t\t\\ForEach(\\hfill\\rc*[h]{Compute current transported residuals}){$i = 1, \\dots, n$}{\t\n\t\t\t\t$[\\mu_i] \\leftarrow \\operatorname{Exp}_{[p]}(h(\\mathbf{x}_i))$\\;\n\t\t\t\t$\\varepsilon_{[\\mu_i]} \\leftarrow \\operatorname{Log}_{[\\mu_i]}([y_i])$\\;\n\t\t\t\t$\\epsilon_i \\leftarrow \\operatorname{Transp}_{[\\mu_i], [p]}(\\varepsilon_{[\\mu_i]})$\\;\n\t\t}\n\t\t\\ForEach(\\hfill\\rc*[h]{PLS fit to residuals}){$j = 1, \\dots, J$}{\n\t\t\t\\rc*[h]{$m_{}\\,m_j$ vector:}\n\t\t\t$\\boldsymbol{\\psi}_j \\leftarrow \\sum_{i=1}^n  \\left\\{ \\re{\\langle b^{(l)}_j(\\mathbf{x}_i) \\partial_r, \\epsilon_i\\rangle_i} \\right\\}_{(r,l)=(1,1), \\dots, (m_{},1), \\dots, (m_{},m_j)}$\\;\n\t\t\t$\\widehat{\\boldsymbol{\\Theta}}_j = \\{\\hat\\theta_j^{(r,l)}\\}_{r,l} \\leftarrow$ \\Solve{\\ $\\left(\\boldsymbol{\\Psi}_j + \\mathbf{R}_j\\right) \\operatorname{vec}(\\boldsymbol{\\Theta}) = \\boldsymbol{\\psi}_j$\\ }\\;\n\t\t}\n\t\t$\\hat\\jmath \\leftarrow \\argmin[j\\in\\{1,\\dots,J\\}] \\sum_{i=1}^n \\| \\epsilon_i -  \\sum_{r,l} \\hat{\\theta}^{(r,l)}_j b_j^{(l)}(\\mathbf{x}) \\partial_r \\|_i^2$; \\hfill\\rc*[h]{Select base-learner}\\\\\n\t\t$\\boldsymbol{\\Theta}_{\\hat\\jmath} \\leftarrow \\boldsymbol{\\Theta}_{\\hat\\jmath} + \\eta\\, \\widehat{\\boldsymbol{\\Theta}}_{\\hat\\jmath}$; \\hfill\\rc*[h]{Update selected model coefficients}\\\\\n\t}\n\t\\caption{Component-wise Riemannian $L^2$-Boosting}\\label{riemmanianL2boosting}\n\\end{algorithm\n\n\n\n\n\n \nThe pole $[p]$ is, in fact, usually not a priori available. Instead we typically assume $[p]=\\argmin[q\\in\\mathcal{Y}^*]\\mathds{E}\\left(d^2([Y], [q])\\right)$ is the overall Fr\u00e9chet mean, also often referred to as \\textit{Riemannian center of mass} for Riemannian manifolds or as \\textit{Procrustes mean} in shape analysis \\citep{DrydenMardia2016ShapeAnalysisWithApplications}. Here, we estimate it as $[p] = \\operatorname{Exp}_{[p_0]}(h_0)$ in a preceding Riemannian $L^2$-Boosting routine. The constant effect $h_0\\in T_{[p_0]}\\mathcal{Y}^*_{/G}$ in the intercept-only special case of our model is estimated with Algorithm \\ref{riemmanianL2boosting} based on a preliminary pole $[p_0]\\in \\mathcal{Y}^*_{/G}$.\nFor shapes and forms, a good candidate for $p_0$ can be obtained as the standard functional mean of a reasonably well aligned sample $y_1, \\dots, y_n \\in \\mathcal{Y}$ of representatives.\n\n\nThe proposed Riemannian $L_2$-Boosting algorithm is available in the \\texttt{R} \\citep{R} package \\texttt{manifoldboost}\\if00{ (\\url{github.com/Almond-S/manifoldboost})}\\fi. The implementation is based on the package \\texttt{FDboost} \\citep{FDboost}, which is in turn based on the model-based boosting package \\texttt{mboost} \\citep{HothornBuehlmann2010}.\n\n\n\n\n\\subsection{Realistic shape and form simulation studies}\n\\label{sec_simulation}\n\nTo evaluate the proposed approach, we conduct \\old{a series of }simulation studies for both form and shape regression for irregular curves.\nWe compare sample sizes $n\\in\\{54, 162\\}$ and average grid sizes $k=\\frac{1}{n}\\sum_{i=1}^n k_i \\in\\{40,100\\}$ as well as an extreme case with $k_i=3$ for each curve but $n=720$, i.e.\\ where only random triangles are observed (yet, with known parameterization over $[0,1]$). We additionally investigate the influence of nuisance effects and compare different inner product weights.\nWhile important results are summarized in the following, comprehensive visualizations can be found in Supplement\\old{ary}~\\ref{sec:bottles_appendix}.\n\n\\textbf{Simulation design:} We simulate models of the form $[\\mu] = \\operatorname{Exp}_{[p]}\\left(\\beta_\\kappa + f_1(z_1)\\right)$ with overall mean $[p]$, a binary \\old{covariate}\\new{effect} with levels $\\kappa\\in\\{0,1\\}$ and a smooth effect of \\old{a metric covariate }$z_1 \\in [-60,60]$. \nWe choose a cyclic cubic B-spline basis with $27$ knots for $T_{[p]}\\mathcal{Y}^*_{/G}$, \\new{placing them irregularly at 1/27-quantiles of unit-speed parameterization time-points of the curves. \nC}ubic B-splines with $4$ \\new{regularly placed} knots \\new{are used} for covariates in smooth effects. \nTrue models are \\old{obtained }based on the \\texttt{bot} dataset from R package \\texttt{Momocs} \\citep{Bonhommeetal2014RPackMomocs} comprising outlines of 20 beer ($\\kappa = 0$) and 20 whiskey ($\\kappa = 1$) bottles of different brands. A \\old{metric covariate}\\new{smooth} effect is induced by the 2D viewing transformations resulting from tilting the planar outlines in a 3D coordinate system along their longitudinal axis by an angle of up to 60 degree towards the viewer ($z_1 = 60$) and away ($z_1 = -60$) (i.e. in a way not captured by 2D rotation invariance). \\old{Besides e}\\new{E}stablishing \\old{the }ground truth models based on a fit to the bottle data, we simulate new responses $[y_1], \\dots, [y_n]$ via residual re-sampling (Supplement \\ref{sec:bottles_appendix})\nto preserve realistic autocorrelation. \nSubsequently, we randomly translate, rotate and scale $y_1, \\dots, y_n \\in \\mathcal{Y}$ somewhat around the aligned form/shape representatives to obtain realistic samples.\\\\\nThe implied residual variance $\\frac{1}{n}\\sum_{i=1}^n \\|\\epsilon_i\\|_i^2 = \\frac{1}{n}\\sum_{i=1}^n d_i^2([y_i],[\\mu_i])$ on simulated datasets ranges around 105\\% of the predictor variance $\\frac{1}{n}\\sum_{i=1}^n \\|h(\\mathbf{x}_i)\\|_i^2 = \\frac{1}{n}\\sum_{i=1}^n d_i^2([\\mu_i],[p])$ in the form scenario and around 65\\% in the shape scenario. All simulation\\new{s} \\old{settings }were repeated 100 times, \\old{fitting the generated data with}\\new{fitting} models \\old{including}\\new{with} the model \\old{components}\\new{terms} specified above and three additional nuisance effects: a linear effect $\\beta z_1$ (orthogonal to $f_1(z_1)$), an effect $f_2$ of the same structure as $f_1$ but depending on an independently uniformly drawn variable $z_2$, and a constant effect $h_0\\in T_{[p]}\\mathcal{Y}^*_{/G}$ to test centering around $[p]$. \\old{For the model fit, all b}\\new{B}ase-learners are regularized to 4 degrees of freedom \\new{(step-length $\\eta = 0.1$)}. \\old{We specify a step-length of $\\eta = 0.1$, and e}\\new{E}arly-stopping is based on 10-fold cross-validation.\\\\\n\\begin{figure}[!h]\n\t\\centering\n\t\\includegraphics[width = 0.73\\textwidth]{figure/factorized_effect_plots}\n\t\\includegraphics[width = 0.256\\textwidth]{figure/analysis_excerpt}\n\t\\caption{\\textit{Left:} First \\textit{(row 1)} and second \\textit{(row 2)} main components of the smooth effect $f_1(z_1)$ in the form scenario obtained from TP factorization. Normalized component directions are visualized as bottle outlines after transporting them to the true pole \\textit{(gray solid outline)}. Underlying truth \\textit{(orange solid lines / areas)} are plotted together with\n\t\tfive example estimates for $n=162$ and $k=100$ \\textit{(black solid lines)} and the extremely sparse $k_i=3$ setting \\textit{(gray dashed lines)}. \n\t\t\\textit{Center:} Conditional means for both bottle types with fixed metric covariate $z_1=0$ in the shape scenario with $n=54$ and $k=40$. Five example estimates \\textit{(black solid outlines)} are plotted in front of the underlying truth \\textit{(olive-green areas)}.\n\t\t\\textit{Right:} rMSE of shown example estimates \\textit{(jittered colored diamonds)} contextualized with boxplots of rMSE distributions observed in respective simulation scenarios. \n\t}\\label{fig:sim_effect_plots} \n\\end{figure}\n\\textbf{Form scenario:} In the form scenario, the smooth covariate effect $f_1$ offers a particularly clear interpretation. TP factorization decomposes the true effect into its two relevant components, where the first (major) component corresponds to the bare projection of the tilted outline in 3D into the 2D image plane and the second to additional perspective transformations (Fig.\\ \\ref{fig:sim_effect_plots}). For this effect, we observe a median relative mean squared error  $\\operatorname{rMSE}(\\hat{h}_j) = \\sum_{i=1}^n \\|\\hat{h}_j(\\mathbf{x}_i) - h_j(\\mathbf{x}_i)\\|_i^2 / \\sum_{i=1}^n \\|h(\\mathbf{x}_i)\\|_i^2$ of about 3.7\\% of the total predictor variance for small data settings with $n=54$ and ${k} = 100$ (5.9\\% with $ k = 40$), which reduces to 1.5\\% for $n=162$ (for both $k = 40$ and $ k = 100$). It is typical for functional data that, from a certain point, adding more (highly correlated) evaluations per curve leads to distinctly less improvement in the model fit than adding further observations \\citep[compare, e.g., also][]{Stoecker2019FResponseLSS}.\nIn the extreme $k_i=3$ scenario, we obtain an rMSE of around 15\\%, which is not surprisingly considerably higher than for the moderate settings above.\nEven in this extreme setting (Fig.\\ \\ref{fig:sim_effect_plots}), the effect directions are captured well, while the size of the effect is underestimated. Rotation alignment based on only three points (which are randomly distributed along the curves) might considerably differ from the full curve alignment, and averaging over these sub-optimal alignments masks the full extend of the effect. Still, results are very good given the sparsity of information in this case.\nHaving a simpler form, the binary effect $\\beta_\\kappa$ is also estimated more accurately with an rMSE of around 1.5\\% for $n=54$, $ k = 100$ (1.9\\% for $ k = 40$) and less than 0.8\\% for $n=162$ (for both $ k = 40$ and $ k = 100$). The pole estimation accuracy varies on a similar scale.\n\n\\textbf{Shape scenario:}\nQualitatively, the shape scenario shows a similar picture. For $ k = 40$, we observe median rMSEs of 2.8\\% ($n = 54$) and 2.2\\% ($n = 162$) for $f_1(z_1)$, and 1.5\\% and 0.6\\% for the binary effect $\\beta_\\kappa$. For $k = 100$, accuracy is again slightly higher.\n\n\\textbf{Nuisance effects and integration weights:}\nNuisance effects in the model where generally rarely selected and, if selected at all, only lead to a marginal loss in accuracy. The constant effect is only selected sometimes in the extreme triangle scenarios, when pole estimation is difficult.\nWe refer to \\citet{brockhaus2017boosting}, who perform gradient boosting with functional responses and a large number of covariate effects with stability selection, for simulations with larger numbers of nuisance effects and further discussion in a related context, as variable selection is not our main focus here. \nFinally, simulations indicate that inner product weights implementing a trapezoidal rule for numerical integration  are slightly preferable for typical grid sizes ($ k = 40, 100$), whereas weights of $1/k_i$ equal over all grid points within a curve gave slightly better results in the extreme $k_i=3$ settings.\n\nAll in all, the simulations show that Riemannian $L_2$-Boosting can adequately fit both shape and form models in a realistic scenario and captures effects reasonably well even for a comparably small number of sampled outlines or evaluations per outline.\n\n\n\n\n\\subsection{Cellular Potts model parameter effects on cell form}\n\\label{sec_cells}\n\nThe stochastic biophysical model proposed by \\cite{Thueroff2019SingleCellCollectiveDynamics}, a cellular Potts model (CPM), simulates migration dynamics of cells (e.g. wound healing or metastasis) in two dimensions.\nThe progression of simulated cells is the result of many consecutive local elementary events sampled with a Metropolis-algorithm according to a Hamiltonian.\nDifferent parameters controlling the Hamiltonian have to be calibrated to match real live cell properties \\citep{schaffer2021phd}. Considering whole cells, parameter implications on the cell form are not obvious.\nTo provide additional insights, we model the cell form in dependence on four CPM parameters considered particularly relevant: the bulk stiffness $x_{i1}$, membrane stiffness $x_{i2}$, substrate adhesion $x_{i3}$, and signaling radius $x_{i4}$ are subsumed in a vector $\\mathbf{x}_i$ of metric covariates for $i=1,\\dots,n$.\nCorresponding sampled cell outlines $y_i$ were provided by Sophia Schaffer in the context of \\cite{schaffer2021phd}, who ran underlying CPM simulations and extracted outlines.\nDeriving the intrinsic orientation of the cells from their movement trajectories, we parameterize $y_i: [0,1] \\rightarrow \\mathds{C}$, clockwisely relative to arc-length such that $y_i(0) = y_i(1)$ points into the movement direction of the barycenter of the cell. With an average of $k = \\frac{1}{n}\\sum_{i=1}^{n}k_i \\approx 43$ samples per curve \\citep[after sub-sampling preserving $95\\%$ of their inherent variation, as described in][Supplement]{Volkmann2020MultiFAMM}, the evaluation vectors $\\mathbf{y}_i \\in \\mathds{C}^{k_i}$ are equipped with an inner-product implementing trapezoidal rule integration weights. Example cell outlines are depicted in Supplement Figure \\ref{fig:celldataexamples}. The results shown below are based on cell samples obtained from 30 different CPM parameter configurations. For each configuration, 33 out of 10.000 Monte-Carlo samples were extracted as approximately independent. This yields a dataset of $n = 990 = 30 \\times 33$ cell outlines. \n\nAs positioning of the irregularly sampled cell outlines $y_i$, $i=1,\\dots,n$, in the coordinate system is arbitrary, we model the cell forms $[y_i]\\in\\mathcal{Y}^*_{/\\operatorname{Trl} + \\operatorname{Rot}}$. Their estimated overall form mean $[p]$ serves as pole in the additive model\n\\newcommand{{\\ddot{\\jmath}}}{{\\ddot{\\jmath}}}\n\\begin{align*}\n\t[\\mu_i] = \\operatorname{Exp}_{[p]}\\big( h(\\mathbf{x}_i) \\big) = \\operatorname{Exp}_{[p]}\\big( \\sum_{j} \\beta_j x_{ij} + \\sum_{j} f_j(x_{ij}) + \\sum_{j\\neq {\\ddot{\\jmath}}} f_{j{\\ddot{\\jmath}}}(x_{ij}, x_{i{\\ddot{\\jmath}}}) \\big)\n\\end{align*}\nwhere the conditional form mean $[\\mu_i]$ is modeled in dependence on tangent-space linear effects with coefficients $\\beta_j \\in T_{[p]}\\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}$ and non-linear smooth effects $f_j$ for covariate $j=1,\\dots, 4$, as well as smooth interaction effects $f_{j{\\ddot{\\jmath}}}$ for each pair of covariates $j \\neq {\\ddot{\\jmath}}$. All involved (effect) functions are modeled via a cyclic cubic P-spline basis $\\{b_{0}^{(r)}\\}_r$ with $7$ (inner) knots and a ridge penalty, and quadratic P-splines with $4$ knots for the covariates $x_{ij}$ equipped with a second order difference penalty for the $f_j$ and ridge penalties for interactions. \nCovariate effects are mean centered and interaction effects $f_{j{\\ddot{\\jmath}}}(x_j, x_{\\ddot{\\jmath}})$ are centered around their marginal effects $f_{j}(x_j), f_{\\ddot{\\jmath}}(x_{\\ddot{\\jmath}})$, which are in turn centered around the linear effects $\\beta_j x_j$ and $\\beta_{\\ddot{\\jmath}} x_{\\ddot{\\jmath}}$, respectively. \nResulting predictor terms involve 69 (linear effect) to 1173 (interaction) basis coefficients but are penalized to a common degree of freedom of 2 to ensure a fair base-learner selection.\nWe fit the model with a step-size of $\\eta = 0.25$\nand stop after 2000 boosting iterations observing no further meaningful risk reduction, since no need for early-stopping is indicated by 10-fold form-wise cross-validation.\nDue to the increased number of data points and coefficients, the irregular design, and the increased number of iterations, the model fit takes considerably longer than in Section \\ref{sec_sheep}, with about 50 initial minutes followed by 8 hours of cross-validation. However, as usual in boosting, model updates are large in the beginning and only marginal in later iterations, such that fits after $1000$ or $500$ iterations would already yield very similar results.   \n\nObserving that the most relevant components point into similar directions, we jointly factorize the \\old{additive model }predictor as $\\new{\\hat{h}}(\\mathbf{x}_i) = \\sum_r \\xi^{(r)} \\new{\\hat{h}}^{(r)}(\\mathbf{x}_i)$ with TP factorization. The first component explains about 93\\% of the total predictor variance (Supplement Fig. \\ref{fig:cellmodelfactorizedvarimp}), indicating that, post-hoc, a good share of the model can be reduced to the geodesic model $[\\new{\\hat{\\mu}}_i] = \\operatorname{Exp}_{[p]}(\\xi^{(1)} \\new{\\hat{h}}^{(1)}(\\mathbf{x}_i))$ illustrated in Figure \\ref{fig:cells_factorized_effect_plots}. A positive effect in the direction $\\xi^{(1)}$ makes\ncells larger and more keratocyte / croissant shaped, a negative effect -- pointing into the opposite direction -- makes them smaller and more mesenchymal shaped / elongated\n\\begin{figure}[t]\n\t\\includegraphics[width=5.5in]{figure/cell_plots}\n\t\\caption{\\textit{Center:} the main direction $\\xi^{(1)}$ of the model illustrated as vectors pointing from the overall mean cell form $[p]$ (\\textit{grey curve}) to the form $\\operatorname{Exp}_{[p]}(\\xi^{(1)})$ (\\textit{blue dots}), which are both oriented as cells migrating rightwards.\n\t\\textit{Left:} Effects of the bulk stiffness $x_{i1}$ into the direction $\\xi^{(1)}$.\n\tA vertical line from 0, corresponding to $[p]$, to 1, corresponding to the full extent of $\\xi^{(1)}$, underlines the connection between the plots and helps to visually asses the amount of change for a given value of $x_{i1}$.\n\t\\textit{Right:}\n\tThe overall effect of $x_{i1}$ and membrane stiffness $x_{i2}$, comprising linear, smooth and interaction effects, as a 3D surface plot. The heat map plotted on the surface shows only the interaction effect $f^{(1)}_{12}(x_{i1}, x_{i2})$ illustrating deviations from the marginal effects, which are of particular interest for CPM calibration.\n\t}\\label{fig:cells_factorized_effect_plots}\n\\end{figure}\nThe bulk stiffness $x_{i1}$ turns out to present the most important driving factor behind the cell form, explaining over 75\\% of the cumulative variance of the effects (Supplement Fig. \\ref{fig:cellcovfactorizedvarimp}). Around 80\\% of its effect are explained by the linear term reflecting gradual shrinkage at the side of the cells with increasing bulk stiffness. \n\n\\section{Geometry of functional forms and shapes}\n\\label{chap_diffgeo}\n\n\\newcommand{\\re}[1]{\\operatorname{Re}\\!\\left(#1\\right)}\n\\newcommand{\\im}[1]{\\operatorname{Im}\\!\\left(#1\\right)}\n\\newcommand{\\dir}{\\times}\n\nRiemannian manifolds of planar shapes (and forms)\nare discussed in various textbooks at different levels of generality, in finite \\citep{DrydenMardia2016ShapeAnalysisWithApplications, Kendall1999} or potentially infinite dimensions  \\citep{SrivastavaKlassen2016,Klingenberg1995RiemannianGeometry}. \nStarting from the Hilbert space $\\mathcal{Y}$ of curve representatives $y$ of a single shape or form observation, we successively characterize its quotient space geometry under translation, rotation and re-scaling including the respective tangent spaces. Building on that, we introduce Riemannian exponential and logarithmic maps and parallel transports needed for model formulation and fitting, and the sample space of (irregularly observed) functional shapes/forms.\n\n\n\\newcommand{\\operatorname{Trl}}{\\operatorname{Trl}}\n\\newcommand{\\operatorname{Scl}}{\\operatorname{Scl}}\n\\newcommand{\\operatorname{Rot}}{\\operatorname{Rot}}\n\\newcommand{\\!\\mycal{1}}{\\!\\mycal{1}}\n\\newcommand{\\boldsymbol{0}}{\\mycal{0}}\n\nTo make use of complex arithmetic, we identify  the two-dimensional plane with the complex numbers, $\\mathds{R}^2 \\cong \\mathds{C}$, and consider a planar curve to be a function $y: \\mathds{R} \\supset \\mathcal{T} \\rightarrow \\mathds{C}$, element of a separable complex Hilbert space $\\mathcal{Y}$ with a complex inner product $\\langle \\cdot,\\cdot\\rangle$ and corresponding norm $\\|\\cdot\\|$. \nThis allows simple scalar expressions for\nthe group actions of translation $\\operatorname{Trl} = \\{y \\overset{\\operatorname{Trl}_\\gamma}{\\longmapsto} y + \\gamma\\, \\!\\mycal{1} : \\gamma\\in\\mathds{C}\\}$ with $\\!\\mycal{1}\\in\\mathcal{Y}$ canonically given by $\\!\\mycal{1}: t\\mapsto \\frac{1}{\\|t\\mapsto 1\\|}$ the real constant function of unit norm; re-scaling $\\operatorname{Scl} = \\{y \\overset{\\operatorname{Scl}_\\lambda}{\\longmapsto} \\lambda \\cdot (y - \\boldsymbol{0}_y) + \\boldsymbol{0}_y : \\lambda \\in \\mathds{R}^+\\}$ around the centroid $\\boldsymbol{0}_y = \\langle \\!\\mycal{1}\\,, y \\rangle \\!\\mycal{1}$ (which we consider more natural than using $\\boldsymbol{0}$, the zero element of $\\mathcal{Y}$, mostly chosen in the literature); and rotation $\\operatorname{Rot} = \\{y \\overset{\\operatorname{Rot}_u}{\\longmapsto} u\\cdot (y-\\boldsymbol{0}_y) + \\boldsymbol{0}_y : u \\in \\S^1\\}$ around $\\boldsymbol{0}_y$ with $\\S^1 = \\{u\\in\\mathds{C}:|u|=1\\} = \\{\\exp(\\omega\\i):\\omega\\in\\mathds{R}\\}$\nreflecting counterclockwise rotations by $\\omega$ radian measure. Concatenation yields combined group actions $G$ as direct products, such as the rigid motions $G = \\operatorname{Trl} \\dir \\operatorname{Rot} = \\{\\operatorname{Trl}_\\gamma \\circ \\operatorname{Rot}_u  : \\gamma \\in \\mathds{C}, u \\in \\S^1 \\} \\cong \\mathds{C} \\times \\S^1$ (see  Supplement \\ref{sec:groupactions} for more details).\nThe two real-valued component functions of $y$ are identified with the real part $\\re{y}: \\mathcal{T} \\rightarrow \\mathds{R}$ and imaginary part $\\im{y}:\\mathcal{T} \\rightarrow \\mathds{R}$ of $y=\\re{y} + \\im{y} \\i$. \nWhile the complex setup is used for convenience, the real part of $\\langle \\cdot, \\cdot \\rangle$ constitutes an inner product $\\re{\\langle y_1, y_2 \\rangle} = \\langle \\re{y_1}, \\re{y_2} \\rangle + \\langle \\im{y_1}, \\im{y_2} \\rangle$ for $y_1, y_2 \\in \\mathcal{Y}$ on the underlying real vector space of planar curves.\nTypically $\\re{y},\\ \\im{y}$ are assumed square-intregrable with respect to a measure $\\nu$ and we consider the canonical inner product $\\langle y_1, y_2 \\rangle = \\int \\conj{y}_1 y_2 d\\nu$ where $\\conj{y}$ denotes the conjugate transpose of $y$, i.e. $\\conj{y}(t) = \\re{y}(t) - \\im{y}(t)\\i$ is simply the complex conjugate, but for vectors $\\mathbf{y}\\in\\mathds{C}^k$, the vector $\\conj{\\mathbf{y}}$ is also transposed. \nFor curves, we typically assume $\\nu$ to be the Lebesgue measure on $\\mathcal{T} = [0,1]$;\nfor landmarks, a standard choice\nis the counting measure on $\\mathcal{T} = \\{1,\\dots,k\\}$.\n \nThe ultimate response object is given by the \\emph{orbit} $[y]_{G}=\\{g(y) : g\\in G\\}$ (or short $[y]$) of $y\\in\\mathcal{Y}$, the equivalence class under the respective combined group actions $G$:\n\\new{with $G=\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}$,}\n$\\new{[y] =} [y]_{\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}} = \\{\\lambda u\\, y+ \\gamma\\, \\!\\mycal{1} : \\lambda \\in \\mathds{R}^+, u \\in \\S^1, \\gamma \\in \\mathds{C}\\}$ is referred to as the \\emph{shape} of $y$ and\\new{, for $G=\\operatorname{Trl} \\dir \\operatorname{Rot}$,}  $\\new{[y] =} [y]_{\\operatorname{Trl} \\dir \\operatorname{Rot}}=\\{u y + \\gamma\\,\\!\\mycal{1} :  u \\in \\S^1, \\gamma\\in\\mathds{C}\\}$ as its \\emph{form} or \\emph{size-and-shape}.\n$\\mathcal{Y}_{/G} = \\{[y]_G : y\\in\\mathcal{Y}\\}$ denotes the quotient space of $\\mathcal{Y}$ with respect to $G$. The description of the Riemannian geometry of $\\mathcal{Y}_{/G}$ involves, in particular, a description of the tangent spaces $T_{[y]}\\mathcal{Y}_{/G}$ at points $[y]\\in \\mathcal{Y}_{/G}$, which can be considered local vector space approximations to $\\mathcal{Y}_{/G}$ in a neighborhood of $[y]$. For a point $q$ in a manifold $\\mathcal{M}$ the tangent vectors $\\beta \\in T_{q}\\mathcal{M}$ can, i.a., be thought of as gradients $\\dot{c}(0)$ of paths $c:\\mathds{R} \\supset (-\\delta, \\delta) \\rightarrow \\mathcal{M}$ at $0$ where they pass through $c(0) = q$. Besides their geometric meaning, they will also play an important role in the regression model, as additive model effects are formulated on tangent space level.\nChoosing suitable representatives $\\widetilde{y}^G \\in [y]_G \\subset \\mathcal{Y}$ (or short $\\widetilde{y}$) of orbits $[y]_G$, we use an identification of tangent spaces with  suitable linear subspaces $T_{[y]_G}\\mathcal{Y}_{/G} \\subset \\mathcal{Y}$.\n\n\\textbf{Form geometry:}\nStarting with translation as the simplest invariance, an orbit $[y]_{\\operatorname{Trl}}$ can be one-to-one identified with its centered representative $\\widetilde{y}^{\\operatorname{Trl}} =y - \\langle y, \\!\\mycal{1}\\rangle\\, \\!\\mycal{1}$\nyielding an identification $\\mathcal{Y}_{/\\operatorname{Trl}} \\cong \\{y\\in\\mathcal{Y} : \\langle y, \\!\\mycal{1}\\, \\rangle = 0\\}$ with a linear subspace of $\\mathcal{Y}$. Hence, also $T_{[y]}\\mathcal{Y}_{/\\operatorname{Trl}} = \\{y\\in\\mathcal{Y} : \\langle y, \\!\\mycal{1}\\, \\rangle = 0\\}$.\nFor rotation, by contrast,\nwe can only find local identifications with Hilbert subspaces (i.e.\\ charts) around reference points $[p]_{\\operatorname{Trl} \\dir \\operatorname{Rot}}$ we refer to as ``poles''. Moreover, we restrict to $y, p \\in \\mathcal{Y}^* = \\mathcal{Y} \\setminus [\\boldsymbol{0}\\,]_{\\operatorname{Trl}}$ eliminating constant functions as degenerate special cases in the translation orbit of zero.\nFor each $[y]_{\\operatorname{Trl}\\dir\\operatorname{Rot}}$ in an open neighborhood around $[p]_{\\operatorname{Trl}\\dir\\operatorname{Rot}}$ which can be chosen with $\\langle \\widetilde{y}^{\\operatorname{Trl}}, \\widetilde{p}^{\\operatorname{Trl}} \\rangle \\neq 0$, $y$ can be uniquely rotation aligned to $p$, yielding a one-to-one identification of the form $[y]_{\\operatorname{Trl} \\dir \\operatorname{Rot}}$ with the aligned representative given by $\\widetilde{y}^{\\operatorname{Trl}\\dir\\operatorname{Rot}} = \\frac{\\langle \\widetilde{y}^{\\operatorname{Trl}}, \\widetilde{p}^{\\operatorname{Trl}} \\rangle}{|\\langle \\widetilde{y}^{\\operatorname{Trl}}, \\widetilde{p}^{\\operatorname{Trl}} \\rangle|} \\widetilde{y}^{\\operatorname{Trl}} = \\argmin[{y' \\in [y]_{\\operatorname{Trl}\\dir\\operatorname{Rot}}}] \\|y' - p\\|$ (compare Fig. \\ref{fig:quotientgeometry}). While $\\widetilde{y}^{\\operatorname{Trl} \\dir \\operatorname{Rot}}$ depends on $p$, we omit this in the notation for simplicity. \nAll $\\widetilde{y}^{\\operatorname{Trl}}$ rotation aligned to $\\widetilde{p}^{\\operatorname{Trl}}$ lie on the hyper-plane determined by $\\im{\\langle \\widetilde{y}^{\\operatorname{Trl}}, \\widetilde{p}^{\\operatorname{Trl}}\\rangle} = 0$ (Figure \\ref{fig:quotientgeometry}), which yields\n$T_{[p]}\\mathcal{Y}_{/\\operatorname{Trl} + \\operatorname{Rot}}^* = \\{y\\in\\mathcal{Y} : \\langle y, \\!\\mycal{1} \\rangle = 0,\\ \\im{\\langle y, p \\rangle} = 0 \\}$ \\new{with normal vectors $\\zeta^{(1)} = \\!\\mycal{1}, \\zeta^{(2)} = \\i\\, \\!\\mycal{1}, \\zeta^{(3)} = \\i\\, p$}. \nNote that, despite the use of complex arithmetic, $T_{[p]}\\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}^*$ is a real vector space not closed under complex scalar multiplication.\nThe geodesic distance of $[y]_{\\operatorname{Trl} \\dir \\operatorname{Rot}}$ to the pole $[p]_{\\operatorname{Trl} \\dir \\operatorname{Rot}}$ is given by $d([y]_{\\operatorname{Trl}\\dir\\operatorname{Rot}}, [p]_{\\operatorname{Trl}\\dir\\operatorname{Rot}}) = \\|\\widetilde{y}^{\\operatorname{Trl} \\dir \\operatorname{Rot}} - \\widetilde{p}^{\\operatorname{Trl}}\\| = \\underset{y'\\in[y]_{\\operatorname{Trl}\\dir\\operatorname{Rot}}, p'\\in[p]_{\\operatorname{Trl}\\dir\\operatorname{Rot}}}{\\operatorname{argmin}}\\|y'-p'\\|$. It reflects \\old{both }the length of the shortest path (i.e. the geodesic) between the forms and the minimum distance between the orbits as sets.\n\n\\textbf{Shape geometry:}\nTo account for scale invariance in shapes $[y]_{\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}$,\nthey are identified with normalized representatives $\\widetilde{y}^{\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}} = \\frac{\\widetilde{y}^{\\operatorname{Trl}\\dir\\operatorname{Rot}}}{\\|\\widetilde{y}^{\\operatorname{Trl}\\dir\\operatorname{Rot}}\\|}$. \nMotivated by the normalization, we borrow the well-known geometry of the sphere $\\S = \\{y \\in \\mathcal{Y} : \\|y\\| = 1\\}$, where $T_{p}\\S = \\{y\\in \\mathcal{Y} : \\re{\\langle y, p \\rangle} = 0\\}$ is the tangent space at a point $p\\in\\S$ and geodesics are great circles.\nTogether with translation and rotation invariance, the shape tangent space is then given by $T_{[p]}\\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}^* = T_{[p]}\\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}^* \\cap T_{p}\\S = \\{y\\in\\mathcal{Y} : \\langle y, \\!\\mycal{1} \\rangle = 0,\\ \\langle y, p \\rangle = 0 \\}$ \\new{with normal vector $\\zeta^{(4)} = p$ in addition to $\\zeta^{(1)}, \\zeta^{(2)}, \\zeta^{(3)}$ above}. The geodesic distance $d([p]_{\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}, [y]_{\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}) = \\arccos |\\langle \\widetilde{y}^{\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}, \\widetilde{p}^{\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}} \\rangle|$ corresponds to the arc-length between the representatives. This distance is often referred to as \\textit{Procrustres distance} in statistical shape analysis.\\\\\n\nWe may now define the maps needed for the regression model formulation. Let $\\widetilde{y}$\\old{,} \\new{and} $\\widetilde{p}$ \\old{and $\\widetilde{p}'$} be shape/form representatives of $[y]$\\old{,} \\new{and}  $[p]$ \\old{and $[p']$} rotation aligned to the shape/form pole representative $p$. \nGeneralizing straight lines to a Riemannian manifold $\\mathcal{M}$, geodesics $c: (-\\delta, \\delta) \\rightarrow \\mathcal{M}$ can be characterized by their ``intercept'' $c(0)\\in\\mathcal{M}$ and ``slope'' $\\dot{c}(0)\\in T_{c(0)}\\mathcal{M}$.\nThe\n\\emph{exponential map} $\\operatorname{Exp}_{q}: T_q\\mathcal{M} \\rightarrow \\mathcal{M}$ at a point $q\\in\\mathcal{M}$ is defined to map $\\beta \\mapsto c(1)$ for $c$ the geodesic with $q=c(0)$ and $\\beta=\\dot{c}(0)$. It maps $\\beta\\in T_q\\mathcal{M} $ to a point $\\operatorname{Exp}_{q}(\\beta) \\in \\mathcal{M}$ located $d(q,\\operatorname{Exp}_{q}(\\beta)) = \\|\\beta\\|$ apart of the pole $q$ in the direction of $\\beta$.\nOn the form space $\\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}$, the exponential map is simply given by $\\operatorname{Exp}_{[p]\\new{_{\\operatorname{Trl}\\dir\\operatorname{Rot}}}}(\\beta) = \\left[\\widetilde{p}\\new{^{\\operatorname{Trl}\\dir\\operatorname{Rot}}} + \\beta\\right]\\new{_{\\operatorname{Trl}\\dir\\operatorname{Rot}}}$. \nOn the shape space $\\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot} \\new{\\dir}\\old{+} \\operatorname{Scl}}$, identification with exponential maps on the sphere yields $\\operatorname{Exp}_{[p]\\new{_G}}(\\beta) = \\left[\\cos(\\|\\beta\\|) \\widetilde{p}\\new{^G} + \\sin(\\|\\beta\\|) \\frac{\\beta}{\\|\\beta\\|}\\right]\\new{_G}$ \\new{ with $G=\\operatorname{Trl}\\dir\\operatorname{Rot}\\dir\\operatorname{Scl}$}.\nIn an open neighborhood $\\mathcal{U}$, $q\\in\\mathcal{U}\\subset\\mathcal{M}$, $\\operatorname{Exp}_{q}$ is invertible yielding the $\\operatorname{Log}_{q}: \\mathcal{U} \\rightarrow T_q\\mathcal{M}$ map from the manifold to the tangent space at $q$. For forms, it is given by $\\operatorname{Log}_{[p]_\\new{\\operatorname{Trl} \\dir \\operatorname{Rot}}}([y]_\\new{\\operatorname{Trl} \\dir \\operatorname{Rot}}) = \\widetilde{y}^\\new{\\operatorname{Trl} \\dir \\operatorname{Rot}} - \\widetilde{p}^\\new{\\operatorname{Trl} \\dir \\operatorname{Rot}}$ and, for shapes, by $\\operatorname{Log}_{[p]_\\new{G}}([y]_\\new{G}) = d([p]_\\new{G}, [y]_\\new{G}) \\frac{\\widetilde{y}^\\new{G} - \\langle \\widetilde{p}^\\new{G}, \\widetilde{y}^\\new{G} \\rangle \\widetilde{p}^\\new{G}}{\\|\\widetilde{y}^\\new{G} - \\langle \\widetilde{p}^\\new{G}, \\widetilde{y}^\\new{G} \\rangle \\widetilde{p}^\\new{G}\\|}$ with \\new{$G=\\operatorname{Trl} \\dir \\operatorname{Rot}\\dir\\operatorname{Scl}$}\\old{$\\widetilde{y}, \\widetilde{p}$ the form or shape representatives, respectively}.\nFinally, $\\operatorname{Transp}_{q, q'}: T_{q}\\mathcal{M} \\rightarrow T_{q'}\\mathcal{M}$ parallel transports tangent vectors $\\varepsilon \\mapsto \\varepsilon'$ isometrically along a geodesic $c(\\tau)$ connecting $q$ and $q'\\in\\mathcal{M}$ such that the slopes $\\new{\\operatorname{Transp}_{q, q'}(}\\dot{c}(q)\\new{)} \\new{=}\\old{\\cong} \\dot{c}(q')$ are identified and all angles are preserved.\nFor shapes, $\\operatorname{Transp}_{[\\old{p}\\new{y}]_\\new{G}, [p\\old{'}]_\\new{G}}(\\varepsilon) = \\varepsilon - \\langle \\varepsilon, {\\widetilde{p}\\old{'}}^\\new{G} \\rangle \\frac{\\old{\\widetilde{p}}\\new{\\widetilde{y}}^\\new{G} + {\\widetilde{p}\\old{'}}^\\new{G}}{1+\\langle \\old{\\widetilde{p}}\\new{\\widetilde{y}}^\\new{G}, {\\widetilde{p}\\old{'}}^\\new{G} \\rangle}$\\new{, with $G = \\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}$,} takes the form of the parallel transport on a sphere replacing the real inner product with its complex analogue.\nFor forms, it changes only the $\\im{\\langle\\varepsilon, \\widetilde{p}\\old{'}\\rangle}$ coordinate orthogonal to the real $\\old{\\widetilde{p}}\\new{\\widetilde{y}}$-$\\widetilde{p}\\old{'}$-plane as in the shape case, while the remainder of $\\varepsilon$ is left unchanged as in a linear space. This yields $\\operatorname{Transp}_{[\\old{p}\\new{y}]\\new{_G}, [p\\old{'}]\\new{_G}}\\left(\\varepsilon\\right) =  \\varepsilon -  \\im{\\langle \\widetilde{p}\\old{'}\\new{^G}/\\|\\widetilde{p}\\new{^G}\\old{'}\\|, \\varepsilon \\rangle} \\frac{\\old{\\widetilde{p}}\\new{\\widetilde{y}^G}/\\|\\old{\\widetilde{p}}\\new{\\widetilde{y}^G}\\| + \\widetilde{p}\\new{^G}\\old{'}/\\|\\widetilde{p}\\new{^G}\\old{'}\\|}{1+\\langle \\old{\\widetilde{p}}\\new{\\widetilde{y}^G}/\\|\\old{\\widetilde{p}}\\new{\\widetilde{y}^G}\\|, \\widetilde{p}\\new{^G}\\old{'}/\\|\\widetilde{p}\\new{^G}\\old{'}\\| \\rangle}\\i$\\new{, with $G=\\operatorname{Trl}\\dir\\operatorname{Rot}$,} for form tangent vectors. While \\old{this or} equivalent expressions for the parallel transport in the shape case can be found, e.g., in \\cite{DrydenMardia2016ShapeAnalysisWithApplications, Huckemann2010intrinsicMANOVA}, a corresponding derivation for the form case is given in \\old{the} Supplement \\ref{sec:transport}\\old{. This also involves a more detailed} \\new{including a} discussion of the quotient space geometry in differential geometric terms.\n\n\nBased on this understanding of the response space, we may now proceed to consider a sample of curves $y_1, \\dots, y_n \\in \\mathcal{Y}$ representing orbits $[y_1], \\dots, [y_n]$ with respect to group actions $G$. In the functional case, with the domain $\\mathcal{T} = [0,1]$, these curves are usually observed as evaluations $\\mathbf{y}_i = (y_i(t_{i1}), \\dots, y_i(t_{ik_i}))^\\top$ on a finite grid $t_{i1} < \\dots < t_{ik_i} \\in \\mathcal{T}$ which may differ between observations. In contrast to the \\textit{regular} case with common grids, this more general data structure is referred to as \\textit{irregular} functional shape/form data. To handle this setting, we replace the original inner product $\\langle \\cdot, \\cdot\\rangle$ on $\\mathcal{Y}$ by individual\n\n$\\langle y_i , y_i' \\rangle_i = \\conj{\\mathbf{y}}_i \\mathbf{W}_i \\mathbf{y}'_i$ providing inner products on the $k_i$-dimensional space $\\mathcal{Y}_i=\\mathds{C}^{k_i}$ of evaluations $\\mathbf{y}_i, \\mathbf{y}_i'$ on the same grid. The symmetric positive-definite weight matrix $\\mathbf{W}_i$ can be chosen to implement an approximation to integration w.r.t.\\ the original measure $\\nu$ with a numerical integration measure $\\nu_i$ such as given by the trapezoidal rule. Alternatively, $\\mathbf{W}_i = \\frac{1}{k_i} \\mathbf{I}_{k_i}$ with $k_i \\times k_i$ identity matrix $\\mathbf{I}_{k_i}$ presents a canonical choice that is analog to the landmark case for $k_i \\equiv k$. \n\\new{Moreover, data-driven $\\mathbf{W}_i$ could also be motivated from the covariance structure estimated for (potentially sparse) $y_1, \\dots, y_n$ along the lines of \\citet{Yao:etal:2005, stoecker2022efp}. \n\tWhile this is beyond the scope of this paper, potential procedures are sketched in Supplement \\ref{sec::FPC}.}\nWith the inner products given for $i=1,\\dots, n$, the sample space naturally arises as the Riemannian product $\\mathcal{Y}^*_{1/G} \\times \\dots \\times \\mathcal{Y}^*_{n/G}$ of the orbit spaces, with the individual geometries constructed as described above.\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.45\\linewidth]{figure/quotient_geometry}\n\t\\includegraphics[width=0.45\\linewidth]{figure/manifold_maps}\n\t\\caption[quotientgeometry]{\n\t\t\\textit{Left:} Quotient space geometry: assuming $p$ and $y$ centered, translation invariance is not further considered in the plot; given pole representative $p$, we express $y = \\frac{\\re{\\langle p, y\\rangle}}{\\|p\\|^2} p + \\frac{\\im{\\langle p, y\\rangle}}{\\|p\\|^2} ip + (y-\\frac{\\langle p, y\\rangle}{\\|p\\|^2} p) \\in\\mathcal{Y}$ in its coordinates in $p$ and $ip$ direction, subsuming all orthogonal directions in the third dimension. In this coordinate system, the rotation orbit $[y]_{\\operatorname{Rot}}$ corresponds to the dotted horizontal circle, and is identified with the aligned $\\widetilde{y}:=\\widetilde{y}^{\\operatorname{Rot}}$ in the half-plane of $p$; $[y]_{\\operatorname{Rot} \\dir\\operatorname{Scl}}$ is identified with the unit vector $\\widetilde{y}^{\\operatorname{Rot}\\dir\\operatorname{Scl}} = \\frac{\\widetilde{y}}{\\|\\widetilde{y}\\|}$ projecting $\\widetilde{y}$ onto the hemisphere depicted by the vertical semicircle. Form and shape distances between $[p]$ and $[y]$ correspond to the length of the geodesics $c(\\tau)$ (\\textit{thick lines}) on the plane and sphere, respectively. \n\t\t\\textit{Right:} Geodesic line $c(\\tau)$ between $p=c(0)$ and $p'=c(1)$, Log-map projecting $y$ to $\\varepsilon \\in T_p\\mathcal{M}$, parallel transport $\\text{Transp}_{pp'}$ forwarding $\\varepsilon$ to $\\varepsilon' \\in T_{p'}\\mathcal{M}$, and Exp-map projecting $\\varepsilon'$ onto $\\mathcal{M}$ visualized for a sphere. Tangent spaces, identified with subspaces of the ambient space, are depicted as \\textit{gray planes} above the respective poles. The parallel transport preserves all angles between tangent vectors and identifies $\\dot{c}(0) \\cong \\dot{c}(1)$.\n\t}\n\t\\label{fig:quotientgeometry}\n\\end{figure}\n\n\n\n\\section{Discussion and Outlook}\n\\label{chap_discussion}\n\nCompared to existing (landmark) shape regression models, the presented approach extends  linear predictors to more general additive predictors including also, e.g., smooth nonlinear model terms and interactions, and yields the first regression approach for functional shape as well as  form responses.\nMoreover, we propose novel visualizations based on TP factorization that, similar to \\new{FPC}\\old{functional principal component} analysis, enable a systematic decomposition of the variability explained by an additive effect on tangent space level. \nYielding meaningful coordinates for model effects,\nits potential for visualization will be useful also for \\new{FAMs}\\old{functional additive models} in linear spaces\\old{.} \\new{and also beyond our model framework, such as we exemplarily illustrate for the non-parametric approach of \\citet{jeon2020additiveHilbertian} in Supplement \\ref{sec:kernelTPfactorization}}.\n\nInstead of operating on the original evaluations $\\mathbf{y}_i\\in\\mathds{C}^{k_i}$ of response curves $y_i$ as in all applications above, another frequently used approach expands $y_i$, $i=1,\\dots,n$, in a common basis first, before carrying out statistical analysis on coefficient vectors (compare \\old{e.g. }\\citet{RamsaySilverman2005, Morris2015} \\new{and \\citet{MullerYao2008FAM} for smoothing spline, wavelet or FPC representations} in FDA or \\citet{Bonhommeetal2014RPackMomocs} in\\old{ the} shape analysis\\old{ literature}). \\old{Additive s}\\new{S}hape/form regression on the coefficients is, in fact, a special case of our approach, where the inner product is evaluated on the coefficients instead of \\old{on curve }evaluations\\old{, since the entire quotient space geometry and model is determined by the Hilbert space $\\mathcal{Y}$} (\\old{for more details see }Supplement \\ref{sec:coefficientlevel}). \n\nThe proposed model is motivated by geodesic regression.\nHowever, in the multiple linear predictor, a linear effect of a single covariate does, in general, not describe a geodesic for fixed non-zero values of other covariate effects. Or put differently, $\\operatorname{Exp}_{[p]}\\left( h_1 + h_2\\right) \\neq \\operatorname{Exp}_{\\operatorname{Exp}_{[p]}\\left(h_1\\right)}\\left(h_2\\right) \\neq \\operatorname{Exp}_{\\operatorname{Exp}_{[p]}\\left(h_2\\right)}\\left(h_1\\right)$ in general.\nThus, hierarchical geodesic effects of the form $\\operatorname{Exp}_{\\operatorname{Exp}_{[p]}\\left(h_1\\right)}\\left(h_2\\right)$, relevant, i.a., in mixed models for hierarchical/longitudinal study designs \\citep{KimEtAl2018RiemannianNonlinearMixedEffectsModels}, present an interesting future extension of our model.\\old{ For estimation, developments in  model-based boosting for generalized additive models for location, scale and shape \\citep[GAMLSS,][]{Thomas2016} might offer the needed infrastructure.}\nMoreover, an ``elastic'' extension\\old{ for functional shape and form regression} based on the square-root-velocity framework \\citep{SrivastavaKlassen2016} presents a promising direction for future research\\new{, as do other manifold responses}.\n\\old{The generality of underlying concepts also encourages  investigation of our approach for response objects living in other Riemannian manifolds, such as SPD matrices occurring, e.g., as covariance matrices in functional connectivity studies or diffusion tensor imaging. \n\n\\section{Introduction}\n\nIn many imaging data problems, the coordinate system of recorded objects is arbitrary or explicitly not of interest.\nStatistical shape analysis \\citep{DrydenMardia2016ShapeAnalysisWithApplications} addresses this point by identifying the ultimate object of analysis as the \\textit{shape} of an observation, reflecting its geometric properties invariant under translation, rotation and re-scaling, or as its \\textit{form} (or \\textit{size-and-shape}) invariant under translation and rotation. \nThis paper establishes a flexible additive regression framework for modeling the shape or form of planar (potentially irregularly sampled) curves and/or landmark configurations in dependence on scalar covariates. \n\\old{The proposed component-wise Riemannian $L_2$-Boosting algorithm allows estimation of a large number of parameter-intense model terms with inherent model selection.  \nWe further introduce a novel visualization of functional additive effects using a tensor-product factorization,\nwhich we consider essential for practical interpretation.}\nA  rich \\new{shape analysis} literature \\old{on statistical shape analysis }has been developed for 2D or 3D landmark configurations -- presenting for instance selected points of a bone or face -- which are considered elements of Kendall's shape space \\citep[see, e.g.][]{DrydenMardia2016ShapeAnalysisWithApplications}. \nIn many 2D scenarios, however, observed points describe a curve reflecting the outline of an object rather than dedicated landmarks \\citep{AdamsRholfSlice2013GeomMorphometrics21stCentury}. \nConsidering outlines as images of (parameterized) curves shows a direct link to functional data analysis \\citep[FDA,][]{RamsaySilverman2005} and,\nin this context, we speak of functional shape/form data analysis. \nAs in FDA, functional shape/form data can be observed on a common and often dense grid (\\textit{regular/dense} design) or on curve-specific often sparse grids (\\textit{irregular/sparse} design).\nWhile in the regular case, analysis often simplifies by treating curve evaluations as multivariate data, more general irregular designs gave rise to further developments in sparse FDA \\citep[e.g.][]{Yao:etal:2005, GrevenScheipl2017}, explicitly considering irregular measurements instead of pre-smoothing curves.\nTo the best of our knowledge,\nwe are the first to consider\nirregular/sparse designs in the context of\nfunctional shape/form analysis.\n\nShapes and forms are examples of manifold data. \n\\new{\\citet{petersen2019frechet} propose ``Fr\u00e9chet regression'' for random elements in general metric spaces, which requires estimation of a (potentially negatively) weighted Fr\u00e9chet mean for each covariate combination. \n\n\tTheir implicit rather then explicit model formulation\n\trenders model interpretation\n\tdifficult.\n\n\tMore explicit model formulations have been developed for the special case of a Riemannian geometry.  Besides tangent space models \\citep{kent2001functional}, extrinsic models \\citep{lin2017extrinsic} and models based on unwrapping \\citep{jupp1987fitting, mallasto2018wrapped}, a variety of manifold regression models have been designed based on the intrinsic Riemannian geometry.} Starting from geodesic regression \\citep{Fletcher2012GeodesicRegressionRiemannianManifol\n}, which extends linear regression to curved spaces\\new{, these include} \\old{,several generalizations of standard regression models to manifold responses have been developed, such as} MANOVA \\citep{Huckemann2010intrinsicMANOVA}, polynomial regression \\citep{Hinkle2014IntrinsicPolynomials}, \\new{smoothing splines \\citep{kume2007shape}},\nregression along geodesic paths with non-constant speed \\citep{HongNiethammer2014TimeWarpedGeodesicRegression}, or kernel regression \\citep{Davis2010ManifoldKernelRegression} \\new{and Kriging \\citep{pigoli2016kriging}}. \\old{While the \nformulations are more general, landmark shape spaces often serve as an example.} \nHowever, \\new{mostly} only one metric covariate or categorical covariates are considered\\new{, possibly in hierarchical model extensions for longitudinal data \\citep{muralidharan2012sasaki, schiratti2017bayesian}}. By contrast, \\citet{zhu2009intrinsic,shi2009intrinsic,KimEtAl2014RiemannGLM} generalize geodesic regression to regression with multiple covariates \\new{focusing on symmetric positive-definite (SPD) matrix responses}\\old{but with symmetric positive-definite (SPD) matrices as response rather than shapes}. \\citet{CorneaEtAl2017RegRiemannianSymSpaces} \ndevelop a general generalized linear model (GLM) analogue regression framework for responses in a symmetric \\new{manifold} \\old{space -- a class of Riemannian manifolds containing, in particular, Kendall's shape space --} and apply it to shape analysis. Recently, \\citet{lin2020AdditiveModelsSPD} proposed a Lie group additive regression model for Riemannian manifolds focusing \\old{however} on SPD matrices \\old{and not on} \\new{rather than} shapes.\n\nIn FDA, there is a much wider range of developed regression methods \\citep[see overviews in][]{Morris2015,GrevenScheipl2017}.\nAmong the most flexible models \\new{are functional additive models (FAMs)}\nfor (univariate) functional responses \\new{(in contrast to FAMs with functional covariates \\citep{ferraty2011recent}) with different strategies existing to model a) response functions and b) smooth covariate effects. For a), basis expansions in spline \\citep{BrockhausGreven2015}, functional principal component (FPC) bases \\citep{MorrisCarroll2006} or both \\citep{Scheipl2015} are employed as well as wavelets \\citep{meyer2015bayesian}, sometimes directly expanding functions to model on coefficients and sometimes expanding only predictions while keeping the raw measurements. Other approaches effectively evaluate curves on grids or apply pre-smoothing techniques instead \\citep[e.g.,][]{jeon2020additiveHilbertian}.\nFor b), again penalized spline basis approaches are employed \\citep{Scheipl2015, BrockhausGreven2015}, or local linear/polynomial \\citep{MullerYao2008FAM, jeon2022locally} or other kernel-based approaches \\citep{jeon2020additiveHilbertian, jeon2021additiveNonEuclidean}.\nThe different approaches come with different theoretical and practical advantages, but similiarities such as regarding asymptotic behavior   are also  known  from scalar nonparametric regression  \\citep{LiRuppert2008asymptotics}.\nAdvantages of the fully basis expansion based approach summarized in \\cite{GrevenScheipl2017} include its appropriateness for sparse irregular functional data and its modular extensibility to functional mixed models \\citep{Scheipl2015, meyer2015bayesian} and non-standard response distributions \\citep{BrockhausGreven2015, Stoecker2019FResponseLSS}.\n}\\old{\nare the functional additive (mixed) models (FA(M)Ms) of  i.a.\\ \\cite{MorrisCarroll2006,meyer2015bayesian}  for dense functional data \nand the flexible FAMM model class also covering sparse irregular functional data, summarized in \\cite{GrevenScheipl2017}.\n}\n\\old{There is less work on regression f}\\new{F}or bivariate or multivariate functional responses, which are closest to functional shapes/forms but without   invariances\\old{.}\\new{,}  \n\\cite{rosen2009bayesian,zhu2012multivariate,olsen2018simultaneous} consider linear fixed\neffects of scalar covariates, the latter also allowing for warping. \n\\cite{\n\n\tZhuEtal2017multiFAMM,BackenrothEtal2018HeteroFPCA} consider one or more random effects for one grouping variable, linear fixed effects and common dense grids for all functions. \n\\cite{Volkmann2020MultiFAMM} combine the FA\\old{M}M model class of \\cite{GrevenScheipl2017} with multivariate \\old{functional principal component}\\new{FPC} analysis\n\\citep{HappGreven2018} to \\new{model}\\old{obain FA\\old{M}Ms for} multivariate (sparse) functional responses.\n\n \\old{For estimation of FAMMs, \\citet{BrockhausGreven2015} employ a (component-wise) model-based gradient boosting algorithm. Model-based boosting \\citep{HothornBuehlmann2010} is a stage-wise fitting procedure for estimating additive  models with inherent model selection and a slow over-fitting behavior, allowing to efficiently estimate models with a very large number of coefficients. \nRegularization is applied both in each step and via early stopping of the algorithm based, e.g., on cross-validation. The combination of high auto-correlation within functional responses and complex tensor-product covariate effects typical for FAMMs make the double regularization particularly beneficial in this context \\citep{Stoecker2019FResponseLSS}. Gradient boosting with respect to (w.r.t.) a least-squares loss is typically referred to as $L^2$-Boosting \\citep{Buehlmann2003L2boosting}. \nWe introduce a \\textit{Riemannian $L^2$-Boosting} algorithm for estimating interpretable additive regression models for response shapes/forms of planar curves.} \n\n\\new{This paper establishes an interpretable FAM framework for modeling the shape or form of planar (potentially irregularly sampled) curves and/or landmark configurations in dependence on scalar covariates, extending $L_2$-Boosting \\citep{Buehlmann2003L2boosting, BrockhausGreven2015} to Riemannian manifolds for model estimation.}\nThe three major contributions of our regression framework are: \n1.~We introduce additive regression with shapes/forms of planar curves and/or landmarks as response, extending FAMs to non-linear response spaces or, vice versa, extending GLM-type regression on manifolds for landmark shapes both to functional shape manifolds and to include\n(non-linear) additive model effects.\n2.~We propose a novel Riemannian $L_2$-Boosting algorithm for estimating regression models for this type of manifold response, \nand 3.~a visualization technique based on tensor-product factorization yielding intuitive  interpretations even of multi-dimensional smooth covariate effects\nfor practitioners.\nAlthough related tensor-product model transformations based on higher-order SVD have been used, i.a., in control engineering \\citep{Baranyi2013tensor}, \nwe are not aware of any comparable application for visualization in FAMs or other statistical models for object data.\nDespite our focus on shapes and forms, transfer of the model, Riemannian $L_2$-Boosting, and\nfactorized visualization to other Riemannian manifold responses is intended in the generality of the formulation and the design of the provided R package \\texttt{manifoldboost}\\if00{ (developer version on \\url{github.com/Almond-S/manifoldboost})}\\fi.\n\\old{Moreover, while considering potential invariance w.r.t.\\ re-parameterization (``warping'') of curves is beyond the scope of this paper, the presented framework is intended to be combined with an ``elastic'' approach in the square root velocity framework \\citep{SrivastavaKlassen2016} in the future.}\nThe versatile applicability of the approach is illustrated in three different scenarios: \nan analysis of the shape of sheep astragali (ankle bones) represented by both regularly sampled curves and landmarks in dependence on categorical ``demographic'' variables; \nan analysis of the effects of different metric biophysical model parameters (including smooth  interactions) on the form of (irregularly sampled) cell outlines generated from a cellular Potts model;\nand a simulation study with irregularly sampled functional shape and form responses generated from a dataset of different bottle outlines and including metric and categorical covariates.\n\nIn Section \\ref{chap_diffgeo}, we introduce the manifold geometry of irregular curves modulo translation, rotation and potentially re-scaling, which underlies the intrinsic additive regression model formulated in Section \\ref{chap_model}. The Riemannian $L^2$-Boosting algorithm is introduced in Section \\ref{chap_boosting}. Section \\ref{chap_application} analyzes different data problems,   modeling sheep bone shape responses (Section \\ref{sec_sheep}) and cell outlines (Section \\ref{sec_cells}). Section \\ref{sec_simulation} summarizes the results of simulation studies with functional shape and form responses. We conclude  with a discussion in Section \\ref{chap_discussion}.\n\n\n\\section{Additive Regression on Riemannian Manifolds}\n\\label{chap_model}\n\nConsider a data scenario with $n$ observations of a random response covariate tuple $\\left(Y, \\mathbf{X}\\right)$, where the realizations of $Y$ are planar curves $y_i: \\mathcal{T} \\rightarrow \\mathds{C}$, $i=1,\\dots,n$, belonging to a Hilbert space $\\mathcal{Y}$ defined as above and potentially irregularly measured on individual grids $t_{i1} < \\dots < t_{ik_i} \\in \\mathcal{T}$.\nThe response object $[Y]$ is the equivalence class of $Y$ with respect to translation, rotation and possibly scale and the sample $[y_1],\\dots, [y_n]$ is equipped with the respective Riemannian manifold geometry introduced in the previous section. \nFor $i=1,\\dots,n$, realizations $\\mathbf{x}_{i}\\in \\mathcal{X}$ of a covariate vector $\\mathbf{X}$ in a covariate space $\\mathcal{X}$ are observed.  $\\mathbf{X}$ can contain several categorical and/or metric covariates. \n\n\\newcommand{j}{j}\n\\newcommand{J}{J}\nFor regressing the mean of $[Y]$ on $\\mathbf{X}=\\mathbf{x}$, we model the shape/form $[\\mu]$ of \\old{a curve }$\\mu\\in\\mathcal{Y}$ as\n\\begin{equation}\n\t\\label{model_equation}\n\t[\\mu] = \\operatorname{Exp}_ {[p]}\\left(h(\\mathbf{x})\\right) = \\operatorname{Exp}_ {[p]}\\left(\\sum_{j=1}^J h_j(\\mathbf{x})\\right), \n\\end{equation}\nwith \nan additive predictor $h: \\mathcal{X} \\rightarrow T_{[p]}\\mathcal{Y}^*_{/G}$ acting in the tangent space at an ``intercept'' $[p] \\in \\mathcal{Y}^*_{/G}$.\nGeneralizing an additive model ``$Y = \\mu + \\epsilon = p + h(\\mathbf{x}) + \\epsilon$'' in a linear space, we implicitly define $[\\mu]$ as the conditional mean of $[Y]$ given $\\mathbf{X}=\\mathbf{x}$ by assuming zero-mean ``residuals'' $\\epsilon $. In their definition, we follow \\cite{CorneaEtAl2017RegRiemannianSymSpaces}\nbut extend to the functional shape/form and additive case.\nWe assume local linearized residuals $\\varepsilon_{[\\mu]} = \\operatorname{Log}_{[\\mu]}([Y])$ in $T_{[\\mu]}\\mathcal{Y}^*_{/G}$ to have mean $\\mathds{E}\\left(\\varepsilon_{[\\mu]}\\right) = \\boldsymbol{0}$,\nwhich corresponds to $\\mathds{E}\\left(\\varepsilon_{[\\mu]}(t)\\right) = 0$ for ($\\nu$-almost) all $t \\in \\mathcal{T}$.\nHere, we assume $[Y]$ is sufficiently close to $[\\mu]$ with probability 1 such that $\\operatorname{Log}_{[\\mu]}$ is well-defined, which is the case whenever $\\langle \\widetilde{Y}, \\widetilde{\\mu}\\rangle \\neq 0$ for centered shape/form representatives $\\widetilde{Y}$ and $\\widetilde{\\mu}$, an un-restrictive and common assumption \\cite[compare also][]{CorneaEtAl2017RegRiemannianSymSpaces}. \nHowever, residuals $\\varepsilon_{[\\mu]}$ for different $[\\mu]$ belong to separate tangent spaces.\nTo obtain a formulation in a common linear space instead, local residuals are mapped to residuals $\\epsilon = \\operatorname{Transp}_{[\\mu], [p]}(\\varepsilon_{[\\mu]})$ by parallel transporting them from $[\\mu]$ to the common covariate independent pole $[p]$. After this isometric mapping into $T_{[p]}\\mathcal{Y}^*_{/G}$, we can equivalently define the conditional mean $[\\mu]$ via $\\mathds{E}\\left(\\epsilon\\right) = \\boldsymbol{0}$ for the transported residuals $\\epsilon$.\n\\newline\n$\\operatorname{Exp}_{[p]}$  maps the additive predictor $h(\\mathbf{x}) = \\sum_{j=1}^J h_j(\\mathbf{x}) \\in T_{[p]}\\mathcal{Y}^*_{/G}$ to the response space. It is analogous to a response function in GLMs but depends on $[p]$. While other response functions could be used, we restrict to the exponential map here, such that the model contains a geodesic model \\citep{Fletcher2012GeodesicRegressionRiemannianManifold} -- the direct generalization of simple linear regression -- as a special case for $h(\\mathbf{x}) = \\beta x_1$ with a single covariate $x_1$ and tangent vector $\\beta$.\nTypically, it is assumed that $h$ is centered such that $\\mathds{E}\\left(h(\\mathbf{X})\\right) = \\boldsymbol{0}$, and the pole $[p]$ \\old{corresponds to}\\new{is} the overall mean of $[Y]$ defined, like the conditional mean, via residuals of mean zero.\n\n\\subsection{Tensor-product effect functions $h_j$}\n\\label{sec_tensorproduct_effects}\n\\newcommand{r}{r}\n\\newcommand{m}{m}\n\\newcommand{0}{0}\n\\newcommand{{}}{{}}\n\\citet{Scheipl2015} and other authors employ tensor-product (TP) bases for functional additive model terms. This naturally extends to tangent space effects, which we model as\n$$h_j(\\mathbf{x}) = \\sum_{r, l} \\theta_j^{(r,l)}\\, b_j^{(l)}(\\mathbf{x})\\, \\partial_r$$\nwith the TP basis given by the pair-wise products of $m_{}$ linearly independent tangent vectors $\\partial_r \\in T_{[p]}\\mathcal{Y}^*_{/G},\\ r=1,\\dots, m_{},$ and $m_j$ basis functions $b_j^{(l)}: \\mathcal{X} \\rightarrow \\mathds{R},\\ l=1,\\dots,m_j,$ for the $j$-th covariate effect depending on one or more covariates. The real \\old{basis} coefficients can be arranged as a matrix $\\{\\theta_j^{(r,l)}\\}_{r,l} = \\boldsymbol{\\Theta}_j \\in \\mathds{R}^{m_{} \\times m_j}$. \n\\new{\nAlso for infinite-dimensional $T_{[p]}\\mathcal{Y}^*_{/G}$ and a general non-linear dependence on $x$, a basis representation approach requires truncation to finite dimensions $m$ and $m_j$ in practice. Choosing the bases to capture the essential variability in the data, their size can be extended with increasing data size and computational resources.}\n\nWhile, in principle, the basis $\\{\\partial_r\\}_r$ could also vary across effects $j=1,\\dots,J$, we assume a common basis for notational simplicity, which presents the typical choice.\nDue to the identification of $T_{[p]}\\mathcal{Y}^*_{/G}$ with a subspace of the function space $\\mathcal{Y}$,\nthe $\\{\\partial_r\\}_r$ may be specified using a  function basis commonly used in additive models: Let $b_{0}^{(l)}: \\mathcal{T} \\rightarrow \\mathds{R}$, $l = 1,\\dots, m_0$ be a basis of real functions, say a B-spline basis \\new{(other typical bases used in the literature include wavelet \\citep{meyer2015bayesian} or FPC bases \\citep{MullerYao2008FAM})}. Then we construct the tangent space basis as $\\partial_r = \\sum_{l=1}^{m_0} \\left( z_p^{(l, r)} + z_p^{(m_0+l, r)}\\i\\right)  b_0^{(l)}$, employing the same basis for the $1$- and $\\i$-dimension before transforming it with a\\old{ suitable} basis transformation matrix $\\mathbf{Z}_p = \\{z_p^{(l, r)}\\}_{l,r}\\in \\mathds{R}^{2m_0 \\times m_{}}$ with $m_{} < 2m_0$ implementing the linear tangent space constraints \\old{as given in }\\new{(}Section \\ref{chap_diffgeo}\\new{)}.\nPractically, $\\mathbf{Z}_p$ \\old{can be}\\new{is} obtained as null space basis matrix of the matrix $(\\re{\\mathbf{C}}, \\im{\\mathbf{C}})$ with $\\mathbf{C}=\\{\\langle b_0^{(l)}, \\zeta^{(r)}\\rangle\\}_{r,l}$ (or with the empirical inner product on the product space of irregular curves instead) constructed from the normal vectors $\\zeta^{(r)}\\in\\mathcal{Y}$, $r=1,\\dots, 2m_0-m,$ to $T_{[p]}\\mathcal{Y}^*_{/G}$. \nFor closed curves, we additionally choose $\\mathbf{Z}_p$ to enforce periodicity, i.e.\\ $\\partial_r(t) = \\partial_r(t+t_0)$ for some $t_0 \\in \\mathds{R}$ \\citep[compare][]{Hofner2016ConstrainedRegression}. \n\n\\newcommand{\\kappa}{\\kappa}\n\\newcommand{K}{K}\nGiven the tangent space basis, we may now modularly specify the usual additive model basis functions $b_j^{(l)}: \\mathcal{X} \\rightarrow \\mathds{R}$, $l=1,\\dots, m_j$, for the $j$-th covariate effect to obtain the full functional additive model ``tool box'' offered by, e.g., \\citet{BrockhausGreven2015}. A linear effect -- linear in the tangent space -- of the form $h_j(\\mathbf{x}) = \\beta z$ with a scalar (typically centered) covariate $z$ in $\\mathbf{x}$ and $\\beta \\in T_{[p]}\\mathcal{Y}^*_{/G}$ is simply implemented by a single function $b_j^{(1)}(\\mathbf{x}) = z$. \nA smooth effect of the generic form $h_j(\\mathbf{x})(t) = f(z, t)$ can be implemented by choosing, e.g., a B-spline basis \\new{(Asymptotic properties of penalized B-splines and connections to kernel estimators are discussed, e.g., by \\citet{WoodPyaSaefken2016smoothing, LiRuppert2008asymptotics})}.  \nFor a categorical covariate effect of the form $h_j(\\mathbf{x}): \\{1,\\dots, K\\} \\rightarrow T_{[p]}\\mathcal{Y}^*_{/G}$, $\\kappa \\mapsto \\beta_\\kappa$, the basis $\\mathbf{b}_j(\\mathbf{x}): \\kappa \\mapsto \\mathbf{e}_{\\kappa} \\in \\mathds{R}^{K-1}$ maps category $\\kappa$ to a usual contrast vector $\\mathbf{e}_{\\kappa}$ just as in standard linear models. Here, we typically use effect-encoding to obtain centered effects.\nMoreover, TP interactions of the model terms described above, as well as group-specific effects and smooth effects with additional constraints \\citep{Hofner2016ConstrainedRegression} can be specified in the model formula, relying on the \\texttt{mboost} framework introduced by \\citet{HothornBuehlmann2010}, which also allows to define custom effect designs. \nFor identification of an overall mean intercept $[p]$, sum-to-zero constraints yielding $\\sum_{i=1}^{n} h_j(\\mathbf{x}_i) = \\boldsymbol{0}$ for observed covariates $\\mathbf{x}_i$\\old{, $i=1, \\dots, n$,} can be specified, and similar constraints can be used to distinguish linear from non-linear effects and interactions from their marginal effects \\citep{KneibHothornTutz2009}.\nDifferent quadratic penalties can be specified for the coefficients $\\boldsymbol{\\Theta}_j$, allowing to regularize high-dimensional effect bases and to balance effects of different complexity in the model fit (\\old{see also }\\new{cf.} Section \\ref{chap_boosting}). \n\n\\subsection{Tensor-product factorization}\n\\label{chap_model::TPfactorization}\nThe multidimensional structure of the response objects makes it challenging to graphically illustrate and interpret additive model terms, in particular when it comes to non-linear (interaction) effects, or when effect sizes are visually small.\nTo solve this problem, we suggest to re-write \\new{estimated} \\old{the} TP effects $\\new{\\hat{h}}_j$ \\new{with estimated} \\old{for a given  fixed} coefficient matrix $\\new{\\widehat{\\boldsymbol{\\Theta}}}_j$\nas\n\\newcommand{\\new{\\hat{h}}}{\\new{\\hat{h}}}\n$$\n     \\new{\\hat{h}}_j(\\mathbf{x}) = \\sum_{r=1}^{m_j'} \\xi^{(r)}_j \\new{\\hat{h}}_j^{(r)}(\\mathbf{x}) \n$$\nfactorized into $m_j' = \\min(m_j, m_0)$ components consisting of covariate effects $\\new{\\hat{h}}_j^{(r)}: \\mathcal{X} \\rightarrow \\mathds{R},\\ r=1,\\dots,m_j',$ in corresponding orthonormal directions $\\xi^{(r)}_j \\in T_{[p]}\\mathcal{Y}^*_{/G}$ with $\\langle\\xi_j^{(r)},  \\xi_j^{(l)} \\rangle = \\mathds{1}({r = l})$, i.e.\\ $1$ if $r=l$ and 0 otherwise. \nAssuming $\\mathds{E}\\left(b_j^{(l)}(\\mathbf{X})^2\\right)<\\infty$, $l=1,\\dots,m_j$, for the underlying effect basis, the  $\\new{\\hat{h}}_j^{(r)}$ are specified to achieve decreasing component variances $v_j^{(1)} \\geq \\dots \\geq v_j^{(m_j')}\\geq 0$ \ngiven by $v_j^{(r)} = \\mathds{E}\\left(\\new{\\hat{h}}_j^{(r)}(\\mathbf{X})^2\\right)$. In practice, the expectation over the covariates $\\mathbf{X}$ and the inner product $\\langle ., .\\rangle$ are replaced by empirical analogs (compare Supplement Corollary \\ref{corollary:TPfactorization_empirical}). Due to orthonormality of the $\\xi_j^{(r)}$, the component variances add up to the total predictor variance $\\sum_{r=1}^{m_j'} v_j^{(r)}= v_j = \\mathds{E}\\left(\\langle \\new{\\hat{h}}_j(\\mathbf{X}), \\new{\\hat{h}}_j(\\mathbf{X})\\rangle\\right)$.\nMoreover, the TP factorization is optimally concentrated in the first components in the sense that for any $l \\leq m_j'$ there is no sequence of $\\xi^{(r)}_* \\in \\mathcal{Y}$ and $\\new{\\hat{h}}_*^{(r)}:\\mathcal{X}\\rightarrow\\mathds{R}$, such that $\\mathds{E}\\left(\\|\\new{\\hat{h}}_j(\\mathbf{X}) - \\sum_{r=1}^l \\xi^{(r)}_* \\new{\\hat{h}}_*^{(r)}(\\mathbf{X}) \\|^2\\right) < \\mathds{E}\\left(\\|h_j(\\mathbf{X}) - \\sum_{r=1}^l \\xi_j^{(r)} \\new{\\hat{h}}^{(r)}_j(\\mathbf{X}) \\|^2\\right)$, i.e.\\ the series of the first $l$ components yields the best rank $l$ approximation of $\\new{\\hat{h}}_j$. \nThe factorization relies on SVD of (a transformed version of) the coefficient matrix $\\new{\\widehat{\\boldsymbol{\\Theta}}}_j$ and the fact that it is well-defined is a variant of the  Eckart-Young-Mirsky\ntheorem (proof in Supplement \\ref{EYM}). \\newline\nParticularly when large shares of the predictor variance are explained by the first component(s), the decomposition facilitates graphical illustration and interpretation: choosing a suitable constant $\\tau\\neq0$, an effect direction $\\xi^{(r)}_j$ can be visualized by plotting the pole representative $p$ together with $\\operatorname{Exp}_{p}(\\tau\\,\\xi^{(r)}_j)$ on the level of curves, while\naccordingly re-scaled  $\\frac{1}{\\tau}\\new{\\hat{h}}_j^{(r)}(\\mathbf{x})$ is displayed separately in a standard scalar effect plot. \nAdjusting $\\tau$ offers an important degree of freedom for visualizing $\\xi^{(r)}_j$ on an intuitively accessible scale while faithfully depicting $\\xi^{(r)}_j\\new{\\hat{h}}_j^{(r)}(\\mathbf{x})$. \nWhen based on the same $\\tau$, different covariate effects can be compared across the plots sharing the same scale. We suggest $\\tau = \\max_{j} \\sqrt{v_j}$, the maximum total predictor standard deviation of an effect, as a good first choice.\n\nBesides factorizing effects separately, it can also be helpful to apply TP factorization to the joint additive predictor, yielding\n$$\nh(\\mathbf{x}) = \\sum_{r=1}^{m'} \\xi^{(r)} \\new{\\hat{h}}^{(r)}(\\mathbf{x}) = \\sum_{r=1}^{m'} \\xi^{(r)} \\left( \\new{\\hat{h}}_1^{(r)}(\\mathbf{x}) + \\dots + \\new{\\hat{h}}_J^{(r)}(\\mathbf{x})\\right), \\quad m' = \\min(\\sum_j m_j, m_{}),\n$$\nwith again $\\xi^{(r)} \\in T_{[p]}\\mathcal{Y}^*_{/G}$ orthonormal and the corresponding variance concentration in the first components, but now determined w.r.t.\\ entire additive predictors $\\new{\\hat{h}}^{(r)} =  \\sum_{j=1}^J \\new{\\hat{h}}_j^{(r)}$\nspanned by all covariate basis functions in the predictor. \nIn this representation, the first component yields a geodesic additive model approximation where the predictor moves along a geodesic line $c(\\tau) = \\operatorname{Exp}_{[p]}\\left(\\xi^{(1)} \\tau\\right)$ with the signed distance $\\tau \\in \\mathds{R}$ from $[p]$, modeled by a scalar additive predictor $\\new{\\hat{h}}^{(1)}(\\mathbf{x})$ composed of covariate effects analogous to the original model predictor.\nIn Section \\ref{chap_application}, we illustrate its potential in three different scenarios.\n\n \n\n\n\n\\section{Applications and Simulation}\n\\label{chap_application}\n\\input{chap_bones}\n\\input{chap_cells}\n\\input{chap_bottles}\n\n\\input{chap_discussion}\n\n\\section*{Acknowledgement}\nWe sincerely thank Nadja P\\\"ollath for providing carefully recorded sheep astragalus data and important insights and comments, \\new{and} Sophia Schaffer for running and discussing cell simulations and providing fully processed cell outlines\\new{.}\\old{, and Lisa Steyer for frequent and fruitful discussions.}\nMoreover, we gratefully acknowledge funding by grant GR 3793/3-1 from the German research foundation (DFG).\n\n\\bigskip\n\\begin{center}\n{\\large\\bf SUPPLEMENTARY MATERIAL}\n\\end{center}\nSupplementary material with further details is provided in an online supplement.\n\n\n\n\n\\bibliographystyle{chicago}\n\n\\section{Online Supplementary Material to}\n{\\Large Functional additive models on manifolds of planar shapes and forms}\\newline\nby Almond St\\\"ocker\\new{, Lisa Steyer} and Sonja Greven\n\n\\subsection{Geometry of functional forms and shapes}\n\n\\subsubsection{Translation, rotation and re-scaling as normal subgroups}\n\\label{sec:groupactions}\nWe consider the following invariances of a response curve $y \\in\\mathcal{Y}$ with respect to the transformations $\\mathcal{Y} \\rightarrow \\mathcal{Y}$ given by \nthe group actions of translation $\\operatorname{Trl} = \\{y \\overset{\\operatorname{Trl}_\\gamma}{\\longmapsto} y + \\gamma\\,\\!\\mycal{1} : \\gamma\\in\\mathds{C}\\}$ with some $\\!\\mycal{1}\\in\\mathcal{Y}\\setminus\\{\\boldsymbol{0}\\}$ (for curves typically $\\!\\mycal{1}: t\\mapsto \\frac{1}{\\|t\\mapsto 1\\|}$ the real constant function of unit norm), re-scaling $\\operatorname{Scl} = \\{y \\overset{\\operatorname{Scl}_\\lambda}{\\longmapsto} \\lambda \\cdot (y - \\boldsymbol{0}_y) + \\boldsymbol{0}_y : \\lambda \\in \\mathds{R}^+\\}$ around a reference point $\\boldsymbol{0}_y \\in \\mathds{C}$, and rotation $\\operatorname{Rot} = \\{y \\overset{\\operatorname{Rot}_u}{\\longmapsto} u\\cdot (y-\\boldsymbol{0}_y) + \\boldsymbol{0}_y : u \\in \\S^1\\}$ around $\\boldsymbol{0}_y$ with $\\S^1 = \\{u\\in\\mathds{C}:|u|=1\\} = \\{\\exp(\\omega\\i):\\omega\\in\\mathds{R}\\}$ the circle group reflecting counterclockwise rotations by $\\omega$ radian measure.\nIn the literature, the reference point is usually omitted setting $\\boldsymbol{0}_y = \\boldsymbol{0}$, which can be done without loss of generality under translation invariance (i.e. in particular for shapes/forms). \nHowever, keeping other possible combinations of invariances in mind, we explicitly refer to an individual reference point and suggest \nthe centroid $\\boldsymbol{0}_y = \\langle {\\!\\mycal{1}}\\,, y \\rangle \\!\\mycal{1}$ or, more generally, $\\boldsymbol{0}_y = a(y)\\, \\!\\mycal{1}$ for some linear functional $a:\\mathcal{Y} \\rightarrow \\mathds{R}$.\nAssuming $\\operatorname{Trl}_\\gamma(0_{y}) = 0_{\\operatorname{Trl}_\\gamma(y)}$, as for the centroid, the definition of re-scaling and rotation around $\\boldsymbol{0}_y$ ensures that $\\operatorname{Trl}_\\gamma$, $\\operatorname{Scl}_\\lambda$ and $\\operatorname{Rot}_u$ commute -- and that $\\operatorname{Trl}$, $\\operatorname{Rot}$ and $\\operatorname{Scl}$ present normal subgroups of the combined group actions $\\{y \\mapsto \\lambda u y + \\gamma : \\gamma\\in\\mathds{C}, \\lambda\\in\\mathds{R}^+, u\\in\\S^1\\}$ of shape invariances. \nThus, the combined group actions can be written as the direct product (or direct sum) $\\operatorname{Trl} \\dir \\operatorname{Scl} \\dir \\operatorname{Rot} = \\{\\operatorname{Trl}_\\gamma \\circ \\operatorname{Scl}_\\lambda \\circ \\operatorname{Rot}_u: \\gamma\\in\\mathds{C}, \\lambda\\in\\mathds{R}^+, u\\in\\S^1\\} \\cong \\mathds{C} \\times \\mathds{R}^+ \\times \\S^1$ and invariances with respect to $\\operatorname{Trl}$, $\\operatorname{Scl}$, $\\operatorname{Rot}$ can be modularly accounted for in arbitrary order. $\\operatorname{Trl} \\dir \\operatorname{Rot}$, for instance, describe rigid motions.\nThe ultimate response object is then given by the \\emph{orbit} $[y]_{G}=\\{g(y) : g\\in G\\}$ (or short $[y]$), i.e. the equivalence class with respect to the direct sum $G$ generated by the chosen combination of $\\operatorname{Trl}$, $\\operatorname{Scl}$ and $\\operatorname{Rot}$. \n$[y]_{\\operatorname{Trl} \\dir \\operatorname{Scl} \\dir\\operatorname{Rot}}$ is referred to as the \\emph{shape} of $y$ and $[y]_{\\operatorname{Trl} \\dir\\operatorname{Rot}}$ as its \\emph{form} or \\emph{size-and-shape} \\citepsup[compare][]{DrydenMardia2016ShapeAnalysisWithApplications}; studying  $[y]_{\\operatorname{Scl}}$ is closely related to \\emph{directional} data analysis \\citepsup{MardiaJupp2009DirectionalStatistics} where the direction of $y$ is analyzed independent of its size $\\|y\\|$. \n\n\\subsubsection{Parallel transport of form tangent vectors}\n\\label{sec:transport}\n\nTo confirm that the parallel transport in the form space $\\mathcal{Y}^*_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}$ can be carried out via representatives in $\\mathcal{Y}$ as described in the main manuscript, we closely follow \\citesup{Huckemann2010intrinsicMANOVA} in their derivation of shape parallel transport. Necessary differential geometric notions and statements are briefly introduced in the following before stating the main result in Lemma \\ref{lemma:formtransport}. For a more profound introduction, we recommend \\citetsup[in particular, p. 21, 43, 93, 124, 337, 402]{Lee2018RiemannianManifolds}, as well as \\citetsup{tu2011manifolds} for an illustrative introduction into some of the concepts, and \\citetsup[in particular, p.103- 107]{Klingenberg1995RiemannianGeometry} for an introduction in the light of potentially infinite dimensional manifolds.\n\n\n\\newtheorem{theorem}{Theorem}\n\\newtheorem{corollary}{Corollary}\n\\newtheorem{lemma}{Lemma}\n\\newtheorem{proposition}{Proposition}\n\\newtheorem{remark}{Remark}\n\n\\newcommand{d}{d}\n\\newcommand{{\\operatorname{Re}}}{{\\operatorname{Re}}}\n\\newcommand{{\\operatorname{Im}}}{{\\operatorname{Im}}}\n\\newcommand{\\mathcal{N}}{\\mathcal{N}}\n\\newcommand{\\mathcal{H}}{\\mathcal{H}}\n\n\\newcommand{\\pullb}[1]{\\widetilde{#1}}\n\\newcommand{\\pullback}[1]{\\pullb{\\left(#1\\right)}}\n\nThe entire argument crucially relies on properties known for Riemannian submersions between differentiable manifolds $\\widetilde{\\mathcal{M}}$ and $\\mathcal{M}$, which allow to relate the structure of $\\mathcal{M}$ back to  $\\widetilde{\\mathcal{M}}$. A \\textit{submersion} is a smooth surjective function $\\Phi: \\pullb{\\mathcal{M}} \\rightarrow \\mathcal{M}$, for which also the differential $d\\Phi: T_{\\widetilde{q}} \\widetilde{\\mathcal{M}} \\rightarrow T_{q} \\mathcal{M}$, $\\widetilde{q}\\in\\widetilde{\\mathcal{M}}$, $q=\\Phi{(\\widetilde{q})}$, is surjective at each $\\widetilde{q}\\in\\widetilde{\\mathcal{M}}$. For $q \\in \\mathcal{M}$, the $\\Phi^{-1}(\\{q\\})$ are submanifolds of $\\mathcal{M}$, and $T_{\\widetilde{q}} \\widetilde{\\mathcal{M}} = T_{\\widetilde{q}}\\Phi^{-1}(\\{q\\}) \\oplus H_{\\widetilde{q}}\\widetilde{\\mathcal{M}}$ can be decomposed into the \\textit{vertical space} $T_{\\widetilde{q}}\\Phi^{-1}(\\{q\\}) = \\ker(d\\Phi)$ and its orthogonal complement $H_{\\widetilde{q}}\\widetilde{\\mathcal{M}}$, the \\textit{horizontal space}. When restricted to the horizontal space, $d\\Phi\\big|_{H_{\\widetilde{q}}\\widetilde{\\mathcal{M}}} : H_{\\widetilde{q}}\\widetilde{\\mathcal{M}} \\rightarrow T_{q} \\mathcal{M}$ presents a linear isomorphism. A submersion $\\Phi$ is called \\textit{Riemannian submersion} if $d\\Phi\\big|_{H_{\\widetilde{q}}\\widetilde{\\mathcal{M}}}$ is also isometric. It gives rise to an identification $T_{q} \\mathcal{M} \\cong H_{\\widetilde{q}}\\widetilde{\\mathcal{M}}$ of tangent spaces of $\\mathcal{M}$ with horizontal spaces on $\\widetilde{\\mathcal{M}}$. Such an identification underlies the presentation of the response geometry in Section \\ref{chap_diffgeo} of the main manuscript. \\newline\nBy construction, the quotient map $\\Phi: \\mathcal{Y}^* \\rightarrow \\mathcal{Y}^*_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}, y \\mapsto [y]$ presents a Riemannian submersion: \nSince $[p]=\\{u p + \\gamma \\!\\mycal{1}: u\\in\\S^1, \\gamma \\in \\mathds{C}\\}$ embeds $\\S^1\\times \\mathds{R}^2$ in $\\mathcal{Y}$, and, since the tangent spaces of $\\S^1$ and $\\mathds{R}^2$ are well-known, the vertical space is given by $T_p[p] \\cong \\{\\lambda p \\i + \\gamma\\, \\!\\mycal{1} : \\lambda \\in  \\mathds{R} , \\gamma \\in  \\mathds{C} \\} \\subset \\mathcal{Y}$, with orthogonal complement $H_p\\mathcal{Y}^* \\cong \\{y\\in\\mathcal{Y} : \\langle y, \\!\\mycal{1} \\rangle = 0,\\ \\im{\\langle y, p \\rangle} = 0 \\}$ (see also Figure \\ref{fig:quotientgeometry} in the main manuscript for an illustration).\nWhile $\\Phi$ is obviously surjective, surjectivity and isometry of $d\\Phi|_{H_{p}\\mathcal{Y}^*}$ can be seen by expressing $\\Phi$ in terms of the charts for $\\mathcal{Y}^*_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}$: for a given $p \\in [p] \\in \\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}^*$, the map $\\widetilde{(\\cdot)}: [y] \\mapsto \\widetilde{y}^{\\operatorname{Trl} \\dir \\operatorname{Rot}}$ provides a chart $\\mathcal{U}_{[p]} \\rightarrow \\mathcal{V}_{p}$, i.e.\\ an isomorphism from $\\mathcal{U}_{[p]} = \\{[y] \\in \\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}^* : \\langle \\widetilde{p}^{\\operatorname{Trl}}, \\widetilde{y}^{\\operatorname{Trl}}\\rangle \\neq 0\\}$ to $\\mathcal{V}_{p} = \\{y\\in\\mathcal{Y} : \\im{\\langle p, y\\rangle} = 0, \\re{\\langle p, y\\rangle} > 0, \\langle \\!\\mycal{1}, y\\rangle = 0\\}$ used to establish the differential structure on $\\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}^*$. Expressed in this chart, $\\widetilde{\\Phi}(y) = \\widetilde{(\\cdot)} \\circ \\Phi(y) =  \\widetilde{y}^{\\operatorname{Trl} \\dir \\operatorname{Rot}}$ is the identity for all $y\\in\\mathcal{V}_{p}\\subset\\Phi^{-1}\\left(\\mathcal{U}_{[p]}\\right)$. Thus, since $T_p\\mathcal{V}_p = H_p\\mathcal{Y}^*$, also $d\\widetilde{\\Phi}\\big|_{H_p\\mathcal{Y}^*}$ is the identity, which is obviously an isometric isomorphism. The latter carries over to $d\\Phi\\big|_{H_p\\mathcal{Y}^*}$ independent of the given chart.\\newline\nThe isometric isomorphism $d\\Phi\\big|_{H_p\\mathcal{Y}^*}: H_p\\mathcal{Y}^* \\rightarrow T_{[p]}\\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}^*$ yields the identification \\linebreak \n$T_{[p]}\\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}^* \\cong \\{y\\in\\mathcal{Y} : \\langle y, \\!\\mycal{1} \\rangle = 0,\\ \\im{\\langle y, p \\rangle} = 0 \\}$, which we rely on in the main manuscript.\nUnlike there, we denote $d\\Phi\\big|_{H_y\\mathcal{Y}^*}^{-1}: \\xi \\mapsto \\widetilde{\\xi}$ also for tangent vectors in the following, to make the identification of $\\xi=d\\Phi(\\pullb{\\xi}) \\in T_{[y]}\\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}^*$ with the corresponding $\\pullb{\\xi} \\in H_y\\mathcal{Y}^*$, usually referred to as \\textit{horizontal lift}, explicit in the notation. \n\nThe \\textit{covariant derivative} (Levi-Civita connection) $\\nabla^\\mathcal{M}_V W \\in T\\mathcal{M}$ of a vector-field $W\\in T\\mathcal{M}$ along a vector-field $V\\in T\\mathcal{M}$ provides a derivative of vector-fields in the tangent bundle $T\\mathcal{M} = \\{T_q\\mathcal{M} : q\\in\\mathcal{M}\\} $ of a Riemannian manifold $\\mathcal{M}$. As a derivation in $W$ and a linear function in $V$, $\\nabla^\\mathcal{M}$ fulfills a set of properties identifying it as unique generalization of ordinary directional derivatives of the components of $W:q\\mapsto W_q\\in T_q\\mathcal{M}$ into the direction $V_q \\in T_q\\mathcal{M}$. \nFor a submanifold $\\mathcal{M}$ of a linear space $\\mathcal{Y}$, $\\nabla^\\mathcal{M}_V W$ corresponds to the ordinary directional derivative orthogonally projected into $T_q\\mathcal{M}$. \nFor the linear case (with $\\mathcal{M}=\\mathcal{Y}$), the covariant derivative of \na vector field $W(\\tau) := W_{c{(\\tau)}}$ along a differentiable curve $c(\\tau)$ is directly given as\n\\begin{equation}\n\t\\label{covariant_derivative}\n\t\\nabla^\\mathcal{Y}_{\\dot{c}(t)} W(\\tau) = \\dot{W}(\\tau) = \\frac{d}{d\\tau} W(\\tau).\n\\end{equation}\n\nIn analogy to straight lines, geodesic curves $c(\\tau)$ are characterized by \n\\begin{equation*}\n\t\\nabla^\\mathcal{M}_{\\dot{c}(\\tau)} \\dot{c}(\\tau) = \\boldsymbol{0},\n\\end{equation*}\ni.e. curves with zero `second derivative'.\nMore generally, a vector-field $W$ is called parallel along a curve $c(\\tau)$ if\n\\begin{equation}\n\t\\label{parallel}\n\t\\nabla^\\mathcal{M}_{\\dot{c}(\\tau)} W(\\tau) = \\boldsymbol{0}.\n\\end{equation}\nAccording to that the parallel transport $\\operatorname{Transp}_{q, q'}^c : T_q\\mathcal{M} \\rightarrow T_{q'}\\mathcal{M}$ along a curve $c: [\\tau_0, \\tau_1] \\rightarrow \\mathcal{M}$ between $c(\\tau_0) = q,\\ c(\\tau_1) = q' \\in \\mathcal{M}$ is defined to map tangent vectors $\\varepsilon = W(\\tau_0) \\mapsto \\varepsilon' = W(\\tau_1)$ for some  vector field $W$ parallel along $c$ (fulfilling Equation \\ref{parallel}). If the curve $c$ is clear from context, we omit it in the notation. This is especially the case in the following, where $c$ can be chosen as the unique geodesic between two forms $[p]$ and $[p']$ with $\\langle p, p' \\rangle\\neq 0$, yielding a canonical connection (in this case, $c$ corresponds to the line between $p$ and the aligned $\\tilde{p}'$; for $\\langle p, p'\\rangle=0$, by contrast, it is easy to see that for each $u\\in\\S^1$ the line between $p$ and $up'$ corresponds to a different geodesic; the second case can, however, be neglected).\n\nThe possibility to effectively carry out the parallel transport between forms $[p], [p']$ on suitable representatives $p, p'\\in\\mathcal{Y}^*$ stems from the following theorem and subsequent Corollary \\citepsup[compare, e.g,][p. 103-105]{Klingenberg1995RiemannianGeometry}. \n\n\n\\begin{theorem}\n\t\\label{submersion_theorem}\n\tLet $\\Phi: \\pullb{\\mathcal{M}} \\rightarrow \\mathcal{M}$ be a Riemannian submersion between manifolds $\\widetilde{\\mathcal{M}}$ and $\\mathcal{M}$, and $V,W \\in T\\mathcal{M}$ vector-fields. Then\n\t\\begin{equation*}\n\t\t\\nabla^{\\pullb{\\mathcal{M}}}_{\\pullb{V}} \\pullb{W} = \\pullback{\\nabla^\\mathcal{M}_{V}W} + \\frac{1}{2}[\\pullb{V}, \\pullb{W}]^\\perp\n\t\\end{equation*}\n\twhere $\\pullb{Z} \\in H\\pullb{\\mathcal{M}}$ denotes the horizontal lift of $Z  \\in T\\mathcal{M}$ to the horizontal bundle $H\\pullb{\\mathcal{M}} = \\{H_{\\pullb{p}}\\pullb{\\mathcal{M}} : \\pullb{p} \\in \\pullb{\\mathcal{M}} \\}$, $Z= d\\Phi \\left(\\pullb{Z}\\right)$, and $[\\pullb{V}, \\pullb{W}]^\\perp$ is the the Lie bracket $[\\pullb{V}, \\pullb{W}] = \\pullb{V} \\circ \\pullb{W} - \\pullb{W} \\circ \\pullb{V}$ orthogonally projected $(\\cdot)^\\perp: T\\pullb{\\mathcal{M}}\\rightarrow ker\\left(d\\Phi\\right)$ to the vertical space.\n\\end{theorem}\n\n\\begin{corollary}\n\t\\label{corollary}\n\tLet $\\Phi: \\pullb{\\mathcal{M}} \\rightarrow \\mathcal{M}$ be a Riemannian submersion and $c: (\\tau_0, \\tau_1) \\rightarrow \\mathcal{M}$ a smooth curve on $\\mathcal{M}$ with $\\pullb{c}: (\\tau_0, \\tau_1) \\rightarrow \\pullb{\\mathcal{M}}$ its horizontal lift, i.e., $\\Phi\\circ\\pullb{c} = c$, $d\\Phi\\circ\\dot{\\pullb{c}} = \\dot{c}$ and $\\dot{\\pullb{c}}(\\tau)\\in H_{\\pullb{c}(\\tau)}\\pullb{\\mathcal{M}}$ horizontal (i.e. $\\dot{\\pullb{c}} = \\pullb{\\dot{c}}$). Then\n\t\\begin{enumerate}[label = \\roman*)]\n\t\t\\item \\label{corollary_parallel} a vector-field $W = d \\Phi \\circ \\pullb{W} \\in T \\mathcal{M}$ along $c$ is parallel if and only if\n\t\t\\begin{equation*}\n\t\t\t\\nabla_{\\dot{\\pullb{c}}}^{\\pullb{\\mathcal{M}}} \\pullb{W} = \\frac{1}{2}[\\dot{\\pullb{c}}, \\pullb{W}]^{\\perp}\n\t\t\\end{equation*}\n\t\tfor the horizontal vector-field $\\pullb{W} \\in T\\pullb{\\mathcal{M}}$ along $\\pullb{c}$.\n\t\t\\item \\label{corollary_geodesic}$c$ is a geodesic if and only if $\\pullb{c}$ is a geodesic.\n\t\\end{enumerate}\n\\end{corollary}\n\nWhile \\ref{corollary_parallel} yields the basis for confirming the parallel transport computation, \\ref{corollary_geodesic} is the underlying fact behind the identification of geodesics in form and shape spaces with geodesics of suitably aligned representatives.\nNote that, while \\citetsup{Huckemann2010intrinsicMANOVA} generally restrict their discussion to finite dimensional manifolds, the theorem does in fact not have this restriction. Based on these preparations, we can now verify the presented parallel transport along the lines of \\citetsup{Huckemann2010intrinsicMANOVA}, but for forms rather than shapes and explicitly based on a separable Hilbert space $\\mathcal{Y}$ rather than on $\\mathds{C}^k$. Note that, while identifying $H_p\\mathcal{Y}^* \\cong T_{[p]}\\mathcal{Y}_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}^*$ and $\\varepsilon \\cong d \\Phi(\\varepsilon)$ in the main manuscript, they are distinguished here for clarity.\n\n\\begin{lemma}\n\t\\label{lemma:formtransport}\n\tLet $p, p' \\in \\mathcal{Y}^*$ with $\\langle p, p' \\rangle \\neq 0$ centered and mutually rotation aligned representatives of forms $[p], [p'] \\in \\mathcal{Y}^*_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}$ (i.e. $p=\\widetilde{p}^{{\\operatorname{Trl} \\dir \\operatorname{Rot}}}$ for notational simplicity and $p'$ accordingly), let $\\varepsilon \\in T_{[p]}\\mathcal{Y}^*_{/\\operatorname{Trl} \\dir \\operatorname{Rot}}$ with horizontal lift $\\tilde{\\varepsilon}\\in H_p \\mathcal{Y}^*$, and let $\\Phi:y \\mapsto [y]$ denote the quotient map. Then  \n\t\\begin{equation}\n\t\t\\label{transport_equation}\n\t\t\\operatorname{Transp}_{[p], [p']}\\left(\\varepsilon\\right) = d\\Phi\\left( \\tilde{\\varepsilon} - \\im{\\langle p'/\\|p'\\|, \\tilde{\\varepsilon}\\rangle} \\frac{p/\\|p\\| + p'/\\|p'\\|}{1+\\langle p/\\|p\\|, p'/\\|p'\\|\\rangle} \\i \\right)\n\t\\end{equation}\n\timplements the form parallel transport via its horizontal lift.\n\t\n\\end{lemma}\n\\begin{proof}\n\tFor $\\langle p, p' \\rangle \\neq 0$ aligned and centered, the unique unit-speed geodesic (uniqueness can be seen using Corollary \\ref{corollary} \\ref{corollary_geodesic}) between $[p]$ and $[p']$ is described by $\\tau \\rightarrow [p + \\tau \\frac{p'-p}{\\|p'-p\\|}]$. Yet, to simplify the argument, we choose a unit-angular speed parameterization instead. It takes the form $c(\\tau) := [\\pullb{c}(\\tau)] :=  [\\rho(\\tau) \\gamma(\\tau)]$ with $\\gamma(\\tau) = \\cos(\\tau) \\beta + \\sin(\\tau) \\beta'$ where $\\beta = \\frac{p}{\\|p\\|}$ and $\\beta' = \\frac{p' - \\langle \\beta, p'\\rangle \\beta}{\\|p' - \\langle \\beta, p'\\rangle \\beta\\|}=\\frac{\\frac{p'}{\\|p'\\|} - \\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle \\frac{p}{\\|p\\|}}{\\|\\frac{p'}{\\|p'\\|} - \\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle \\frac{p}{\\|p\\|}\\|}$ form an orthonormal basis of the real plain containing the horizontal geodesic.\n\tWith $\\widetilde{c}(0) = p$ and $\\widetilde{c}(\\arccos \\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle)=p'$, $\\widetilde{c}(\\tau)$ describes the line connecting $p$ and $p'$ in polar coordinates.\n\t$[\\gamma(\\tau)]_{\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}$ corresponds to the shape geodesic between $[p]_{\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}$ and $[p']_{\\operatorname{Trl} \\dir \\operatorname{Rot} \\dir \\operatorname{Scl}}$, and $\\rho(\\tau) = \\|\\pullb{c}(\\tau)\\|$ reflects the size of the geodesic $c(\\tau)$. An explicit definition of $\\rho(\\tau)$ is not needed.\n\n\t\n\tDue to the alignment of $p$ and $p'$,  $\\dot{\\gamma}(\\tau)$ and also\n\t\\begin{equation}\n\t\t\\label{W}\n\t\t\\pullb{W}(\\tau) := \\tilde{\\varepsilon} + \\im{\\langle \\beta', \\tilde{\\varepsilon}\\rangle} \\left(\\dot{\\gamma}(\\tau) - \\beta' \\right) \\i\n\t\\end{equation}\n\tare horizontal along $\\pullb{c}(\\tau)$, i.e. $\\widetilde{W}(\\tau) \\in H_{\\widetilde{c}(\\tau)} \\mathcal{Y}^*$ for each $\\tau$, if $\\tilde{\\varepsilon}$ is horizontal, i.e. if $\\im{\\langle p, \\tilde{\\varepsilon}\\rangle} = \\langle \\!\\mycal{1}, \\tilde{\\varepsilon} \\rangle =  0$. More concretely, this holds as\n\t\\begin{align*}\n\t\t\\im{\\langle \\pullb{c}(\\tau), \\pullb{W}(\\tau) \\rangle} \n\t\t&= \\rho(\\tau)\\; \\left( \\im{\\langle \\gamma(\\tau), \\tilde{\\varepsilon} \\rangle}  + \\im{\\langle \\beta', \\tilde{\\varepsilon}\\rangle} \\re{\\langle \\gamma(\\tau), \\dot{\\gamma}(\\tau) - \\beta' \\rangle} \\right)\\\\  \n\t\t&\\overset{\\tilde{\\varepsilon} \\text{ horizontal}}{=} \\rho(\\tau)\\; \\left(\\sin(\\tau) \\im{\\langle \\beta', \\tilde{\\varepsilon} \\rangle} + \\im{\\langle \\beta', \\tilde{\\varepsilon}\\rangle} (0 - \\sin(\\tau) \\underbrace{\\|\\beta'\\|^2}_{=1}) \\right) = 0\n\t\\end{align*}\n\tand, obviously, also $\\langle \\!\\mycal{1}, \\pullb{W}(\\tau) \\rangle = 0$ as this is the case for all involved vectors.\n\tMoreover, $\\pullb{W}$ is smooth and $\\tilde{\\varepsilon} \\mapsto \\pullb{W}\\left(\\arccos \\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle\\right)$ yields the transport formulated in Equation \\eqref{transport_equation}, which follows from basic trigonometric relations.\n\tIn detail, it follows from plugging\n\t\\begin{align*}\n\t\t\\dot{\\gamma}\\left(\\arccos \\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle\\right) -\\beta' &= \\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle\\ \\beta' - \\sqrt{1-\\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle^2}\\ \\beta - \\beta'\\\\ \n\t\t&=  \\left(\\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle - 1\\right) \\overbrace{\\frac{\\frac{p'}{\\|p'\\|} - \\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle \\frac{p}{\\|p\\|}}{\\sqrt{1-\\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle^2}}}^{\\beta'}\\  - \\sqrt{1-\\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle^2}\\ \\frac{p}{\\|p\\|}\\\\\n\t\t&=  \\frac{\\left(\\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle - 1\\right) \\frac{p'}{\\|p'\\|}}{\\sqrt{1-\\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle^2}} \\\\\n\t\t&\\qquad +  \\frac{ - \\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle^2 \\frac{p}{\\|p\\|} + \\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle \\frac{p}{\\|p\\|} - \\left(1-\\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle^2\\right) \\frac{p}{\\|p\\|}}{\\sqrt{1-\\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle^2}} \\\\\n\t\t&= \\frac{-\\left(1-\\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle\\right) \\left(\\frac{p'}{\\|p'\\|} +  \\frac{p}{\\|p\\|}\\right)}{\\sqrt{1-\\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle^2}}\n\t\n\t\n\t\n\t\\end{align*}\n\tand \n\t\\begin{align*}\n\t\t\\im{\\langle \\beta', \\tilde{\\varepsilon} \\rangle} \\overset{\\text{$\\tilde{\\varepsilon}$ horizontal}}{=} \\frac{\\im{\\langle p', \\tilde{\\varepsilon}\\rangle}}{\\sqrt{1-\\langle \\frac{p}{\\|p\\|}, \\frac{p'}{\\|p'\\|}\\rangle^2}}\n\t\\end{align*}\n\tinto the definition of $W(\\tau)=d\\Phi (\\widetilde W(\\tau))$ using \\eqref{W}.\\\\\n\n\t\\newcommand{\\diff^\\mathcal{I}}{d^\\mathcal{I}}\n\t\\newcommand{\\diff^\\mathcal{R}}{d^\\mathcal{R}}\n\t\\newcommand{\\partial_\\mathcal{I}}{\\partial_\\mathcal{I}}\n\t\\newcommand{\\partial_\\mathcal{R}}{\\partial_\\mathcal{R}}\n\n\tHence, due to Corollary \\ref{corollary} \\ref{corollary_parallel}, we mainly need to show\n\t\\begin{equation}\n\t\t\\label{targetequation}\n\t\t\\nabla_{\\dot{\\pullb{c}}}^{\\mathcal{Y}^*} \\pullb{W} = \\frac{1}{2} [\\dot{\\pullb{c}}, \\pullb{W}]^\\perp \n\t\\end{equation}\n\twhere the left-hand side may directly be computed as\n\t\\begin{align*}\n\t\t\\nabla_{\\pullb{\\dot{c}}(\\tau)}^{\\mathcal{Y}^*} \\pullb{W}(\\tau) &\\overset{\\eqref{covariant_derivative}}{=} \\dot{\\pullb{W}}(\\tau) \\overset{\\eqref{W}}{=} -\\im{\\langle \\beta', \\tilde{\\varepsilon}\\rangle} \\gamma(\\tau)  \\i\n\t\\end{align*}\n\tsince $\\ddot{\\gamma}(\\tau) = - \\gamma(\\tau)$.\\\\ \n\t\n\tOn the right-hand side, the orthogonal projection of a vector-field $V(\\tau):= V_{\\pullb{c}(\\tau)} \\in T_{\\pullb{c}(\\tau)}\\mathcal{Y}^*$ along $\\pullb{c}(\\tau)$ into the vertical spaces (of which $\\{ \\gamma(\\tau), \\!\\mycal{1}, \\i\\!\\mycal{1}\\}$ constitute an orthonormal basis) is given by\n\t\\begin{align*}\n\t\tV^\\perp(\\tau) &= \\frac{\\re{\\langle \\i\\, \\pullb{c}(\\tau), V(\\tau) \\rangle}}{\\|\\pullb{c}(\\tau)\\|^2}  \\pullb{c}(\\tau) \\i + \\langle \\!\\mycal{1}, V(\\tau) \\rangle\\, \\!\\mycal{1} \\nonumber\\\\\n\t\t&= \\frac{\\omega^{\\operatorname{Rot}}(V(\\tau))}{\\rho(\\tau)} \\gamma(\\tau) \\i  + \\omega^{\\operatorname{Trl}}(V(\\tau))\\, \\!\\mycal{1}\n\t\\end{align*}\n\twith the 1-forms $\\omega^{\\operatorname{Rot}}$ and $\\omega^{\\operatorname{Trl}}$ defined as\n\t\\begin{align*}\n\t\t\\omega^{\\operatorname{Rot}}(V_p) &:= \\re{\\langle \\i\\, p, V_p \\rangle} = \\re{-\\i\\, \\langle p, V_p \\rangle} \\\\ \n\t\t&= \\re{-\\i\\, \\left(\\re{\\langle p, V_p \\rangle}+\\im{\\langle p, V_p \\rangle} \\i\\right)}\\\\\n\t\t&= \\re{-\\i\\, \\re{\\langle p, V_p \\rangle} + \\im{\\langle p, V_p \\rangle}}\\\\\n\t\t&= \\im{\\langle p, V_p \\rangle}.\n\t\n\t\\end{align*}\n\tand $\\omega^{\\operatorname{Trl}}(V_p) = \\langle \\!\\mycal{1}, V_p \\rangle$ for $p\\in\\mathcal{Y}^*$.\n\t\n\tThus, to confirm \\eqref{targetequation} and complete the proof, it remains to show $\\omega^{\\operatorname{Rot}}([\\dot{\\pullb{c}}, \\pullb{W}]) = - 2 \\im{\\langle\\beta', \\tilde{\\varepsilon}\\rangle} \\rho$ and $\\omega^{\\operatorname{Trl}}([\\dot{\\pullb{c}}, \\pullb{W}]) = 0$. For this, we use some statements on the exterior derivative $d\\omega$ of a 1-form $\\omega$ subsumed in the following auxiliary lemma (proven later):  \n\t\n\t\\begin{lemma}\n\t\t\\label{auxiliary}\n\t\tLet $V, W$ be smooth vector-fields.\n\t\t\\begin{enumerate}[label = \\roman*)]\n\t\t\t\\item \\label{auxiliary:lie} For any smooth 1-form $\\omega$ it holds that \n\t\t\n\t\t\t$\\omega\\left([V,W]\\right) = V\\left(\\omega\\left(W\\right)\\right) - W\\left(\\omega\\left(V\\right)\\right) - d \\omega \\left(V,W\\right)$.\n\t\t\t\\item \\label{auxiliary:omega} For $\\omega^{\\operatorname{Rot}}$ defined above, $d\\omega^{\\operatorname{Rot}}(V,W) = 2\\, \\im{\\langle V, W\\rangle}$.\n\t\t\t\\item For $\\omega^{\\operatorname{Trl}}$ defined above, $d\\omega^{\\operatorname{Trl}}(V,W) = 0$.\n\t\t\\end{enumerate}\n\t\\end{lemma}\n\t\n\tUsing further that\n\t\\begin{align*}\n\t\t\\omega^{\\operatorname{Rot}}\\left(\\dot{\\pullb{c}}(\\tau)\\right)\n\t\t&= \\im{\\langle \\gamma(\\tau), \\rho(\\tau)\\, \\dot{\\gamma}(\\tau)\\rangle} + \\im{\\langle \\gamma(\\tau), \\dot{\\rho}(\\tau)\\, \\gamma(\\tau)\\rangle} = 0\n\t\\end{align*}\n\tand  \n\t\\begin{align*}\n\t\t\\omega^{\\operatorname{Rot}}\\left(\\pullb{W}(\\tau)\\right)\n\t\t&= \\rho(\\tau) \\im{\\langle \\gamma(\\tau), \\tilde{\\varepsilon}\\rangle} + \\rho(\\tau) \\im{\\langle\\beta', \\tilde{\\varepsilon}\\rangle}\\underbrace{\\langle \\gamma(\\tau),  \\dot{\\gamma}(\\tau) - \\beta'\\rangle}_{\\in\\mathds{R}}\\\\\n\t\t&= \\rho(\\tau)\\left(\\sin(\\tau) \\im{\\langle \\beta', \\tilde{\\varepsilon}\\rangle} - \\im{\\langle \\beta', \\tilde{\\varepsilon}\\rangle} \\sin(\\tau)\\right) = 0\n\t\\end{align*}\n\t we then have \n\t\\begin{align*}\n\t\t\\omega^{\\operatorname{Rot}}([\\dot{\\pullb{c}}(\\tau), \\pullb{W}(\\tau)]) &= \n\t\t\\underbrace{\\dot{\\pullb{c}}(\\tau) \\left( \\omega^{\\operatorname{Rot}}(\\pullb{W}(\\tau)) \\right)}_{=0} - \n\t\t\\underbrace{\\pullb{W}(\\tau) \\left( \\omega^{\\operatorname{Rot}}(\\dot{\\pullb{c}}(\\tau)) \\right)}_{=0} -\n\t\td\\omega^{\\operatorname{Rot}} \\left(\\dot{\\pullb{c}}(\\tau), \\pullb{W}(\\tau) \\right)\\\\\n\t\t\t&= -2\\,\\big( \\im{\\langle \\rho(\\tau) \\dot{\\gamma}(\\tau), \\pullb{W}(\\tau)\\rangle} + \\underbrace{\\im{\\langle \\dot{\\rho}(\\tau) \\gamma(\\tau), \\pullb{W}(\\tau)\\rangle}}_{=0 \\text{  ($\\pullb{W}$ horizontal, $\\rho$ and $\\dot{\\rho}$ real)}} \\big)\\\\\n\t\t\t&= -2\\rho(\\tau)\\, \\im{\\langle \\cos(\\tau) \\beta' - \\sin(\\tau) \\beta, \\tilde{\\varepsilon}\\rangle + \\langle \\dot{\\gamma}(\\tau), \\im{\\langle\\beta', \\tilde{\\varepsilon}\\rangle}\\left(\\dot{\\gamma}(\\tau) - \\beta'\\right) \\i\\rangle}\\\\\n\t\t\t&= -2\\rho(\\tau)\\, \\big( \\cos(\\tau) \\im{\\langle\\beta', \\tilde{\\varepsilon}\\rangle} +\n\t\t\t\\im{\\langle\\beta', \\tilde{\\varepsilon}\\rangle} \\underbrace{\\langle \\dot{\\gamma}(\\tau), \\dot{\\gamma}(\\tau) - \\beta'\\rangle}_{\\in\\mathds{R}} \\big) \\\\\n\t\t\t&= -2\\rho(\\tau)\\, \\big( \\cos(\\tau) \\im{\\langle\\beta', \\tilde{\\varepsilon}\\rangle} + \\im{\\langle\\beta', \\tilde{\\varepsilon}\\rangle} - \\im{\\langle\\beta', \\tilde{\\varepsilon}\\rangle} \\cos(\\tau) \\big)\\\\ \n\t\t\t&= -2\\rho(\\tau)\\, \\im{\\langle\\beta', \\tilde{\\varepsilon}\\rangle} \n\t\\end{align*}\n\tand\n\t\\begin{align*}\n\t\t\\omega^{\\operatorname{Trl}}([\\dot{\\pullb{c}}(\\tau), \\pullb{W}(\\tau)]) &= \n\t\t\\underset{=\\langle \\!\\mycal{1}, \\pullb{W}\\rangle=0}\n\t\t{\\dot{\\pullb{c}}(\\tau) \\left( \\underbrace{\\omega^{\\operatorname{Trl}}\\left(\\pullb{W}(\\tau)\\right)} \\right)} - \n\t\t\\underset{=\\langle \\!\\mycal{1}, \\dot{\\pullb{c}}\\rangle=0}\n\t\t{\\pullb{W}(\\tau) \\left( \\underbrace{\\omega^{\\operatorname{Trl}}\\left(\\dot{\\pullb{c}}(\\tau)\\right)} \\right)} -\n\t\t\\underbrace{d\\omega^{\\operatorname{Trl}} \\left(\\dot{\\pullb{c}}(\\tau), \\pullb{W}(\\tau) \\right)}_{= 0} = 0,\\\\\n\t\\end{align*}\n\twhere tangent vectors $\\dot{\\pullb{c}}(\\tau)$ and $\\pullb{W}(\\tau)$ are interpreted as directional derivatives. These are the two equations that remained to show.\n\\end{proof}\n\n\\begin{proof}[Proof of Lemma \\ref{auxiliary}]\n\t\\begin{enumerate}[label = \\roman*)]\n\t\t\\item See, e.g., \\citetsup{Lee2018RiemannianManifolds},  Proposition B.12 on page 402. This is a standard result. Note that based on an alternative (yet also common) definition of the wedge product and, hence, the exterior derivative, \\citetsup{Huckemann2010intrinsicMANOVA} and others write $\\omega\\left([V,W]\\right) = V\\left(\\omega\\left(W\\right)\\right) - W\\left(\\omega\\left(V\\right)\\right) - 2\\, d \\omega \\left(V,W\\right)$ instead. In this case, we also have $d\\omega^{\\operatorname{Rot}}(V,W) = \\, \\im{\\langle V, W\\rangle}$ in \\ref{auxiliary:omega} compensating for the different factor in the proof of Lemma \\ref{lemma:formtransport}. \n\t\t\\item Let $\\{e_r\\}_r$ be an orthonormal $\\mathds{C}$-basis of $\\mathcal{Y}$ (a complete orthonormal system existing since $\\mathcal{Y}$ is separable) and $\\{\\vartheta^{(r)}(y)\\}_r = \\langle e_r, y \\rangle$ the corresponding dual basis. The tangent vectors $\\partial_{{\\operatorname{Re}}, r}\\big|_p \\cong e_r$ and $\\partial_{{\\operatorname{Im}}, r}\\big|_p \\cong \\i\\, e_r$, $p\\in\\mathcal{Y}$ together form an $\\mathds{R}$-basis of $T_p\\mathcal{Y}^* \\cong \\mathcal{Y}$. The dual 1-forms are given by $d^{{\\operatorname{Re}}, r}(V_p) := V_p\\left({\\operatorname{Re}} \\circ \\vartheta^{(r)}\\right) \\cong {\\operatorname{Re}} \\circ \\vartheta^{(r)}(V_p)$ and $d^{{\\operatorname{Im}}, r}(V_p) := V_p\\left({\\operatorname{Im}} \\circ \\vartheta^{(r)}\\right) \\cong {\\operatorname{Im}}\\circ\\vartheta^{(r)}(V_p)$ where we identify tangent vectors either with directional derivatives  $V_p(f)=\\frac{d}{d\\tau}\\left( f\\circ\\operatorname{Exp}_{p}(\\tau V_p)\\right)\\big|_{\\tau=0}$ of functions $f:\\mathcal{M}\\rightarrow\\mathds{R}$ or with elements of $\\mathcal{Y}$, and the equality follows from $\\mathcal{M}=\\mathcal{Y}^*$, and ${\\operatorname{Re}}\\circ\\vartheta^{(r)}$, ${\\operatorname{Im}}\\circ\\vartheta^{(r)}$ linear.\n\t\tWith this given, we have \n\t\t\\begin{align}\n\t\t\t\\label{oneform}\n\t\t\t\\omega^{\\operatorname{Rot}}(V_p) &= \\im{\\langle \\sum_r \\langle e_r, p\\rangle e_r, V_p\\rangle}\\\\ \\nonumber\n\t\t\t&= \\sum_r \\im{\\langle p, e_r\\rangle \\langle e_r, V_p\\rangle}\\\\ \\nonumber\n\t\t\t&= \\sum_r \\re{\\langle e_r, p\\rangle} \\im{\\langle e_r, V_p\\rangle} - \\im{\\langle e_r, p\\rangle} \\re{\\langle e_r, V_p\\rangle}\\\\  \\nonumber\n\t\t\t&= \\sum_r {\\operatorname{Re}}\\circ\\vartheta^{(r)}(p)\\, d^{{\\operatorname{Im}}, j}\\left(V_p\\right) - {\\operatorname{Im}}\\circ\\vartheta^{(r)}(p)\\, d^{{\\operatorname{Re}}, j}\\left( V_p\\right)  \n\t\t\\end{align}\n\t\tand thus, expressing the exterior derivative in terms of wedge products \n\t\t\\begin{align*}\n\t\t\td\\omega^{\\operatorname{Rot}} &= \\sum_r \\sum_l \\partial_{{\\operatorname{Re}}, r} \\left({\\operatorname{Re}}\\circ\\vartheta^{(r)}\\right)\\ d^{{\\operatorname{Re}}, l}\\wedged^{{\\operatorname{Im}}, r} + \n\t\t\t\\partial_{{\\operatorname{Im}}, r} \\left({\\operatorname{Re}}\\circ\\vartheta^{(r)}\\right)\\ d^{{\\operatorname{Im}}, l}\\wedged^{{\\operatorname{Im}}, r}\\\\\n\t\t\t&\\quad -\\partial_{{\\operatorname{Re}}, r} \\left({\\operatorname{Im}}\\circ\\vartheta^{(r)}\\right)\\ d^{{\\operatorname{Re}}, l}\\wedged^{{\\operatorname{Re}}, r} - \n\t\t\t\\partial_{{\\operatorname{Im}}, r} \\left({\\operatorname{Im}}\\circ\\vartheta^{(r)}\\right)\\ d^{{\\operatorname{Im}}, l}\\wedged^{{\\operatorname{Re}}, r}\\\\\n\t\t\t&= \\sum_r \\sum_l d^{{\\operatorname{Re}}, l}\\left(\\partial_{{\\operatorname{Re}}, r}\\right) \\ d^{{\\operatorname{Re}}, l}\\wedged^{{\\operatorname{Im}}, r} + \n\t\t\td^{{\\operatorname{Re}}, l}\\left(\\partial_{{\\operatorname{Im}}, r}\\right) d^{{\\operatorname{Im}}, l}\\wedged^{{\\operatorname{Im}}, r}\\\\\n\t\t\t&\\quad -d^{{\\operatorname{Im}}, l}\\left(\\partial_{{\\operatorname{Re}}, r}\\right) d^{{\\operatorname{Re}}, l}\\wedged^{{\\operatorname{Re}}, r} - \n\t\t\td^{{\\operatorname{Im}}, l}\\left(\\partial_{{\\operatorname{Im}}, r}\\right) d^{{\\operatorname{Im}}, l}\\wedged^{{\\operatorname{Re}}, r}\\\\\n\t\t\t&=\\sum_r  d^{{\\operatorname{Re}}, r}\\wedged^{{\\operatorname{Im}}, r} - d^{{\\operatorname{Im}}, r}\\wedged^{{\\operatorname{Re}}, r}\\\\\n\t\t\t&=2 \\sum_r  d^{{\\operatorname{Re}}, r}\\wedged^{{\\operatorname{Im}}, r}\n\t\t\\end{align*}\n\t\twhich evaluates to\n\t\t\\begin{align*}\n\t\t\td\\omega^{\\operatorname{Rot}}(V,W) &= 2 \\sum_r \\Big( d^{{\\operatorname{Re}}, r}\\left(V\\right) d^{{\\operatorname{Im}}, r}\\left(W\\right) - d^{{\\operatorname{Im}}, r}\\left(V\\right) d^{{\\operatorname{Re}}, r}\\left(W\\right) \\Big) \\\\\n\t\t\t&= 2\\, \\im{\\langle V, W\\rangle}\n\t\t\\end{align*}\n\t\twhere the last equation follows from a computation analogous to \\eqref{oneform}.\n\t\t\n\t\t\\item By choosing w.l.o.g. $e_1 = \\!\\mycal{1}$, we obtain\n\t\t\\begin{align*}\n\t\t\t\\omega^{\\operatorname{Trl}}(V) = d^{{\\operatorname{Re}}, 1} + \\i\\, d^{{\\operatorname{Im}}, 1}\n\t\t\\end{align*}\n\t\twhich immediately yields $d\\langle \\!\\mycal{1}, \\cdot \\rangle = 0$, since $d\\,d^{{\\operatorname{Re}}, 1} = d\\,d^{{\\operatorname{Im}}, 1} = 0$.\t \n\t\\end{enumerate}\n\\end{proof}\n\n\n\n\\subsection{Tensor-product factorization} \\label{EYM}\n\n\\newcommand{\\tp}[2]{\\, #1 \\otimes #2\\,}\n\nThe optimality of the proposed tensor-product factorization follows from the Eckart-Young-Mirsky theorem (EYM) which can be found, e.g., in \\citepsup[page 139]{Gentle2007MatrixAlgebra} for matrices and, in more general terms, in \\citepsup[page 111]{Hsing2015TheoreticalFoundations} for Hilbert-Schmidt operators. In the following, we present a tensor-product version of EYM designed for our needs. The optimality of the tensor-product factorization is then illustrated in two corollaries -- first in a theoretical model setting and second for the empirical decomposition on evaluations which can be practically conducted on given data.\nConsider two real vector spaces $\\mathcal{B}_j$, $j \\in \\{0,1\\}$, with positive semi-definite bilinear forms $\\langle\\cdot,\\cdot\\rangle_j: \\mathcal{B}_j \\times \\mathcal{B}_j \\rightarrow \\mathds{R}$ inducing semi-norms $\\|\\cdot\\|_j$. Assuming $\\mathcal{B}_1$ to be, in fact, a function space of functions $f: \\mathcal{X} \\rightarrow \\mathds{R}$ on some set $\\mathcal{X}$, \nthe (vector space) tensor product $\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}$ of $\\mathcal{B}_0$ and $\\mathcal{B}_1$ is the vector space spanned by all $f \\otimes y: \\mathcal{X} \\rightarrow \\mathcal{B}_0, \\mathbf{x} \\mapsto f(\\mathbf{x})\\, y$ with $f\\in\\mathcal{B}_1$ and $y\\in\\mathcal{B}_0$.\nBy linear extension, a symmetric positive semi-definite bilinear form on $\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}$ is defined by $\\langle \\tp{f}{y}, \\tp{f'}{y'} \\rangle_{\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}} = \\langle f, f' \\rangle_{1} \\, \\langle y, y' \\rangle_{0}$ for all $f,f' \\in \\mathcal{B}_1, y,y' \\in \\mathcal{B}_0$. It induces a semi-norm $\\|\\cdot\\|_{\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}}$ on the tensor product space.\n \n\\newcommand{\\rank}{\\operatorname{rank}} \n\n\\begin{theorem}[Eckart-Young-Mirsky for finite-dimensional tensor-products]\n\t\\label{EckartYoungMirsky}\n\tLet $\\mathcal{B}_0, \\mathcal{B}_1$ be semi-normed vector spaces as defined above and $h = \\sum_{r=1}^{m_0}  \\sum_{l=1}^{m_1} \\theta^{(r,l)} \\tp{b_1^{(l)}}{b_0^{(r)}} \\in \\tp{\\mathcal{B}_1}{\\mathcal{B}_0}$ expressed as a finite linear-combination with $b_1^{(1)}, \\dots, b_1^{(m_1)} \\in \\mathcal{B}_1$,  $b_0^{(1)}, \\dots, b_0^{(m_0)} \\in \\mathcal{B}_0$, and coefficient matrix $\\{\\theta^{(r,l)}\\}_{r,l} = \\boldsymbol{\\Theta} \\in \\mathds{R}^{m_0\\times m_1}$. \n\tThen we can optimally decompose $h = \\sum_{r=1}^m d^{(r)} \\tp{\\xi_{1}^{(r)}}{\\xi_{0}^{(r)}}$ with $m = \\min\\{m_0, m_1\\}$, $d^{(1)} \\geq \\dots \\geq d^{(m)} \\geq 0$ and $\\langle \\xi_{j}^{(r)}, \\xi_{j}^{(l)} \\rangle_j = \\mathds{1}(r=l)$ for $\\xi_{j}^{(r)}\\in\\mathcal{B}_j$\n\t, in the sense that for any $L\\leq m$ \n\t\\begin{equation}\n\t\t\\label{optimalapprox}\n\t\t\\| h - \\sum_{r=1}^L d^{(r)} \\tp{\\xi_{1}^{(r)}}{\\xi_{0}^{(r)}} \\|_{\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}} \\ \\leq \\ \\| h - \\sum_{r=1}^L d^{(r)}_\\star \\tp{\\xi_{1\\star}^{(r)}}{\\xi_{0\\star}^{(r)}} \\|_{\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}}\n\t\\end{equation}\n\tfor all $d^{(r)}_\\star\\in \\mathds{R}$ and $ \\xi_{j\\star}^{(r)} \\in \\mathcal{B}_j$, $j\\in\\{0,1\\}$, $r=1,\\dots, L$.\n\tArranging $\\mathbf{D} = \\operatorname{diag}(d^{(1)}, \\dots, d^{(m)})$ and expressing $\\xi_{j}^{(r)} = \\sum_{l} u_{j}^{(l,r)} b_{j}^{(l)}, r=1,\\dots, m,$ with the coefficient matrices $\\{u_{j}^{(l,r)}\\}_{l,r}=\\mathbf{U}_j \\in \\mathds{R}^{m_j \\times m}$, $j\\in\\{0,1\\}$, an optimal decomposition is obtained as follows: \n\t\t \n\t\\begin{enumerate}[label = \\roman*)]\n\t\t\\item If for $j\\in\\{0,1\\}$ the Gram matrices $\\mathbf{G}_j = \\{\\langle b_{j}^{(r)}, b_{j}^{(l)}\\rangle_j\\}_{r,l}$ are the identity $\\mathbf{G}_j = \\mathbf{I}_{m_j}$, the matrices $\\mathbf{D}$ and $\\mathbf{U}_j$, $j\\in\\{0,1\\}$, are directly determined via SVD of the coefficient matrix $\\boldsymbol{\\Theta} = \\mathbf{U}_0 \\mathbf{D} \\mathbf{U}_1^\\top$.\n\t\t\n\t\t\\item In general, there are suitable matrices $\\mathbf{M}_j\\in\\mathds{R}^{\\rank \\mathbf{G}_j \\times m_j}$, $j\\in\\{0,1\\}$, such that $\\boldsymbol{\\Xi} = \\mathbf{V}_0 \\mathbf{D} \\mathbf{V}_1^\\top$ is the SVD of the matrix $\\boldsymbol{\\Xi} = \\mathbf{M}_0\\boldsymbol{\\Theta}\\mathbf{M}_1^{\\top}$ and $\\mathbf{U}_j = \\mathbf{M}_j^{-} \\mathbf{V}_j$ with generalized inverse $\\mathbf{M}_j^- = \\mathbf{M}_j^\\top (\\mathbf{M}_j \\mathbf{M}_j^\\top)^{-1}$. \n\t\t\\begin{enumerate}\n\t\t\t\\item In general, a suitable matrix is given by $\\mathbf{M}_j = \\sqrt{\\mathbf{G}_j}^{\\top}$ with $\\mathbf{G}_j = \\sqrt{\\mathbf{G}_j}\\sqrt{\\mathbf{G}_j}^\\top$ a Cholesky decomposition.\n\t\t\t\n\t\t\t\\item If the $b_{j}^{(r)}$ can be identified with vectors $\\mathbf{b}_{j}^{(r)}\\in \\mathds{R}^{m_j'}$ of some length $m_j'$, arranged as column vectors of a ``design matrix'' $\\mathbf{B}_j \\in \\mathds{R}^{m_j'\\times m_j}$, such that $\\langle b_{j}^{(r)}, b_{j}^{(l)} \\rangle_j = (\\mathbf{b}_{j}^{(r)})^\\top \\mathbf{W}_j \\mathbf{b}_{j}^{(l)}$, with $r,l = 1,\\dots, m_j$, for a symmetric positive definite weight matrix $\\mathbf{W}_j$, we may equivalently set $\\mathbf{M}_j = \\mathbf{R}_j$ based on the QR-decomposition $\\sqrt{\\mathbf{W}_j}^\\top \\mathbf{B}_j = \\mathbf{Q}_j \\mathbf{R}_j$. In this case, design matrices $\\mathbf{E}_j$ of vector representatives $\\boldsymbol{\\xi}_{j}^{(r)}$ for the $\\xi_{j}^{(r)}$ can, alternatively, be obtained as $\\mathbf{E}_j = \\sqrt{\\mathbf{W}_j}^{-\\top}\\mathbf{Q}_j\\mathbf{V}_j$ (where $\\mathbf{W}_j$ is typically diagonal and, hence, $\\sqrt{\\mathbf{W}_j}^{-\\top}$ fast to compute).\n\t\t\\end{enumerate}\n\t\\end{enumerate}\n\\end{theorem}\n\n\\begin{proof}\n\t\\begin{enumerate}[label = \\textit{\\roman*)}]\n\t\t\\item \\label{factorization_orthonormal} For $j\\in\\{0, 1\\}$, denote the column vectors of $\\mathbf{U}_j$ by $\\mathbf{u}_{j}^{(r)}$, $r=1,\\dots, m$, and consider the space of $m_0\\times m_1$ matrices equipped with the inner product $\\langle \\boldsymbol{\\Theta}_1, \\boldsymbol{\\Theta}_2 \\rangle_{F} = \\operatorname{tr}{\\left(\\boldsymbol{\\Theta}_1^\\top \\boldsymbol{\\Theta}_2\\right)}$, for $\\boldsymbol{\\Theta}_1, \\boldsymbol{\\Theta}_2 \\in \\mathds{R}^{m_0\\times m_1}$, inducing the Frobenius norm $\\|\\cdot \\|_F$. \n\t\t\n\t\tThe EYM for matrices \\citepsup[e.g.][page 139]{Gentle2007MatrixAlgebra} states that the matrix $\\boldsymbol{\\Theta}_L = \\sum_{r=1}^{L} d^{(r)} \\mathbf{u}_{0}^{(r)} (\\mathbf{u}_{1}^{(r)})^\\top$ is the best rank $L$ approximation of $\\boldsymbol{\\Theta}$, in the sense that\n\t\t$$\n\t\t\\| \\boldsymbol{\\Theta} - \\boldsymbol{\\Theta}_L \\|_F \\leq \\| \\boldsymbol{\\Theta} - \\sum_{r=1}^L d_\\star^{(r)} \\mathbf{u}_{0\\star}^{(r)} (\\mathbf{u}_{1\\star}^{(r)})^\\top\\|_F \\text{ for any } d_\\star^{(r)} \\in \\mathds{R}, \\mathbf{u}_{j\\star}^{(r)} \\in \\mathds{R}^{m_j}, r = 1, \\dots, m.\n\t\t$$\n\t\t\n\t\tTo apply the theorem, we point out that, provided the Gram matrices $\\mathbf{G}_j=\\mathbf{I}_{m_j}$, the $\\{b_{j}^{(r)}\\}_{r=1,\\dots,m_j}$ and, thus, also $\\{\\tp{b_1^{(r)}}{b_0^{(l)}}\\}_{r,l}$ are orthonormal bases of finite-dimensional subspaces $\\mathcal{A}_j \\subset \\mathcal{B}_j$ and $\\tp{\\mathcal{A}_1}{\\mathcal{A}_0} \\subset \\tp{\\mathcal{B}_1}{\\mathcal{B}_0}$, respectively, forming Hilbert spaces. \n\t\tHence, the basis representation map sending $\\xi_j^{(r)} \\mapsto \\mathbf{u}_j^{(r)}$ to its coefficient vector w.r.t.\\ $\\{b_{j}^{(r)}\\}_r$ presents an isometric isomorphism from $\\mathcal{A}_j$ to $\\mathds{R}^{m_j}$.\n\t\tAccordingly, the basis representation $\\tp{\\mathcal{A}_1}{\\mathcal{A}_0} \\rightarrow \\mathds{R}^{m_0 \\times m_1}$, $h \\mapsto \\boldsymbol{\\Theta}$ presents an isometric isomorphism identifying $\\tp{\\xi_1^{(r)}}{\\xi_0^{(l)}}$ with $\\mathbf{u}_{0}^{(l)} (\\mathbf{u}_{1}^{(r)})^\\top$. The isometry follows from $\\langle h_1, h_2 \\rangle = \\sum_{r, l, r', l'} \\theta_1^{(r,l)} \\theta_2^{(r',l')} \\langle \\tp{b_1^{(r)}}{b_0^{(l)}} , \\tp{b_1^{(r')}}{b_0^{(l')}} \\rangle = \\sum_{r, l} \\theta_1^{(r,l)} \\theta_2^{(r,l)} = \\operatorname{tr}{\\left(\\boldsymbol{\\Theta}_1^\\top\\boldsymbol{\\Theta}_2\\right)}$ for basis representations $h_1 \\mapsto \\boldsymbol{\\Theta}_1$ and $h_2 \\mapsto \\boldsymbol{\\Theta}_2$.\n\t\tThis lets us carry over the EYM for matrices to $\\tp{\\mathcal{A}_1}{\\mathcal{A}_0}$ yielding the desired inequality \\eqref{optimalapprox} restricted to $\\xi_{j\\star}^{(r)}\\in \\mathcal{A}_j \\subset \\mathcal{B}_j$. The property $d_1\\geq \\dots \\geq d_m \\geq 0$ and orthonormality of the $\\xi_j^{(r)}$ are also inherited from the SVD.\n\t\t\n\t\tMoreover, we can project any $\\xi_{j\\star}^{(r)} \\in \\mathcal{B}_j$ as $\\xi_{j\\parallel}^{(r)} = \\sum_l \\langle b_j^{(l)}, \\xi_{j\\star}^{(r)} \\rangle_j\\, b_j^{(l)}$ into $\\mathcal{A}_j$ and define $\\xi_{j\\perp}^{(r)} = \\xi_{j\\star}^{(r)} - \\xi_{j\\parallel}^{(r)}$, which yields an analogous decomposition $h_\\star =  \\sum_{r=1}^L d^{(r)}_\\star \\tp{\\xi_{1\\star}^{(r)}}{\\xi_{0\\star}^{(r)}} = h_\\parallel + h_\\perp$ with $h_\\parallel\\in\\tp{\\mathcal{A}_1}{\\mathcal{A}_0}$ and $\\langle h, h_\\perp \\rangle_{\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}} = \\langle h_\\parallel, h_\\perp \\rangle_{\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}} = 0$. Thus, we have $\\| h - h_\\star \\|_{\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}}^2 = \\| h - h_\\parallel \\|_{\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}}^2 + \\|h_\\perp\\|_{\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}}^2 \\geq \\| h - h_\\parallel \\|_{\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}}^2 \\overset{\\text{EYM}}{\\underset{\\text{on} \\tp{\\mathcal{A}_1}{\\mathcal{A}_0}}\\geq} \\| h - \\sum_{r=1}^L d^{(r)} \\tp{\\xi_{1}^{(r)}}{\\xi_{0}^{(r)}} \\|_{\\tp{\\mathcal{B}_1}{\\mathcal{B}_0}}$, which completes the proof.\n\t\t\n\t\t\\item We represent $b_{j}^{(r)} = \\sum_{l=1} M_{j}^{(l,r)} a_j^{(l)}$ in an orthonormal basis $\\{a_j^{(l)}\\}_l$ of the Hilbert space $\\mathcal{A}_j \\subset \\mathcal{B}_j$ spanned by $\\{b_{j}^{(r)}\\}_r$ as in \\ref{factorization_orthonormal} with the coefficients forming the matrix $\\mathbf{M}_j = \\{M_{j}^{(l,r)}\\}_{l,r}$, for $j\\in\\{0,1\\}$, such that $\\boldsymbol{\\Xi} = \\mathbf{M}_0 \\boldsymbol{\\Theta} \\mathbf{M}_1^{\\top}$ is the coefficient matrix of $h$ w.r.t.\\ $\\{a_{j}^{(l)}\\}_l$. Hence, due to \\ref{factorization_orthonormal}, the matrices $\\boldsymbol{\\Xi} = \\mathbf{V}_0 \\mathbf{D} \\mathbf{V}_1^\\top$ obtained by SVD fulfill the desired properties where the $\\mathbf{V}_j$ are the coefficient matrices of the $\\{\\xi_{j}^{(r)}\\}_r$ w.r.t.\\ $\\{a_{j}^{(r)}\\}_r$. We may set $\\mathbf{U}_j = \\mathbf{M}_j^- \\mathbf{V}_j$ to represent $\\{\\xi_{j}^{(r)}\\}_r$ in the original basis $\\{b_{j}^{(r)}\\}_r$ instead,  since, due to $\\mathbf{M}_j \\mathbf{M}_j^- = \\mathbf{I}_{\\rank \\mathbf{G}_j}$, we have $a_{j}^{(r)} = \\sum_{l=1} M_{j}^{-(l,r)} b_j^{(l)}$ for $\\mathbf{M}^- = \\{M_{j}^{-(l,r)}\\}_{l,r}$.\n\t\t\\begin{enumerate}[label = \\textit{\\alph*)}]\n\t\t\t\\item \\label{factorization_general} Constructing the orthonormal basis $\\{a_{j}^{(r)}\\}_r$ via $a_{j}^{(r)} = \\sum_{l=1} M_{j}^{-(l,r)} b_j^{(l)}$ with $\\mathbf{M}_j^- = \\{M_{j}^{-(l,r)}\\}_{l,r} = \\sqrt{\\mathbf{G}_j}^{\\top-}$ is straight forward yielding \n\t\t\t\\begin{align*}\n\t\t\t\t\\{\\langle a_{j}^{(r)}, a_{j}^{(l)} \\rangle \\}_{r,l} &= \\mathbf{M}_j^{-\\top} \\mathbf{G}_j \\mathbf{M}_j^{-}\\\\\n\t\t\t &=\\left(\\sqrt{\\mathbf{G}_j}^\\top\\sqrt{\\mathbf{G}_j}\\right)^{-1} \\sqrt{\\mathbf{G}_j}^\\top \n\t\t\t\\sqrt{\\mathbf{G}_j} \\sqrt{\\mathbf{G}_j}^{\\top}\n\t\t\t\\sqrt{\\mathbf{G}_j} \\left(\\sqrt{\\mathbf{G}_j}^\\top\\sqrt{\\mathbf{G}_j}\\right)^{-1}\\\\ \n\t\t\t&= \\mathbf{I}_{\\rank \\mathbf{G}_j}.\n\t\t\t\\end{align*}\n\t\t\t\\item As in this case, $\\mathbf{G}_j = \\mathbf{B}_j^\\top \\sqrt{\\mathbf{W}}_j \\sqrt{\\mathbf{W}}_j^\\top \\mathbf{B}_j = \\mathbf{R}_j^\\top \\mathbf{Q}_j^\\top\\mathbf{Q}_j \\mathbf{R}_j \\overset{\\mathbf{Q}_j \\text{ orthogonal}}{=} \\mathbf{R}_j^\\top \\mathbf{R}_j$ the choice $\\mathbf{M}_j = \\mathbf{R}_j$ is equivalent to \\ref{factorization_general}. Accordingly, $\\mathbf{U}_j = \\mathbf{R}_j^{-} \\mathbf{V}_j$ and thus $\\mathbf{E}_j = \\mathbf{B}_j \\mathbf{U}_j = \\mathbf{B}_j \\mathbf{R}_j^{-} \\mathbf{V}_j = \\sqrt{\\mathbf{W}}^{-\\top}\\mathbf{Q}_j \\mathbf{V}_j$. \n\t\t\\end{enumerate}\n\t\\end{enumerate}\n\\end{proof}\n\n\n\\begin{corollary}[Tensor-product factorization]\n\tLet $\\{b_0^{{(r)}}\\}_{r=1,\\dots,m_0}$ elements of a Hilbert space $\\mathcal{Y}$ with norm $\\|\\cdot\\|$ and $b_{1}^{(l)} \\in \\mathcal{L}^2(\\mathcal{X})=\\{f\\!:\\!\\mathcal{X} \\rightarrow \\mathds{R}\\, : f\\!\\circ\\!\\mathbf{X} \\text{ measurable, } \\mathds{E}\\left(\\|f(\\mathbf{X})\\|^2\\right)<\\infty\\}$, $l=1,\\dots, m_1$, square-integrable functions of a random covariate vector $\\mathbf{X}$ taking values in $\\mathcal{X}$. Let further $h(\\mathbf{x}) = \\sum_{r=1}^{m_0}\\sum_{l=1}^{m_1} \\theta^{(r,l)} \\, b_{1}^{(l)}(\\mathbf{x}) \\, b_{0}^{(r)}$ for $\\mathbf{x} \\in \\mathcal{X}$.\n\tThen we can optimally decompose $h(\\mathbf{x}) = \\sum_{r=1}^m h^{(r)}(\\mathbf{x})\\, \\xi^{(r)}$ with $m = \\min\\{m_0, m_1\\}$, $\\xi^{(1)}, \\dots, \\xi^{(m)}$ orthonormal and $h^{(r)}\\in\\mathcal{L}^2(\\mathcal{X})$ with $\\mathds{E}\\left(h^{(1)}(\\mathbf{X})^2\\right)\\geq\\dots\\geq \\mathds{E}\\left(h^{(m)}(\\mathbf{X})^2\\right)$,  in the sense that for any $L\\leq m$ \n\t$$\n\t\\mathds{E}\\left( \\| h(X) - \\sum_{r=1}^L h^{(r)}(X)\\, \\xi^{(r)} \\|^2 \\right) \\leq \\mathds{E}\\left( \\| h(X) - \\sum_{r=1}^L h^{(r)}_\\star(X)\\, \\xi^{(r)}_\\star \\|^2 \\right),\n\t$$\n\tfor any other $\\xi^{(r)}_\\star \\in \\mathcal{Y}$ and $h^{(r)}_\\star \\in \\mathcal{L}^2(\\mathcal{X})$, $r=1,\\dots, L$.\n\tAn optimal decomposition is obtained by specifying $\\xi^{(r)} = \\xi_0^{(r)}$ and $h^{(r)} = d^{(r)}\\, \\xi_1^{(r)}$ as in Theorem \\ref{EckartYoungMirsky} with $\\langle \\cdot, \\cdot \\rangle_0=\\langle \\cdot, \\cdot \\rangle$ the inner product of $\\mathcal{Y}$ and $\\langle f, f' \\rangle_1 = \\mathds{E}\\left( f(X) \\, f'(X)\\right)$ for $f, f'\\in\\mathcal{L}^2(\\mathcal{X})$.\n\\end{corollary}\n\n\\begin{proof}\n\tAfter applying Theorem \\ref{EckartYoungMirsky}, it remains to check that $\\|h\\|_{\\tp{\\mathcal{L}^2(\\mathcal{X})}{\\mathcal{Y}}}^2 = \\mathds{E}\\left(\\|h(X)\\|^2\\right)$. Indeed, this holds for all simple $h = \\tp{f}{y}$, since\n\t$$\n\t\t\\langle \\tp{y}{f}, \\tp{y'}{f'} \\rangle_{\\tp{\\mathcal{L}^2(\\mathcal{X})}{\\mathcal{Y}}} = \\langle y, y' \\rangle \\, \\mathds{E}\\left( f(X) \\, f'(X)\\right) = \\mathds{E}\\left( \\langle f(X)\\, y, f'(X)\\, y' \\rangle \\right)\n\t$$\n\tfor any $y,y'\\in\\mathcal{Y}$ and $f,f'\\in\\mathcal{L}^2(\\mathcal{X})$, and, therefore, carries over to all $h \\in \\tp{\\mathcal{L}^2(\\mathcal{X})}{\\mathcal{Y}}$ in the vector space.\n\\end{proof}\n\n\n\n\\newcommand{\\ddot{\\imath}}{\\ddot{\\imath}}\n\n\\begin{corollary}[Tensor-product factorization, empirical version]\n\\label{corollary:TPfactorization_empirical}\n\tLet $\\mathcal{F}(\\mathcal{X}, \\mathds{R})$ and $\\mathcal{F}(\\mathcal{T}, \\mathds{C})$ denote the sets of functions $\\mathcal{X} \\rightarrow\\mathds{R}$ and $\\mathcal{T} \\rightarrow \\mathds{C}$, respectively, which are both considered real vector spaces.\n\tLet $b_{0}^{(r)}\\in\\mathcal{F}(\\mathcal{T}, \\mathds{C})$, $r=1,\\dots, m_0$, and $b_{1}^{(l)}\\in\\mathcal{F}(\\mathcal{X}, \\mathds{R})$, $l=1,\\dots, m_1$. \n\tConsider $h(\\mathbf{x})(t) = \\sum_{r=1}^{m_0}\\sum_{l=1}^{m_1} \\theta^{(r,l)} \\, b_{1}^{(l)}(\\mathbf{x}) \\, b_{0}^{(r)}(t)$ for $\\mathbf{x} \\in \\mathcal{X}$, $t \\in \\mathcal{T}$ evaluated, for $i=1,\\dots,n$, at $\\mathbf{x}_i \\in \\mathcal{X}$ and $t_{i,\\iota} \\in \\mathcal{T}, \\iota= 1,\\dots,k_i$. \n\tThen we can decompose $h(\\mathbf{x}) = \\sum_{r=1}^m h^{(r)}(\\mathbf{x})\\, \\xi^{(r)}$ with $m = \\min\\{m_0, m_1\\}$ optimally, in the sense that for any $L\\leq m$ and any other functions $\\xi^{(r)}_\\star: \\mathcal{T} \\rightarrow \\mathds{C}$ and $h^{(r)}_\\star: \\mathcal{X} \\rightarrow \\mathds{R}$, $r=1,\\dots, L$, \n\t\\begin{align}\n\t\\label{tensorfactorization_empirical}\n\t    \\sum_{i=1}^n w_{1i} \\frac{1}{n} \\sum_{\\ddot{\\imath}=1}^n \\sum_{\\iota=1}^{k_i} w_{0\\ddot{\\imath}\\iota} | h(\\mathbf{x}_i)&(t_{\\ddot{\\imath}\\iota}) - \\sum_{r=1}^L h^{(r)}(\\mathbf{x}_i)\\, \\xi^{(r)}(t_{\\ddot{\\imath}\\iota}) |^2\\nonumber\\\\ \n\t    &\\leq \\\\\n\t    \\sum_{i=1}^n w_{1i} \\frac{1}{n} \\sum_{\\ddot{\\imath}=1}^n \\sum_{\\iota=1}^{k_i} w_{0\\ddot{\\imath}\\iota} | h(\\mathbf{x}_i)&(t_{\\ddot{\\imath}\\iota}) - \\sum_{r=1}^L h_\\star^{(r)}(\\mathbf{x}_i)\\, \\xi_\\star^{(r)}(t_{\\ddot{\\imath}\\iota}) |^2,\\nonumber    \n\t\\end{align}\n\twith integration/sample weights $w_{0i\\iota} \\geq 0$ and $w_{1i} \\geq 0$. An optimal decomposition is obtained by specifying $\\xi^{(r)} = \\xi_{0}^{(r)}$ and $h^{(r)} = d^{(r)}\\, \\xi_{1}^{(r)}$, $r=1,\\dots, m$, specified as in Theorem \\ref{EckartYoungMirsky} with $\\langle y, y' \\rangle_0 = \\frac{1}{n}\\sum_{\\ddot{\\imath}=1}^n \\sum_{\\iota=1}^{k_i} w_{0\\ddot{\\imath}\\iota} \\re{y^\\dagger(t_{\\ddot{\\imath}\\iota}) y'(t_{\\ddot{\\imath}\\iota})}$ for $y, y'\\in\\mathcal{F}(\\mathcal{T}, \\mathds{C})$ and $\\langle f, f' \\rangle_1 = \\sum_{i=1}^n  w_{1i} f(x_i) f'(x_i)$ for $f, f'\\in\\mathcal{F}(\\mathcal{X}, \\mathds{R})$.\n\\end{corollary}\n\n\\begin{proof}\n\tAgain, we confirm $\\|h\\|_{\\tp{\\mathcal{F}(\\mathcal{X}, \\mathds{R})}{\\mathcal{F}(\\mathcal{T}, \\mathds{C})}}^2 = \\sum_{i=1}^n w_{1i} \\sum_{\\iota=1}^{k_i} w_{0\\ddot{\\imath}\\iota} \\left( h(\\mathbf{x}_i)(t_{\\ddot{\\imath}\\iota}) \\right)^2$ by showing\n\t\\begin{align*}\n\t\t\\langle \\tp{y}{f}, \\tp{y'}{f'} \\rangle_{\\tp{\\mathcal{F}(\\mathcal{X}, \\mathds{R})}{\\mathcal{F}(\\mathcal{T}, \\mathds{C})}} \n\t\t\t&= \\langle y, y' \\rangle_0 \\, \\sum_{i=1}^n  w_{1i} f(x_i) f'(x_i) = \\sum_{i=1}^n  w_{1i} \\langle f(x_i) y, f'(x_i) y'\\rangle_1\\\\ \n\t\t\t&= \\sum_{i=1}^n  w_{1i} \\frac{1}{n}\\sum_{\\ddot{\\imath}=1}^n\\sum_{\\iota=1}^{k_i} w_{0\\ddot{\\imath}\\iota} \\re{\\left( f(x_i) y(t_{\\ddot{\\imath}\\iota}) \\right)^\\dagger f'(x_i) y'(t_{\\ddot{\\imath}\\iota})}\n\t\\end{align*}\n\tfor any $y,y'\\in\\mathcal{F}(\\mathcal{T}, \\mathds{C})$ and $f,f'\\in\\mathcal{F}(\\mathcal{X}, \\mathds{R})$.\n\\end{proof}\n\n\\begin{remark}\nFor the regular case with $k_1 = \\dots = k_n =: k$ and for all $\\iota = 1,\\dots,k$ also $t_{i\\iota} = t_{1\\iota} =: t_{\\iota}$ and $w_{0i\\iota} = w_{01\\iota} =: w_{0\\iota}$ equal for all observations $i = 1,\\dots, n$, Inequality \\eqref{tensorfactorization_empirical} simplifies to \n\\begin{align*}\n\t    \\sum_{i=1}^n w_{1i} \\sum_{\\iota=1}^{k_i} w_{0\\iota} | h(\\mathbf{x}_i)&(t_{\\iota}) - \\sum_{r=1}^L h^{(r)}(\\mathbf{x}_i)\\, \\xi^{(r)}(t_{\\iota}) |^2\\nonumber\\\\ \n\t    &\\leq \\\\\n\t    \\sum_{i=1}^n w_{1i} \\sum_{\\iota=1}^{k_i} w_{0\\iota} | h(\\mathbf{x}_i)&(t_{\\iota}) - \\sum_{r=1}^L h_\\star^{(r)}(\\mathbf{x}_i)\\ \\xi_\\star^{(r)}(t_{\\iota}) |^2.\\nonumber    \n\t\\end{align*}\n\\end{remark}\n\n\n\\subsection{Shape differences in astragali of wild and domesticated sheep}\n\\label{sec:bones_appendix}\n\n\\begin{table}[H]\n\t\n\t\\caption{\\label{tab:data-summary}Distribution of covariate levels over the sheep \\new{population}\\old{breed}s in the data set.}\n\t\\centering\n\t\\begin{tabular}[t]{l|r|r|r|r|r|r|r}\n\t\t\\hline\n\t\t\\multicolumn{1}{c|}{ } & \\multicolumn{3}{c|}{Sex} & \\multicolumn{4}{c}{Age\\_group} \\\\\n\t\t\\cline{2-4} \\cline{5-8}\n\t\t& female & male & na & juvenile & subadult & adult & na\\\\\n\t\t\\hline\n\t\tKarakul & 21 & 19 & 1 & 1 & 5 & 35 & 0\\\\\n\t\t\\hline\n\t\tMarsch & 18 & 5 & 0 & 5 & 5 & 13 & 0\\\\\n\t\t\\hline\n\t\tSoay & 21 & 25 & 12 & 7 & 8 & 13 & 30\\\\\n\t\t\\hline\n\t\tWild\\_sheep & 21 & 20 & 0 & 5 & 18 & 14 & 4\\\\\n\t\t\\hline\n\t\\end{tabular}\n\\end{table}\n\n\n\\begin{table}[H]\n\t\\centering\n\t\\begin{tabular}[t]{l|r|r|r|r|r|r}\n\t\t\\hline\n\t\t\\multicolumn{1}{c|}{ } & \\multicolumn{3}{c|}{Mobility} & \\multicolumn{3}{c}{Status} \\\\\n\t\t\\cline{2-4} \\cline{5-7}\n\t\t& confined & pastured & free & domestic & feral & wild\\\\\n\t\t\\hline\n\t\tKarakul & 31 & 10 & 0 & 41 & 0 & 0\\\\\n\t\t\\hline\n\t\tMarsch & 23 & 0 & 0 & 23 & 0 & 0\\\\\n\t\t\\hline\n\t\tSoay & 0 & 0 & 58 & 0 & 58 & 0\\\\\n\t\t\\hline\n\t\tWild\\_sheep & 0 & 0 & 41 & 0 & 0 & 41\\\\\n\t\t\\hline\n\t\\end{tabular}\n\\end{table}\n\n\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=0.9\\linewidth]{figure/sheep_dataexamples}\n\t\\caption[Sheep bone examples]{Six example sheep astragalus shape configurations consisting of landmarks \\textit{(blue dots)} and semi-landmarks describing two outline curves \\textit{(black dots)} recorded in male Karakul and wild sheep of different age. Points are weighted such that the total weight of each curve corresponds to three landmarks \\textit{(weights reflected in point-size)}. Shapes are depicted aligned to their overall mean shape \\textit{(grey circles)}.}\n\t\\label{fig:sheepdataexamples}\n\\end{figure}\n\n\n\n\n\\subsection{Cellular Potts model parameter effects on cell form}\n\nIn the graphics below, the CPM parameters are abbreviated as\n\\begin{itemize}\n\t\\item[\\textsf{b}:] bulk stiffness $x_{i1} \\in [0.003, 0.015]$\n\t\\item[\\textsf{m}:] membrane stiffness $x_{i2} \\in [0.001, 0.015]$\n\t\\item[\\textsf{a}:] substrate adhesion $x_{i3} \\in [30, 70]$\n\t\\item[\\textsf{r}:] signaling radius $x_{i4} \\in [5, 40]$ \n\\end{itemize}\n\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=0.9\\linewidth]{figure/cell_cov_factorized_varimp}\n\t\\caption[cells: factoriced effect varimp]{\n\tTensor-product effect factorization: Predictor variance share explained\nby each effect direction \\textit{(separated by vertical lines)} relative to the total predictor variance of the effects of\neach covariate \\textit{(left)} and of the overall model \\textit{(right)}. \nLinear effect components are presented together with the respective nonlinear effects of a covariate -- they point, however, in individual directions. Interaction effects are listed separately. We observe that for many covariates the nonlinear\neffect is already almost entirely captured by its first component.}\n\t\\label{fig:cellcovfactorizedvarimp}\n\\end{figure}\n\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=0.9\\linewidth]{figure/cell_model_factorized_varimp}\n\t\\caption[cells: factorized model varimp]{\n\tTensor-product model factorization: Predictor variance shares into the first three directions \\textit{(dashed vertical lines)} resulting from joint model factorization (unlike individual factorization of effects in Figure \\ref{fig:cellcovfactorizedvarimp}).\n\tHorizontal bars reflect the variance of the single covariate effects within each model predictor component. They roughly -- but due to potential correlation not precisely -- add up to the predictor component variance shares. \n\t} \n\t\\label{fig:cellmodelfactorizedvarimp}\n\\end{figure}\n\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=0.6\\linewidth]{figure/cell_dataexamples}\n\t\n\t\\vspace{-10pt}\n\t\\rule{0.6\\linewidth}{1pt}\n\t\\vspace{10pt}\n\t\n\t\\includegraphics[width=0.6\\linewidth, page=2]{figure/cell_dataexamples}\n\t\\caption[Cell data examples]{\n\t\t\\textit{Top:}  Example cell outline \\textit{(black)}, one randomly selected out of 33 for each of six different CPM parameter (covariate) configurations chosen for visualization, aligned to the overall mean form \\textit{(grey)}. Note that while panel scales are individually adjusted for better visibility, contrasting plotted forms with the overall mean, which is equal in all plots, also allows to compare their sizes across panels. Headers show parameter deviations from a standard configuration with $b = 0.009$, $m = 0.003$, $a = 50$ and $r = 20$. Dashed lines indicate point correspondences. Cell outlines are oriented as cells migrating rightwards and not connected between $y(0)$ and the point left of it (while outlines are modeled as closed forms in the model).\n\t\t\\textit{Bottom:} Predictions for the corresponding mean form  of our cell form model described in Section \\ref{sec_cells}.}\n\t\\label{fig:celldataexamples}\n\\end{figure}\n\n\n\n\n\n\n\\subsection{Realistic shape and form simulation studies}\n\\label{sec:bottles_appendix}\n\n\\subsubsection{Sampling of response observations}\n\nResponse curves are generated separately for the shape and form scenario as follows: we obtain the true underlying models by fitting original beer and whisky bottles and 3D rotated versions of them, four successively rotated towards the viewer and four away from the viewer, and compute transported residuals $\\epsilon_i$ of a total of $N=360$ bottle outlines $y_1, \\dots, y_N$ (20 whisky and 20 beer brands, each from 9 different angles $z_1$). \nFor each simulated dataset, a sample of the desired size $n$ is randomly drawn (with replacement) from the model residuals $\\epsilon_1, \\dots, \\epsilon_N$. To obtain irregular data with an average grid length $k=\\frac{1}{n}\\sum_{i=1}^n k_i$, we subsample the original evaluations $\\epsilon_i(t_{i1}), \\dots, \\epsilon_i(t_{iK_i})$, with original grid sizes $K_i \\geq 123$, in two steps: first we randomly pick three evaluations as minimal sample size; then we draw evaluations independently with $\\frac{k-3}{K_i-3}$ probability to enter the dataset. \nTo preserve the original covariate distribution of the data, covariates are not randomly picked but we select batches of 9 beer and 9 whisky bottles with $z_1 \\in [-60, 60]$ as in the original dataset. Sample sizes $n$ are, therefore, multiples of 18. \nWith the conditional means $[\\mu_i]$ determined by the covariates, the evaluated residuals $\\epsilon_i$ (on $k_i$ points) are parallel transported to $\\varepsilon_{[\\mu_i], i}\\in T_{[\\mu_i]}\\mathcal{Y}^*_{i/G}$, into the tangent space of the true conditional mean, to generate the simulated shape/form dataset $[y_i] = \\operatorname{Exp}_{[\\mu_i]}(\\varepsilon_{[\\mu_i], i})$, $i=1, \\dots, n$. \n\n\\subsubsection{Simulation results}\n\nIn order to systematically and efficiently assess model behavior, we vary key aspects of the model setup and compare fitting performance in selected settings. \nHere, we list the different aspects and how they are referred to in subsequent graphical visualizations:\n\\begin{itemize}\n\t\\item \\textsf{Scenario}: Shape or form responses.\n\t\\item \\textsf{Sample size} $n$ of curves and mean \\textsf{grid size} $k$ that curves are evaluated on. \n\t\\item \\textsf{Setting}: simulations adjusted in an additional aspect compared to a \\textsf{default} setup\n\t\\begin{itemize}[leftmargin=1.5in]\n\t\t\\item[\\textsf{equal weight}:] Constant inner product weights $w_{i\\iota} = \\frac{1}{k_i}$, $\\iota = 1,\\dots, k_i$, are utilized for curve evaluations $y_i(t_{i1}), \\dots, y_i(t_{ik_i})$ instead of trapezoidal rule weights (\\textsf{default}).\n\t\t\\item[\\textsf{no nuisance}:] No constant and smooth nuisance effects $h_0$ and $f_2(z_2)$ are included into the model, which are included by \\textsf{default}. \n\t\t\\item[\\texttt{pre-aligned}:] This setting concerns the pre-alignment of the curves $y_1, \\dots, y_n$ representing the forms/shapes in the simulated data. \n\t\tNote, however, that due to alignment to the pole $p$ in the very beginning of the Riemannian $L^2$-Boosting algorithm, all of this only effects the preliminary pole $p_0$ used for estimation of $p$. \n\t\tIn the models fit in the paper, we estimated $p_0$ by using a  functional $L^2$-Boosting algorithm (without any alignment), \n\t\twhich makes sense for typical data where the curves occur roughly aligned. Consequently, this aspect translates to a ``good or worse starting point $p_0$'', which is then replaced by $p$ in the actual model fit.\n\t\tIn \\textsf{pre-aligned} settings, simulated response curves $\\tilde{y}_i = \\operatorname{Exp}_{\\mu_i}(\\varepsilon_{\\mu_i, i})$ are directly used for fitting. \n\t\tIn the \\textsf{default}, by contrast, the model is fit on random representatives of $[y_i]$ to mimic realistic scenarios, where $y_i = \\lambda u \\tilde{y}_i + \\gamma \\in [\\tilde{y}_i]$ with \n\t\t$u = \\exp(\\i \\omega)$, $\\omega \\sim N(0, \\frac{\\pi}{20})$, \n\t\twith $\\gamma = \\sigma_1\\gamma_1 + \\sigma_2^2\\gamma_2 \\i$, $\\gamma_1, \\gamma_2 \\sim N(0, 1)$, $(\\sigma_1^2, \\sigma_2^2) = \\frac{1}{n k} \\sum_{i=1}^n \\sum_{\\iota=1}^{k_i} (\\re{\\tilde{y}_i(t_\\iota)}, \\im{\\tilde{y}_i(t_\\iota)})$, \n\t\tand with $\\lambda = 1$ for forms and $\\lambda \\sim \\operatorname{Gamma}(10^2, 10^{-2})$ for shapes. \n\t\t\n\t\\end{itemize}\n\\end{itemize}\n\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=.8\\linewidth]{figure/simplot_metainfo}\n\t\\caption[Simulations: Meta-Info]{\\textit{Top, left:} Noise-to-signal ratio: distribution of empirical residual variance / predictor variance ratio in all simulations. \\textit{Top, right:} Runtime distribution of model fits and subsequent cross-validations (always running 600 boosting iterations). \\textit{Bottom:} Distribution of stopping iteration $m_{stop}$ selected by 10-fold curve-wise cross-validation for different simulation settings. All plots displayed separately for the shape and form scenario (top and bottom row within sub-panel, respectively).}\n\t\\label{fig:simplotmetainfo}\n\\end{figure}\n\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=6in, page=3]{figure/simplot_prediction}\n\t\\caption[Simulations: Prediction Accuracy]{Accuracy in estimating the unconditional mean (pole) and conditional means (predictions), where the MSE is averaged over the covariate values in the dataset.}\n\t\\label{fig:simplotprediction}\n\\end{figure}\n\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=6in]{figure/simplot_effects}\n\t\\caption[Simulations: Effect estimation accuracy]{Accuracy of estimated effects on tangent space level.}\n\t\\label{fig:simploteffects}\n\\end{figure}\n\n\\newpage\n\\subsection{Coefficient level modeling}\n\\label{sec:coefficientlevel}\n\n\\newcommand{\\check{y}}{\\check{y}}\n\\newcommand{\\bycoef}{\\check{\\mathbf{y}}} \n\nIn the main manuscript, we consider the space of complex valued functions $\\mathcal{Y}$ mostly a vector space over $\\mathds{R}$ and utilize real coefficients to formulate the tensor-product effect structure in Section \\ref{sec_tensorproduct_effects} in corresponding bases.\nIn particular for form tangent spaces, identified with real linear subspaces that do not correspond to complex subspaces, this is useful to implement respective constraints via basis transforms. \nBy contrast, here we represent $h_j(\\mathbf{x}) = \\sum_{r, l} \\vartheta_{j}^{(r,l)} b_j^{(l)}(\\mathbf{x}) b_{0}^{(r)}$ with complex coefficients $\\vartheta_{j}^{(r,l)} \\in \\mathds{C}$, $r=1,\\dots,m_0$, $l=1,\\dots, m_j$, with (possibly all real-valued) basis functions $b_0^{(1)}, \\dots, b_0^{(m_0)}\\in \\mathcal{Y}$  corresponding to the basis used for construction of the tangent space basis $\\{\\partial_r\\}_r$ in Section \\ref{sec_tensorproduct_effects}. This representation lets us illustrate the link between evaluation level and coefficient level modeling of shapes and forms:\\newline\nConsider the case where $y_i \\in \\mathcal{Y}$, $i=1,\\dots, n$, can be expanded as $y_i = \\sum_{r=1}^{m_0} \\check{y}_i^{(r)} b_0^{(r)}$ in the same basis with \ncomplex coefficient vectors $\\bycoef_i = (\\check{y}_i^{(1)}, \\dots, \\check{y}_i^{(m_0)})^\\top \\in \\mathds{C}^{m_0}$, and let also  \nthe pole $[p] = [\\sum_{r=1}^{m_0} \\check{p}^{(r)} b_0^{(r)}]$, $\\check{\\mathbf{p}}_i = (\\check{p}^{(1)}, \\dots, \\check{p}^{(m_0)})^\\top$, be expanded accordingly.  \nWith $\\check{\\!\\mycal{1}} = (\\check{\\!\\mycal{1}}^{(1)}, \\dots, \\check{\\!\\mycal{1}}^{(m_0)})$ the coefficient vector of $\\!\\mycal{1} = \\sum_{r=1}^{m_0} \\check{\\!\\mycal{1}}^{(r)} b_0^{(r)}$ (for B-splines simply $\\check{\\!\\mycal{1}} = \\frac{1}{|\\mathcal{T}|}(1,\\dots, 1)^\\top$), we have $u y_i + \\gamma\\,\\!\\mycal{1} = \\sum_{r=1}^{m_0} (u\\, \\check{y}_i^{(r)} + \\gamma\\,\\check{\\!\\mycal{1}}\\,) b_0^{(r)}$ for $u,\\gamma \\in \\mathds{C}$, such that basis representation yields an isomorphism between shapes/forms $[y]$ of curves and the shapes/forms $[\\bycoef]$ of their coefficients as alternative ``landmarks''. Moreover, when choosing inner products on $\\mathcal{Y}$ and $\\mathds{C}^{m_0}$ such that $y \\rightarrow \\bycoef$ is isometric, it follows that $[y] \\rightarrow [\\bycoef]$ presents an isometric isomorphism. \n\nUnder these assumptions, modeling the mean shape/form $[\\check{\\boldsymbol{\\mu}}]= \\operatorname{Exp}_{[\\check{\\mathbf{p}}]}\\left(\\check{\\mathbf{h}}(\\mathbf{x})\\right)$ of the coefficients $\\bycoef_i$, with predictor $\\check{\\mathbf{h}}(\\mathbf{x})=\\sum_{j = 1}^J \\check{\\mathbf{h}}_j(\\mathbf{x}) \\in \\mathds{C}^{m_0}$ and $\\check{\\boldsymbol{\\mu}}_i = (\\check{\\mu}_i^{(1)}, \\dots, \\check{\\mu}_i^{(m_0)})^\\top \\in \\mathds{C}^{m_0}$, is equivalent to our presented model on the original level of curves, if coefficient level effects $\\check{\\mathbf{h}}_j(\\mathbf{x}) = \\sum_{r,l} \\vartheta_j^{(r,l)} b_j^{(l)}(\\mathbf{x}) \\mathbf{e}_r$ are specified with the canonical basis $\\mathbf{e}_r = \\left(\\mathds{1}\\left(r = 1\\right), \\dots, \\mathds{1}\\left(r = m_0\\right)\\right)^\\top$, since  \n$[\\mu] = [\\sum_{r=1}^{m_0} \\check{\\mu}^{(r)} b_0^{(r)}] \\overset{(*)}= \\operatorname{Exp}_{[\\sum_{r=1}^{m_0} \\check{p}^{(r)} b_0^{(r)}]}\\left(\\sum_{r=1}^{m_0} \\check{h}^{(r)}(\\mathbf{x}) b_0^{(r)} \\right) = \\operatorname{Exp}_{[p]}\\left(h(\\mathbf{x})\\right)$ with $\\check{\\mathbf{h}}(\\mathbf{x}) = (\\check{h}^{(1)}(\\mathbf{x}), \\dots, \\check{h}^{(m_0)}(\\mathbf{x}))^\\top$. For shapes, equality $(*)$ follows from \n\t\\begin{align*}\n\t\t\\operatorname{Exp}_{\\sum_{r=1}^{m_0} \\check{p}^{(r)} b_0^{(r)}}\\left(\\sum_{r=1}^{m_0} \\check{h}^{(r)}(\\mathbf{x}) b_0^{(r)} \\right) &= \n\t \\cos(\\|\\check{\\mathbf{h}}(\\mathbf{x})\\|) \\sum_{r=1}^{m_0} \\check{p}^{(r)} b_0^{(r)} + \\sin(\\|\\check{\\mathbf{h}}(\\mathbf{x})\\|)  \\frac{\\sum_{r=1}^{m_0}\\check{h}^{(r)}(\\mathbf{x})b_0^{(r)}}{\\|\\check{\\mathbf{h}}(\\mathbf{x})\\|}  \\\\ \n\t\t&= \n\t\\sum_{r=1}^{m_0} \\left(\\cos(\\|\\check{\\mathbf{h}}(\\mathbf{x})\\|) \\check{p}^{(r)} + \\sin(\\|\\check{\\mathbf{h}}(\\mathbf{x})\\|) \\frac{\\check{h}^{(r)}(\\mathbf{x})}{\\|\\check{\\mathbf{h}}(\\mathbf{x})\\|} \\right) b_0^{(r)}\\\\ \n\t\t&= \\sum_{r=1}^{m_0} \\mathbf{e}_r^\\top \\operatorname{Exp}_{\\check{\\mathbf{p}}}\\left(\\check{\\mathbf{h}}(\\mathbf{x}) \\right) b_0^{(r)} = \\sum_{r=1}^{m_0}\\check{\\mu}^{(r)} b_0^{(r)}\n\t\\end{align*}\nwhere $\\operatorname{Exp}$ is the exponential map on the sphere (first on the function space and then on the coefficient level), we use that due to the isometry $\\|\\check{\\mathbf{h}}(\\mathbf{x})\\| = \\|\\sum_{r=1}^{m_0} \\check{h}^{(r)}(\\mathbf{x}) b_0^{(r)}\\|$ and, we assume w.l.o.g. $\\|p\\| = \\|\\check{\\mathbf{p}}\\| = 1$ and $\\langle p, \\!\\mycal{1}\\rangle = \\sum_r \\check{p}^{(r)} = 0$. Accordingly for forms.\n\nHowever, the expansion $y_i \\approx \\sum_{r=1}^{m_0} \\check{y}_i^{(r)} b_0^{(r)}$ is typically only approximate. In terms of the inner product, $\\langle y_i , y_i' \\rangle_i^0 = \\bycoef_i^{\\dagger} \\check{\\mathbf{W}} \\bycoef_i'$, with $\\check{\\mathbf{W}}$ the Gramian matrix of $\\{b_0^{(r)}\\}_r$, presents an alternative empirical substitute for the inner product $\\langle y_i, y_i' \\rangle$ of curves $y_i, y_i'\\in\\mathcal{Y}$, which is computed on the coefficients instead of $\\langle y_i , y_i' \\rangle_i = \\conj{\\mathbf{y}}_i \\mathbf{W}_i \\mathbf{y}'_i$ computed on evaluation vectors $\\mathbf{y}_i = (y_i(t_{i1}), \\dots, y_i(t_{i k_i}))^\\top,\\mathbf{y}'_i = (y_i'(t_{i1}), \\dots, y_i'(t_{i k_i}))^\\top$ as suggested in Section \\ref{chap_diffgeo}.\nWhen, for dense grids, it can be assumed that both $\\langle y_i , y_i' \\rangle_i^0 \\approx \\langle y_i , y_i' \\rangle_i \\approx \\langle y_i, y_i' \\rangle$ approximate the inner product on the level of curves well, the approach based on the coefficients $\\bycoef_i$ may be computationally preferable, guaranteeing regular and typically more sparse representations that necessitate operations on smaller design matrices (in particular when utilizing the linear array framework \\citepsup{BrockhausGreven2015}). By contrast, in comparably sparse irregular scenarios, expanding single observed $y_i$ in a basis in a first step might involve unwanted pre-smoothing. To give a consistent presentation on the original level of curves, we rely on an evaluation based approach in all applications presented in the main manuscript.\n\n\n\n\\new{\n\t\\subsection{Functional Principal Component Representation}\n\t\\label{sec::FPC}\n\t\n\tVarious approaches in the literature \\citepsup[e.g.,][]{MullerYao2008FAM, Scheipl2015, cederbaum2015functional, Volkmann2020MultiFAMM} have employed \n\tfunctional principal component (FPC) basis representations for modeling functional responses in regression models.\n\tIn combination with covariance smoothing \\citepsup[e.g.,][]{Yao:etal:2005, cederbaum2018fast} \n\tthis can be particularly useful in sparse/irregular scenarios, allowing to estimate the functional covariance structure from single curve evaluations. \n\tIn fact, two variants of corresponding approaches directly fit into our proposed framework, either a) representing  curves using predicted FPC scores or b) estimating inner products based on the covariance structure. \n\tIn the following, we outline both approaches and briefly discuss related perspectives beyond the scope of this paper. \n\t\n\tPrediction of FPC scores and inner products are carried out along the lines of \\citetsup{Yao:etal:2005} and, in the complex case, \\citetsup{stoecker2022efp}. \n\tAssume we have given (an estimate of) the complex covariance surface $C(s,t) = \\mathbb{E}\\left(Y^\\dagger(s) Y(t)\\right)$ of the process $Y$ generating the curve samples $y_1, \\dots, y_n$ in the data, with point-wise mean $\\mathbb{E}(Y(t)) = 0$ for all $t\\in\\mathcal{T}$ without loss of generality in the following. Under standard assumptions, this yields\n\ta (truncated) FPC basis $b_0^{(r)}: \\mathcal{T} \\rightarrow \\mathbb{C}$, $r = 1,\\dots,m_0$ with respective eigenvalues $\\lambda_1 \\geq \\dots \\geq \\lambda_{m_0} \\geq 0$.\n\tObserving only evaluation vectors $\\mathbf{y}_i = (y_{i1}, \\dots, y_{ik_i})^\\top = (y_i(t_{i1}) + \\epsilon_{i1}, \\dots, y_i(t_{ik_i}) + \\epsilon_{ik_i})^\\top$ at time-points $t_{i1}, \\dots, t_{ik_i} \\in \\mathcal{T}$ subject to some iid. white noise measurement errors $\\epsilon_{i1},\\dots, \\epsilon_{ik_i} \\sim N(0, \\sigma^2)$, predicted FPC score vectors $\\bycoef_{i}$, comprising predicted basis coefficients of $y_i$ expanded in the basis $\\{b_0^{(r)}\\}_r$, can be obtained via the conditional expectations\n\t\\begin{equation}\n\t\t\\label{FPCscore}\n\t\t\\bycoef_{i} = \\mathbb{E}\\left( (\\langle b_0^{(1)}, Y \\rangle, \\dots, \\langle b_0^{(m_0)}, Y \\rangle )^\\top \\mid \\mathbf{Y}_i + \\mathbf{\\epsilon}_i = \\mathbf{y}_i \\right) = \\mathbf{\\Lambda} \\mathbf{B}^\\dagger_i \\mathbf{\\Sigma}_i^{-1} \\mathbf{y}_i\n\t\\end{equation}\n\tunder a working normality assumption, with matrices $\\mathbf{\\Lambda} = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_{m_0})$, $\\mathbf{B}_i$ with columns $(b_0^{(r)}(t_{i1}), \\dots, b_0^{(r)}(t_{ik_i}))^\\top$, $r=1,\\dots, m_0$, and $\\mathbf{\\Sigma}_i$ the covariance matrix of $\\mathbf{Y}_i + \\mathbf{\\epsilon}_i = (Y(t_{i1}) + \\epsilon_{i1}, \\dots, Y(t_{ik_i}) + \\epsilon_{i1})^\\top$ obtained from corresponding evaluations of $C$ plus $\\sigma^2$ on the diagonal.\n\t\n\tApproach a) directly analyses shapes of the predicted score vectors $\\bycoef_{1}, \\dots, \\bycoef_{n}$ as described in Section \\ref{sec:coefficientlevel} of the supplementary material.\n\t\n\tApproach b) uses \\eqref{FPCscore} to motivate integration weights $\\mathbf{W}_i = \\mathbf{\\Sigma}_i^{-1} \\mathbf{B}_i \\mathbf{\\Lambda} \\mathbf{B}^\\dagger_i \\mathbf{\\Sigma}_i^{-1}$ for the empirical inner products $\\langle y_i, y'_i \\rangle_i = \\mathbf{y}_i^\\dagger \\mathbf{W}_i \\mathbf{y}'_i$, $i=1, \\dots, n$, introduced in Section \\ref{chap_diffgeo} of the main manuscript for $\\mathbf{y}_i, \\mathbf{y}'_i \\in \\mathbb{C}^{k_i}$, such that with this choice\n\t\\begin{equation*}\n\t\t\\langle y_i, y'_i \\rangle_i = \\mathbf{y}_i^\\dagger \\mathbf{W}_i \\mathbf{y}'_i = \\mathbb{E}\\left( \\langle Y, Y' \\rangle \\mid \\mathbf{Y}_i + \\mathbf{\\epsilon}_i = \\mathbf{y}_i, \\mathbf{Y}'_i + \\mathbf{\\epsilon}'_i = \\mathbf{y}'_i \\right)\n\t\\end{equation*}\n\tfor an independent copy $Y'$ of $Y$, with $\\mathbf{Y}'_i$ and $\\mathbf{\\epsilon}'_i$ defined as $\\mathbf{Y}_i$ and $\\mathbf{\\epsilon}_i$. \n\n\tApproach b) might be refined by approximating $\\|y_i\\|^2$ with $\\mathbb{E}\\left( \\langle Y, Y \\rangle \\mid \\mathbf{Y}_i + \\mathbf{\\epsilon}_i = \\mathbf{y}_i\\right)$ and $\\langle b^*, y_i \\rangle$ with $\\mathbb{E}\\left( \\langle b^*, Y \\rangle \\mid \\mathbf{Y}_i + \\mathbf{\\epsilon}_i = \\mathbf{y}_i\\right)$ for a known function $b^*:\\mathcal{T} \\rightarrow \\mathbf{C}$ as described by \\citetsup{stoecker2022efp}, which is slightly different from $\\langle y_i, y_i \\rangle_i$ and $\\langle b^*, y_i \\rangle_i$, respectively. However, basing all computations on $\\langle y_i, y'_i \\rangle_i$ as described in the main manuscript, holds the advantage of a unified definition of the shape geometry on evaluation vectors and curves.\n\t\n\tBoth a) and b) rely, however, on the covariance $C(s,t)$ of the process $Y$ underlying the realizations $y_i$, \n\twhile we ultimately analyze shapes/forms $[y_i]$, $i=1,\\dots, n$, presenting equivalence classes. In practice, this might in many cases not be a problem, when the $y_i$ are in fact roughly aligned and not as arbitrarily recorded as they might be in theory. \n\tHowever in general, it renders FPC based approaches for such settings more complicated and beyond the scope of this work \\citepsup[compare][for related work in a different non-regression setting]{stoecker2022efp}. \n\tCarrying out the FPC alternatively on tangent space level (i.e. in a linear space) would require computation of $\\operatorname{Log}_{[p]}([y_i])$ at some shape/form $[p]$ involving already computation/prediction of inner products.\n\t\n\tWe leave such considerations to future research, and focus instead on simpler weight matrices $\\mathbf{W}_i$ which are known to also work reasonably well in regression scenarios with sparsely/irregularly sampled functional response \\citepsup{Scheipl2015, Scheipletal2016, BrockhausGreven2015, brockhaus2017boosting, rugamer2018boosting, Stoecker2019FResponseLSS}.\n}\n\n\n\\new{\n\\subsection{Tensor-product structure in non-parametric regression}\n\\label{sec:kernelTPfactorization}\n\n\\renewcommand{\\k}{\\mathcal{K}}\n\\newcommand{\\hat{P}}{\\hat{P}}\n\nWe illustrate the broad applicability of the proposed TP factorization (Section 3.2) for the example of \\emph{Additive Regression with Hilbertian Responses} proposed by \\citet{jeon2020additiveHilbertian} showing that also approaches avoiding (finite-dimensional) basis representations may lead to the desired form of effect estimates $\\hat{h}_j(\\mathbf{x})$.\nHence, although they do not consider manifold valued responses, TP factorization can be directly applied to visualize and investigate their effect estimates.\nWe adapt relevant equations to fit our notation and refer for details to their work.\n\n\\citet{jeon2020additiveHilbertian} consider regression with an additive predictor $h(\\mathbf{x}) = \\sum_{j=1}^{J} h_j(x_j)$ with $h_j(x_j)$ depending on the $j$th scalar covariate in $\\mathbf{x} = (x_1, \\dots, x_J)^\\top$.\nIn Section 2.5 p.~2679, they point out that the estimator $\\hat{h}_j(x_j)$ of $h_j(x_j)$ is a linear smoother if the initial estimate of their back-fitting algorithm is (as, e.g., in all their numerical studies). Assuming this in the following, the expression becomes\n\\begin{equation*}\n\t\\hat{h}_j(x_j) = \\frac{1}{n} \\sum_{i=1}^n w_{ij}^{[g]}(x_j)\\, y_i\n\\end{equation*}\nwith weight functions $w_{ij}^{[g]}(x_j)$, $i=1,\\dots, n$, $j=1,\\dots, J$ after $g$ fitting iterations.\nIn fact, this immediately has the desired TP form given in Section \\ref{sec_tensorproduct_effects}, setting $m = m_j = n$, $\\theta_j^{(r,l)} = \\frac{1}{n}\\mathds{1}(r=l)$, $b_j^{(l)} = w_{lj}^{[g]}$ and  $\\partial_r = y_r$ for all $l,r = 1,\\dots,n$ and $j$. Here, tangent vectors are naturally identified with elements of the Hilbert space, as we are in the linear case.\n\nIt might seem odd to have the effect basis functions $b_j^{(i)}$ only implicitly defined depending on the fitting iteration. \nYet in fact, the $w_{ij}^{[g]}$ are all in the span of \n\\begin{equation*}\n\tb_j^{(i)}(x_j) = \\frac{\\k_j(x_j, x_{ij})}{\\sum_{i=1}^n \\k_j(x_j, x_{ij})}, \\quad i = 1,\\dots,n\n\\end{equation*}\nwith some kernels $\\k_j$ evaluated around covariate realizations $\\mathbf{x}_i = (x_{i1}, \\dots, x_{iJ})^\\top$, $i=1,\\dots,n$.\nThis can be seen by re-writing the definition of $w_{ij}^{[g]}$ \\citep[Sec. 2.5, p.~2679]{jeon2020additiveHilbertian}:\n\\begin{align*}\n\tw_{ij}^{[g]}(x_j) &= \\frac{\\k_j(x_j, x_{ij})}{\\hat{P}_j(x_j)} - 1 - \\sum_{\\jmath \\neq j} \\int_{0}^{1} w_{i\\jmath}^{[g-\\mathds{1}(\\jmath \\geq j)]}(x_\\jmath) \\frac{\\hat{P}_{j\\jmath}(x_j, x_\\jmath)}{\\hat{P}_{j}(x_j)} \\, dx_\\jmath \\\\\n\t&= \\frac{\\k_j(x_j, x_{ij})}{\\hat{P}_j(x_j)} - 1 - \\sum_{l =1}^{n} \\sum_{\\jmath \\neq j} \\int_{0}^{1} w_{i\\jmath}^{[g-\\mathds{1}(\\jmath \\geq j)]}(x_\\jmath) \\frac{\\k_j(x_j, x_{lj}) \\k_\\jmath(x_\\jmath, x_{l\\jmath})}{\\hat{P}_{j}(x_j)} \\, dx_\\jmath \\\\\n\t&= \\underbrace{\\frac{\\k_j(x_j, x_{ij})}{\\hat{P}_j(x_j)}}_{=\\, n\\, b^{(i)}_j(x_j)} - 1 - \\sum_{l =1}^{n} \\frac{\\k_j(x_j, x_{lj})}{\\hat{P}_{j}(x_j)} \\underbrace{\\sum_{\\jmath \\neq j} \\int_{0}^{1} w_{i\\jmath}^{[g-\\mathds{1}(\\jmath \\geq j)]}(x_\\jmath) \\k_\\jmath(x_\\jmath, x_{l\\jmath}) \\, dx_\\jmath}_{=:\\, a_{lj}^{[g]} \\in \\mathbb{R} \\text{ or $\\mathbb{C}$, respectively}}, \\\\\n\t&= n \\sum_{l=1}^{n} (\\mathds{1}(l=i) - \\frac{1}{n} - a_{lj}^{[g]})\\, b^{(l)}_j(x_j),\n\\end{align*}\nwhere by definition\n\\begin{equation*}\n\t\\hat{P}_j(x_j) = \\frac{1}{n} \\sum_{i=1}^n \\k_j(x_j, x_{ij}), \\quad \\hat{P}_{j\\jmath}(x_j, x_\\jmath) = \\frac{1}{n} \\sum_{i=1}^n \\k_j(x_j, x_{ij}) \\k_\\jmath(x_\\jmath, x_{i\\jmath}).\n\\end{equation*}\nand \nby construction $1 \\equiv \\sum_{i=1}^{n} b_j^{(i)}(x_j)$.\n(Starting values for the back-fitting algorithm presented in the paper\nare given simply by $w_{ij}^{[0]} = 0$ or the Nadaraya-Watson-type estimator $w_{ij}^{[0]} = \\frac{1}{n} \\sum_{i=1}^{n} (\\frac{\\k_j(x_j, x_{ij})}{\\hat{P}_j(x_j)} -1) \\, y_i$.)\\\\\nConsequently, also this non-parametric approach leads to the TP effect structure\n\\begin{equation*}\n\t\\hat{h}_j(x_j) = \\sum_{r=1}^n\\sum_{l=1}^n \\hat{\\theta}_j^{(r,l)} \\underbrace{\\frac{\\k_j(x_j, x_{lj})}{\\sum_{i=1}^n \\k(x_j, x_{ij})}}_{=b_j^{(l)}(x_j)} \\, \\underbrace{y_r}_{\\partial_r}\n\\end{equation*}\nwith $\\hat{\\theta}_j^{(r,l)} = \\mathds{1}(l=r) -\\frac{1}{n} - a_{lj}^{[g]}$.\n}", "meta": {"timestamp": "2022-07-08T02:17:12", "yymm": "2109", "arxiv_id": "2109.02624", "language": "en", "url": "https://arxiv.org/abs/2109.02624"}}
{"text": "\\section{Introduction}\n\\label{sec:intro}\n\n\nObtaining an accurate picture of galaxy formation and evolution are amongst the most outstanding questions in astrophysics today. There have been many studies examining this process in some detail over the past 30 years or so, including extensive Hubble Space Telescope campaigns \\citep[e.g.][]{Conselice2003,Beckwith2006,Grogin2011,Duncan2019} and ground-based imaging studies \\citep[e.g.][]{Steidel2004,Mundy2017}. One of the most critical aspects for studying galaxy formation and evolution is to determine the redshifts of objects such that their evolutionary connections can be made, either through mass or number density selections to connect systems at different points in their evolution \\citep[e.g.][]{Mundy2015}.\n\n\nTwo major ways in which galaxy studies progress is through the investigation of galaxies at different redshifts, and by using the emission of light (direct or indirect) from these systems to measure the amount of current star formation and past assembly of stellar mass.  The process of star formation is fundamental for understanding the assembly of galaxies and how the universe was reionized. However, our resulting understanding of the star formation history of galaxies is largely based on observations of single integrated light measures of SFR within entire galaxies, over  a variety of redshifts \\citep[e.g.][]{Madau2014}.   While much work in this area has been done, we are just starting to understand {\\em how} and {\\em why} star formation is distributed within galaxies.   We also know that the sources of reionization are likely galaxies at $z > 6$ \\citep[e.g.][]{Duncan2015,Robertson2015}, yet we do not know from which types of galaxies, or modes of galaxy formation, ionizing radiation is emitted from.  Both of these problems can be addressed in unique ways by searching for and characterizing emission line systems in the distant universe, and perhaps by examining analogue galaxies at lower redshifts, such as the 'green peas' and Lyman-$\\alpha$ emitters at $z \\sim 1.5$ \\citep[e.g.][]{Izotov2017,matthee2021}\n\n\n\nThere are in general three ways in which galaxies in the distant universe are discovered. The most reliable method is through spectroscopic surveys which, by accurately measuring the wavelengths of a few features in the spectrum of a galaxy, its redshift can be determined with a high accuracy \\citep[e.g.][]{Zhou2019}.  However, spectroscopy of significant samples is still very difficult to obtain, and only future programs and telescopes such as MOONS \\citep{moons2020}, WEAVE \\citep{weave2020}, and 4MOST \\citep{4most2019} will make significant progress towards obtaining significant numbers of redshifts for galaxies. Furthermore, the future of this field is 'blind' spectroscopy, with IFUs such as MUSE, where pre-selection is not required, as it is for most traditional spectroscopic surveys\n\n\nAlternatively one can use photometric redshifts to locate the redshifts of distant galaxies by fitting intrinsic spectral energy distributions (SEDs) to stellar population models \\citep[e.g.][]{Dahlen2013}. An alternative simplified version of this is to use the Lyman-break dropout method, whereby one finds the Lyman-break by searching for galaxies that have a faint, or statistically non-existing flux in an observed filter, traditionally the U-band \\citep[e.g.][]{Koo1980,Guhathakurta1990,Steidel1993,Steidel2004}.\n\nThe final method is to use line emission as detected in narrow-band imaging. Searching for and examining distant galaxies using line emission through narrow-band imaging has a long history.  Traditionally, the Lyman-$\\alpha$ line  is used to locate and study distant galaxies, \\citep{rhoads2000,malhotra2004,Stark2010,Ouchi2010}. However, the radiative transfer of Ly-$\\alpha$ is very complex \\citep[e.g.][]{Gronke2016} as it is a resonant line, such that its photons scatter with neutral hydrogen, and increase the likelihood of dust absorption due to the short wavelengths. Thus, Ly-$\\alpha$ cannot typically be used to trace the SFR in most galaxies, unless the escape fraction of Ly-$\\alpha$  photons is known \\citep[however, the work of][has calibrated Ly-$\\alpha$ SFRs with rest frame equivalent widths to within 0.3 dex]{Sobral2019}.  It is often more direct and simplier to use the non-resonant H$\\alpha$ \\citep[e.g.][]{Geach2008,Sobral2013,Sobral2014}, or other similar lines such as [\\ion{O}{II}] or even [\\ion{O}{III}] \\citep[e.g.][]{Khostovan2015}. Likewise, if two lines can be measured from the \\ion{H}{ii} regions (such as \\ion{O}{iii}/\\ion{O}{ii}), their ratios gives an indication for the content of the radiation coming from these star forming areas \\citep[e.g.][]{Izotov2017} and/or dust.\n\n\n\\subsection{H$\\alpha$ Emitters}\n\nFor line emitters, Ly-$\\alpha$ is ideal line for finding distant galaxies due to its brightness, but it is difficult to use this line for measuring astrophysical quantities from these galaxies \\citep[see][]{Sobral2019}. The emission from H$\\alpha$, [\\ion{O}{III}] and [\\ion{O}{II}] lines are, on the other hand, much less affected by dust than Ly-$\\alpha$ or UV fluxes. These lines are also good for identifying and characterising galaxies which may have high dust attenuation and thus very faint Ly-$\\alpha$ emission.   In this paper we present a general search for very faint line emitters using SHARDS \\citep{Perez2013} data and describe a detailed analysis of the H$\\alpha$ emitters we discover.  \n\nThere has been much progress in this over the past few years. For example, we know that bright H$\\alpha$ emitters are very common at low and intermediate redshifts up to $z \\sim 3$ \\citep[e.g.][]{Sobral2012,Sobral2013}, but more difficult to find at higher-z due to technological limitations, as H$\\alpha$ quickly redshifts into the NIR and redder wavelengths at modest redshifts.    We also have not yet probed the lowest mass galaxies at these epochs.  These may also be moderate redshift counter parts to the Lyman-continuum leaking Green Pea galaxies found in the local universe, for example \\cite{Jaskot2013} and \\cite{Yang2017}. They could also be analogues of the numerous  systems that reionized the universe \\citep[e.g.,][Griffiths et al. 2021 submitted]{naidu2020}. \n\n\n\nGround-based surveys have also used this H$\\alpha$ line as a tracer of star formation in distant galaxies \\citep[e.g.][]{Sobral2009,Sobral2012}, finding many sources and a steep luminosity function ($\\alpha_{Ha} = -1.6$). H$\\alpha$ can also be used to investigate the widely variable star formation rates (SFRs) of dwarf galaxies predicted by hydrodynamical simulations \\citep[e.g.][]{Ceverino2016a,Ceverino2016b,Smit2016,Sparre2017,Emami2019}.  Dwarf galaxies are not just simple nearby systems, but are the most common galaxy type in the universe \\citep[e.g.][]{Conselice2016}, but these are extremely difficult to locate and study in the distant Universe.  Deep emission line surveys remains one of the best ways to find and study these systems outside the very local Universe, with Ly-$\\alpha$ surveys providing some of the lowest mass galaxies \\citep{Oteo2015,Matthee2016,Santos2020}.\n\n\n\nDwarf galaxies with stellar masses of M$_* < 10^{9.5}$ M$_{\\odot}$ typically experience bursty episodes of star formation on timescales of a few, to tens of megayears \\citep{Rodighiero2011,Shen2014,Sparre2017}. In order to investigate the bursty star formation of these distant dwarf galaxies, it is necessary to use observables that trace star formation on different time scales. Hydrogen recombination lines such as H$\\alpha$ and H$\\beta$ are produced by short lived O-stars which have lifetimes of a few megayears such that their emission quickly reaches an equilibrium after the star formation is quenched, allowing us to trace galaxy SFRs on these short timescales. On the other hand, far-ultraviolet (FUV) continuum photons (1300\\AA $< \\lambda <$ 2000\\AA) are produced by both O- and B-stars which have much longer lifetimes of ~ 100 Myr such that the FUV emission takes much longer to reach equilibrium. Thus, through the comparison of H$\\alpha$ and FUV it is possible to determine variations in a galaxy's star formation rate on timescales of less than 100 Myr \\citep[e.g.][]{Glazebrook1999,Iglesias2004,Lee2009,Weisz2012,Dominguez2015,Ceverino2018}.  This is crucial to determine how star formation occurs in these most common galaxies, which are also thought to be a dominate production of Lyman-continuum photons which reionized the universe \\citep[e.g.,][Griffiths et al. 2021 submitted]{Duncan2015}. \n\nIn this paper we describe a survey for these line emitters using the SHARDS Frontier Fields (FF) dataset obtained with the Gran Telescopio de Canarias (GTC).  The SHARDS-HFF survey is a medium band survey of the Frontier Fields, which we combine with the existing deep HST imaging.   Our data is unique in that our sources are found in this very deep medium-band imaging from in the Hubble-FF area.  We use this data to search for line emitters and to describe their basic properties and  reveal information about their star formation histories.   The purposes of this paper are two-fold: to describe our data and methodology for finding line emitters in the FFs, and to examine the nearest line emitters which are the H$\\alpha$ systems.  Further papers in this series will describe the other line emitters, including an investigation of the Lyman-continuum emission for systems at $z \\sim 2-3$ (Griffiths et al. 2021 submitted).\n\n\nThis paper is thus organised as follows: In Section~\\ref{sec:obs} we discus the SHARDS medium-band observations and data reduction, as well as ancillary data used. Section~\\ref{sec:calib} details the photometric calibration of SHARDS imaging matched to the deep HST broad-band data. Photometric redshifts and stellar population parameters are calculated in Section~\\ref{sec:photz}. We describe corrections of the broad-band continuum colours and the selection of line emitters in Section~\\ref{sec:cand}. We present result and investigate the properties of the H$\\alpha$ emission line galaxies in Sections~\\ref{sec:res} and \\ref{sec:discussion}, and draw conclusions in Section~\\ref{sec:conc}. Throughout this paper we adopt a $\\Lambda$ cold dark matter cosmological model with $\\Omega_{\\Lambda} = 0.7$, $\\Omega_{\\textrm{M}} = 0.3$ and $H = 70$ km s$^{-1}$ Mpc$^{-1}$. All magnitudes are given in the AB system \\citep{Oke1974}.\n\n\n\\section{Observations and data reduction}\n\\label{sec:obs}\n\nThe following analysis presented in this paper is based on new medium-band observations of the Hubble Frontier Fields (HFF) galaxy clusters \\citep{Lotz2017}, Abell 370 and MACS J1149.5+2223 and their corresponding parallel fields. We also utilise the datasets, and multi-wavelength photometric catalogues made available through the HFF-DeepSpace project \\citep{Shipley2018}. We explain the nature of this data and how we use it in our analyses in the following subsections.\n\n\\subsection{Data Sources}\n\n\\subsubsection{Medium-band observational data}\n\nMedium-band (MB) optical imaging data of the Abell 370 and MACS J1149.5+2223 clusters (hereafter A0370 and M1149 respectively), as well as their parallel fields were obtained as part of the SHARDS Frontier Fields survey (SHARDS-FF; PI:P\\'erez-Gonz\\'alez). The SHARDS-FF (P\\'erez-Gonz\\'alez et al. in prep) survey is an ongoing observation program of two of the Hubble Frontier Fields (HFF) galaxy clusters, obtaining subarcsec-seeing imaging in 25 contiguous filters within the wavelength range 5000-9500~\\AA, reaching an average spectral resolution $R\\sim50$. Observations are performed using the Optical System for Imaging and low-Intermediate-Resolution Integrated Spectroscopy (OSIRIS) instrument, at the 10.4 m Gran Telescopio de Canarias (GTC) at the Observatorio del Roque de los Muchachos, in La Palma. OSIRIS's 8.5\\arcmin x 7.8\\arcmin~field of view (FOV) covers both the cluster and parallel HST fields in a single pointing. With observations beginning in December 2015.  A total of 240 hours observational time was granted for the program. On completion, the observations will reach at least 3-$\\sigma$ sensitivity of m$\\sim$27 in all the 25 contiguous medium-band filters. \n\nIndividual images are reduced using a dedicated OSIRIS pipeline \\citep{Perez2013}. The pipeline performs bias subtraction and flat fielding as well as illumination correction, background gradient subtraction and fringing removal. Additionally the pipeline implements World Coordinate System (WCS) alignment which includes field distortions, two-dimensional calibration of the passband and zero point and stacking of individual frames. \n\nIn order to search for emission line galaxies at a range of redshifts, we use all available SHARDS data; at the onset of this study, observations of A0370 have been carried out with four SHARDS filters (F517W17, F823W17, F913W25 and F941W33), while M1149 has been observed in three (F883W35, F913W25 and F941W33). Filter data is provided in Table~\\ref{tab:filters} and we show response curves in Figure~\\ref{fig:filters}. We take this opportunity to note that at the time of writing, SHARDS-FF observations of A0370 have recently been completed in all 25 contiguous medium-band filters in which raw data is publicly available as soon as the observations are completed. Reduced images and catalogues will be published in P{\\'e}rez-Gonz{\\'a}lez et al. (in prep.) and are available on direct request. \n\n\\begin{table}\n\\caption{SHARDS Filter details.}\n\\label{tab:filters}\n\\centering\n\\begin{tabular}{ccc}\n  \\hline\n  Filter  &  Central Wavelength  &  Width \\\\\n          &       [\\AA]          &  [\\AA] \\\\\n  \\hline\n  F517W17  &  5170  &  165  \\\\\n  F823W17  &  8230  &  147  \\\\\n  F883W35  &  8830  &  336  \\\\\n  F913W25  &  9130  &  278  \\\\\n  F941W33  &  9410  &  333  \\\\\n  \\hline\n \\end{tabular}\n\\end{table}\n\n\\begin{figure*}\n\t\\includegraphics{images/filters}\n\t\\caption{Transmission curves of the SHARDS and HST filters used for the selection of emission line galaxies. All curves have been individually normalised.}\n\t\\label{fig:filters}\n\\end{figure*}\n\n\\subsubsection{Ancillary observations and catalogues}\n\nFor differential flux measurements and the selection of emission line galaxies, we require deep broad-band (BB) multi-wavelength data covering the same wavelengths as the medium-band filters. For this reason we make use of catalogues and imaging data made available as part of the Hubble Frontier Fields (HFF) Deep Space project \\citep{Shipley2018}. These data combine up to 17 ACS/WFC3 filters with ultra-deep Ks band imaging, and Spitzer-IRAC, when available. The HFF-DeepSpace dataset also includes calibrated catalogues, photometric redshifts, lensing magnification factors as well as original imaging data, models and calibration information, providing an ideal ancillary dataset for our candidate selection. We note here that HFF-DeepSpace observations cover only a fraction of the $\\sim$70 square arcminute area surveyed by SHARDS-FF for each cluster. Considering both the cluster and parallel field, this constitutes roughly $\\sim$35\\%~and 38\\% of the total SHARDS-FF coverage for A0370 and M1149 respectively. In Figure~\\ref{fig:obsoverlay} we show the HFF-DeepSpace detection image FOV for both clusters and their parallel fields overlaid on SHARDS F913W data.  \n\n\\begin{figure*}\n\t\\includegraphics{images/obs_overlay}\n\t\\caption{Observational field-of-view comparison. SHARDS F913W25 observations of the A0370 (left) and M1149 (right) fields covering roughly $\\sim$70\\arcmin$^2$ each. Overlaid in green and red are the composite detection image FOVs for the Hubble space telescope cluster and parallel fields respectively (note that not all of the area within the green and red lines is observed by both WFC3 and ACS). The Hubble data we use comes from  from a combination of the F814W, F105W, F125W, F140W and F160W bands, we refer the reader to \\citet{Shipley2018} and \\citet{Lotz2017} for more information.  Essentially, the Hubble field of view is only about 50\\% of that of the SHARDS imaging field of view.}\n\t\\label{fig:obsoverlay}\n\\end{figure*}\n\n\n\\subsection{Photometric calibration}\n\\label{sec:calib}\n\nCandidate line emitter selection requires accurate photometric calibration between all observations. This presents a significant challenge when combining ground and space based data due to the differences is spatial resolution, PSFs etc. In order to optimally combine the data it is necessary to avoid the degradation of the HST imaging to the lower resolutions of the ground based SHARDS observations, while conversely not artificially up-scaling the low resolution images, likely to provide unreliable flux measurements. To circumvent these issues, we opt to calibrate the medium-band flux measurements following the same procedures used to construct the HST catalogues \\citep{Shipley2018} and examine this methodology in the sections below.        \n\n\\subsubsection{Total Flux Measurements}\n\nAll observations are first matched to the PSF of the SHARDS F883W filter ($\\sim$1.0\\arcsec).  This is done by deriving convolution kernels for each band using the PSFs provided as part of both the SHARDS and HFF-DeepSpace datasets. For all HFF-DeepSpace imaging we use 2\\arcsec~apertures to recalculate photometry in SExtractor's dual image mode, utilising the deep detection images from the HFF-DeepSpace data \\citep[see][section 3.3]{Shipley2018}. We perform aperture photometry for SHARDS bands individually, constructing a final catalogue by matching sources detected in the SHARDS filters to the HFF-DeepSpace IDs.  \n\nTo correct for flux falling outside of the 2\\arcsec~apertures, we derive total flux values. Aperture photometry is adjusted by applying a correction factor, derived on a source-by-source basis from a reference band (typically F160W). Firstly, for each galaxy the reference band total flux ($f_{ref,tot}$) is calculated from the SExtractor \\citep{Bertin1996} AUTO flux using a growth curve, in combination with the Kron radius \\citep[for full details see][]{Shipley2018}. A conversion factor is then calculated from the ratio of the reference bands total flux to the corresponding 2\\arcsec~aperture flux ($f_{ref}(r)$). Following the equation:\n\n\\begin{equation}\n    f_{i,tot} = f_{i}(r)\\frac{f_{ref,tot}}{f_{ref}(r)}\n    \\label{eq:fluxconv}\n\\end{equation}\n\n\\noindent where r is the aperture radius, total fluxes for all other bands ($f_{i,tot}$) can be derived based on the measure aperture flux, $f_{i}(r)$.\n\nIt should be noted here that this method of producing our catalogues only provides SHARDS medium-band flux measurements for sources which are also found in the detection HST images (i.e. sources that are emitting in the continuum).   Below we describe how we apply further flux corrections which are required to robustly combine the medium-band imaging with HFF-DeepSpace data.  \n\nTypically, when investigating galaxies within the fields of massive clusters such as those in this study, magnifications effects resulting from strong gravitational lensing should be considered. Our H$\\alpha$ sample, the focus of this paper and which is described in \\S 3.3, is however situated at, or below the cluster redshifts such that corrections for these lensing effects are unnecessary.\n\nIn order to obtain consistent measurements between broad- and medium-band photometry, we apply further flux corrections. We correct for Galactic extinction, taking values given by the NASA Extragalactic Database extinction law calculator\\footnote{http://ned.ipac.caltech.edu/help/extinction\\_law\\_calc.html} for the centre of each field and filter \\citep[for a full breakdown of extinctions used for HFF-DeepSpace filters, see Table 5 of][]{Shipley2018}. Further, flux values are normalised to a zero-point of 25.   \n\n\\subsubsection{Geometric effects}\n\nGeometric effects due to incidence angles of the GTC/OSIRIS light beam result in spatially varying effective central wavelengths (CWL) within each filter. This must be corrected for.   Sources within each SHARDS observation are detected with similar, but not identical, effective central wavelengths depending on their location within the image. These effects have been calibrated for previously in \\citet{Perez2013}.  We also need to included this correction in this work. The variation of the filter's central wavelengths will not only affect the SED and SPS fitting, but also calculations of flux densities, this in turn will induce some shift in the calculated equivalent width and excess significance parameters used for the selection of candidate emitters. Thus, accurate calibration of geometric effects need to be undertaken.\n\nHere, we present a summary of this calibration procedure which is described in detail in \\citet{Perez2013} (Section~3.3). The central wavelength calibration is performed through day-time imaging and laboratory obtained spectroscopic data. Using a pinhole mask, spectra are obtained covering the entire FOV, from which the transmission curve is measured. The shape and width of the curve remain relatively constant while the central wavelength shows systematic variations around the optical axis (to the left of the FOV). Central wavelengths are fit with a function that depends on the distance from the optical axis squared ($r^2$). Given the source location and the position of the optical axis, the central wavelength can be computed following the equation:\n\n\\begin{equation}\n\tCWL\\left(X,Y\\right)=A+B\\times\\left[\\left(X-X_0\\right)^2+\\left(Y-Y_0\\right)^2\\right]\n    \\label{eq:cwl}\n\\end{equation}\n\n\\noindent where $X,~Y$ $X_0,~Y_0$ are the pixel locations of the sources, and the optical axis respectively. The values $A$ and $B$ are the fitting coefficients, and along with the values of $X_0$ and $Y_0$ are filter dependent and do not vary with time. The values of these are presented in Table~1 of \\citet{Perez2013}. Calibrations were tested for repeatability and found to provide robust measurements over different nights.\n\n\n\\subsection{Updated Photometric Redshifts and Stellar Population Parameters}\n\\label{sec:photz}\n\nWe utilise our photometric catalogues combining broad-band data from HFF-DeepSpace with our calibrated medium-band photometry to obtain high quality photometric redshifts and stellar population parameters for all galaxies within both of the clusters, and their parallel fields. The photometric fitting codes EAZY \\citep{Brammer2008} and FAST \\citep{Kriek2009} are used for this as default parameter files are provided in the HFF-DeepSpace dataset. This allows for the most robust comparison to existing catalogue values. For a detailed description of parameter selection and verification see Sections~5.2 and 5.4 of \\citet{Shipley2018}.\n\n\\subsubsection{Photometric Redshifts} \\label{sec:eazy}\n\nPhotometric redshifts are computed with the SED fitting code EAZY\\footnote{https://github.com/gbrammer/eazy-photoz/} \\citep{Brammer2008}. We use a linear combination of 12 galaxy templates based on Flexible Stellar Population Synthesis (FSPS) models \\citep{Conroy2009,Conroy2010} and trained on the UltraVISTA photometric catalogues \\citep{Muzzin2013}. We implement a redshift prior based on F160W apparent magnitudes and use a scaled default template error function. \n\nDue to the variations in the effective central wavelength of the SHARDS filters, EAZY must be run on a source-by-source basis. We employ a custom python code to automate this process and incorporate the object specific filter response curves. In summary; for each object in our catalogue, the code uses a pre-defined set of EAZY parameters in combination with our photometric data to create a set of temporary input files. The SHARDS filter response curves are shifted to the effective central wavelength before EAZY is run on the single object. Once results are obtained for each object, the code then compiles the data into a single catalogue. Spectroscopic redshifts are used when available, otherwise the peak of the photometric redshift distribution (\\textit{z\\_peak}) is taken as the redshift. We compare our updated results with spectroscopic and photometric redshifts from the HFF-DeepSpace catalogues in Figures~\\ref{fig:specz} and \\ref{fig:comp} respectively.\n\nWe fit photometric redshifts for a total of 24,183 galaxies over the four fields, 502 of which have accurate spectroscopic redshift measurements available from a variety of sources. For these 502 objects, we compare the results of our photometric redshift fits to the spectroscopic measurements,  findind an average scatter of the residual of $|\\Delta z|/(1+z) = 0.073$, and using the common definition of catastrophic outlier, $|\\Delta z| > 0.15(1+z_{spec})$ \\citep[e.g.][]{Ilbert2009,Dahlen2013}, we find a rate of 9\\%. We show this comparison in Figure~\\ref{fig:specz}.\n\n\\begin{figure}\n\t\\includegraphics{images/specz}\n\t\\caption{Comparison of the new photometric redshifts we fit in this study ($z_{phot,mb}$) to the 502 available spectroscopic measurements. Dotted line shows the one-to-one relation. We find an average scatter of 0.073 and a catastrophic outlier rate of 9\\% (see text).}\n\t\\label{fig:specz}\n\\end{figure}\n\n\\subsubsection{Stellar Population Parameters}\n\\label{sec:sps}\n\nStellar population parameters such as stellar mass, star formation rates and ages are estimated for our sample of galaxies with FAST\\footnote{http://w.astro.berkeley.edu/~mariska/FAST.html} \\citep{Kriek2009}. For input parameters we again follow the methodology of \\citet{Shipley2018}; employing a Chabrier IMF \\citep{Chabrier2003}, solar metallicity, minimum star-formation age of 10 Myr, Calzetti dust attenuation law \\citep{Calzetti2000}, $0 < A_{V} < 6$ mag and exponentially declining star formation histories with a minimum e-folding time of $\\log_{10}(\\tau/yr) = 7$. We use a Bruzal \\& Charlot SPS model library \\citep{Bruzual2003} which includes random bursts (of duration between $3x10^{7} - 3x10^{8}$ yr) superimposed onto models of continuous star formation histories (SFHs) with a probability that 50\\% of galaxies have experienced a burst of the past 2 Gyr.  We use a solar metallicity here as this is what has been used previously in the HFF-DeepSpace catalogue. Furthermore, the use of solar metallicity only affects the stellar mass measurements, and if our systems are significantly less than solar this would produce stellar masses which are higher than they should be by +0.3 dex, which is not enough to alter any of our conclusions.  A higher solar metallicty may also affect our dust measurements, as the galaxies would be slightly redder when fit and thus would require less dust. FAST models do not currently include emission lines, however their effect on stellar mass estimates has been shown to be small \\citep[e.g.][]{Banerji2013}, which would especially be the case for lower mass galaxies.\n\nIn order to obtain the most accurate stellar population parameters, we again shift the SHARDS filter response curves on an object to object basis to account for variations in central wavelength and use our updated photometric redshift estimates (see Section~\\ref{sec:eazy}). While the inclusion of the SHARDS medium-band photometry is likely to improve estimates of these parameters, only mass-to-light ratios are well constrained due to their dependence on rest-frame optical colours which are covered by the HFF-DeepSpace photometry. We show in the central, and right panels of Figure~\\ref{fig:comp} that our results on measuring these features with our broad band and narrow band imaging are in good agreement with HFF-DeepSpace parameters.\n\n\\begin{figure*}\n\t\\includegraphics{images/comp}\n\t\\caption{Photometric redshift and stellar population parameter for all objects in both the cluster and their corresponding parallel fields obtained in this work, compared to HFF-DeepSpace catalogue values. Left panels shows a direct comparison of photometric redshifts estimated with EAZY from HFF-DeepSpace catalogues and this work (HFF-DeepSpace + SHARDS filters). The grey dotted line represents a one-to-one ratio. Middle and right panels are similar, but for the FAST estimates of stellar mass and star formation rate respectively. In all of the plots, the subscript `mb' (medium-band) represents estimates obtained in this work, while the subscript `ds' (DeepSpace) denotes the values obtained from the HFF-DeepSpace catalogues.}\n\t\\label{fig:comp}\n\\end{figure*}\n\n\n\\section{Emission Line Galaxy Selection}\n\\label{sec:cand}\n\nIn order to select emission line objects via differential flux measurements, we require overlapping wavelength coverage of both the medium and broad-band imaging. We utilise F606W and F105W broad-band imaging to match the F517W and F941W medium-band filters respectively, while F814W is matched to both the F823W and F913W bands (see Figure~\\ref{fig:filters}). We note here that the sky area covered by the F105W band is less than half the size of that covered by the F814W imaging (only $\\sim$42\\% and $\\sim$34\\% the size of the F814W science area for the cluster and parallel fields respectively). For the robust selection of emission line objects we employ a two parameter selection criterion based on emission line equivalent width and having an excess significance, as is well established in previous studies \\citep[e.g.][]{Matthee2015,Santos2016,Sobral2017,arrabal2018}. These criterion assures that the objects selected show a real colour excess, and not an excess due to random scatter or measurement uncertainties. \n\n\\subsection{Broad-band Continuum}\n\nBefore applying the two parameter selection criterion, it is important to note that the medium-band filter profiles are not centrally aligned with the corresponding broad-band filters. Thus, sources with significant intrinsic colour in the continuum can complicate not only the selection of emission line objects, but also result in over or underestimation of line fluxes. In order to correct for this we derive an effective broad-band magnitude (called BB') to account for the slope in the continuum and achieve a mean zero (BB-MB) colour \\citep{Sobral2012,Sobral2013}. To obtain the most accurate estimates we first remove all point sources from the catalogue using the \\textit{star\\_flag} identifier. A further cut is then performed to select sources within 2$\\sigma$ of the scatter around the median of the broad-band colour. We then perform a linear fit which is used to correct the initial broad-band magnitude following the form:\n\n\\begin{equation}\n\t\\left(BB'-MB\\right)=\\left(BB-MB\\right)-M(RB-BB)+C\n    \\label{eq:bbcor}\n\\end{equation}\n\n\\noindent where $RB$ is the reference band (taken as the closest neighbouring broad-band filter), and M and C are the coefficients of the linear fit. For sources in which one of the broad-band colours is not available, the median correction is applied. We show (BB-MB) colours before and after corrections have been applied in Figure~\\ref{fig:bbfix}.  The net effect of this is to reduce our number of candidates significantly.  We take a conservative apporach here so as to remove as much as possible contamination and to retain a population of certain line emitters.\n\n\\begin{figure}\n\t\\includegraphics{images/BBFix}\n\t\\caption{Colour-colour plot showing broad-band continuum magnitude corrections for all objects in both Frontier Field clusters and their corresponding parallel fields. We show object colours before (grey contours) and after (blue contours) corrections have been applied to achieve a mean-zero BB-MB colour.}\n\t\\label{fig:bbfix}\n\\end{figure}\n\n\\subsection{Line Emitter Selection}\n\\label{sec:selection}\n\nTo carry out a selection for our emission line candidates, first we make a check to determine if the medium-band excess is high enough to be considered an emission line object. This is done by setting a lower limit on equivalent width (EW). The observed equivalent width is the ratio between the flux of an emission line (medium-band) and the continuum (broad-band). We follow the basic procedure used in previous searches and specify a traditional excess criteria of rest-frame EW of 25\\AA~\\citep[e.g.][]{Ouchi2008}. Previous studies \\citep{Ouchi2010} have found that higher EW cuts ($>$25\\AA) help to minimise contamination from low redshift interlopers, while \\citet{Sobral2017} are able to implement much lower cuts through the use of narrower filters. Due to the limited sample size and low expected EW at our modest redshifts we use the lower limit of 25\\AA. To obtain an EW from the observed medium-band excess, magnitudes ($m_i$) is converted to flux densities ($f_i$) for each filter ($i$) via the equation: \n\n\\begin{equation}\n\tf_i=\\frac{c}{\\lambda_{i,centre}^2}10^{-0.4(m_i+48.6)}\n    \\label{eq:flux}\n\\end{equation}\n\n\\noindent where $c$ is the speed of light and $\\lambda_{i,centre}$ is the filter's central wavelength (note that for the medium-band filters, this is taken as the corresponding broad-band central wavelength as the continuum colour corrections of Equation~\\ref{eq:bbcor} have been applied). Line fluxes and equivalent widths are then computed using:\n\n\\begin{equation}\n\tf_{line} = \\Delta\\lambda_{MB} \\left(\\frac{f_{MB}-f_{BB}}{1-\\frac{\\Delta\\lambda_{MB}}{\\Delta\\lambda_{BB}}}\\right)\n    \\label{eq:fline}\n\\end{equation}\n\n\\begin{equation}\n\tEW = \\Delta\\lambda_{MB}\\left(\\frac{f_{MB}-f_{BB}}{f_{BB}-f_{MB}\\frac{\\Delta\\lambda_{MB}}{\\Delta\\lambda_{BB}}}\\right)\n    \\label{eq:ew}\n\\end{equation}\n\n\\noindent respectively, where $\\Delta\\lambda_{MB}$ and $\\Delta\\lambda_{BB}$ are the widths of the medium- and broad-band filters respectively. Here, $f_{MB}$ and $f_{BB}$ are the flux densities calculated via Eq.~\\ref{eq:flux}. Depending on the specific filters used, this equation breaks down at certain MB excess when $\\Delta\\lambda_{MB}/\\Delta\\lambda_{BB} \\to f_{BB}/f_{MB}$ such that the denominator tends to 0. When this occurs we set the EW of the sources to > 1500\\AA~as this can also be helpful to identify sources that may not be real, or where problems may have arisen.\n\nThe second parameter, the excess significance \\citep[$\\Sigma$, e.g.][]{Bunker1995} is used to quantify the real flux excess compared to an excess due to random scatter. The excess significance can be written as \\citep{Sobral2013}: \n\\begin{equation}\n\t\\Sigma=\\frac{1-10^{-0.4(BB-MB)}}{10^{-0.4(ZP-MB)}\\sqrt{\\pi r_{ap}^2(\\sigma_{px,BB}^2-\\sigma_{px,MB}^2))}}\n    \\label{eq:sig}\n\\end{equation}\nwhere $MB$ and $BB$ are the broad- and medium-band magnitudes respectively. Here, $ZP$ represents the normalised zero point (25) and $r_{ap}$ is the aperture radius in pixels. The root-mean-square (rms) of the background pixel values, $\\sigma_{px}$, are estimated by randomly placing empty apertures across the respective images. A selection criteria of $\\Sigma$ > 3 is used to classify sources as potential line emitters \\citep{Sobral2013}.\n\nWe show the selection of potential line emitters in Figure~\\ref{fig:selection}. From $\\sim$25,000 detected objects across the four fields (two clusters and their respective parallels), 666 candidates are found in the cluster fields with a further 432 in the parallels, providing a total of 1,098 candidate line emitters. Through the use of our photometric redshift estimates with the detection filter throughput information, it is possible to assign suspected emissions lines to objects within our sample. This method uses our above criteria and removes a significant number of candidate sources. Ultimately, we are left with only a small fraction which are reliable emitters.  One of the main reasons for this is that we are fairly conservative about the line matches with wavelength.  This is one of he deepest narrow band searches yet, and it is also possible that many of the lines are from rare ionization lines that are not the common lines often seen in narrow-band searches, such as H$\\alpha$, \\ion{O}{II} and \\ion{O}{III}.  The comparison of our numbers agrees with the expected uncertainties with previous work such as \\citet{Sobral2013}. For example; we find 42 robust H$\\alpha$ candidates as well as 33 \\ion{O}{II}, 63 \\ion{O}{III}$+$H$\\beta$ and 3 suspected Ly$\\alpha$ line emitters. We also find 42 candidate emission line galaxies between redshifts $2 < z < 3.5$ which we use to investigate escape fractions in Paper II (Griffiths et al. 2021, submitted). In Figure~\\ref{fig:candidates} we show the updated photometric redshifts for our candidate emission line galaxies as a function of HFF-DeepSpace photo-z (i.e. without SHARDS medium-band filters) and updated stellar mass (see Section~\\ref{sec:sps}), highlighting a few example emission lines only.\n \n\\begin{figure*}\n\t\\includegraphics{images/selection}\n\t\\caption{Medium-band excess as a function of F913W medium-band magnitude for the A0370 and M1149 clusters (top left and bottom left panels respectively) and their parallel fields (top right and bottom right panels). The horizontal dashed lines represents a rest-frame EW cut of 25\\AA~while solid lines show the average 3.0$\\Sigma$ colour significance. All objects are shown as grey points while galaxies that meet the selection criterion are coloured by photometric redshift.}\n\t\\label{fig:selection}\n\\end{figure*}\n\n\\begin{figure*}\n\t\\includegraphics{images/candidates}\n\t\\caption{Left panel: redshift comparison of emission line selected candidates with ($z_{phot,mb}$), and without ($z_{phot,ds}$) SHARDS medium-band filters. We show a few example emission lines based on the detected medium-band filter and photometric redshift, other emission line candidates are shown as grey points. Candidates selected from the A0370 and M1149 fields are shown as circles and squares respectively, while filled and empty points represent the cluster and parallel fields. The redshift of the clusters are shown by dashed vertical lines at 0.375 (A0370) and 0.543 (M1149). The inset shows the same again from $2 < z < 7.5$, illustrating the detection of 3 Lyman-$\\alpha$ emitters. Right panel: Stellar mass as a function of photometric redshift for all candidate objects. Coloured points correspond to the same emission line selected candidates as in the left panel.}\n\t\\label{fig:candidates}\n\\end{figure*}\n\n\\subsection{H$\\alpha$ Emitter Selection}\n\\label{sec:ha}\n\nWe select H$\\alpha$ emitters, which are the focus of the rest of this paper, from our emission line sample based on a combination of the detected filters and updated photometric redshifts. We utilise filter throughput information for all medium- and broad-band filter combinations to determine the redshift range in which the detected emission line coincides with H$\\alpha$. When a galaxy is selected as an emission line candidate via the two parameter selection criterion (as described in Section~\\ref{sec:cand}) and its photometric redshift coincides with the H$\\alpha$ emission line for the detection filter combination, the object is marked as a H$\\alpha$ emitter. We find a total of 42 objects meeting this criterion over the four fields (the two clusters and their corresponding parallel fields), and summarise the redshift range for all filter combinations in Table~\\ref{tab:haz}.  Other line emitters will be the focus of future papers.\n\nGiven the luminosity functions of \\citet{Sobral2013} at $z=0.4$, we would expect to find roughly 4 H$\\alpha$ emitters within our fields. This is consistent with our results if the overdensity in the A0370 cluster field can be attributed to the sources being cluster members. This is further supported by the work of \\citet{Stroe2017} in which 9 narrow band filters are used to select over 3000 H$\\alpha$ emitters from 19 galaxy clusters and their large scale environments (beyond 2 Mpc from cluster centre). Their results indicate that cluster fields are overdense in H$\\alpha$ emitters, with the luminosity function showing a strong dependence on the dynamic state of the cluster. Considering recent simulations suggesting A0370 is a merging system \\citep{Molnar2020} along with the volume probed by the SHARDS data, our results are consistent with the work of \\citet{Stroe2017} finding similar number densities of H$\\alpha$ emitters, considering our deeper depth. This also confirms our approach towards finding these emitters is sound and consistent with previous work.\n\nWe also visually inspect all objects which are within a crowded environment or contain contaminating flux from neighbouring sources. In total we find four candidates which can not be successfully isolated. These objects are excluded from our analysis as the overlapping light profiles have the potential to result in unreliable photometry, as well as systematic shifts in our morphological and structural measurements. \n\nA total of 33 of the remaining 38 candidate galaxies are selected from the A0370 field.  This is expected as the redshift of the A0370 cluster ($z = 0.38$) corresponds to that of our detection range such that we are able to select both foreground galaxies and cluster members. We show the distribution of our H$\\alpha$ candidates within the A0370 field in Figure~\\ref{fig:locations}. However, as M1149 is at the slightly higher redshift of $z=0.54$ we are unable to probe the cluster population with our permitted filter combinations.      This is a strong indication that our H$\\alpha$ emitters are located mostly within dense environments.  \n\nWe note here that this process is undertaken for all chosen emission lines, however a full analysis of all these emitters is beyond the scope of this paper and will be discussed in future publications. \n\n\\begin{figure}\n\t\\includegraphics[width=\\columnwidth]{images/locations}\t\n\t\\caption{Hubble Space Telescope F814W image of the A0370 cluster field with locations of candidate H$\\alpha$ emission line sources denoted by open blue circles. Orange circles mark the location of objects found to be undergoing a recent burst of star formation following the analysis in Section~\\ref{sec:bursts}. The HST/UVIS F275W FOV is displayed by the black contour. } \n\t\\label{fig:locations}\n\\end{figure}\n\n\\begin{table}\n\\caption{Detection filter redshift range and cluster candidate counts of H$\\alpha$ emitters for the MACS J1149.5+2223 (M1149) and Abell 370 (A0370) fields.}\n\\label{tab:haz}\n\\centering\n\\begin{tabular}{ccccc}\n  \\hline\n  MB  &  BB  &  Redshift Range  & \\# M1149 & \\# A0370 \\\\\n  \\hline\n  F823W  &  F814W  &  0.24 $<$ z $<$ 0.27 & - & 3 \\\\\n  F883W  &  F814W  &  0.32 $<$ z $<$ 0.37 & 2 & - \\\\\n  F913W  &  F814W  &  0.37 $<$ z $<$ 0.41 & 1 & 19 \\\\\n  F941W  &  F105W  &  0.41 $<$ z $<$ 0.46 & 2 & 11 \\\\\n  \\hline\n \\end{tabular}\n\\end{table}\n\n\\subsubsection{Correction for [N\\small\\textsc{II}] line contamination}\n\nCalculations of H$\\alpha$ emission line fluxes and equivalent widths must take into consideration the contribution of flux from neighbouring [\\ion{N}{II}] lines at $\\lambda_{rest}$ = 6548, and 6583\\AA. As these lines are situated in close proximity to H$\\alpha$, their flux is typically included in photometric measurements, acting to artificially increase line fluxes and EWs measured. To account for this, we implement a correction based on the work of \\citet{Villar2008} where it was found that the fractional contribution of the [\\ion{N}{II}] flux decreases with increasing EW due to lower metallicities. The flux ratio, $F_{\\rm{N_{II}}} / F_{\\rm{H_{\\alpha}}}$ and equivalent width, EW(H$\\alpha$+[\\ion{N}{II}]) are related by:\n\n\\begin{equation}\n\t\\log\\left({F_{\\rm{N_{II}}}/F_{\\rm{H_{\\alpha}}}}\\right) = -5.78 + 7.63x - 3.37x^2 + 0.42x^3\n    \\label{eq:ffix}\n\\end{equation}\n\n\\noindent where the value \"x\" is log(EW(H$\\alpha$+[\\ion{N}{II}])). We use this relation to correct all H$\\alpha$ fluxes in this study whereby we a  find a median correction of $\\sim$25\\%.   The range of masses for our sample overlap with the range of masses for which this criteria was measured and determined.  \n\n\\subsubsection{Extinction correction}\n\\label{sec:extcor}\n\nThe H$\\alpha$ emission line is much less affected by dust obscuration than other common lines such as Lyman-$\\alpha$ or [\\ion{O}{II}], however dust effects are non-negligible and should be accounted for.  This is particularly the case when deriving star formation rates. When spectroscopy is available for individual sources, the amount of extinction can be estimated by comparing the intrinsic Balmer decrement with that observed. As spectroscopy of each individual sources is often unfeasible, dust extinctions can also be estimated via the comparison of H$\\alpha$ and far-infrared based SFRs \\citep{Ibar2013,Koyama2013}.\n\n\nThere are many ways in which to account for dust extinction in our sample, some of which we explore below, and throughout the paper. Some studies employ a simple approach of applying a constant value of dust extinction throughout a galaxy. As determined by \\citet{Kennicutt1992} a 1 mag extinction of H$\\alpha$ can be adopted, and is often a good approximation for the true extinction \\citep[e.g.][]{Ly2007,Geach2008,Sobral2009}.  However, this extinction has been found to be stellar mass dependent in the local universe \\citep{Garn2010b}, in clusters \\citep{Sobral2016}, and at high redshift \\citep{Sobral2012}, so it must be used with care.\n\nHowever, as SEDs are readily available for our galaxy candidates, we estimate dust attenuation based on stellar E(B-V) values derived during fitting (see Section~\\ref{sec:photz}) and the assumption of $A_{\\rm{v,stars}} = 0.44A_{\\rm{v,gas}}$ found in local starburst galaxies \\citep{Calzetti2000}.  This is a major assumption we are making, and we later investigate how our results would differ if we were to assume other relationships, such as equality between the stellar and gaseous components.  We find all galaxies have a dust content such that 0 $<$ E(B-V) $<$ 0.7 with mean, median and dispersion values for our dust corrections are 0.16, 0.06, and 0.20, respectively.  \n\nWe also investigate the situation whereby we make the assumption that\n$A_{\\rm{v,stars}} = A_{\\rm{v,gas}}$, which in some higher redshift situations is more likely the case \\citep[e.g.,][]{reddy2015}.   We describe in detail how this assumption would change our results when we discuss the effects of star formation.  The net effect is that the measured star formation rate in the UV would be $\\sim1.85$ times lower with this equality in the dust extinction. This produces a change of $-0.26$ dex when plotting features in log space. As we discuss later, this affects in no significant way the conclusions we draw from this study. This shows that these galaxies are overall not very dusty systems, which fits in with them typically being low-mass galaxies.\n\n\\subsubsection{Star formation rates}\n\\label{sec:sfrs}\n\nA galaxy's star formation activity changes throughout its lifetime due to various physical processes such as major/minor mergers, gas accretion, and supernovae feedback. These fluctuations all contribute to the luminosity of the H$\\alpha$ emission line, which is sensitive to instantaneous star formation on timescales of the order $\\sim$10 Myr \\citep[e.g.][]{Kennicutt1998}. The burstiness of star formation activity can be investigated through the direct comparison of H$\\alpha$ derived SFRs with those estimated via UV continuum emission, which provides time averaged SFRs over a much longer $\\sim$100 Myr period \\citep[e.g.][]{Kennicutt1998}. It is also the case that the H$\\alpha$ equivalent width (EW) is a good indicator for the specific star formation rate \\citep{khostovan2020}. \n\nAs the UV continuum traces star formation over a wider range of stellar ages than that of H$\\alpha$, our calibrations must trace a galaxy's SFR history for at least the last 100 Myr.  As such, we derive UV star formation rates from rest-frame 1600\\AA~emission (L$_{\\rm{UV}}$) for our H$\\alpha$ sample.  The UV flux corresponds to the F275W band, which is provided in the HFF-DeepSpace dataset for both clusters.  The luminosities for these galaxies are first corrected for reddening using the stellar E(B-V) values derived from SED fitting before SFRs are obtained from the \\citet{Kennicutt1998} relation adapted from a Salpeter, to \\citet{Chabrier2003} IMF following \\citet{Muzzin2010}:\n\n\n\\begin{equation}\n\t\\rm{SFR(M}_{\\odot} yr^{-1}) = 0.8 \\times 10^{-28} L_{\\rm{UV}} (erg~s^{-1}~Hz^{-1})\n    \\label{eq:uvsfr}\n\\end{equation}\n\n\\noindent To provide a consistent set of calibrations, we convert corrected H$\\alpha$ luminosities into star formation rates using the standard calibration of \\citet{Kennicutt1998}, again modified to a \\citet{Chabrier2003} IMF following \\citet{Muzzin2010}:\n\n\\begin{equation}\n\t\\rm{SFR(M}_{\\odot} yr^{-1}) = 4.5 \\times 10^{-42} L_{H_{\\alpha}} (erg~s^{-1})\n    \\label{eq:hasfr}\n\\end{equation}\n\n\\noindent assuming Case B recombination at T$_e = 10^4$K and continuous star formation. All measurements are based on line luminosities in which the [\\ion{N}{II}] contamination has been accounted for and dust attenuation is assumed to follow $A_{\\rm{v,stars}} = 0.44A_{\\rm{v,gas}}$, as discussed in Section~\\ref{sec:extcor}. While this calibration assumes continuous star formation on timescales of over 100 Myr, it is relatively robust to variations in recent star formation history (if it has been fairly continuous when averaged over periods of tens of Myr, for more discussion see \\citet{Kennicutt1998}).\n\nWe use these star formation rates to investigate the possibility of bursty star formation histories of these galaxies at moderate redshifts. We note that estimates based on Equations~\\ref{eq:uvsfr} and \\ref{eq:hasfr} assume a constant SFR for more than 100 Myr. However, for a robust comparison of H$\\alpha$ and UV SFRs in this analysis, the absolute SFR scale is less important than a consistent set of calibrations. As H$\\alpha$ and UV luminosities depend on the shape of the upper IMF for a given age, we expect the H$\\alpha$ to UV ratio to vary no more than 30\\% for typical IMF slopes \\citep[e.g.][]{Glazebrook1999}. \n\n\n\\subsection{Structure and Morphologies}\\label{sec:morfometryka}\n\nWe have used the \\textsc{Morfometryka} application \\citep{morfometryka} version 8.2 to measure the structure, non-parametric morphology, and Sersic profiles of the sources investigated here in the HST/F814W band \\citep{Lotz2017}. We briefly describe how \\textsc{Morfometryka} works below; please refer to \\cite{morfometryka} for full details. \n\n\\textsc{Morfometryka} takes as input the galaxy stamp image and the related PSF, then segments it and measures basic geometric parameters (e.g. centre, axis length, position angle). Next, it quantifies the radial light distribution I(R) from which the Petrosian Radius Rp \\citep{Petrosian} and the half-light radius is estimated. For subsequent measurements, a Petrosian Region with the same geometric parameters as the galaxy and with a radius of 1.5 Rp is used. We also use this region to measure classic non-parametric morphology indicators, like the CAS system parameters \\citep{Conselice2003a, Conselice2014}. Additionally, our method fits an 1D Sersic profile \\citep{Sersic} for the quantified I(R) and uses the retrieved initial parameters to fit an 2D Sersic profile directly to the image of the central source. The structural and morphological measurements used in Section~ \\ref{sec:structure_sizes} are summarised in Table~\\ref{tab:results_morphology}, including errors estimated for the Sersic Index, Asymmetry, Concentration and Half-Light Radius.\n\n\n\\begin{table*}\n\\caption{Summary of the structure and morphology measurements of selected sample}\n\\label{tab:results_morphology}\n\\input{morphov2.tex}\n\\begin{minipage}[c]{\\textwidth}\n\\textbf{Table columns:} (1) HFF-DeepSpace catalogue ID; (2) Right ascension in degrees; (3) Declination in degrees; (4) Photometric redshift estimated using the EAZY software, including all available SHARDS and HFF-DeepSpace bands; (5) H$\\alpha$ SFRs and associated error, calculated using the \\citet{Kennicutt1998} conversion factor following corrections, as detailed in Section~\\ref{sec:ha}; (6) Stellar mass in units of log solar masses, estimated with FAST utilising all available bands with typical errors of 0.2 dex; (7) Sersic index for a 2D profile (8) Concentration value; (9) Asymmetry value; (10) Half-Light Radius estimated from the radial light distribution I(R); (11) Label of star formation type.\n\\end{minipage}\n\\end{table*}\n\n\n\\section{Results}\n\\label{sec:res}\n\n\\subsection{Emission line candidates}\n\nUsing the methods outlined in Section~\\ref{sec:cand}, we select emission line candidates based on the differential flux measurements using medium- and broad-band data. SHARDS fluxes are calibrated to match broad-band HST measurements, and are corrected for filter geometric effects (Equation~\\ref{eq:cwl}) and broad-band continuum colour (Figure~\\ref{fig:bbfix}). We then implement a two parameter selection criterion, based partially on agreement of line identification with photometric redshift, in order to minimise contamination from low redshift interlopers and the effects of random scatter. This selection process yields our candidate emission line galaxies from both cluster and their parallel fields (see Figure~\\ref{fig:selection} and Section~\\ref{sec:selection}).\n\nOne of the ways we identify correctly emission lines is by updating photometric redshift estimates through integrating all available SHARDS medium-band data. Using the SED fitting code EAZY \\citep{Brammer2008} on a source-by-source basis, SHARDS filter response curves are shifted to the effective central wavelength of each object, allowing for robust photometric redshift estimates (see Figure~\\ref{fig:comp}). In a similar manner, we update HFF-DeepSpace stellar population parameters using the FAST \\citep{Kriek2009} code.\n\nOur updated photometric redshift estimates allow for the identification of the emission lines responsible for the observed flux excess, as shown in Figure~\\ref{fig:candidates}. Our sample is dominated by [\\ion{O}{II}] and [\\ion{O}{III}]+H$\\beta$ emitters, as described earlier in the paper.  The photometric redshift criteria, along with the line emitter criteria ensures that our sample is a pure one with minimal contamination.   It may also appear that there is a continuous distribution of values, but this selection is done before we narrow down our targets, and many of these systems on the right hand side of Figure~7 are from rarer emission lines that are not often seen in shallower surveys.   Furthermore, we do find a concentration of objects at the expected common line emitter redshifts, and gaps at other redshifts, such as around $z \\sim 1.1$.\n\nWhile we find only 3 Lyman alpha candidates, one of which is a previously unknown $z\\sim7$ object, which we will explore in future works.   As mentioned, this paper focuses on the H$\\alpha$ emitters.    The reason for this is that the H$\\alpha$ emitters are the closest systems to us and due to being the reddest emission line are affected less by dust and redshift effects. They therefore are easier to study, and give us some idea for how line emitters in the relatively nearby universe behave, which we can then compare with the higher redshift emitters.\n\nIn the next subsections we explore the properties of these H$\\alpha$ emitters and give some idea of their origin.  This includes characterising their physical properties and comparing these to other known line emitters and field galaxies.  Specifically, we explore the star formation history of these objects and their structure, arguing that they are a population late time infall into cluster galaxies with induced star formation.\n\n\n\\subsection{Star formation main-sequence}\n\nFirst, we investigate the star formation rates of our H$\\alpha$ emitters and how these correlate with other properties. A crucial tool for understanding galaxy evolution is the star forming main sequence, or SFR-stellar mass relation. We present in Figure~\\ref{fig:mainseq}, H$\\alpha$ derived SFRs as a function of stellar mass. These H$\\alpha$ SFRs are calculated using the \\citet{Kennicutt1998} conversion factor following corrections as detailed in Section~\\ref{sec:ha}, while stellar masses are measured using the SPS fitting code, FAST \\citep{Kriek2009}. The green line in Figure~\\ref{fig:mainseq} shows the z = 0.4 (redshift of our sample) main sequence parameterization of \\citet{Speagle2014}, derived from rest-frame UV continuum based star formation rates.\n\nIn Figure~\\ref{fig:mainseq} we also show the SFR$_{\\rm{H\\alpha}}$-M$_*$ relation for the $z=0.4$ H$\\alpha$ emitter sample selected from the narrow-band High-Z Emission Line Survey \\citep[HiZELS;][]{Sobral2013} in blue, and from the study of the rich cluster Cl 0939+4713 \\citep{Koyama2013} in orange. \n\nAs can be seen, we find a strong SFR$_{\\rm{H\\alpha}}$ enhancement of more than a factor of two for almost all galaxies in our sample over the \\citet{Speagle2014} parameterization. This is indicative of a recent star burst in our galaxy sample's star formation history, the excess in SFR$_{\\rm{H\\alpha}}$ is further enhanced at low masses if one is to consider the \\citet{Whitaker2014} parameterization, which finds a steep low mass slope and has been shown to be consistent with not only UV and IR star formation indicators, but also that of H$\\alpha$. We note here however that care should be taken when making assumptions based on the \\citet{Whitaker2014} parameterization as it is only constrained down to $log(M_*/M_{\\odot}) = 8.4$.  Regardless, it is clear that these systems all have very large star formation rates for their mass and can be considered to be actively star forming galaxies, as expected given their identification as line emitters.  We later compare this H$\\alpha$ star formation rate to the UV measured star formation rate to locate galaxies which exhibit `bursty' star formation histories. Note that as in \\S 3.3.2 if we consider an equivalence between the extinction in stars and gas, rather than the 0.44 factor,  we would obtain a decrease in 0.26 dex in the log SFR axis.  This would however, not change the results discussed above. \n\n\\begin{figure*}\n\t\\includegraphics{images/mainSeq}\t\n\t\\caption{Star formation vs. mass diagram for our z $\\sim$ 0.4 line emitters with H$\\alpha$ derived star formation rates. Black dotted line shows the selection function while the green line shows the main-sequence parameterization from \\citet{Speagle2014} at z = 0.4. Dark, and light coloured bands have a width of a factor of 2 and 10 respectively. SFR$_{\\rm{H\\alpha}}$-M$_*$ relations of H$\\alpha$ selected galaxies (at $z=0.4$) from HiZELS \\citep{Sobral2013} and a rich cluster study \\citep{Koyama2013} are shown as orange and blue solid lines respectively. Errors are extrapolated from uncertainties in luminosity and as such should be taken as lower bounds on the full error. See the text for a discussion of this trend under different assumptions on the dust extinction.}\n\t\\label{fig:mainseq}\n\\end{figure*}\n\n\n\n\n\n\n\\subsection{Bursty star formation}\n\\label{sec:bursts}\n\nFrom the previous sections it is clear that we have discovered a highly starforming population of moderate to low-mass galaxies at moderate redshifts.  The nature of these objects is however a mystery, and in the next sections we determine the properties of these systems, including their possible bursting nature, as well as their structure, environment and other properties.  \n\n\\subsubsection{Comparison of H$\\alpha$ and UV derived SFRs}\n\\label{sec:sfr}\n\nIn this section we discuss the star formation rate history for our sample by comparing the H$\\alpha$ to FUV fluxes.  Ideally, these two star formation rates should be the same if they measure the same aspects of star formation. However, for galaxies in which the star formation rate is more episodic or bursting, the measured H$\\alpha$ SFR will often be higher than that measured from UV. The reason for this is that during a star formation episode, the star formation rate measured with H$\\alpha$ will be higher during the initial burst than the UV measured star formation as detailed in Section~\\ref{sec:intro}.  After some time, the H$\\alpha$ measured star formation will become more similar to the UV measured one, especially for more constant star formation histories.\n\n\nIn Figure~\\ref{fig:SFRComp} we present a comparison of SFRs derived from H$\\alpha$ and UV luminosities in order to provide a direct visualisation of possible bursty star formation activity. The H$\\alpha$ line fluxes are initially computed via Equation~\\ref{eq:fline} to account for a non-zero continuum before we apply corrections for dust extinction and [\\ion{N}{II}] line contamination. UV luminosities are derived directly from reddening corrected rest-frame 1600\\AA~emission, corresponding to the F275W band at $z \\sim 0.4$. In order to obtain SFRs from H$\\alpha$ and UV corrected luminosities we use the method described in Section~\\ref{sec:sfrs}. To gain further insight into our sample and the possibility of bursty star formation histories, we include the evolutionary track of two example galaxies modelled with Starburst99\\footnote{http://www.stsci.edu/science/starburst99/} \\citep{Leitherer1999}. \n\nIn yellow, we show the evolutionary track of an individual stellar population model of initial mass 10$^6$ M$_{\\odot}$, undergoing a single burst of star formation, while model ages of 1 Myr, 5 Myr and 10 Myr are represented by the yellow points. While in green we show the evolutionary track of a model stellar population with a constant rate of star formation of 1 M$_{\\odot}$ yr$^{-1}$ (green line), and indicate the SFR at ages of 1 Myr, 10 Myr and 100 Myr by the green points.\n\nFor simulated galaxies with a constant star formation history, H$\\alpha$ derived SFRs usually match closely to values derived from the UV. In Figure~\\ref{fig:SFRComp}, we show the one-to-one ratio of H$\\alpha$ and UV SFRs, representing the equilibrium value of a constant star formation rate. The 0.3 dex margin accounts for variations around the main sequence due to changes in star formation activity, such as mergers, gas flows or AGN feedback \\citep{Tacchella2016}. We find 65\\% of our sample show an excess in H$\\alpha$ SFR with respect to UV, while only 4 galaxies display significant enhancement of over 0.3 dex. Note that these numbers are still largely the same when we consider the dust extinction equivalence between stars and gas, as discussed in \\S. 3.3.2. Only three more systems fall below the equivalence line in this case for this case of less dust extinction.  This is in indicative of a recent burst in star formation activity. We do however note that a number of systems exhibit SFR$_{\\rm{H\\alpha}}$/SFR$_{\\rm{UV}}$ ratios of less than one, which can be attributed to galaxies in the post-burst phase, or consisting of a relatively young stellar population formed through a single burst of star formation. We discuss this further in Section~\\ref{sec:ssfr} below.\n   \nThere are however some important caveats in the direct comparison of SFRs, including the dust assumptions as already mentioned. Firstly, the conversion from luminosities to SFRs induce uncertainties due to unknown factors, such as; the initial mass function (IMF), stellar populations and metallicities \\citep{Boselli2009} . Uncertainties in the IMF arise as a result of galaxies with a less abundant population of high-mass stars to ionize hydrogen, these objects will have similarly low ratios of H$\\alpha$-to-FUV as those undergoing bursty star formation \\citep{Lee2009}. For low-mass galaxies, these effects becomes particularly important, and luminosity to SFR conversions are expected to differ from the standard \\citet{Kennicutt1998} prescriptions \\citep{Ly2016}. Further sources of uncertainty arises from possibly more complicated dust parameterizations \\citep{Kewley2002} and [\\ion{N}{II}]/H$\\alpha$ flux ratios. However, low-mass galaxies such as those in our sample are less affected by dust than higher redshift systems \\citep{reddy2015}, while luminosity to SFR conversions and variations in [\\ion{N}{II}]/H$\\alpha$ line ratios are not sufficient to account for the excess in H$\\alpha$ derived SFRs of our sample. \n\n\\begin{figure}\n\t\\includegraphics{images/SFR}\n\t\\caption{Comparison of the H$\\alpha$ and UV SFRs for our sample of H$\\alpha$ emitters. H$\\alpha$ line fluxes are corrected for dust extinction and [\\ion{N}{II}] line contamination, while the UV luminosities are dust corrected using stellar E(B-V) values derived from SED fitting. See the text for alternative parameterisations of these factors. Candidates from this study are marked by blue \\& orange points and green crosses (as described in Fig~\\ref{fig:mainseq}). Black dotted line shows a one-to-one ratio with a 0.3 dex margin, accounting for typical scatter in star formation activity due to mergers, gas flows or AGN feedback \\citep{Tacchella2016}. We include two stellar population models from Starburst99 \\citep{Leitherer1999} for reference. The green track shows a model stellar population with a constant star formation history of 1 M$_{\\odot}$ yr$^{-1}$, with ages of 1 Myr, 10 Myr and 100 Myr marked by green points. While the yellow track shows a burst of star formation for a single stellar population with an initial mass of 10$^6$ M$_{\\odot}$ and ages of 1 Myr and 5 Myr shown as yellow points.  Note that these models would need to be scale up in both directions to match the mass of individual galaxies in our sample by a factor of mass which is larger than 10$^{6}$ M$_{\\odot}$. }\n\t\\label{fig:SFRComp}\n\\end{figure}\n\n\\subsubsection{Specific star formation rates}\n\\label{sec:ssfr}\n\nThe relatively high SFR$_{\\rm{H\\alpha}}$/SFR$_{\\rm{UV}}$ ratios present in our sample can be attributed to bursts of star formation, in which H$\\alpha$ flux from short-lived O-stars is boosted with respect to that of the UV. However, it can also be explained by the assumption of  predominantly young galaxies formed in a single burst of star formation.  As can be seen in (Figure~\\ref{fig:SFRComp}), models with a bursting star formation are able to match these values better than a constant star formation rate, except for the bursts which have a higher H$\\alpha$ based star formation rate.\n\nIn order to further investigate the origin of the observed enhancement in SFR$_{\\rm{H\\alpha}}$, we present the H$\\alpha$ specific star formation rate (sSFR) as a function of UV sSFR in Figure~\\ref{fig:ssfr}.  The sSFR is defined as the measured star formation rate divided by the stellar mass.  It gives some idea of the whether the star formation ongoing when a galaxy is observed is an important component of its mass formation.  Again, we show the evolutionary tracks of two Starburst99 models as discussed in Figure~\\ref{fig:SFRComp}, and Section~\\ref{sec:sfr}.\n\n\nAs shown by the evolutionary tracks in Figure~\\ref{fig:ssfr}, the specific SFRs of a galaxy undergoing constant star formation (green line) eventually settles into equilibrium with little deviation. This is however not the case for a young galaxy formed under a single burst of star formation (yellow track), in which an enhancement in H$\\alpha$ is expected for the first $\\sim$5-10 Myr, but rapidly decreases with age as star formation is shut off. Thus, for a recent burst of star formation we would expect galaxies to be above the one-to-one relation of sSFR$_{\\rm{H\\alpha}}$ to sSFR$_{\\rm{UV}}$. After some time these sources with ages $>$10 Myr after the burst would drop below this high excess. For our sample as a whole, we find no reason to favour young galaxy ages over bursty star formation histories as deviations from the one-to-one relation are typically within the margins expected due to perturbations such as gas flows or mergers. We note that $\\sim$15\\% of our sources exhibit a deficit in sSFR$_{\\rm{UV}}$ over that of H$\\alpha$ which could be attributed to young ages, but is more likely due to those galaxies being in the post-burst phase. These results would not change if we considered the alternative formalism for the dust extinction in gas compared to stars, as described in the previous section.\n\nAs can be seen in Figure~\\ref{fig:ssfr}, our bursty sample, as defined by having a significantly larger SFR in H$\\alpha$ than in the UV are not well fit by either the constant star formation or the single burst. However, if we consider the alternative equivalence between extinction in stars and gas, these points do approach the constant SFH models. This is however likely due to the fact that the bursts we see are occurring on top of an existing stellar mass.  From Figure~\\ref{fig:ssfr}, we can see that the difference is about a factor of $\\sim 30$, showing that the burst ongoing is about $\\sim 3$\\% of the galaxies mass, which would be consistent with an older system which is undergoing a new burst of star formation.\n\n\\begin{figure}\n\t\\includegraphics{images/sSFRTracks}\n\t\\caption{Specific star formation rates derived from H$\\alpha$ and UV luminosities. These H$\\alpha$ line fluxes are corrected for dust extinction and [\\ion{N}{II}] line contamination (Section~\\ref{sec:ha}) while UV luminosities are dust corrected using stellar E(B-V) values derived from SED fitting. Stellar masses used in the calculation of the sSFR are obtained through SPS modelling (see Section~\\ref{sec:sps}). Candidates from this study are marked by blue \\& orange points and green crosses (as described in Fig~\\ref{fig:mainseq}). We include two stellar population models from Starburst99 \\citep{Leitherer1999} for reference. The green track shows a model stellar population with a constant star formation history of 1 M$_{\\odot}$ yr$^{-1}$, with ages of 1 Myr, 10 Myr, 100 Myr and 1 Gyr marked by green points. While the yellow track shows a single burst of star formation of a single stellar population and an initial mass of 10$^6$ M$_{\\odot}$ and ages of 1 Myr, 5 Myr and 10 Myr shown as yellow points. }\n\t\\label{fig:ssfr}\n\\end{figure}\n\n\n\\subsection{Structure and Sizes}\n\\label{sec:structure_sizes}\n\nWe use the methodology explained in Section~\\ref{sec:morfometryka} to measure the structures and sizes of our galaxies to better understand their origin in more detail. From the previous sections we know that these H$\\alpha$ systems are undergoing star formation, with a small number undergoing bursts.   The sizes and structures of these systems may illuminate how the star formation within these systems triggered.\n\nIn Figure~\\ref{fig:MRe} we show the distribution of the sizes vs. mass for our sample.  As can be seen, these systems are similar in size to galaxies of similar mass based on the size-mass relation for nearby galaxies \\citep[from][]{Lange2015}. We also compare our objects to the size-mass relations derived from $z=0.4$ star-forming galaxies selected by H$\\alpha$ emission. \\citet{Stott2013} and \\citet{PaulinoAfonso2017} both of whom contain samples of ($>400$) H$\\alpha$ emitters from HiZELS \\citep{Sobral2013}. From these size-mass relations (dotted and dashdot lines in Figure~\\ref{fig:MRe}) we show that (with the exception of ID:4080) our galaxies are similar to typical H$\\alpha$ emitters of corresponding mass and redshifts. \n\nIn terms of other aspects of the structure, we find that there are a range of Sersic indices for our systems, but that most of them have values of around $n=1$, suggesting that these systems are more disk-like, rather than spheroidal like.  This can also be gleamed by investigating the visual morphologies of these systems shown in Figure~\\ref{fig:nM} where most of them appear to be disk-like objects rather than ellipticals.\n\nFurthermore, the bursting galaxies, quite interestingly,  do not show any signs of being mergers.  They are not found in the merger space of the C-A diagram (Figure~\\ref{fig:AC}), and do not appear to have a merger morphology when examined by eye (see Figure~\\ref{fig:mosaic}). This implies that these systems do not have their star formation triggered directly by merging activity.  In fact, the bursts themselves are amongst the lowest asymmetry objects in the early-type galaxy region.   This is a strong sign that other mechanisms, besides merging, are responsible for producing the star formation within these systems.\n\n\\begin{figure}\n\t\\includegraphics[width=\\columnwidth]{images/M-Re.pdf}\n\t\\caption{The size-mass relation for our sample of galaxies. Points are coloured according to the different methods in which our sample has been selected and measured. Solid, dotted and dash-dot lines show the size-mass relations from \\citet{Lange2015}, \\citet{Stott2013} and \\citet{PaulinoAfonso2017}, respectively.}\n\t\\label{fig:MRe}\n\\end{figure}\n\n\\begin{figure}\n\t\\includegraphics[width=\\columnwidth]{images/n_M.pdf}\n\t\\caption{The relation between Sersic index, $n$ and the stellar mass.  Here we see the differences between our sample's structure with mass, showing that many of the bursty systems are found at the high-mass end of our relation, as well as have Sersic indices suggestive of disk galaxies. Sizes of the points are proportional to the $\\tilde{\\chi}^2$ of the Sersic profile fitting, with bigger points represent more uncertain fits.}\n\t\\label{fig:nM}\n\\end{figure}\n\n\\begin{figure}\n\t\\includegraphics[width=\\columnwidth]{images/A_C.pdf}\n\t\\caption{Diagram showing the concentration vs. asymmetry diagram for our sample. The points are defined in the same way as in Figure~\\ref{fig:MRe}.  The lines denoting the differences between various $z = 0$ systems is shown from \\citet{Bershady2000}. As can be seen very few to none of our systems have a morphology consistent with undergoing major mergers.}\n\t\\label{fig:AC}\n\\end{figure}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\textwidth]{images/mosaic.png}\n    \\caption{Mosaic of the 38 sources in the HST/F814W band \\citep{Lotz2017} with structural and morphology measurements summarised in Table~\\ref{tab:results_morphology}, the unique identifier of each object is shown in the bottom left corner for reference. A label using the same designation of the plots with No UV/Non-Bursty/Bursty is shown in the top left. Images presented here are the input stamps used with \\textsc{Morfometryka}, external sources were extracted and replaced with a noise matching the stamp noise level.}\n    \\label{fig:mosaic}\n\\end{figure*}\n\n\\section{Discussion}\n\\label{sec:discussion}\n\nWe find a significant number (42) low-mass star forming galaxies selected by H$\\alpha$ emission in narrow band filters though deep imaging of small pointed fields. Our analysis shows that all of these systems are above the star formation main sequence, and thus have an enhancement of star formation. This is likely partially due to our procedure for finding such systems, as we are more likely to find higher star formation rates in H$\\alpha$ given the requirement that a significant detection exists within our medium-band imaging. Regardless, this is an interesting, highly starforming population of galaxies. We discover that about half of our systems can be defined as `bursts' which have a significantly higher star formation rate in the H$\\alpha$ compared with the UV measured star formation rate.  We show that these systems can be due to very young starbursts within star formation events which occurred in the the past few Myr.  Therefore, we have discovered that some of these galaxies have their star formation triggered by a recent galaxy formation event.\n\nOne issue that we have not discussed in any detail in this paper, thus far, is that many of our objects, but not all, are within the cluster environment.  As Figure~\\ref{fig:locations} shows, the distribution of our galaxies is such that many of them are within the A0370 cluster.  This cluster is at the redshift of our sources and it is likely that these galaxies are within the cluster itself.  The fact that we see more of these systems within a cluster than without might be the result of just having an overdensity in clusters compared to field galaxies, with a steeper slope mass function \\citep[e.g.][]{Penny2008}.  However, it is worth considering that the cluster environment produces or influences the star formation within these galaxies, in particular for bursts.    As can be seen from the images in Figure~\\ref{fig:locations}, many of our bursts are near larger galaxies.  This may be a sign that interactions, even perhaps high-speed ones such as galaxy harassment \\citep[e.g.][]{Moore1998} are producing this effect.  Similar results are found by \\citet{Stroe2017} in which higher densities of H$\\alpha$ emitters are found in merging clusters over more relaxed systems. Further, \\citet{Dressler1983} found signatures of increased star formation in the spectra of cluster galaxies, thought to be induced by ram pressure and later confirmed by the simulations of \\citet{Bekki2003}. More recent simulations propose large-scale, low pressure shocks induced by merger processes could trigger star formation in cluster galaxies \\citep{Roediger2014}.\n\nWhen we examine the morphologies of these galaxies (Figure~\\ref{fig:nM}) we find that most of them appear to to disk-like, which is consistent with their low stellar masses.  One of the burst galaxies appears to be a spiral (ID:4080), which may have recently fallen into the cluster, but the other three do not seem to have any detailed substructure. However, as we show and discuss in Section~\\ref{sec:structure_sizes}, these systems are of typical size for their mass.     We conclude that therefore these systems are likely in some type of evolutionary stage whereby infalling field galaxies are undergoing rapid star formation, likely due to the cluster environment triggering this.  On the other hand they could be galaxies stripped of mass by the cluster environment, but it is unlikely that this process would be efficient enough to produce so many systems.    Future observations of these galaxies, through e.g., spectroscopy, should help us to better understand the origins of these systems which appear to be a new class of low mass galaxy.\n\n\\section{Conclusions}\n\\label{sec:conc}\n\nIn this paper we investigate the selection of emission line galaxies from two of the Hubble Frontier Fields clusters using SHARDS medium-band imaging.  Our Major findings are:\n\nI. With the full suite of SHARDS observations now available (for A0370), we carry out an investigation to  discover and study emission line selected galaxies over a wide redshift range. While lacking the wide area coverage studies such as this typically use, deep imaging through the full set of 25 contiguous medium-band filters combined with the strong lensing potential of the Frontier Field clusters provides an essential tool for the identification and study of line emitters at a variety of redshifts. Particularly those at high redshifts which would typically be too faint to be detected in field observations or observations carried out with smaller ground based telescopes. \n\nII. We identify 1,098 candidate emission line galaxies based on a modified version of a well established two parameter selection criterion. SHARDS photometry is calibrated to match HST images and used to update photometric redshift estimates and stellar population parameters. Using a strict criteria, we are able to match a fraction of these to well known emission lines, such as H$\\alpha$, the focus of this paper. Many of the other sources are likely from more obscure emission lines.\n\nIII. We discover 38 predominantly mid to low-mass H$\\alpha$ emitters, which is the focus of the latter parts of this paper.  After correcting for dust extinction and [\\ion{N}{II}] line contamination we derive H$\\alpha$-based star formation rates. Overall, 27 of these candidates have corresponding HST UV data which enables us to investigate potential bursts of star formation in our galaxies recent histories by comparing the two indicators.  Most of our H$\\alpha$ emitters are undergoing star formation at high enough values such that these galaxies fall above the main sequence relation between star formation and stellar mass.\n\nIV. We investigate  bursty star formation histories for these star forming galaxies using different star formation indicators that trace different time scales. We find a significant enhancement ($>0.3$ dex) of SFR$_{\\rm{H\\alpha}}$ over that of SFR$_{\\rm{UV}}$ for 4 of our candidates. This fact, implies a recent burst in star formation within these systems.  These systems are likely within the foreground Frontier Fields clusters, and thus their bursty star formation is probably induced by cluster processes. This result is robust to different considerations of the dust extinction we use to correct the H$\\alpha$ based star formation rates.\n\nV. Enhancements in the H$\\alpha$ SFR are typically attributed to bursty star formation, but can also result from a young population of galaxies. In order to differentiate we compare the H$\\alpha$ and UV sSFR to the evolutionary tracks of two model galaxies. We find no reason to favour a young first generation population for the majority of our sample, but note that up to 10\\% of our galaxies show a tentative indication of young ages over bursty star formation histories.  \n\nWe conclude that SHARDS imaging, combined with deep Hubble Space Telescope data set can successfully identify emission line galaxies over a wide range of redshifts. Future papers will investigate the higher redshift emitters in a similar way.   These methods, coupled with the strong lensing potential of the Frontier Fields clusters enables the identification of a population of low mass, faint galaxies likely undergoing a burst in star formation.  These objects are fairly unique and should be followed up as they are likely tracing important processes in the formation of galaxies within dense environments.  \n\n\\section*{Acknowledgements}\n\nThis work was supported by the Science and Technology Facilities Council in the form of a studentship to AG. LF acknowledges funding from Coordena\\c{c}\\~{a}o de Aperfei\\c{c}oamento de Pessoal de N\\'{i}vel Superior - Brasil (CAPES) - Finance Code 001. DRG thanks CONACyT for the research grant CB-A1-S-22784. This paper is (partly) based on SHARDS-FF data. SHARDS-FF is currently funded by Spanish Government grant PGC2018-093499-BI00. Based on observations made with the Gran Telescopio Canarias (GTC), installed at the Spanish Observatorio del Roque de los Muchachos of the Instituto de Astrof\\'{i}sica de Canarias, in the island of La Palma.  This work is also based partially on data and catalog products from HFF-DeepSpace, funded by the National Science Foundation and Space Telescope Science Institute (operated by the Association of Universities for Research in Astronomy, Inc., under NASA contract NAS5-26555).\n\n\\section*{Data availability}\nThe data underlying this article will be shared on reasonable request to the corresponding author.\n\n\n\n\n\\bibliographystyle{mnras}\n\n", "meta": {"timestamp": "2021-09-07T02:39:37", "yymm": "2109", "arxiv_id": "2109.02620", "language": "en", "url": "https://arxiv.org/abs/2109.02620"}}
{"text": "\\section{Introduction}\\label{sec:intro}\nCreating novel interpretations of existing musical compositions is and has always been an essential part of musical practice. Before the advent of recorded music, listening to a piece of music mostly meant listening to a version of it, in many cases, performed by musicians other than the original composer or performer.\nWhile this practice, along with musical notation, allows compositions to remain known for many decades or even centuries, it also provides room for artistic expression. Oftentimes, versions that are reproduced faithfully with respect to the original composition are seen as tributes to honor the composers; however, versions that are altered with the limitless creativity of humans often demonstrate how an existing idea can be transformed into something that goes beyond the original intention. Regardless of the ways in which musical versions are created, they are fundamental to the world\u2019s musical heritage.\n\nIn this article, we consider a musical version to be ``any rendition or performance of \nan existing musical composition.''\nAnother widely used term in the literature is ``cover songs''; however, there are three main reasons we avoid using that term in this article. \nFirstly, the scope of the term is not clear as many authors limit its use to certain genres (e.g., pop and rock) and time periods (e.g.,~after the 1950s). \nSecondly, it has negative historical and economic connotations that date back to when the term emerged: record labels often asked White musicians to record (or cover) the songs of Black musicians as a marketing strategy to reach certain demographics. \nLastly, the term does not represent the wide range of musical versions that are the subject of this article (see Fig.~\\ref{fig:version_types}). \nTherefore, we employ the term ``musical versions'' throughout this article for being as inclusive as possible.\n\n\\begin{figure}[tb]\n\\centering\n\\includegraphics[width=\\columnwidth]{figures/fig1.pdf}\n\\caption{Examples of version types and the (subjective) degrees to which a list of musical dimensions may be altered. Both the version types and the musical dimensions are based on~\\cite{serra2010audio} with a few additions. The version type ``Performance'' refers to a recording of a written classical work, while ``Standard'' refers to a recording of a folk or a jazz tune where there are often improvisational aspects.}\n\\label{fig:version_types}\n\\end{figure}\n\nThis survey article focuses on the literature studying musical version identification and retrieval from a computational perspective.\nA key concept we refer to throughout this article is the concept of similarity, which we aim to quantify and model in a way that would reflect musical versions as semantically closer than non-versions.\nFollowing the literature, we consider that all versions of the same composition belong to the same group, or clique.\n\nStudies in audio-based music information retrieval (MIR) focus on extracting information from audio signals (tracks), which is then exploited to develop technologies that can be used for various applications including music retrieval, recommendation, and classification. \nFollowing a query-by-example paradigm, such applications require a notion of musical similarity. However, considering the complexity of information carried by musical audio signals, defining a single notion of similarity is a rather difficult and perhaps futile goal. \nTherefore, the scope of musical similarity\nfor various MIR tasks \ncan be situated on a two-dimensional plane characterized by specificity and granularity~\\cite{grosche2012audio}. The two ends of the specificity axis contain high- and low-specificity systems that are differentiated by the degree of similarity between their queries and targets. While high-specificity systems aim to identify the exact musical tracks (e.g.,~music fingerprinting), low-specificity systems are concerned with broader descriptions of music (e.g.,~genre, mood, and instruments) to retrieve tracks that are related to a given query from high-level musical properties. In terms of granularity, MIR tasks are situated on a spectrum that goes from fragment- to document-level retrieval scenarios. In fragment-level scenarios, queries and targets are short fragments of audio tracks while in document-level scenarios, they are mostly entire audio tracks.\n\nDefining the concept of similarity that connects musical versions is a challenging task. As humans, we can, in most cases, easily identify two tracks as versions of one another. However, considering the wide range of version types~\\cite{serra2010audio} (see Fig.~\\ref{fig:version_types}), constructing a comprehensive similarity definition is extremely difficult. Such versions may incorporate various differences in musical dimensions~\\cite{serra2010audio}, including differences in timbre, tempo, structure, lyrics, recording conditions (``noise''), and so on (Fig.~\\ref{fig:version_types}).\nFor example, a ``radio edit'' of a track may have minor differences in recording quality, have sections removed, and have non-explicit lyrics, but all other musical dimensions remain mostly unchanged.\nLive versions of a track often have higher degrees of variation: they may have small differences in the melody, key, and lyrics; more drastic variations in tempo, timing, structure, and timbre/instrumentation; and lots of background noise from the live recording environment.\nRemixes, on the other hand, may have very little in common with the original, sharing only lyrics and melody for example, which may be superimposed on \nmusical content from a different track.\n\nDue to such differences, the connections that link musical versions together vary depending on each case. For instance, while some version pairs may share the same melodic phrases,\nothers may share only the lyrics. Therefore, modeling the information shared by various types of versions requires a similarity notion that incorporates multiple musical dimensions. Formulating such a notion from a computational perspective is the main focus of the line of research often referred to as version identification (VI).\nNote that, in this article, we focus on techniques that address a wide variety of versions simultaneously. However, there are a number of subfields (discussed in Sec.~\\ref{sec:sub-fields}) that are built specifically for particular types of versions.\nOn the aforementioned specificity--granularity plane, VI can be situated as a task that is mid-specificity and document-level, as the degree of similarity is neither based on high-level concepts nor exact characteristics of signals and the queries and targets are often entire tracks.\n\nThe first efforts toward VI emerged in the early 2000s~\\cite{foote2000arthur}, and it has remained an active field of research ever since. VI systems are designed in a query-by-example fashion: given a query, the goal is to retrieve all the different interpretations of the same musical composition from a corpus. \nThe main consideration for building a VI system is to overlook the differences in musical characteristics and focus on the \nshared information connecting version pairs.\nHowever, instead of aiming to directly quantify \nthis shared information, such\nsystems create representations that are invariant to the aforementioned differences. In light of this, VI research, as other music retrieval~\\cite{muller2019} and classification~\\cite{nam2019} studies in MIR, benefits from the advancements from many scientific disciplines such as signal processing, machine learning, non-linear time series analysis, computational biology, etc.\n\nThe potential impact of VI research can be examined from three perspectives. Firstly, from an MIR perspective, it holds the promise of better quantifying (leading ultimately to better understanding of) the connections between musical versions.\nPrevious research focuses on using the harmonic and the melodic characteristics of musical audio to formulate the concept of similarity for this task. Although providing plausible results, solely relying on these two musical characteristics is not sufficient to cover all the variety that can be found across versions. Therefore, exploring novel ideas that exploit a more comprehensive definition of similarity in the VI context is still an ongoing challenge. Moreover, from a more musicological aspect, understanding how versions are connected and how they evolve through time may reveal valuable lessons for better analysis of artistic inspirations in the music creation process.\nSecondly, from an industrial perspective, building accurate and scalable VI systems benefits various needs in the current music ecosystem, including, but not limited to, detection of copyright infringements in media platforms (e.g., YouTube, Apple, and Spotify) and for author and composer societies (e.g., SACEM\\footnote{\\url{https://www.sacem.fr/en}}, SGAE\\footnote{\\url{http://www.sgae.es/en-EN/SitePages/index.aspx}}, and ASCAP\\footnote{\\url{https://www.ascap.com/}}), organizing and navigating through vast music corpora, and automatically identifying versions in live performances.\nDue to the \nrapid\nincrease in the amount of new musical content created and uploaded to media platforms, automating such application scenarios is becoming increasingly important.\nLastly, from a\nlistener\nperspective, finding new interpretations of a favorite song may have value for the listening experience and for music appreciation. \nThe existence of many websites\\footnote{\\url{https://secondhandsongs.com/}}\\textsuperscript{,}\\footnote{\\url{https://www.whosampled.com/}} that are dedicated to annotating and sharing this information can be seen as an indicator of user interest in versions in general.\n\nIn this article, we aim to provide a review of the key ideas and approaches proposed in 20 years of scientific literature around VI research and connect them to current practice. For more than a decade, VI systems suffered from the accuracy--scalability trade-off, with attempts to increase accuracy that typically resulted in cumbersome, non-scalable systems. Recent years, however, have witnessed the rise of deep learning--based approaches that take a step toward bridging the accuracy--scalability gap, yielding systems that can realistically be deployed in industrial applications. Although this trend positively influences the number of researchers and institutions working on VI, it may also result in obscuring the literature before the deep learning era. To appreciate two decades of novel ideas in VI research and to facilitate building better systems, we now review some of the successful concepts and applications proposed in the literature and study their evolution throughout the years. To facilitate understanding some of the concepts mentioned in this article, we include audio examples on our supplementary website\\footnote{\\url{https://furkanyesiler.github.io/musical_version_id_spm/}}.\n\nThe rest of the article is organized as follows. We give a chronological survey of the evolution of VI systems in Section~\\ref{sec:survey}. We introduce the building blocks of VI systems and describe them in detail in Section~\\ref{sec:blocks}. We then introduce a set of ideas that can be used in any VI system regardless of their building blocks in Section~\\ref{sec:beyond_blocks}. We overview the publicly available datasets and evaluation metrics for VI in Section~\\ref{sec:data_and_eval}. Finally, in Section~\\ref{sec:issues}, we present some current open issues to provide ideas for researchers interested in working on VI.\n\\section{A Historical Survey of VI Systems}\\label{sec:survey}\nThis section aims to provide a survey of the evolution of VI systems across 20~years of research (see Fig.~\\ref{fig:vi_history} for a timeline overview of important milestones). \nWe discuss how pioneering VI approaches were inspired by earlier music retrieval systems and how they were continuously improved over time to address the complex VI use case.\nThroughout this section, various system components will be referenced which are later explained in Sections~\\ref{sec:blocks} and~\\ref{sec:beyond_blocks}. \n\n\\begin{figure}[tb]\n\\centering\n\\includegraphics[width=\\columnwidth]{figures/fig2.pdf}\n\\caption{Milestones of VI research over the past 20~years.}\n\\label{fig:vi_history}\n\\end{figure}\n\n\n\\subsection{1995--2005: Precursors of VI Systems}\n\\label{sec:music_retrieval}\n\nDepending on the context, the musical similarity is typically assessed from editorial or social metadata such as tags or genre, from symbolic data such as Musical Instrument Digital Interface (MIDI)\\footnote{https://en.wikipedia.org/wiki/MIDI} representations, or from the audio content itself such as waveform or spectral representations. Considering such kinds of similarity assessments, pioneering works of VI mainly relied on symbolic and audio data.\n\n\\minisec{Comparing discrete sequences} In the mid-1990s, pioneering music retrieval systems originally relied on symbolic musical representations, for instance, MIDI. In this formalism, a musical excerpt was described as a series of symbols: for instance, a monophonic melody could be described as a sequence of n-grams of intervals between consecutive notes, and the same principle was extended to polyphonic lines, encoding relative pitch values and durations in the n-gram. The similarity between musical excerpts, each represented as a series of symbols, was then evaluated with standard text-based comparison methods, such as regular expressions.\nHowever, symbolic representations only exist for very specific corpora, while audio content is now commonly available and often the main source of musical creation.\n\nThe first attempts to establish musical similarity directly from audio content aimed at reducing the problem to the already known symbolic case. For instance, for query-by-humming applications,\na short hummed or whistled audio input was processed with a pitch tracker, and intervals between consecutive pitches were encoded into a series of symbols and used to query a corpus of musical scores encoded in the same way. The more complex case of polyphonic audio content was also reduced to the monophonic symbolic case by extracting a sequence of the most salient pitches.\n\nThe conversion of salient pitches into sequences of symbols ultimately relied on the limited efficiency of the then-available pitch estimation algorithms. It quickly appeared that they were not accurate enough for strict string matching, which motivated the introduction of sequence comparison algorithms based on dynamic programming (DP, see Section~\\ref{sec:tempotime}). The principle was to estimate the similarity between two sequences by counting the symbol insertions or deletions that are required to align them. This method was well-suited for musical sequence comparison because symbol (e.g.,~note) insertion and deletion could be penalized based on musical plausibility (e.g.,~subsequent notes could be considered more likely to be within a small interval, thus large intervals could be more heavily penalized). Text-based comparison methods were abandoned in favor of DP comparisons, and these became the \\textit{de facto} standard for musical sequence comparison (see~\\cite{doras2020phd} for a summary).\n\nHowever, symbolic representations are inherently discrete and turned out not to be expressive enough to embed all the musical complexity of real audio content. This fostered the development of alternative, real-valued representations. \n\n\\minisec{Comparing real-valued sequences} One of the first attempts at using real-valued representations for VI purposes was proposed by Foote et al.~\\cite{foote2000arthur}. The idea was to represent an audio excerpt by its energy profile (the root-mean-square signal power over short windows) and compare the resulting real-valued sequences via DP.\n\nIn a more musically motivated approach, another idea was to model\nmusic as a stochastic process, in particular, as a Markov model where each state corresponds to a chord. An audio excerpt was then represented as a Markov chain, and its similarity with others was assessed with an appropriate metric, such as the Kullback-Leibler divergence. \nTo estimate chords more accurately, Bello et al.~\\cite{bello2005robust} proposed training a hidden Markov model based on a simple chord vocabulary using an initial beat-synchronous pitch class profile (PCP, also known as ``chroma'') sequence (see Section~\\ref{sec:input_repr}). The idea was to use this model to infer the most probable chord sequence that could have generated an observed PCP sequence and to use such a chord sequence as a proxy to evaluate musical similarity.\n\nIt then appeared that the PCP sequence itself was particularly well-suited for musical comparison: it could be deterministically computed directly from the audio and adequately represented the relative intensity of each pitch class of the equal-tempered scale. This idea was used by M\\\"uller et al.~\\cite{muller2005audio}, who proposed assessing the similarity between audio excerpts by comparing PCP features in a frame-wise manner, achieving tempo invariance using several representations of the same excerpt computed at different sampling rates.\n\nAlthough not directly related as they focus on the retrieval of the music recordings that match the query in an exact manner, music fingerprinting algorithms (see~\\cite{grosche2012audio}) were proposed around the same time, which hash connections between characteristic audio ``landmarks'' (i.e.,~patterns of large spectral peaks).\nThese algorithms were however not invariant to tempo, timbre, or pitch changes. Although they were extremely efficient for exact matching throughout the entire duration of a track, they were not designed to match different versions and usually not used for VI purposes.\n\n\\subsection{2005--2010: The First Efficient VI Systems}\n\\label{sec:cover_detection}\n\nIn the mid-2000s, the topic of musical similarity was being studied from many dimensions: from music fingerprinting to classifying musical genres. At the same time, VI\nstarted to gain more attention from the community. Pioneering attempts were logically built upon existing music retrieval approaches: for instance, using pitch trackers to extract \ndominant melodies (see Section~\\ref{sec:input_repr}) as a discrete input representation was one of the first proposals for VI. However, the complexity of the task pushed researchers toward exploring solutions that were better suited for this particular problem.\n\n\\minisec{Comparing harmonic features} Most of the first robust VI methods followed the same principle: they used a harmonic progression (typically a sequence of enhanced PCP), transposed it to ensure key invariance (see Section~\\ref{sec:transposition_invariance}), and computed a similarity score between pairs of those resulting sequences. \nFor instance, G\\'omez et al.~\\cite{gomez2006automatic} proposed the use of an enhanced PCP-based representation, the harmonic pitch class profile (HPCP, see Section~\\ref{sec:input_repr}). Key invariance was achieved by normalizing HPCP with its estimated global key, and similarity was assessed via DP. The same year, an approach involving beat-synchronous PCP representations (see Section~\\ref{sec:tempotime}) was proposed and later elaborated by Ellis and Poliner~\\cite{ellis2007identifyingcover}. \nKey invariance was achieved using each possible relative PCP rotation, and similarity was assessed via cross-correlation between transposed sequences. \nThis method yielded the best performance on the first Music Information Retrieval Evaluation eXchange\\footnote{https://www.music-ir.org/mirex} (MIREX) ``audio cover song identification'' (i.e.,~VI) contest that took place in 2006. \nThese approaches were improved further for the following 2007 and 2008 MIREX editions. \nFor instance, Serr\\`a et al.~\\cite{serra2008chroma} used another method to ensure key invariance, named optimal transposition index (OTI, see Section~\\ref{sec:transposition_invariance}), which transposed one track relative to the other so that they share the same common global key.\n\n\\minisec{Improving dynamic programming} In the same work, Serr\\`a et al.~\\cite{serra2008chroma} also introduced DP-based local alignment with musically motivated constraints to account for tempo and structure differences between versions, obtaining state-of-the-art results in 2007 and 2008.\nThe following year, Serr\\`a et al.~\\cite{serra2009cross} adapted several concepts commonly used to study recurrences in physical or biological systems. \nThe idea was to compare PCP sequences using a cross recurrence plot (CRP), a representation highlighting common subsequences. \nThe global similarity was assessed via recurrence quantification analysis measurement, which in essence quantifies the importance of the similarity patterns in the CRP. \nThis algorithm was enhanced further using similarity transitivity: if A and B are similar and B and C are similar, then it is likely that A and C are similar too~\\cite{serra2009unsupervised} (see Section~\\ref{sec:clique_enhancement}). \nThe combination of this algorithm with the previous approach~\\cite{serra2009cross}, dubbed $Q^*_{\\text{max}}$, remained the state-of-the-art method in VI for more than a decade.\n\n\\subsection{2010--2015: Improving Accuracy \\& Scalability}\n\nBy the early 2010s, successful VI systems based on harmonic representations and local alignment had achieved promising accuracy. However, it appeared that harmonic information was not the only musical facet that could be used to adequately model music complexity. At the same time, DP algorithms were too computationally expensive to address industrial applications and the ever-increasing size of modern music corpora.\n\n\\minisec{Improving accuracy} A strategy for improving accuracy was to investigate alternative input features: for instance, some representations were designed to account for the characteristics of the human auditory perception system (see~\\cite{doras2020phd} for a review).\n\nAnother strategy was to consider the combination of existing features: the idea was that different features embed complementary information, and combining them would improve the accuracy compared to using each input feature separately (see Section~\\ref{sec:feature_fusion}). Several combinations were investigated: for instance, HPCP extracted from the original mixture, the separated vocals, and the separated accompaniment~\\cite{foucard2010multimodal}; dominant melody, bass line, and the harmony (i.e.,~HPCP)~\\cite{salamon2012melodybass}; or timbral features and HPCP~\\cite{tralie2017cover}. It was shown that these combined representations improved the accuracy compared to cases where each feature was considered alone.\n\nA third strategy was derived from the observation that some methods performed better for certain tracks than others and that an ensemble system could blend existing systems' strengths (see Section~\\ref{sec:ensemble_systems}). Following this line of thought, different approaches based on classifiers~\\cite{ravuri2010cover}, rank aggregation~\\cite{osmalsky2016enhancing}, and similarity network fusion~\\cite{tralie2017cover, chen2018} were investigated to merge scores obtained from various systems. In all cases, ensemble systems improved upon the accuracy of single systems.\n\n\\minisec{Improving scalability} All successful VI algorithms described so far rely on variants of DP sequence comparison which scale quadratically with the length of the sequences.\nThis time complexity quickly becomes prohibitive when querying large corpora; the sequence comparison computation with the query track must be done on the fly for every track in the corpus.\nThe problem of scalability is common to all information retrieval systems: in order to scale, they generally require a very lightweight similarity estimation function (e.g.,~a simple Euclidean distance), which in turn implies a data representation that can be computed offline and conveniently stored for fast lookup (e.g.,~a small vector of real numbers).\n\nA first direction to improve scalability was to transform input features into a more compact representation, ideally a lightweight matrix (or vector) that could be compared via Frobenius norm (or Euclidean distance). A popular compacting transformation was the 2D~Fourier transform of the PCP sequences (see Section~\\ref{sec:transposition_invariance}) because it provides key and time-shift invariance with respect to\nthe original input~\\cite{bertin2012large}.\n\nAnother approach to reducing the size of input features was based on the assumptions that similarity between versions mainly depends on certain parts of the audio and that comparing short segments should be more efficient than comparing an entire track (see Section~\\ref{sec:struct}). Different methods were investigated, for instance, detecting and isolating only the segments of interest, such as those exhibiting some degree of repetition~\\cite{silva2015music}.\n\nA second direction was directly inspired by fast indexing and retrieval methods that proved their efficiency in text-based retrieval contexts: instead of comparing audio features, the idea was to devise a hashing function and to store audio hashes in an index (see Section~\\ref{sec:fast_indexing}). For instance, locality-sensitive hashing (LSH) was used to obtain the same hash for similar audio shingles, allowing an extremely fast lookup of musically similar audio excerpts~\\cite{casey2006song}. \nAlong the same vein, inverted file indexing was also adapted to the music retrieval context. \nThe original idea was to index text documents by the keywords they contain. In a musical context, a codebook of audio-based tags plays the role of the keywords. These tags were obtained by vector clustering~\\cite{kurth2008efficient} or seeding (indexing of fixed-length short regions)~\\cite{martin2012blast}. \n \nBoth lightweight input features and fast indexing yielded efficient lookup times (as fast as a few seconds on a one million track corpus) but exhibited poor accuracy compared to previous systems.\n\nFinally, a third direction was to combine different methods in a two-step pruning approach: the first step aimed to discard tracks from the corpus that could be easily distinguished as non-versions using scalable systems~\\cite{cai2016} or ``weak rejectors''~\\cite{osmalsky2016enhancing}, while the second step involved a more accurate method operating on the remaining candidates (see Section~\\ref{sec:pruning}).\n\n\\subsection{2015--today: Toward Data-driven VI Systems}\n\\label{sec:trends}\n\nIn the mid-2010s, the VI community was confronted with a dilemma: \naccurate systems, which could not scale up to industry-sized corpora, versus fast systems, which struggled in accuracy and were not suitable for practical use.\n\nMany other MIR applications during this period were also confronted with a plateau in their performance gains. Inspired by advances made in other fields (e.g.,~computer vision, natural language processing, and speech processing), the community initiated a paradigm shift from ad-hoc hand-crafted feature extraction toward data-driven feature learning. \nThis new perspective created a new opportunity for VI: to build more expressive representations of the audio, while still enabling a faster similarity estimation function.\n\n\\minisec{The use of feature learning} The representation learning paradigm aims at identifying and disentangling the underlying structures in the original data that can explain its relevant characteristics. \nVarious attempts to learn a mid-level representation from the PCP-based descriptors were previously proposed, for instance with Markov models or $k$-means. \nHumphrey et al.~\\cite{humphrey2013data} were however the first to explicitly propose using data-driven learned features to represent a track as a single embedding vector and estimate similarities between tracks by computing Euclidean distance between such embeddings.\nTheir method used $k$-means and linear discriminant analysis (LDA) to learn an embedding space and was the first to reach a meaningful accuracy on a very large corpus (one million tracks). \n\nThe impressive results of the data-driven learning approaches in other fields also fostered the use of representation learning in VI. \nA common approach became to train a convolutional neural network (ConvNet) to extract a compact representation (the embedding) out of a low- or mid-level spectral representation of the audio. \nFor instance, Xu et al.~\\cite{xu2018key} proposed training a ConvNet mapping PCP descriptors of each version of a composition to the same class. More generic spectral representations have also been investigated, such as the constant-Q transform (CQT)~\\cite{yu2019temporal}. \nDifferent variants of ConvNets were proposed to address the tempo invariance problem, including temporal pyramid pooling~\\cite{yu2019temporal} or standard max-pooling that have proven their efficiency in the image domain for dealing with different image scales (see Section~\\ref{sec:tempotime}). These methods yielded promising accuracy and lookup times on a large corpus of tens of thousands of tracks.\n\n\\minisec{The rise of metric learning} \nMetric learning is a subset of representation learning that aims to learn a compact data representation fitting a given similarity estimation function (e.g.,~Euclidean distance). \nThe underlying motivation is that the learned representations and similarity estimation functions could yield better performances for high-dimensional data, compared to their ad-hoc counterparts.\nFollowing this idea, Doras et al.~\\cite{doras2019cover} proposed learning an embedding of dominant melody, while Yesiler et al.~\\cite{yesiler2019accurate} proposed an approach based on a learned PCP feature that approximates estimated chords. \nIn both cases, the principle was to learn to project tracks into compact embedding vectors so that the pairwise distance (e.g.,~Euclidean or cosine distance) is smaller for versions than for non-versions.\nThis was typically achieved using an objective function such as a triplet loss, yielding promising results on datasets of up to fifty thousand tracks. In a similar vein, Zalkow and M\u00fcller~\\cite{zalkow2020} proposed learning an embedding of short audio shingles and demonstrated the efficiency of this approach for Western classical music.\n\nRecently, feature learning--based approaches have yielded the current state-of-the-art performance. On the one hand, a musically informed approach combining various complementary musical features, such as melody and harmony, yielded a competitive accuracy and lookup times~\\cite{doras2020combining}. On the other hand, a very deep architecture applied to a generic spectral representation proved to be expressive enough to yield a similar accuracy without any prior musical knowledge~\\cite{du2020bytecover}. \n\n\\begin{figure}[tb]\n\\centering\n\\includegraphics[width=\\columnwidth]{figures/fig3.pdf}\n\\caption{Performance (as measured by mean average precision) of different VI systems evaluated on several datasets throughout the years.} \n\\label{fig:map_summary}\n\\end{figure}\n\n\nThroughout the last 20 years, VI systems have evolved to improve their accuracy on increasingly large corpora. This trend can be seen in Fig.~\\ref{fig:map_summary}, which summarizes the mean average precision (MAP) scores that such systems obtained on different VI datasets over the years (see Section~\\ref{sec:datasets} for details about these datasets and Section~\\ref{sec:metrics} for details about MAP). While high performance scores could only be obtained on smaller datasets with hundreds of tracks in the early years, recent VI systems are nowadays able to reach similar performances on datasets with thousands of tracks.\n\n\n\\section{Building blocks of VI systems}\\label{sec:blocks}\nFollowing the historical perspective presented in Section~\\ref{sec:survey}, we now present a deeper dive into VI systems by dissecting them into their building blocks. Following the literature, we consider five main components that are for (1) feature extraction, (2) transposition, (3) tempo/timing and (4) structure invariance, and (5) similarity estimation (see Fig.~\\ref{fig:bblocks}). While each of these blocks addresses a key challenge in the VI workflow and is proven to improve system accuracy, there is no requirement that VI systems incorporate all of them.\nIn fact, some of the techniques presented below may address multiple challenges at once.\n\n\\begin{figure}[tb]\n\\centering\n\\includegraphics[width=\\columnwidth]{figures/fig4.pdf}\n\\caption{Illustration of building blocks of VI systems detailed in Sections~\\ref{sec:blocks}~and~\\ref{sec:beyond_blocks}.} \n\\label{fig:bblocks}\n\\end{figure}\n\n\\subsection{Feature Extraction}\n\\label{sec:input_repr}\nExtracting useful information from high-dimensional audio signals is the first step in VI systems. Considering the nature of the problem, the representations that are rich in relevant characteristics (e.g.,~harmonic or melodic) and ignore the commonly varied ones (e.g.,~timbre, harmonization, or noise) are favored.\n\n\\begin{figure}[tb]\n\\centering\n\\includegraphics[width=\\columnwidth]{figures/fig5.pdf}\n\\caption{Common input features for VI systems, extracted for the song ``Don't Stop Believin''' by Journey (included in the supplementary website). The y-axes represent musical notes (in subfigures a, b, and d) and pitch classes (in subfigure c), the x-axes represent time, and the color scale indicates the energy/intensity of such notes/pitch classes on a given time frame.}\n\\label{fig:spm_inputs}\n\\end{figure}\n\n\\minisec{Melody} Humans are very good at identifying a known song when the isolated melody is played. \nFollowing this intuition, melody-based representations are a natural choice for VI systems and have been explored in the literature since the early days~\\cite{marolt2008mid, salamon2012melodybass, doras2019cover}. Early melody estimation systems were based on sub-harmonic summation while the recent systems are typically implemented as ConvNets.\nMainly, two types of melody representations are considered: the dominant melody, which represents a single pitch trajectory that is generated by the most dominant instrument (i.e., singing voice or a solo instrument), and the bass melody, which encodes low-frequency bass information that may be relevant for VI. An example of a dominant melody representation can be seen in Fig.~\\ref{fig:spm_inputs}~(a). \nMelody representations are usually high-resolution features, both in time and frequency, in order to model the subtle variations generated by continuous pitch instruments like violin or singing voice. \nWhen used for VI, they are downsampled along both axes, as such levels of granularity increase computational complexity and, furthermore, incorporate detail that is detrimental to detecting variations of the same underlying musical piece. \n\n\n\\minisec{Multi-pitch} Another type of high-resolution feature is a multi-pitch representation, which captures information about the pitch trajectories for each source in a track, covering both melodic and harmonic content (an example can be seen in Fig.~\\ref{fig:spm_inputs}~(b)).\nSimilar to dominant melody, multi-pitch representations are also typically downsampled along both axes and have been shown to be a useful feature for VI~\\cite{doras2020combining}.\n\n\\minisec{Pitch class profile} Perhaps the most exploited musical characteristic in VI systems is the harmonic content~\\cite{ellis2007identifyingcover, serra2008chroma, serra2009cross, bertin2012large, humphrey2013data, chen2018, yesiler2019accurate}. PCP has been the primary representation used to analyze the harmonic content in musical audio recordings for a long time. They are derived frame-wise by collapsing the energies within a certain frequency range (commonly 50 to 5,000\\,Hz) into an octave-independent and usually 12-bin histogram that represents the relative intensities of the 12 semitones found in the Western musical tradition (see Fig.~\\ref{fig:spm_inputs}~(c) for an example). \nAn important variant of PCP representations is the HPCP~\\cite{gomez2006automatic, serra2009cross}, which has been, and still is, used in VI extensively. \nIt produces a more robust summary of the tonal content than plain PCP by incorporating additional steps such as harmonic weighting and spectral whitening. Along with HPCP, another PCP variant that has been used by many systems is chroma energy distribution normalized statistics, or CENS~\\cite{muller2005audio}. \nIt is obtained by incorporating quantization and smoothing operations that alleviate the issues with sensitive characteristics of local PCP distributions, namely articulation variations and local tempo deviations. \nHowever, the search for more robust PCP-like features is still ongoing.\nA recent trend is to train neural networks to estimate ``deep'' PCP features from audio. \nFor example, cremaPCP is a learned variant of PCP that estimates pitch-class information needed to predict chord sequences, and it shows performance improvements over PCP and HPCP features in the VI context~\\cite{yesiler2019tacos, doras2020combining}.\n\n\\minisec{Chords} Another idea for exploiting the harmonic content is to extract chord progressions from the audio signals~\\cite{bello2007audio, khadkevich2013large}. \nThey can be seen as an abstraction over PCP features to obtain a more robust summary of the harmonic content, and the fact that they can be represented as discrete codes (i.e.,~chord symbols) makes them highly useful for reducing computational complexity for similarity estimations and disk usage for data storage. \nAlthough the motivation for using chord progressions can be easily justified, issues in chord estimation algorithms make them less appealing for VI. For example, most research in automatic chord estimation uses a rather small target vocabulary (24 chords), which is insufficient to correctly transcribe tracks from certain genres (e.g.,~jazz and blues) and may lead to inaccurate input representations.\n\n\\minisec{Self-similarity} An interesting way to make features more robust across musical versions is to consider their evolution~\\cite{tralie2015cover, tralie2017cover}.\nA common way to model musical structure is via self-similarity matrices, which represent the distances between the feature vectors of each time frame and every other frame.\nSelf-similarity matrices are attractive input representations for VI systems because they are invariant to several musical characteristics including timbre, transposition, and noise, as they only encode relative differences between features\nfrom different time frames, discarding their global offset.\n\n\\minisec{Constant-Q transform} For many years, the input representations for VI systems were either\nmid- or high-level audio descriptors, mainly due to the fact that low-level representations contained too many redundant and noisy signals for developing an accurate system. \nHowever, as deep learning methods become more popular, VI researchers have begun experimenting with low-level descriptors like CQT~\\cite{yu2019temporal, jiang2020}.\nThe CQT is a spectral representation of an audio signal, which is obtained by using a set of frequency filters with a constant-Q factor (see Fig.~\\ref{fig:spm_inputs}~(d) for an example).\nThe representation is quite convenient for considering pitch transpositions, as the filters are logarithmically scaled and match the pitches on the Western musical scale, which is an important advantage of CQT over other spectral representations like plain short-time Fourier transform. \nConsidering that many deep melody- or harmony-based representations are extracted from the CQT, it has become a natural choice for deep learning--based VI systems that follow a data-driven, feature-learning paradigm.\n\n\\subsection{Transposition Invariance}\n\\label{sec:transposition_invariance}\nTransposing a song to a different key is a common practice in musical performances and is among the most common variations in musical versions. Thus, it is desirable for VI systems to be completely invariant to transpositions.\nWhen not adequately accounted for, transpositions can drastically lower a VI system's performance.\n\n\\minisec{Transposition to a common key} A straightforward idea to deal with transpositions is to estimate the key of each track and transpose them to a common key (typically C~major or A~minor)~\\cite{gomez2006song}. However, the accuracy of the key estimation algorithm is critical to the success of this approach, as the errors propagate to the overall system.\n\n\\minisec{Relative feature encoding} When using dominant melody or chord representations as input, instead of using the exact frequencies or chord symbols, the same information can be represented as intervals, starting from a given offset (e.g.,~the first note, the mean frequency, or the first chord of a track)~\\cite{sailer2006finding, doras2019cover}.\nWith this, the representations are disentangled from their key offset, which results in them being transposition-invariant.\n\n\\minisec{Exhaustive search over all possible transpositions} Another approach to this problem is to obtain similarity estimations between a track and all possible transpositions of the other track~\\cite{ellis2007identifyingcover, khadkevich2013large}. Especially when using PCP representations as input, their octave-independent characteristic limits the search space to 12 transpositions in total. \nThis approach has the advantage of being more robust than approaches based on direct key estimation but requires a higher computation complexity in the similarity estimation step.\n\n\\minisec{Optimal transposition index} A more computationally efficient approach for considering all possible transpositions is to estimate the OTI for a pair of tracks, which indicates the \npitch transposition interval \nneeded to make the tracks at hand the most similar~\\cite{serra2008chroma, serra2009cross}.\nIn practice, the similarity between global feature vectors of the first track and the transposed versions of the second track is estimated to find the index that results in the highest similarity.\nPrevious works have shown that OTI is very successful with PCP features.\n\n\\minisec{Using magnitudes of the 2D Fourier transform} The magnitude and phase components of the Fourier transform represent the energy of each sinusoidal frequency and its rotational offset, respectively. Therefore, discarding the latter provides shift-invariance with respect to the axes on which the Fourier transform is applied. \nIn VI, applying a 2D Fourier transform on short patches of PCP features and discarding the phase is a practice that is used for obtaining representations invariant to pitch transpositions~\\cite{bertin2012large, humphrey2013data}.\n\n\\minisec{Using convolutional and pooling layers} A typical approach for achieving translation invariance in machine learning is to use convolutional and pooling layers. \nConvolutional layers aim to capture local information with kernels that traverse the entire input, which in the case of VI are the 2D input representations presented in Section~\\ref{sec:input_repr}. The output produced by convolutional layers can be aggregated using pooling layers so that the results are invariant to the exact location of a pattern of interest.\nThe key point for translation invariance using \nthis approach\nis to have kernel sizes much smaller than the input representations, so that the kernels can model similar patterns even when they are present at different locations of the input~\\cite{doras2019cover}.\nIn order to take advantage of the octave-independent characteristic of PCP representations, convolution kernels can be combined with a simple pre-processing step and max-pooling operation~\\cite{xu2018key, yesiler2019accurate}.\nFirst, PCP features are copied and concatenated in the pitch class dimension, so that the resulting representation contains all possible transpositions for each track.\nNext, a series of convolution operations of size 12 in the pitch class dimension are applied, generating activations for each transposition.\nFinally, a max-pooling layer of size (12 $\\times$ 1) selects the largest value across transpositions, which can be considered as the most useful transposition, for each activation.\nAlthough proven to be useful for PCP representations, applying this technique for other input representations is not straightforward and has not been tested in the literature.\n\n\\subsection{Tempo and Timing Invariance}\n\\label{sec:tempotime}\nOther common types of transformations in musical performances are tempo and timing changes.\nTempo differences can occur across entire tracks by changing the global speed, or within certain segments with changes in local contexts.\nTiming differences often occur on the note level and consist of sustaining, repeating, shortening, or removing notes, mainly for conveying artistic expressions.\n\n\\minisec{Beat synchronization} Tempo differences result from changes in the duration of bars; however, the number of bars is not affected by such changes. \nTherefore, by using beat-synchronous features, the temporal content of the input can be represented in units of beats, rather than units of seconds (or frames)~\\cite{ellis2007identifyingcover, bertin2012large}.\nTo compute beat-synchronous features, a beat estimation step is performed for each track, and the feature content that falls into each beat interval is aggregated to obtain one feature vector per beat. \nWhile the efficiency of this approach highly depends on the beat estimation algorithm, empirical results have proven this technique to be helpful for handling variations in tempo.\n\n\\minisec{Dynamic programming alignment} Perhaps the most standard way of dealing with tempo and timing modifications is to perform alignment using DP algorithms. \nSuch algorithms aim to find the optimal alignment between a pair of time series given certain constraints on the solution space.\nIn the VI context, global alignment methods like dynamic time warping (DTW)~\\cite{gomez2006song, serra2008chroma} and local alignment methods like the Smith-Waterman algorithm (SWA)~\\cite{serra2009cross, chen2018} have been proven useful for achieving tempo and timing invariance.\nAlthough resulting in higher system performances compared to other strategies against tempo and timing changes, DP methods require higher (typically quadratic) computation costs\n\n\\minisec{Increasing strides in deep neural layers} The striding operation in neural networks is a common method for downsampling a given input, and it may be useful for being robust to timing variations~\\cite{doras2019cover}. \nAlthough prone to aliasing, this method of downsampling applied on abstract representations found in deeper layers of neural networks is commonly seen in deep learning models, as well as some VI systems.\n\n\\minisec{Using recurrent kernels} In the machine learning community, recurrent kernels are a popular choice for dealing with sequential data. \nThey often incorporate a notion of ``memory'' that facilitates differentiating between past and future events, which gives them the capacity to preserve non-stationary characteristics.\nThe classic formulation of recurrent kernels is prone to a number of issues while training, including the inability to consider long-term dependencies and not being suitable for parallel processing. \nConsidering that the VI systems typically process full-length songs rather than short fragments, the issues around recurrent kernels make them less appealing for VI research; thus, they have been under-explored in the literature~\\cite{ye2019}.\n\n\\minisec{Using dilated convolutional layers} The receptive field of convolution kernels can be increased by separating kernel elements from each other by a dilation factor while keeping the number of parameters constant. \nUsing multiple kernels with different dilation rates can help efficiently \nprocess\na given input for various tempi using convolutional kernels~\\cite{jiang2020}.\n\n\\subsection{Structure Invariance}\n\\label{sec:struct}\nAnother common source of variance across versions is the musical structure.\nThis includes removing, repeating, or changing the order of the existing sections, or introducing new ones.\n\n\\minisec{Segmenting sections} An intuitive solution for dealing with structural changes is to first perform segmentation in order to identify sections and later estimate the segment-wise similarities between two tracks~\\cite{gomez2006automatic, cai2016}. \nAlthough this is an ideal solution for achieving structure invariance, segmentation algorithms are currently error-prone, and comparing wrongly segmented sections may drastically reduce the system performance.\n\n\n\n\\minisec{Extracting music thumbnails} To avoid the computational complexity of using all segments and performing segment-wise similarity estimation, some VI systems extract single or multiple ``thumbnails,'' or short representative clips, for each track and use them as a representation of it~\\cite{silva2018summarizing}. Such thumbnails can be selected using various criteria, the most common being selecting the most repeated subsequences. This technique assumes (1)~that the most repeated section will correspond to the most informative or characteristic one, and (2)~that all versions of a particular song include that section due to its importance. \nWhile these assumptions hold true for many versions of many popular songs, extreme stylistic changes may present difficulties for this method.\n\n\n\\minisec{Sequence windowing} Following the idea of comparing subsequences rather than entire tracks, this technique avoids a segmentation or thumbnailing step by dividing a representation into short, overlapping segments of a fixed size (also called shingles) using a predetermined hop length between offsets of consecutive windows~\\cite{muller2005audio, casey2006song, bertin2012large}. \nAfter obtaining multiple shingles for each input, such shingles can be aggregated by computing their mean or median~\\cite{bertin2012large}, the distances obtained between an item and multiple shingles can be aggregated~\\cite{zalkow2020}, or each shingle can be used individually for fragment-level retrieval.\n\n\n\\minisec{Local alignment} Apart from being helpful for tempo and timing invariance, alignment algorithms can also be useful for achieving structure invariance~\\cite{serra2009cross, chen2018}. The key consideration for this is to avoid global alignment algorithms (e.g.,~DTW) since they struggle in cases with structural changes. Local alignment algorithms (e.g.,~SWA), on the other hand, are ideal candidates for achieving structure invariance due to their goal of finding alignments among subsequences. \n\n\\minisec{Using convolutional and pooling layers} As introduced in Section~\\ref{sec:transposition_invariance}, convolutional and pooling layers can be combined to achieve translation invariance. Such a property can be useful for achieving structure invariance as it makes the VI systems less sensitive to the exact locations of patterns (e.g.,~when the ordering of sections are changed). Moreover, to handle cases where certain sections of a track are repeated, using max-pooling for downsampling can be useful.\n\n\\minisec{Using global pooling operations} It is common practice to use local pooling operations for downsampling purposes.\nGlobal pooling, however, takes the downsampling aspect a step further to aggregate information from an entire dimension to output a single value~\\cite{xu2018key, yu2019temporal, doras2019cover}. \nThe main purpose of this is to be invariant to track length and to obtain a fixed number of features per track.\nChoosing the pooling operation is a crucial aspect of this technique and can be done in an intuitive way. \nThe average-pooling operation considers all the temporal frames as equally important. \nOn the one hand, this may hurt system performance when versions include new or missing segments, but, on the other hand, the most repeated sections will contribute to the results more than the others. \nThe max-pooling operation chooses only the time frames with the highest value for each channel, and assuming that the frames with the highest value are the most informative ones (per channel), this operation is better suited against changes in structure. However, during the backpropagation phase of training, the gradients will flow only from the selected time frames for each channel, which may introduce some instability during earlier training steps due to weights being randomly initialized and updated only through such selected time frames.\n\n\\minisec{Using attention modules} Although pooling operations have been the most popular choice for structure invariance in deep learning--based VI, they consider each frame independently and ignore their relationships. \nAttention modules address this issue by inferring links between frames so that the models can select which frames to highlight or ignore based on the information contained in each frame. \nA popular attention technique in the machine learning community is self-attention, popularized by the Transformer architecture. \nSelf-attention passes information about each frame to all the others and modifies the features in each time step using this information.\nIdeally, only the most informative frames (e.g.,~the most repeated ones) are highlighted, and the structural changes are overlooked. \nAlthough widely used in the applied machine learning literature,\nthe self-attention idea has been under-explored in VI~\\cite{jiang2020}.\nMulti-channel attention is another technique that can be considered as a link between attention and global pooling techniques. \nThe goal is to learn a weighted average of time frames for each feature (or channels of convolutional layers) independently~\\cite{yesiler2019accurate}.\nCompared to self-attention, there are two main differences: (1) different sets of kernels are trained to learn the attention weights, which are later applied to the input of the module for performing a weighted average, and (2) using convolution kernels for computing the weights provides attention only within a local context.\n\n\\subsection{Similarity Estimation}\n\\label{sec:similarity_estimation}\nThe main goal of VI systems is to estimate similarities between pairs of tracks in a way that versions of a musical composition return higher similarity scores than non-versions. \nThe techniques used for achieving invariances are crucial for this purpose. \nHowever, the similarity estimation algorithm must also be chosen carefully to succeed. \nBased on the literature, we consider two main types of similarity estimation: knowledge- and data-driven approaches.\n\n\\subsubsection{Knowledge-driven Approaches} Knowledge-driven approaches use heuristic-based algorithms that are often selected based on domain knowledge.\nThe characteristics of the invariant representations obtained from the previous steps (e.g.,~whether representations lie in Euclidean space) play an important role in the decision of which algorithm to use for this final step.\n\n\\minisec{Conventional similarity measures} When the invariant representations obtained from previous blocks are suitable, similarity measures such as cross-correlation~\\cite{ellis2007identifyingcover}, the Euclidean distance~\\cite{marolt2008mid}, or the dot product~\\cite{muller2005audio} are simple, yet effective choices for similarity estimation. \n\n\\minisec{Dynamic programming alignment} As described in Sections~\\ref{sec:tempotime} and~\\ref{sec:struct}, DP techniques are often used for tempo, timing, and structure invariance, and they eliminate the need for adopting further similarity estimation steps as they \nprovide a measure of it. \nIn the cases of alignment algorithms like DTW, the cost of the optimal solution can be used as a distance measure~\\cite{foote2000arthur, gomez2006song, gomez2006automatic}. \nIn the case of local alignment algorithms like SWA, the length of the longest-aligned subsequence is typically considered as a measure of similarity~\\cite{serra2009cross, chen2018, tralie2017cover}.\n\n\\subsubsection{Data-driven Approaches} Data-driven approaches aim to learn a function that transforms the data \nto facilitate similarity estimation through conventional measures (e.g.,~Euclidean distance).\nSuch functions are learned using training data, in a supervised or unsupervised fashion.\nIn supervised cases, the semantic relationships (i.e.,~version or non-version) between pairs of items are used for obtaining effective similarity functions.\nThe learned functions depend on the inductive biases of the models and the training process.\n\n\n\\minisec{Data projection}\nAlong with a few classification approaches, early works for data-driven VI considered \ndata projection\nalgorithms like PCA and LDA~\\cite{bertin2012large, humphrey2013data}. \nThey are used for transforming invariant representations obtained in the previous blocks into more compact embedding vectors. \nSuch systems can be considered as hybrid approaches that connect knowledge- and data-driven similarity estimation, as they incorporate rule-based algorithms for their initial steps.\n\n\\minisec{Vector clusters} Another common approach to \nderive\ninput representations was based on vector clustering, such as $k$-means. A learned dictionary of cluster centroids can be used to efficiently encode data with a small number of components. This approach has been used to encode input representations into chord series~\\cite{bello2005robust}, hashcodes~\\cite{kurth2008efficient}, or embedding vectors~\\cite{humphrey2013data}.\n\n\\minisec{Model-based error} Another early data-driven approach to VI was to study model-based errors~\\cite{serra2012, foster2015}. \nIn this approach, a simple parametric model that describes the temporal evolution of the feature sequence is fit to the data.\nThis modeling can be performed on the basis of a single musical piece or from multiple pieces that form a version clique.\nAfter that, the model is used to predict future samples of a new feature sequence coming from a candidate piece (i.e.,~fragments of the sequence are used as input to the model, and this outputs the most reasonable continuation).\nIf the model produces a small error, one concludes the candidate piece is a version of the piece that was used to train the model.\n\n\\minisec{Classification-based training} Classification-based training approaches are perhaps the most popular in supervised learning. \nIn VI, three main formulations exist. \nFirstly, distance/similarity scores can be obtained from multiple systems and used to train a classifier to make a final decision (see Section~\\ref{sec:ensemble_systems}). \nSecondly, a cross-similarity matrix of two tracks can be computed, and a convolutional network used to determine whether the inputs are versions of each other or not (i.e.,~binary classification)~\\cite{lee2018}. \nAlthough computing cross-similarity matrices introduces a computational load for the similarity estimation step, convolutional networks can replace the quadratic-complexity alignment algorithms like SWA, which results in a considerable improvement in terms of overall computational requirements. \nThirdly, the training process can be formulated by considering each clique as a separate class (i.e.,~multi-class classification)~\\cite{xu2018key, yu2019temporal, jiang2020}. \nOne important consideration is that, during inference, it is likely that the system will encounter tracks from cliques that are not in the training data, and using a pure classification strategy, it would not be possible to correctly identify those cases. \nTo handle this, a typical solution is to consider the output of the penultimate layer of the model as the embedding for each track.\nTraining then aims to make the embeddings from different classes linearly separable, which is a way of constructing a similarity function.\n\n\\minisec{Similarity-based training} In recent years, the most popular training formulation in VI is similarity-based, using metric learning approaches~\\cite{doras2019cover, yesiler2019accurate, jiang2020}. \nThe supervision signals in this context only require the information about whether two items are similar (as in, belong to the same clique) or not. \nDuring training, instead of predicting the classes of each item, these systems focus on manipulating the distances of items directly by pulling together similar items and pushing apart the dissimilar ones. \nThe most popular training objectives for this approach are contrastive and triplet losses. \n\n\n\n\\section{Beyond building blocks: improving accuracy and scalability}\\label{sec:beyond_blocks}\nThis section introduces a set of ideas that can be incorporated in VI systems regardless of their building blocks, mainly for improving their accuracy or scalability.\nWe group these ideas into six main categories: version set enhancement, feature fusion, ensemble systems, pruning, fast indexing, and data augmentation.\n\n\n\\subsection{Version Set Enhancement} \n\\label{sec:clique_enhancement}\nVersions of the same composition can be viewed as items of the same set, or clique. Using this intuition, community detection algorithms have been studied to refine the obtained distances between queries and items in a corpus.\nSpecifically, one can construct a fully connected graph based on similarities obtained with any system, eliminate certain edges based on a threshold of some quantity (e.g.,~a distance threshold), and, assuming transitive relations, complete the missing links in this graph~\\cite{serra2009unsupervised}. \nThis process is highly efficient and can lead to better retrieval performance by finding undetected versions and cleaning up the noisy results. \nAs a side benefit, it is possible to use measures of centrality on these completed graphs to estimate the original performance from which subsequent versions arose~\\cite{serra2009unsupervised}.\n\n\\subsection{Feature Fusion} \n\\label{sec:feature_fusion}\nConsidering the complexity of the VI task, no single feature has been able to capture all possible transformations that exist across versions. \nFor instance, while the majority of VI systems work by matching sequences of pitch-based features, this leaves a blind spot for certain genres where the notes do not carry the dominant musical expression, such as '80s hip-hop and drum solos~\\cite{tralie2017cover}. \nAt the same time, features that ignore notes are missing crucial information that helps much of the time~\\cite{tralie2015cover}. \nHence, intuitive solutions for such issues have been proposed to find ways of combining information from various musical dimensions.\n\n\\minisec{Early fusion}\nMusic, in general, has repeated structures that can be represented as a graph, where each node represents a small snippet of audio and edges exist with high weights between snippets that are similar, according to some chosen features.\nDifferent features can lead to different noisy observations of an ideal structure graph and, in most cases, no single feature reliably picks up on all aspects of the \ncomplex musical\nstructure.\nTo address this, it is possible to use a technique known as similarity network fusion~\\cite{tralie2017cover, chen2018} to reconstruct a cleaner graph from these noisy observations, particularly if they contain complementary information.\nIt is possible to adapt these features so that cleaner cross-similarity measures can be obtained between versions, and this can significantly improve the system accuracy over each feature alone~\\cite{tralie2017cover}.\n\n\\minisec{Late fusion}\nAnother possibility for feature fusion is to combine information from various features at later stages of systems. \nFor example, cross-similarity matrices for the same pair of tracks but obtained with different features can be aggregated using simple schemes like taking the maximum or the minimum~\\cite{foucard2010multimodal}. \nIn addition, embedding vectors obtained with systems that use different features can be concatenated and projected into a new space, which can then be shaped by combined characteristics of all input features~\\cite{doras2020combining}.\n \n\\subsection{Ensemble Systems}\n\\label{sec:ensemble_systems}\nOne common strategy to boost overall accuracy is to incorporate the output from multiple pipelines. \nSuch systems, known as ensemble systems, thereby leverage the joint strength of disparate workflows. \nAlthough the motivation behind some ensemble systems is similar to that of feature fusion (combining information from various musical dimensions), here, we describe VI systems that combine multiple systems after they return distance or similarity scores between queries and a corpus of tracks.\n\n\\minisec{Training a classifier}\nA first approach for aggregating\nscores obtained from various systems is to train a shallow classifier that takes a set of scores as input and returns a binary decision (i.e.,~version/non-version). \nDepending on the characteristics of the classifier, non-linear relationships between the input scores may be explored. \nIn VI, this strategy has been explored to combine systems that use different input features (e.g.,~PCP, dominant melody, and bass line)~\\cite{salamon2012melodybass}, and different similarity estimation steps (e.g.,~local alignment and cross-correlation)~\\cite{ravuri2010cover}.\n\n\\minisec{Similarity normalization and aggregation}\nIn cases where the distance scores obtained from various systems are well-calibrated, aggregation of such distances can be trivial with simple schemes like taking the mean, the maximum, or the minimum.\nHowever, when there is a mismatch regarding the scale of such scores, additional operations like simple normalizations are needed to alleviate the issue~\\cite{degani2013heuristic}.\n\nAnother approach to handle scores of different scales is to consider the global ranking of all tracks in a corpus for each system since rankings are automatically invariant to the scale of the similarity scores obtained from a system.\nRank aggregation can then be used to create a\nglobal ranking that incorporates the individual ranks from each system. \nThe Kendall tau distance~\\cite{osmalsky2016enhancing} is an objective function for measuring the agreement of rankings and indicates the number of pairs of ranked tracks that have a reversed order.\nA Kemeny optimal global ranking~\\cite{osmalsky2016enhancing} is a ranking that minimizes the Kendall tau distance to each individual system's rank. \nUnfortunately, finding such a ranking is NP-hard. \nHowever, using an initial guess based on heuristics such as mean and median rank aggregation, followed by local Kemenization, in which greedy swaps are performed until the Kendall tau distance is minimized, can lead to superior performance over individual systems in practice~\\cite{osmalsky2016enhancing}.\n\n\\minisec{Late similarity network fusion}\nIn addition to promoting cliques \nfrom a single system, it is also possible to fuse graphs from multiple similarity networks. \nSimilarity network fusion can again be used\nto enhance cliques, but \nthe algorithm operates at the track level instead of the time frame level (as done in feature fusion).\nDue to normalizations based on local neighborhoods within each network, this technique can fuse similarity measures from any set of systems, and it has been shown in practice to improve accuracy when fusing\nPCP-based systems that use different alignment schemes~\\cite{chen2018}, as well as between systems built on timbral and PCP-based features~\\cite{tralie2017cover}.\n\n\\subsection{Pruning} \n\\label{sec:pruning}\nAs many information processing systems suffer from the accuracy--scalability trade-off, a plausible solution is to design multi-step systems where the scalability and the accuracy of multiple systems may complement one another.\nFor this, fast algorithms can prune the corpus to allow slow but better-performing systems to operate on a reduced set of data for improving the computation times.\n\n\\minisec{Scalable VI systems}\nLately, deep learning--based VI systems have made substantial contributions for bridging the accuracy--scalability gap, but before them, scalable VI systems were not sufficient for obtaining confident results. \nHowever, considering their far-from-random performances, such early systems became a natural choice to be used as the first step of pruning-based, multi-step systems~\\cite{cai2016}. The general tendency was to use systems that encode tracks into compact embedding vectors as the first step, mainly to take advantage of the fast lookup times. \nAfterward, local alignment--based systems were used on the pruned set of tracks to obtain the final results. \nPruning systems nonetheless need to have good recall (at the expense of good precision, if necessary).\n\n\\minisec{Weak rejectors}\nThere is a multitude of features that are similar for versions of the same track, but which are not strong enough indicators to confidently label them as versions of one another. \nStill, having a large collection of such ``weak rejectors'' can be used to narrow down candidates, leading to improved scalability.\nAmong such features are bag of words of PCP features, duration and tempo of a recording~\\cite{osmalsky2016enhancing}, and structure-related descriptors~\\cite{yesiler2019tacos}. \nAlthough not applicable in the audio-based VI literature,\ntextual bag of words can also be applied to title and lyrics information, if they are available~\\cite{correya2018large}.\n\n\\subsection{Fast Indexing} \n\\label{sec:fast_indexing}\nLike every other information retrieval system, VI systems store and index tracks for future lookup and comparison. \nThis perspective motivated other strategies to devise new music representations, inspired by text-based content indexing. \nDifferent kinds of efficient text indexing algorithms were then adapted to music retrieval: for instance, and among others, inverted file indexing and LSH. \n\n\\minisec{Inverted file indexing} In a text-based indexing context, the idea is to establish a list of keywords (the codebook) and to use these keywords to index the documents where they appear. \nIn a VI context, the codebook contains encodings of audio shingles, which are used as indexes to the full audio tracks. \nFor instance, a $k$-means approach was proposed to learn a codebook from the set of all PCP vectors present in a corpus. \nThe inverted index was built using the closest code to each PCP frame as an index to the full song~\\cite{kurth2008efficient}. \nAnother proposal built the codebook encoding each PCP sequence as a major/minor chord series and then using short chord subsequences as index entries~\\cite{martin2012blast}.\n\n\\minisec{Locality-sensitive hashing} The basic idea of LSH indexing is to devise a hashing function that will guarantee that similar contents are encoded by the same hash with a high probability, while the probability that different contents are mapped to the same hash remains low.\nThere are various ways to generate hashing functions that will satisfy these properties~\\cite{casey2006song}.\nSeveral authors adapted this principle to the VI context and proposed encoding shingles of input representations with an LSH scheme (e.g.,~using dominant melody~\\cite{marolt2008mid}, chord progression~\\cite{khadkevich2013large}, or PCP~\\cite{casey2006song}).\n\nThe recent deep learning--based systems (see Section~\\ref{sec:similarity_estimation}), which also encode tracks into compact vector embeddings, have superseded these fast-indexing approaches. \nHowever, techniques like LSH could still be considered to further speed up the retrieval process of deep learning--based systems (see Section~\\ref{sec:scalability_tradeoffs}).   \n\n\n\\subsection{Data Augmentation} \nWith the increasing interest in data-driven VI systems, domain-specific strategies for robust representation learning are becoming more important. \nFor this, we now introduce data augmentation strategies that are inspired by musical characteristics that can be modified while creating versions of a composition.\n\n\\minisec{Pitch transposition}\nTo simulate pitch transpositions that are typically observed between versions, different strategies can be used based on the input representation. \nFirstly, regardless of the input representation, a pitch shift transformation can be applied to the audio signal before feature extraction. \nWhile this operation is universal and does not depend on the type of input, it can introduce some artifacts on the signal and may require more computational resources than its alternatives. \nSecondly, if the PCP features are used as input representations, a simple circular shift along the frequency axis is sufficient to simulate a pitch shift operation, thanks to their octave-independent characteristics~\\cite{xu2018key, yesiler2019accurate}. \nLastly, if melody or CQT representations are used, shifting the values by a certain number of rows along the frequency axis can be useful. \nHowever, unlike PCP features, these representations are not octave-independent, and the behavior of this operation at the boundary bins (the lowest and the highest) should be considered.\n\n\\minisec{Tempo}\nAs with pitch shift transformations, increasing or decreasing the tempo of a track can be done \nbefore the feature extraction step.\nA common alternative to this is to apply interpolation functions (e.g.,~linear) to the 2D input representations (e.g.,~PCP, melody, or CQT)~\\cite{doras2019cover}.\n\n\\minisec{Timing}\nTo simulate minor timing variations where some notes are sustained, repeated, shortened, or removed, similar operations can be applied to randomly selected frames from 2D input representations. \nFor this, such frames can be duplicated, silenced (by replacing them with zero vectors), or simply removed~\\cite{yesiler2019accurate}.\n\n\\minisec{Input patch sampling}\nSimilar to the idea of shingling, the input patch sampling strategy is to randomly select fixed-size patches from the input representations to use in the training process~\\cite{yesiler2019accurate, yu2019temporal}. \nThis can be viewed as simulating structural changes where some sections are removed from a version. Note that the sizes of the patches for this strategy (e.g.,~120--180\\,s) are generally larger than the sizes used for shingling (e.g.,~30--60\\,s).\n\n\\minisec{Duration}\nWhile applying the input patch sampling transformation, the sizes of the patches can be varied to mitigate bias toward a certain representation length~\\cite{yu2019temporal}.\n\n\\minisec{Noise}\nLastly, several transformations to audio signals can be applied to simulate the differences in recording conditions. Some examples are additive noise, low-pass filters, and MP3 transcoding.\n\n\n\\section{Datasets and Evaluation Metrics}\\label{sec:data_and_eval}\nThis section presents an overview of the publicly available datasets and the most widely used evaluation metrics for VI. Although there exist different datasets and evaluation methods for various subproblems within VI (see Section~\\ref{sec:sub-fields}), we here focus only on the most frequently used ones.\n\n\\subsection{Datasets}\\label{sec:datasets}\nFinding data for developing and evaluating MIR systems is a challenging issue, mainly due to the fact that musical audio is often subject to copyright. \nHistorically, the impact of this issue on VI was that the researchers were limited to developing and evaluating their systems using in-house private corpora, which made unified benchmarking of systems a difficult task. \nThese datasets had varying characteristics, such as the size of the corpus, the cardinality of cliques, the distribution of musical genres, and so on. \nHowever, with the help of online communities like SecondHandSongs.com, where editors and users annotate musical versions in terms of their connections with previous musical compositions, this issue is mostly alleviated today. \nTherefore, for the remainder of this section, we focus only on the public benchmarks and publicly available datasets that have been frequently used for VI. \nA summary of such datasets can be seen in Table~\\ref{tab:vi-datasets}.\n\n\\begin{table}[tb!]\n\\caption{Publicly available VI datasets}\n\\label{tab:vi-datasets}\n\\begin{threeparttable}\n\\resizebox{\\textwidth}{!}{%\n\\centering\n\\begin{tabular}{l p{0.15\\linewidth} p{0.15\\linewidth} p{0.15\\linewidth} p{0.35\\linewidth}}\n\\hline\\hline\n\\textbf{Dataset} & \\textbf{Training subset\\tnote{1}} & \\textbf{Validation subset\\tnote{1}} & \\textbf{Test subset\\tnote{1}} & \\textbf{Content}   \\\\\n\\hline\n\nMIREX collection & - & - & 330 (30) + \\newline 670 noise tracks & Proprietary collection \\\\\ncovers80~\\cite{ellis2007covers80}& - & - & 160 (80) & Full audio tracks and metadata  \\\\\n\nSecondHandSongs~\\cite{bertin2011million} & 12,960 (4,128) & - & 5,236 (1,726) &\nPre-extracted features (a wide range including PCP, timbral features, beat, etc.) and metadata \\\\\n\nYouTubeCovers~\\cite{silva2015music} & 100 (50) & -  & 250 (50) & \nPre-extracted features (3 PCP variants) and metadata \\\\\n\nSHS-100K~\\cite{xu2018key, yu2019temporal} & 84,340 (4,611) & 10,883 (1,842) & 10,547 (1,692) & YouTube URLs and metadata  \\\\\nDa-TACOS~\\cite{yesiler2019tacos} & 83,904 (14,499) & 14,000 (3,500) & 13,000 (1,000) +  \\newline 2,000 noise tracks & \nPre-extracted features (3 PCP variants, timbral features, and 4 rhythm features) and metadata \\\\\nSHS5+ \\& SHS4-~\\cite{doras2019cover} & 62,311 (7,460) & - & 48,483 (19,445) & Pre-extracted features (CQT, 2 melody, and PCP variants) and metadata\\\\\n\\hline\\hline\n\\end{tabular}%\n}\n\\begin{tablenotes}\n\\item[1] Values outside and inside the parentheses indicate the number of tracks and unique cliques, respectively.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\n\\minisec{MIREX collection} The ``audio cover song identification'' competition in MIREX stood out as the only platform for benchmarking in the early days of VI. The dataset used in this competition is private and includes 1,000~tracks from a variety of genres. 670~among them are considered as ``noise\u2019\u2019 tracks that do not belong to the same clique as any others. The rest of the data is organized into 30~cliques with 11~versions each. While the query set consists of only those with multiple versions (330~tracks), the corpus includes the entire collection of 1,000~tracks. The inclusion of noise tracks that are not queried is done mainly to imitate the distribution of industrial corpora, and it influenced some of the forthcoming publicly available VI datasets.\n\n\\minisec{covers80} Apart from the MIREX collection, the first dataset curated for VI research is covers80~\\cite{ellis2007covers80}, and it was released publicly as opposed to the former. It includes full-length audio files for 160~tracks divided into 80~cliques with 2~versions each and no noise tracks. The majority of this data is taken from the uspop2002 dataset\\footnote{\\url{https://labrosa.ee.columbia.edu/projects/musicsim/uspop2002.html}}, and the rest was taken from a few commercial ``cover albums.'' The major advantage of this dataset is that it includes audio files for all tracks, which enables researchers to develop and evaluate systems that use novel input representations. However, the limited size is an important drawback as the reported results may not be statistically significant for a true comparison of systems.\n\n\\minisec{SecondHandSongs} The next publicly available dataset for VI was the SecondHandSongs (SHS) dataset~\\cite{bertin2011million}. It is a subset of the Million Song Dataset~\\cite{bertin2011million}\\footnote{\\url{http://millionsongdataset.com/}}, and the version annotations are obtained using the SecondHandSongs.com API\\footnote{\\url{https://secondhandsongs.com/page/API}}. It includes a training set with 12,960~tracks split into 4,128~cliques and a test set with 5,236~tracks split into 726~cliques, without any noise tracks. With the release of SHS, VI research\nentered into a new era, which led to development of scalable systems that can leverage and be evaluated on large datasets. However, due to legal issues, this dataset includes only pre-extracted features that were obtained using the EchoNest API\\footnote{\\url{https://en.wikipedia.org/wiki/The_Echo_Nest}}. Therefore, the VI systems developed and evaluated using this dataset have a strict limitation in the input representations they can use, which may reduce accuracy and hinder system deployment in the real world due to their proprietary nature.\n\n\\minisec{YouTubeCovers} Following the idea of sharing pre-extracted features, the YouTubeCovers dataset was released with a larger set of harmonic features compared to SHS~\\cite{silva2015music}. It includes a total of 350~tracks from 50~cliques and is further split into a training subset with 100~tracks (2~per clique) and a test subset with 250~tracks (5~per clique) with no noise tracks. However, having the same cliques in both training and test subsets may result in biased evaluations. Moreover, like covers80, the rather small evaluation set may lead to statistically insignificant results. \nAlthough there are still research papers using this dataset, the URL shared in the original publication for obtaining the dataset is no longer maintained. \n\n\\minisec{SHS-100K} With deep learning--based systems getting more prominent, a need for larger datasets has emerged. Addressing this need, SHS-100K includes a total of 108,869~tracks split into 9,202~cliques (no noise tracks), which was a considerable increase compared to the largest dataset until then~\\cite{xu2018key}. The version annotations are collected from SecondHandSongs.com, and the dataset includes YouTube links for the tracks, rather than any pre-extracted features. It was initially divided into training, validation, and test subsets with 101,968~tracks (8,177~cliques), 3,918~tracks (909~cliques), and 2,983~tracks (116~cliques), respectively. However, in a later publication, the dataset was split in a different way mainly to have a larger test set, having 84,340~tracks (4,611~cliques), 10,883~tracks (1,842~cliques), and 10,547~tracks (1,692~cliques) for training, validation, and test, respectively~\\cite{yu2019temporal}.\n\n\\minisec{Da-TACOS} Another dataset that addressed the need for larger corpora is Da-TACOS~\\cite{yesiler2019tacos}. It includes a benchmark set with 15,000~tracks that split into 1,000~cliques with 13~versions each and 2,000~noise tracks. Like many others, the version annotations are obtained using the API of SecondHandSongs.com. To enable researchers to experiment with not only harmonic but also rhythmic and timbral characteristics, it includes a large set of pre-extracted features along with the metadata that is linked to the composition and performance IDs used in SecondHandSongs.com. Therefore, even though the audio files are not available, researchers can use the detailed metadata to recover the tracks themselves. Along with the benchmark set, the authors also released a framework called ``acoss'' for feature extraction and benchmarking designed for VI. It includes feature extraction functions with the hyperparameters used for preparing Da-TACOS, and open-source implementations for 7~VI systems. Furthermore, a training set for Da-TACOS was recently released, containing a training subset with 83,904~tracks (14,499~cliques), and a validation subset with 14,000~tracks (3,500~cliques). \n\n\\minisec{SHS-5+ \\& SHS-4-} The last dataset we introduce in this section is SHS-5+ \\& SHS-4-~\\cite{doras2019cover}. It includes a training subset, SHS-5+, with 62,311~tracks in 7,460~cliques, and all the cliques have at least 5~versions each (hence the name). The test subset, SHS-4-, includes 48,483~tracks in 19,445~cliques, with 2 to 4~versions per clique, and it is the largest benchmark set for VI to date. Neither subset includes noise tracks. The version annotations are obtained using the SecondHandSongs.com API, and the dataset includes a large set of pre-extracted features, including CQT and melodic representations. \n\n\\subsection{Evaluation Metrics}\\label{sec:metrics}\nAs previously introduced, VI systems aim to model \nthe shared information between\nversions of the same underlying composition in order to provide a similarity score. The ability of a system to correctly assess the similarities between a query track and a corpus of tracks\nis usually evaluated by metrics that operate on a ranked sequence of results. Such a ranked sequence is obtained by first estimating similarity scores between a query and all the tracks in a corpus and then sorting the items in the corpus with respect to their similarities to the query. Below, we introduce the set of metrics typically used in VI (see~\\cite{doras2020phd} for details). \n\n\\minisec{Precision and recall} Being perhaps the most typical metrics in information retrieval, precision and recall provide rank-independent measures of system performance, which means that the order of the items does not affect the outcome. Precision gives the ratio of the retrieved items that are relevant\\footnote{Here, we use the term ``relevant'' to denote the items in the corpus that are versions of the query.} to all the retrieved ones (in other words, the accuracy of the system's predictions). Recall, on the other hand, gives the ratio of the retrieved items that are relevant to all the relevant items in the corpus (in other words, how well the system finds the relevant items). \nIn VI, they are typically computed as Precision@$K$ (P@$K$) or Recall@$K$ (R@$K$) at a cut-off rank $K$, meaning only the first $K$ results are considered. Note that although precision and recall are rank-independent (i.e.,~the order of the items does not matter), P@$K$ and R@$K$ require a cut-off rank by definition and can be considered as rank-aware (i.e.,~the items have to be placed below rank $K$).\n\n\\minisec{Mean average precision} Although precision and recall are common metrics, their rank-independent characteristics are not useful for tasks where the order of the retrieved items is important. A possible alternative for considering the ranks of the results is mean average precision (MAP). For this, average precision (AP) for all queries are computed and averaged. AP for a single query is obtained by averaging P@$K$ scores over all $K$ where a relevant item is returned, which makes AP a rank-aware metric in contrast to precision. Therefore, MAP assesses not only the number of relevant items in the results but also their ranks.\n\n\\minisec{Mean reciprocal rank} Another rank-aware metric for assessing system performances is mean reciprocal rank (MRR). It is the average of reciprocal rank scores obtained for all the queries, where reciprocal rank is the multiplicative inverse of the rank of the first relevant item. \nTherefore, this metric is more appropriate for cases where either there is only one relevant item in the corpus, or if only the position of the first relevant item is important. Note that when there is a single relevant item in the corpus, MRR is equal to MAP.\n\n\\minisec{Mean rank of the first relevant item} The last metric we introduce is the mean rank of the first relevant item (MR1). As MRR, this metric also uses only the first relevant item, and the only difference between them is the multiplicative inverse function in MRR. However, MR1 may be easier to interpret as ranks are taken directly. Note that the scale of differences between MR1 scores are the same everywhere, but such differences between MRR scores are scaled down when \nhigher\nranks are considered. For example, when comparing two cases where the first relevant items have ranks 1 and 11, the \nreciprocal ranks\nare 1.00 and 0.09, respectively. On the other hand, when comparing two cases where the ranks of the first relevant items are 40 and 50, the \nreciprocal ranks\nare 0.025 and 0.020, respectively.    \n\n\n\\section{Open issues and future directions}\\label{sec:issues}\nAlthough VI research has made substantial progress in the last 20 years, there are many challenges that have yet to be addressed.\nHere, we outline some current open issues to provide guidance for researchers that are interested in contributing to the field.\nThese challenges include, but are not limited to, (1) the task definition itself, (2) evaluation methodologies, (3) trade-offs that arise when scaling VI systems, (4) accuracy gaps on certain under-represented types of content, and (5) the variety of applications and the different treatments they require.\n\n\\subsection{Task Definition}\nAs discussed in Section~\\ref{sec:intro}, the definition of a musical version, especially a ``cover song,'' may differ in various contexts. To avoid such differences, we have so far favored a quite permissive definition in this article, labeling all the tracks that are derived from a musical composition as versions. Although this definition is convenient for introducing and discussing VI from an academic research point of view, applications that consider legal aspects of VI (e.g.,~detecting copyright infringements) may require different definitions that are more suitable for their purposes.\n\nThe definition of a version for legal applications often needs to be based upon the rightsholders (typically songwriters and recording labels) rather than the musical connections.\nFor example, a composer may copyright a new arrangement of a folk song that is in the public domain. According to our inclusive definition of a version, the arrangement would be a version of the original, but legally, it may be a separate entity.\nThese rights themselves are often not well-defined, and there are a number of famous lawsuits\\footnote{\\url{https://www.bbc.com/culture/article/20190605-nine-most-notorious-copyright-cases-in-music-history}} about whether or not the creators of a track must pay royalties to the rightsholders.\nPublishing data, which provides a legal link between track metadata and composition metadata, often exists in text form, linking songwriters/composers, track titles, and recording/composition identifiers such as ISRCs/ISWCs\\footnote{\\url{https://isrc.ifpi.org/}}.\n\n\nThere are cases where this metadata is the only information separating two nearly identical tracks (such as an original release and a remastered release) into different legal entities.\nThus, purely audio-based VI for legal applications is not possible in many cases,\nand any successful system must consider additional information such as editorial metadata to disambiguate unclear cases.\n\nApart from the legal perspective, current VI systems are typically built around a particular notion of which musical features are important for determining whether two tracks are versions of one another.\nThese notions fit well for some music traditions, such as Western pop and classical music; \nhowever, there are other musical traditions that break some of these assumptions.\nFor example, in Indian art music, certain melodic phrases or motifs are crucial for identifying the ragas (roughly speaking, modes) that tracks belong to. From a Western point of view, two tracks that include versions of the same melodic phrase would mostly mean that they originate from the same composition; however, in Indian art music, it can simply mean that they belong to the same raga.\n\n\\subsection{Evaluation Methodologies} \nEvaluation of VI systems is typically performed on well-curated datasets (see Section~\\ref{sec:datasets}) using mostly rank-aware evaluation metrics (see Section~\\ref{sec:metrics}).\nThese datasets are dominated by music with singing voice, music with well-defined versions, music from mostly pop, rock, and jazz genres, and generally do not contain duplicates. Industry-scale music corpora, however, often have quite different distributions. In such corpora, near-exact duplicates are very common,\nit is difficult to apply the concept of versions for the majority of music,\ngenres like rap and electronic music constitute a large proportion of the data,\nplus a non-negligible subset of the tracks does not contain singing voice.\nThis presents a challenge when extrapolating the performance of a system on an industry-scale corpus using evaluations performed on well-curated datasets. Furthermore, there are several limitations of the commonly used evaluation metrics. Firstly, they are highly sensitive to the number of relevant tracks in the corpus per query, which may not be appropriate for VI since some compositions may have hundreds of versions while some others may have only a few, or even zero. Additionally, the metrics mostly consider only the rank ordering of the tracks and not the distances between the query and the retrieved items, which makes it difficult to assess how well-separated the relevant items are from the irrelevant ones. \n\nNear-duplicates (i.e.,~content that is the same but distorted enough so that a standard music fingerprinting algorithm will not provide a match) present a particular challenge: \ngiven a query, VI systems will naturally assign a lower distance to a near-duplicate compared to other versions that may contain several changes in musical characteristics. \nThis highlights both of the previously mentioned issues regarding the evaluation metrics.\nFor example, for P@$K$, the presence of duplicates can both increase and decrease the metric in an unpredictable way, as this changes the number of relevant tracks for a given query.\nConsider this toy example: corpus A has no duplicates, and corpus B \nis an extended version of corpus A, where each item has one near-duplicate. \nNow consider a VI system that, for a given query, would return 5 relevant results out of the first 10 for corpus A (i.e.,~P@$10=0.5$).\nIf the relevant results are in positions 1 through 5, the equivalent query for corpus B would have 10 relevant results (i.e.,~P@$10=1.0$).\nConversely, if the relevant results are in positions 6 through 10 (for corpus A), the equivalent query for corpus B would have 0 relevant results (i.e.,~P@$10=0.0$).\n\nMusic without well-defined versions, such as ambient music and soundscapes, leave an open question: how should VI systems handle this type of content?\nPractically, a VI system should not retrieve any matches when no versions are present.\nHowever, this kind of content typically does not have \nclear melodic, harmonic, or structural characteristics.\nAs a result, the features VI has historically used are typically close to zero everywhere and are confidently, and incorrectly, clustered together as a tight group by VI systems.\n\nThe genre distributions of research datasets may introduce a bias toward certain input representations and musical characteristics. For example, the success of PCP representations in VI is fairly easy to explain for datasets having many tracks from the pop, rock, and jazz genres. However, the performances of state-of-the-art systems on other genres where rhythmic and timbral properties are highlighted is an under-explored issue in VI. Considering the popularity of hip-hop and electronic music genres (and their sub-genres), this is clearly an issue to be addressed in VI research to be useful in the current music ecosystem. \n\nFinally, VI research, except for a subfield focusing on Western classical music, has under-explored how systems behave for instrumental music, largely due to the existing datasets being dominated by music with singing voice. \nAs a result, the performance of current VI systems on instrumental music is not well understood, and thus the performance on industry-scale corpora, which contain a considerable percentage of instrumental content, cannot be inferred.\n\n\\subsection{Scalability Trade-offs} \n\\label{sec:scalability_tradeoffs}\nAlthough VI has clear industrial applications in the current music ecosystem, the scope of scalability-related discussions in research papers is rather limited. It has been demonstrated that vector-based techniques provide large benefits in computation and memory requirements compared to alignment-based ones, but no systematic evaluation of vector-based techniques has been performed from a scalability perspective. The scalability considerations in such works are typically limited to the size of the embedding vectors. However, the computations that produce these vectors (e.g.,~feature extraction algorithms, or deep neural network layers) are mostly ignored. Therefore, here, we highlight two directions to cover this under-explored perspective.\n\nFirstly, for the vector-based techniques, in particular, there is an additional accuracy--scalability trade-off that arises especially in industry-scale datasets: retrieving the $K$ nearest results for a query.\nConsider a vector-based system, with vectors in $\\mathbb{R}^d$ under the Euclidean norm. \nIn order to retrieve the $K$ nearest results from a corpus of $N$ items in an exact manner, at least $O(N\\log{N})$ operations are required, and these exact algorithms are difficult to improve due to the ``curse of dimensionality.''\nEven if pruning techniques are employed to reduce $N$, when performing large numbers of $K$ nearest neighbor look-ups, the computational efficiency can greatly influence the speed and cost of deploying a VI system.\nIn practice, approximate nearest neighbor search algorithms\\footnote{For example, \\url{https://github.com/erikbern/ann-benchmarks}} are used.\nThese algorithms provide efficient approximations for finding the $K$ nearest neighbors for high-dimensional data over large corpora and introduce a trade-off between the time-per-query and the recall (i.e.,~the percentage of true nearest neighbors returned): the higher the recall, the slower the query.\nThe degree of trade-off varies by algorithm and dataset, but roughly, at 80\\% recall, these algorithms provide a speedup of a factor between 100 and 1000 over an exact computation.\nFor a fixed speedup, the achieved recall typically decreases as the dimensionality of the vectors increases, which highlights an additional motivation for the dimensionality reduction or data projection methods described in Section~\\ref{sec:similarity_estimation}.\n\nSecondly, to better understand the time and memory complexities of VI systems, additional metrics such as floating point operations per second (FLOPS) and peak memory usage can be reported. The goal then would be to use such metrics to compare entire workflows when performing matching for many queries against a large corpus, from feature extraction up through the final similarity estimation steps. Although not all VI research needs to aim for industrial-level scalability, such information can be useful for comparing application-oriented VI systems.\n\n\\subsection{Accuracy Gaps} \nImproving the accuracy of VI systems has been the main goal of most VI research, and we now highlight a number of ideas to accelerate the progress toward this goal.\nThe commonly used features described in Section~\\ref{sec:input_repr} have been successful at capturing relevant information for quantifying similarities between versions for most mainstream music. However, in some edge cases (e.g.,~cross-genre versions, \\textit{a cappella} versions, and versions of drum solos), such features may fail drastically. Therefore, to further improve the accuracy of current systems, other musical dimensions should be considered. The biggest challenge here is to find ways to exploit these uncommon musical characteristics while keeping in mind the principal invariances a VI system must consider. For example, for certain cases, a particular rhythmic pattern may facilitate identification, but using only rhythmic information for identifying versions would certainly fail in a large body of popular music where the rhythmic patterns are similar for various compositions.\n\nA musical dimension that has not been considered in previous works is the lyrics.\nLyrics could be a major factor for improving VI accuracy for versions \nthat share the same lyrics/rhyme patterns but little else,\nsuch as those \nwith no prominent melodic or harmonic characteristics to rely on,\nthose with greatly varying singing styles, or those with lyrics as the only connecting property.\nEstimating lyrics from directly polyphonic audio is a challenging task that has been explored, but with limited success.\nHowever, as has been done for harmony and melody, features capturing approximate lyric information (e.g.,~phoneme-related features of the singing voice) could be a useful, complementary signal to the existing feature set.\nAdditionally, onset and rhythm information is not yet well-captured by the existing features, yet they are key features for determining similarity between certain types of music, such as rap and electronic music.\n\nTrack metadata, such as tags describing the high-level musical properties, can also be a powerful, under-explored signal for VI~\\cite{correya2018large}. Typically, systems aim to be invariant to such high-level characteristics, but using tags such as ``vocal'' or ``instrumental'' as a way of conditioning may improve system performances, as they could inform the systems about what kind of properties they should focus on exploiting.\n\nAs for more extreme examples, there are categories of music where none of the existing or aforementioned dimensions adequately capture what makes tracks similar or not, such as sound art, soundscapes (e.g.,~rain sounds, city streets), ambient music (e.g.,~singing bowls, drones), etc. \nIn these cases, the composition is closely tied to the properties of the recording itself, such as the precise sound qualities and placement of events in time.\nTo address such cases, music fingerprinting techniques could be applied, e.g.,~as a pre-processing step of a VI system. \n\nAnother potential direction for improving accuracy is to fully embrace data-driven representation learning. While hand-designing features has proven to be useful for VI, it introduces a bias toward what VI systems are able to model.\nAlternatively, end-to-end learning paradigms could be explored, where given a sufficient amount of data, a system learns which properties of the track are most important for the task.\nIn particular, these techniques give systems the potential to uncover relations beyond melody/harmony that are relevant for identifying matching versions.\nThese techniques have seen some success in other related domains such as speech recognition and have not yet been explored for VI.\n\nFinally, there is an opportunity for VI systems to place more focus on post-processing operations, such as version set enhancement methods.\nSuch operations are generally computationally cheap and are proven to improve accuracy. However, except for a few research papers in the early 2010s, this research direction has been on standby.\n\n\\subsection{Emphasis on Subfields and  Applications}\\label{sec:sub-fields}\nMost VI research focuses on the general problem of identifying and retrieving versions of tracks, but there are a number of under-explored subfields and practical applications of these systems.\nCertain subfields of VI focus on particular types of versions in order to address or exploit specific characteristics and challenges. For example, in versions of Western classical music~\\cite{zalkow2020} (see the column ``Performances'' in Fig.~\\ref{fig:version_types}), the musical variations are typically limited to those in tempo, timing, key, ``noise,'' and possibly structure (e.g.,~the presence or absence of repeats).\nTherefore, the systems designed to be used in such cases focus more on being robust to noise and timing distortions while assuming \nmelodic and harmonic characteristics are likely to be shared between versions.\n\nRather than identifying full tracks, there is the interesting subproblem of identifying versions of short queries, or phrases (e.g.,~3--15 seconds)~\\cite{muller2005audio}.\nConsidering the difficulties that music fingerprinting systems have with identifying versions (even live performances), such an application scenario could address a particular need for end-users. \nMoreover, given the ability to identify versions of short phrases, their origins could be identified, which could enable musicologists to create phylogenetic trees of musical phrases. \nUsing these, musical influences within and between musical genres and styles can be studied to have a better understanding of the evolution of musical practice.\n\n\nAnother less-explored application is in ``setlist identification,'' wherein the task is to identify versions from a long recording consisting of a sequence of versions of different tracks.\nThe long recording could be, for example, a live recording of a concert, a DJ set, or a medley.\nSuch long recordings are usually processed with overlapping windows that span typically 1--2 minutes. However, this is error-prone, as the windows may cross multiple tracks. To avoid this, a segmentation step can be performed beforehand, but such algorithms may also introduce erroneous segments, especially when live tracks are interrupted briefly for banter or applause. Therefore, solving problems other than identification performance may be crucial for a VI system to be used for setlist identification.\n\nIn some applications, the typical setup of having a fixed corpus does not hold, and instead, the VI problem exists in an ``online'' setting.\nIn this case, the goal is to build a graph of connections over time (e.g.,~as new tracks are added to a corpus) in an online fashion.\nIn this application, there are many open problems, such as how to avoid error propagation (e.g.,~by applying version set enhancement methods), and exploring efficient ways to perform the online steps.\n\nFinally, applications that match audio to editorial metadata have not been well-explored.\nA common example is matching a registered ``composition'' which exists purely as text metadata to a corpus of tracks.\nIn this context, once there is at least one track connected to a composition, standard VI techniques can be employed.\nFurther, it is common to match new tracks to existing ``compositions'' which already have several matching tracks, moving the problem from track-to-track matching to track-to-clique matching.\n\nVI has come a long way in the last 20~years: from early, symbolic sequence--based approaches to recent, representation learning--based ones, a great number of techniques and ideas have been studied to approach this problem that is deeply connected to the history of musical practice. However, there is still a long way to go before we can consider the problem as solved, because, all in all, ``there is nothing that says a great song cannot be interpreted at any time in any way.''\\footnote{Phil Ramone.}\n\\section*{Acknowledgments}\nF. Yesiler is supported by the MIP-Frontiers project, the European Union\u2019s Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No. 765068.\n\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\n\\bibliographystyle{IEEEtran}\n", "meta": {"timestamp": "2021-09-07T02:34:17", "yymm": "2109", "arxiv_id": "2109.02472", "language": "en", "url": "https://arxiv.org/abs/2109.02472"}}
{"text": "\\section{Introduction}\n\nDimensionality reduction and manifold learning can be used for feature extraction and data visualization. Dimensionality reduction methods can be divided into three categories which are spectral methods, probabilistic methods, and neural network-based methods \\cite{ghojogh2021data}. Some of the probabilistic methods are neighbor embedding methods where the probabilities of neighborhoods are used in which attractive and repulsive forces are utilized for neighbor and non-neighbor points, respectively. Some of the well-known neighbor embedding methods are Student's t-distributed Stochastic Neighbor Embedding (t-SNE) \\cite{maaten2008visualizing,ghojogh2020stochastic}, LargeVis \\cite{tang2016visualizing}, and Uniform Manifold Approximation and Projection (UMAP) \\cite{mcinnes2018umap}. Interestingly, both t-SNE and UMAP are state-of-the-art methods for data visualization. \nThe reason behind the name of UMAP is that it assumes and approximates that the data points are uniformly distributed on an underlying manifold. The term ``projection\" in the name of algorithm is because it sort of projects, or embeds, data onto a subspace for dimensionality reduction. \n\nThe theory behind UMAP is based on algebraic topology and category theory. \nThe main idea of UMAP is constructing fuzzy topological representations for both high-dimensional data and low-dimensional embedding of data and changing the embedding so that its fuzzy topological representation becomes similar to that of the high-dimensional data. UMAP has been widely used for DNA and single-cell data visulization and feature extraction \\cite{becht2019dimensionality,dorrity2020dimensionality}. It is noteworthy that t-SNE has also been used for single-cell applications \\cite{kobak2019art}.  \nSome other applications of UMAP are visualizing deep features \\cite{carter2019activation}, art \\cite{vermeulen2021application}, and visualizing BERT features in natural language processing \\cite{coenen2019visualizing,levine2019sensebert}. \nThis paper is a tutorial and survey paper on UMAP and its variants. \n\nThe remainder of this paper is organized as follows. We explain the details of UMAP algorithm in Section \\ref{section_UMAP} and explain the category theory and algebraic topology behind it in Section \\ref{section_categoryTheory_algebraicTopology}. Explaining UMAP as neighbor embedding and comparison with t-SNE and LargeVis are explained in Section \\ref{section_compare_tSNE_LargeVis}. Then, we discuss the repulsive forces, negative sampling, and effective cost function of UMAP in Section \\ref{section_discussion_repulsive_forces}. DensMAP, parametric UMAP, and progressive UMAP are introduced in Sections \\ref{section_DensMAP}, \\ref{section_parametric_UMAP}, and \\ref{section_progressive_UMAP}, respectively. Finally, Section \\ref{section_conclusion} concludes the paper. \n\n\\section*{Required Background for the Reader}\n\nThis paper assumes that the reader has general knowledge of calculus, probability, linear algebra, and basics of optimization. \nThe required background on algebraic topology and category theory are explained in the paper. \n\n\\section{UMAP}\\label{section_UMAP}\n\n\\subsection{Data Graph in the Input Space}\n\nConsider a training dataset $\\ensuremath\\boldsymbol{X} = [\\ensuremath\\boldsymbol{x}_1, \\dots, \\ensuremath\\boldsymbol{x}_n] \\in \\mathbb{R}^{d \\times n}$ where $n$ is the sample size and $d$ is the dimensionality. We construct a $k$-Nearest Neighbors ($k$NN) graph for this dataset. \nIt has been empirically observed that UMAP requires fewer number of neighbors than t-SNE \\cite{sainburg2020parametric}. Its default value is $k=15$.\nWe denote the $j$-th neighbor of $\\ensuremath\\boldsymbol{x}_i$ by $\\ensuremath\\boldsymbol{x}_{i,j}$.\nLet $\\mathcal{N}_i$ denote the set of neighbor points for the point $\\ensuremath\\boldsymbol{x}_i$, i.e., $\\mathcal{N}_i := \\{\\ensuremath\\boldsymbol{x}_{i,1}, \\dots, \\ensuremath\\boldsymbol{x}_{i,k}\\}$.\nWe treat neighbor relationship between points stochastically. \nInspired by SNE \\cite{hinton2003stochastic} and t-SNE \\cite{maaten2008visualizing,ghojogh2020stochastic}, we use the Gaussian or Radial Basis Function (RBF) kernel for the measure of similarity between points in the input space.\nThe probability that a point $\\ensuremath\\boldsymbol{x}_i$ has the point $\\ensuremath\\boldsymbol{x}_j$ as its neighbor can be computed by the similarity of these points:\n\\begin{align}\\label{equation_UMAP_p_Directional}\np_{j|i} := \n\\left\\{\n    \\begin{array}{ll}\n        \\exp\\big(\\!-\\frac{\\|\\ensuremath\\boldsymbol{x}_i - \\ensuremath\\boldsymbol{x}_j\\|_2 - \\rho_i}{\\sigma_i}\\big) & \\mbox{if } \\ensuremath\\boldsymbol{x}_j \\in \\mathcal{N}_i \\\\\n        0 & \\mbox{Otherwise},\n    \\end{array}\n\\right.\n\\end{align}\nwhere $\\|.\\|_2$ denotes the $\\ell_2$ norm.\nThe $\\rho_i$ is the distance from $\\ensuremath\\boldsymbol{x}_i$ to its nearest neighbor:\n\\begin{align}\\label{equation_umap_rho}\n\\rho_i := \\min\\{\\|\\ensuremath\\boldsymbol{x}_i - \\ensuremath\\boldsymbol{x}_{i,j}\\|_2\\, |\\, 1\\leq j\\leq k\\}.\n\\end{align}\nThe $\\sigma_i$ is the scale parameter which is calculated such that the total similarity of point $\\ensuremath\\boldsymbol{x}_i$ to its $k$ nearest neighbors is normalized. By binary search, we find $\\sigma_i$ to satisfy:\n\\begin{align}\\label{equation_umap_sigma}\n\\sum_{j=1}^k \\exp\\!\\Big(\\!\\!-\\!\\frac{\\|\\ensuremath\\boldsymbol{x}_i - \\ensuremath\\boldsymbol{x}_{i,j}\\|_2 - \\rho_i}{\\sigma_i}\\Big) = \\log_2(k).\n\\end{align}\nNote that t-SNE \\cite{maaten2008visualizing} has a similar search for its scale using entropy as perplexity. These searches make the neighborhoods of various points behave similarly because the scale for a point in a dense region of dataset becomes small while the scale of a point in a sparse region of data becomes large. In other words, UMAP and t-SNE both assume (or approximate) that points are uniformly distributed on an underlying low-dimensional manifold. This approximation is also included in the name of UMAP. \n\nEq. (\\ref{equation_UMAP_p_Directional}) is a directional similarity measure. To have a symmetric measure with respect to $i$ and $j$, we symmetrize it as:\n\\begin{align}\\label{equation_UMAP_p}\n\\mathbb{R} \\ni p_{ij} := p_{j|i} + p_{i|j} - p_{j|i}\\, p_{i|j}. \n\\end{align}\nThis is a symmetric measure of similarity between points $\\ensuremath\\boldsymbol{x}_i$ and $\\ensuremath\\boldsymbol{x}_j$ in the input space. \n\n\\subsection{Data Graph in the Embedding Space}\n\nLet the embeddings of points be $\\ensuremath\\boldsymbol{Y} = [\\ensuremath\\boldsymbol{y}_1, \\dots, \\ensuremath\\boldsymbol{y}_n] \\in \\mathbb{R}^{p \\times n}$ where $p$ is the dimensionality of embedding space and is smaller than input dimensionality, i.e., $p \\ll d$. \nNote that $\\ensuremath\\boldsymbol{y}_i$ is the embedding corresponding to $\\ensuremath\\boldsymbol{x}_i$.\nIn the embedding space, the probability that a point $\\ensuremath\\boldsymbol{y}_i$ has the point $\\ensuremath\\boldsymbol{y}_j$ as its neighbor can be computed by the similarity of these points:\n\\begin{align}\\label{equation_UMAP_q}\n\\mathbb{R} \\ni q_{ij} := (1 + a\\, \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b})^{-1},\n\\end{align}\nwhich is symmetric with respect to $i$ and $j$. The variables $a>0$ and $b>0$ are hyperparameters determined by the user. By default, we have $a \\approx 1.929$ and $b \\approx 0.7915$ \\cite{mcinnes2018umap}, although it has been empirically seen that setting $a=b=1$ does not qualitatively impact the results \\cite{bohm2020unifying}. \n\n\\subsection{Optimization Cost Function}\n\nUMAP aims to make the data graph in the low-dimensional embedding space similar to the data graph in the high-dimensional embedding space. \nIn other words, we treat Eqs. (\\ref{equation_UMAP_p}) and (\\ref{equation_UMAP_q}) as probability distributions and minimize the difference of these distributions to make similarities of points in the embedding space similar to similarities of points in the input space. \nA measure for the difference of these similarities of graphs is the fuzzy cross-entropy defined as:\n\\begin{align}\\label{equation_umap_cost}\nc_1 := \\sum_{i=1}^n \\sum_{j=1, j \\neq i}^n \\Big(p_{ij} \\ln(\\frac{p_{ij}}{q_{ij}}) + (1 - p_{ij}) \\ln(\\frac{1 - p_{ij}}{1 - q_{ij}})\\Big),\n\\end{align}\nwhere $\\ln(.)$ is the natural logarithm. \nThe definition of this cross-entropy is in the field of fuzzy category theory which we will explain in Section \\ref{section_categoryTheory_algebraicTopology} (see Eq. (\\ref{equation_fuzzy_cross_entropy})).\n\nThe first term in Eq. (\\ref{equation_umap_cost}) is the \\textit{attractive force} which attracts the embeddings of neighbor points toward each other. \nThis term should only appear when $p_{ij} \\neq 0$ which means either $\\ensuremath\\boldsymbol{x}_j$ is a neighbor of $\\ensuremath\\boldsymbol{x}_i$, or $\\ensuremath\\boldsymbol{x}_i$ is a neighbor of $\\ensuremath\\boldsymbol{x}_j$, or both (see Eq. (\\ref{equation_UMAP_p})). \nThe second term in Eq. (\\ref{equation_umap_cost}) is the \\textit{repulsive force} which repulses the embeddings of non-neighbor points away from each other. As the number of all permutations of non-neighbor points is very large, computation of the second term is non-tractable in big data. Inspired by Word2Vec \\cite{mikolov2013distributed} and LargeVis \\cite{tang2016visualizing}, UMAP uses \\textit{negative sampling} where, for every point $\\ensuremath\\boldsymbol{x}_i$, $m$ points are sampled randomly from the training dataset and treat them as non-negative (negative) points for $\\ensuremath\\boldsymbol{x}_i$. As the dataset is usually large, i.e. $m \\ll n$, the sampled points will be actual negative points with high probability.\nThe summation over the second term in Eq. (\\ref{equation_umap_cost}) is computed only over these negative samples rather than \\textit{all} negative points. \n\n\nUMAP changes the data graph in the embedding space to make it similar to the data graph in the input space. \nEq. (\\ref{equation_umap_cost}) is the cost function which is minimized in UMAP where the optimization variables are $\\{y_i\\}_{i=1}^n$:\n\\begin{align*}\n&\\min_{\\{\\ensuremath\\boldsymbol{y}_i\\}_{i=1}^n} c_1 := \\min_{\\{\\ensuremath\\boldsymbol{y}_i\\}_{i=1}^n} \\sum_{i=1}^n \\sum_{j=1, j \\neq i}^n \\Big(p_{ij} \\ln(p_{ij}) - p_{ij} \\ln(q_{ij}) \\\\\n&~~~~~~~ + (1 - p_{ij}) \\ln(1 - p_{ij}) - (1 - p_{ij}) \\ln(1 - q_{ij})\\Big) \\\\\n&= \\!\\!\\min_{\\{\\ensuremath\\boldsymbol{y}_i\\}_{i=1}^n} -\\sum_{i=1}^n \\sum_{j=1, j \\neq i}^n\\!\\!\\! \\Big( p_{ij} \\ln(q_{ij}) + (1 - p_{ij}) \\ln(1 - q_{ij})\\Big) \n\\end{align*}\nAccording to Eqs. (\\ref{equation_UMAP_p_Directional}), (\\ref{equation_UMAP_p}), and (\\ref{equation_UMAP_q}), in contrast to $q_{ij}$, the $p_{ij}$ is independent of the optimization variables $\\{y_i\\}_{i=1}^n$. Hence, we can drop the constant terms to revise the cost function:\n\\begin{align}\\label{equation_umap_cost2}\n&c_2 := -\\sum_{i=1}^n \\sum_{j=1, j \\neq i}^n \\Big(p_{ij} \\ln(q_{ij}) + (1 - p_{ij}) \\ln(1 - q_{ij})\\Big),\n\\end{align}\nwhich should be minimized. \nTwo important terms in this cost function are:\n\\begin{align}\n&c^a_{i,j} := - \\ln(q_{ij}), \\label{equation_umap_cost2_attractive} \\\\\n&c^r_{i,j} := - \\ln(1-q_{ij}), \\label{equation_umap_cost2_repulsive}\n\\end{align}\nand we can write:\n\\begin{align}\nc_2 &:= \\sum_{i=1}^n \\sum_{j=1, j \\neq i}^n \\Big(p_{ij}\\, c^a_{i,j} + (1 - p_{ij})\\, c^r_{i,j}\\Big) \\label{equation_umap_cost2_withAttractiveAndRepulsiveForces}\\\\\n&\\overset{(a)}{=} 2\\sum_{i=1}^n \\sum_{j=i+1}^n \\Big(p_{ij}\\, c^a_{i,j} + (1 - p_{ij})\\, c^r_{i,j}\\Big), \\label{equation_umap_cost2_withAttractiveAndRepulsiveForces_2}\n\\end{align}\nwhere $(a)$ is because $p_{ij}=p_{ji}$, $c_{i,j}^a=c_{j,i}^a$, and $c_{i,j}^r=c_{j,i}^r$ are symmetric. \n\nThe Eqs. (\\ref{equation_umap_cost2_attractive}) and (\\ref{equation_umap_cost2_repulsive}) are the attractive and repulsive forces in Eq. (\\ref{equation_umap_cost2}), respectively. The attractive force attracts the neighbor points toward each other in the embedding space while the repulsive force pushes the non-neighbor points (i.e., points with low probability of being neighbors) away from each other in the embedding space. \nAccording to Eq. (\\ref{equation_umap_cost2_withAttractiveAndRepulsiveForces}), $c^a_{i,j}$ and $c^r_{i,j}$ occur with probability $p_{ij}$ and $(1- p_{ij})$, respectively.\nFor every point, we call it the \\textit{anchor} point and we call its neighbor and non-neighbor points, with large and small $p_{ij}$, as the \\textit{positive} and \\textit{negative} points, respectively. \n\n\\SetAlCapSkip{0.5em}\n\\IncMargin{0.8em}\n\\begin{algorithm2e}[!t]\n\\DontPrintSemicolon\n    \\textbf{Input}: Training data $\\{\\ensuremath\\boldsymbol{x}_i\\}_{i=1}^n$\\;\n    Construct $k$NN graph\\;\n    Initialize $\\{\\ensuremath\\boldsymbol{y}_i\\}_{i=1}^n$ by Laplacian eigenmap\\;\n    Calculate $p_{ij}$ and $q_{ij}$ for $\\forall i,j \\in \\{1, \\dots, n\\}$ by Eqs. (\\ref{equation_UMAP_p}) and (\\ref{equation_UMAP_q})\\;\n    $\\eta \\gets 1$, $\\nu \\gets 0$\\;\n    \\While{not converged}{\n        $\\nu \\gets \\nu + 1$ \\quad // epoch index\\; \n        \\For{$i$ from $1$ to $n$}{\n            \\For{$j$ from $1$ to $n$}{\n                $u \\sim U(0,1)$\\;\n                \\If{$u \\leq p_{ij}$}{\n                    $\\ensuremath\\boldsymbol{y}_i \\gets \\ensuremath\\boldsymbol{y}_i - \\eta \\frac{\\partial c^a_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_i}$\\;\n                    $\\ensuremath\\boldsymbol{y}_j \\gets \\ensuremath\\boldsymbol{y}_j - \\eta \\frac{\\partial c^a_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_j}$\\;\n                    \\For{$m$ iterations}{\n                        $l \\sim U\\{1, \\dots, n\\}$\\;\n                        $\\ensuremath\\boldsymbol{y}_i \\gets \\ensuremath\\boldsymbol{y}_i - \\eta \\frac{\\partial c^r_{i,l}}{\\partial \\ensuremath\\boldsymbol{y}_i}$\\;\n                        // The next line does not exist in original UMAP:\\;\n                        $\\ensuremath\\boldsymbol{y}_l \\gets \\ensuremath\\boldsymbol{y}_l - \\eta \\frac{\\partial c^r_{i,l}}{\\partial \\ensuremath\\boldsymbol{y}_l}$ \\label{algorithm_umap_update_negativeSample}\\;\n                    }\n                }\n            }\n        }\n        $\\eta \\gets 1 - \\frac{\\nu}{\\nu_\\text{max}}$\\;\n    }\n    \\textbf{Return} $\\{\\ensuremath\\boldsymbol{y}_i\\}_{i=1}^n$\\;\n\\caption{UMAP algorithm}\\label{algorithm_umap}\n\\end{algorithm2e}\n\\DecMargin{0.8em}\n\n\\subsection{The Training Algorithm of UMAP}\n\nThe procedure of optimization in UMAP is shown in Algorithm \\ref{algorithm_umap}. As this algorithm shows, a $k$NN graph is constructed from the training data $\\{\\ensuremath\\boldsymbol{x}_i\\}_{i=1}^n$. UMAP uses Laplacian eigenmap \\cite{belkin2001laplacian,ghojogh2021laplacian}, also called spectral embedding, for initializing the embeddings of points denoted by $\\{\\ensuremath\\boldsymbol{y}_i\\}_{i=1}^n$. \nUsing Eqs. (\\ref{equation_UMAP_p}) and (\\ref{equation_UMAP_q}), $p_{ij}$ and $q_{ij}$ are calculated for all points. Stochastic Gradient Descent (SGD) is used for optimization where optimization is performed iteratively. In every iteration (epoch), we iterate over points twice with indices $i$ and $j$ where the $i$-th point is called the \\textit{anchor}. For every pair of points $\\ensuremath\\boldsymbol{x}_i$ and $\\ensuremath\\boldsymbol{x}_j$, we update their embeddings $\\ensuremath\\boldsymbol{x}_i$ and $\\ensuremath\\boldsymbol{x}_j$ with probability $p_{ij}$ (recall Eq. (\\ref{equation_umap_cost2})). \nIf $p_{ij}$ is large, it means that the points $\\ensuremath\\boldsymbol{x}_i$ and $\\ensuremath\\boldsymbol{x}_j$ are probably neighbors (in this case, the $j$-th point is called the \\textit{positive} point) and their embeddings are highly likely to be updated to become close in the embedding space based on the attractive force. \nFor implementing it, we can sample a uniform value from the continuous uniform distribution $U(0,1)$ and if that is less than $p_{ij}$, we update the embeddings. \nWe update the embeddings $\\ensuremath\\boldsymbol{y}_i$ and $\\ensuremath\\boldsymbol{y}_j$ by gradients $\\partial c^a_{i,j} / \\partial \\ensuremath\\boldsymbol{y}_i$ and $\\partial c^a_{i,j} / \\partial \\ensuremath\\boldsymbol{y}_j$, respectively, where $\\eta$ is the learning rate. \n\n\n\nFor repulsive forces, we use negative sampling as was explained before. If $m$ denotes the size of negative sample, we sample $m$ indices from the discrete uniform distribution $U\\{1, \\dots, n\\}$. These are the indices of points which are considered as \\textit{negative} samples $\\{\\ensuremath\\boldsymbol{y}_l\\}$ where $|\\{\\ensuremath\\boldsymbol{y}_l\\}|=m$. \nAs the size of dataset is usually large enough to satisfy $n \\gg m$, these negative points are probably valid because many of the points are non-neighbors of the considered anchor. \nIn negative sampling, the original UMAP \\cite{mcinnes2018umap} updates only the embedding of anchor $\\ensuremath\\boldsymbol{y}_i$ by gradient of the repulsive force $\\partial c^a_{i,j} / \\partial \\ensuremath\\boldsymbol{y}_i$. One can additionally update the embedding of negative point $\\ensuremath\\boldsymbol{y}_l$ by gradient of the repulsive force $\\partial c^a_{i,j} / \\partial \\ensuremath\\boldsymbol{y}_l$ \\cite{damrich2021umap}. \nThe mentioned gradients are computed in the following lemmas. \n\n\n\\begin{lemma}[\\cite{mcinnes2018umap}]\nThe gradients of attractive and repulsive cost functions in UMAP are:\n\\begin{align}\n& \\frac{\\partial c^a_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_i} = \\frac{2ab \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2(b-1)}}{(1 + a\\, \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b})} (\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j), \\\\\n& \\frac{\\partial c^r_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_i} = \\frac{-2b}{(\\varepsilon+\\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^2) (1 + a\\, \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b})} (\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j), \n\\end{align}\nwhere $\\varepsilon$ is a small positive number, e.g. $\\varepsilon = 0.001$, for stability to prevent division by zero when $\\ensuremath\\boldsymbol{y}_i \\approx \\ensuremath\\boldsymbol{y}_j$.\nLikewise, we have:\n\\begin{align*}\n& \\frac{\\partial c^a_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_j} = \\frac{2ab \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2(b-1)}}{(1 + a\\, \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b})} (\\ensuremath\\boldsymbol{y}_j - \\ensuremath\\boldsymbol{y}_i), \\\\\n& \\frac{\\partial c^r_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_j} = \\frac{-2b}{(\\varepsilon+\\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^2) (1 + a\\, \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b})} (\\ensuremath\\boldsymbol{y}_j - \\ensuremath\\boldsymbol{y}_i).\n\\end{align*}\n\\end{lemma}\n\\begin{proof}\nFor the first equation, we have:\n\\begin{align}\n\\frac{\\partial c^a_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_i} &= \\frac{\\partial c^a_{i,j}}{\\partial q_{ij}} \\times \\frac{\\partial q_{ij}}{\\partial \\ensuremath\\boldsymbol{y}_i} = \\frac{-1}{q_{ij}} \\times \\Big( \\frac{-1}{(1 + a\\, \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b})^{2}} \\nonumber\\\\\n&\\times 2ab (\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j) \\times \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2(b-1)} \\Big) \\nonumber\\\\\n&\\overset{(\\ref{equation_UMAP_q})}{=} \\frac{2ab \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2(b-1)}}{(1 + a\\, \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b})} (\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j).\n\\end{align}\nFor the second equation, we have:\n\\begin{align}\n&\\frac{\\partial c^r_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_i} =  \\nonumber\\\\\n&\\frac{\\partial c^r_{i,j}}{\\partial q_{ij}} \\times \\frac{\\partial q_{ij}}{\\partial \\ensuremath\\boldsymbol{y}_i} = \\frac{1}{1-q_{ij}} \\times \\Big( \\frac{-1}{(1 + a\\, \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b})^{2}} \\nonumber\\\\\n&\\times 2ab (\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j) \\times \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2(b-1)} \\Big) \\nonumber\\\\\n&= \\frac{-2ab \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2(b-1)}}{(1-q_{ij}) (1 + a\\, \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b})^2} (\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j). \\label{equation_proof_derivative_c_r_q}\n\\end{align}\nThe term in the numerator can be simplified as:\n\\begin{align*}\n& -2ab \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2(b-1)} \\!= -2b (a \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b}) \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{-2} \\\\\n&\\overset{(\\ref{equation_UMAP_q})}{=} -2b\\, (q_{ij}^{-1} - 1) \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{-2}. \n\\end{align*}\nThe term in the denominator can be simplified as:\n\\begin{align*}\n& (1-q_{ij}) (1 + a\\, \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b})^2 \\overset{(\\ref{equation_UMAP_q})}{=} (1-q_{ij}) q_{ij}^{-2} \\\\\n& = q_{ij}^{-2} - q_{ij}^{-1} = q_{ij}^{-1} (q_{ij}^{-1} - 1).\n\\end{align*}\nHence, Eq. (\\ref{equation_proof_derivative_c_r_q}) can be simplified as:\n\\begin{align*}\n\\frac{\\partial c^r_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_i} &= \\frac{-2b\\, (q_{ij}^{-1} - 1) \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{-2}}{q_{ij}^{-1} (q_{ij}^{-1} - 1)} (\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j) \\\\\n&= \\frac{-2b}{\\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^2\\, q_{ij}^{-1}} (\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j) \\\\\n&\\overset{(\\ref{equation_UMAP_q})}{=} \\frac{-2b}{\\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^2\\, (1 + a\\, \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^{2b})} (\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j).\n\\end{align*}\nIf we add $\\varepsilon$ for stability to the squared distance in the denominator, the equation is obtained. Q.E.D.\n\\end{proof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Supervised and Semi-supervised Embedding}\\label{section_supervised_semisupervised_umap}\n\nThe explained UMAP algorithm is unsupervised. \nWe can have supervised and semi-supervised embedding by UMAP \\cite{sainburg2020parametric}. For supervised UMAP, we can use UMAP cost function, Eq. (\\ref{equation_umap_cost2}), regularized by a classification cost function such as cross-entropy or triplet loss. \nIn semi-supervised case, some part of dataset has labels and some part does not. We can iteratively alternate between UMAP's cost function, Eq. (\\ref{equation_umap_cost2}), and a classification cost function. In this way, the embeddings are updated by UMAP and fine-tuned by class labels and this procedure is repeated iteratively until convergence of embedding. \n\n\n\\section{Justifying UMAP's Cost Function by Algebraic Topology and Category Theory}\\label{section_categoryTheory_algebraicTopology}\n\nUMAP is a neighbor embedding method where the probability of neighbors for every point is used for optimization of embedding. However, its cost function, Eq. (\\ref{equation_umap_cost}), can be justified by algebraic topology \\cite{may1992simplicial,friedman2012survey} and category theory \\cite{mac2013categories,riehl2017category}. Specifically, its theory heavily uses the fuzzy category theory \\cite{spivak2012metric}. \nIn this section, we briefly introduce the theory behind UMAP. The reader can skip this section if they do not wish to know the theory behind UMAP's cost function.\n\nFirst, we introduce some concepts which will be used for theory of UMAP:\n\\begin{itemize}\n\\item A \\textit{simplex} is a generalization of triangle to arbitrary dimensions. \n\\item A \\textit{simplicial complex} is a set of points, line segments, triangles, and their $d$-dimensional counterparts \\cite{may1992simplicial}. \n\\item A \\textit{fuzzy set} is a mapping $\\mu: \\mathcal{A} \\rightarrow [0,1]$ from carrier set $\\mathcal{A}$ where the mapping is called the membership function \\cite{zadeh1965fuzzy}. We can denote a fuzzy set by $(\\mathcal{A}, \\mu)$.\n\n\\item In category theory, a \\textit{category} is a collection of objects which are linked by arrows. For example, objects can be sets and the arrows can be functions between sets \\cite{mac2013categories}. \nA \\textit{morphism} is a mapping from a mathematical structure to another structure without changing type (e.g., morphisms are functions in set theory). \nA \\textit{functor} is defined as a mapping between categories. \n\\textit{Adjunction} is a relation between two functors. The two functors having an adjunction are \\textit{adjoint} functors, one of which is the \\textit{left adjoint} and the other is the \\textit{right adjoint}. \n\n\\item A \\textit{topology} is a geometrical object which is still preserved by continuous deformations such as stretching and twisting but without tearing and making or closing holes. A \\textit{topological space} is a set of topologies whose operations are continuous deformations. \n\n\\item We define the category $\\ensuremath\\boldsymbol{\\Delta}$ whose objects are finite order sets $[n] = \\{1, \\dots, n\\}$ with order-preserving maps as its morphisms {\\citep[Definition 1]{mcinnes2018umap}}. \nA simplicial set is a functor from $\\ensuremath\\boldsymbol{\\Delta}$ to the category of sets {\\citep[Definition 2]{mcinnes2018umap}}. \n\n\\item Consider a category of fuzzy sets, denoted by \\textbf{Fuzz} {\\citep[Definition 4]{mcinnes2018umap}}. The category of fuzzy simplicial sets, denoted by \\textbf{sFuzz}, is the category of objects with functors from $\\ensuremath\\boldsymbol{\\Delta}$ to \\textbf{Fuzz} and natural transformations as its morphisms {\\citep[Definition 5]{mcinnes2018umap}}. \n\n\\item An extended-pseudo-metric space is a set $\\mathcal{X}$ and a mapping $d: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}_{\\geq 0} \\cup \\{\\infty\\}$ where for $x,y \\in \\mathcal{X}$, we have $d(x,y) \\geq 0$ and $x=y \\implies d(x,y)=0$ and $d(x,y) = d(y,x)$ and $d(x,z) \\leq d(x,y) + d(y,z)$ {\\citep[Definition 6]{mcinnes2018umap}}. The mapping $d$ can be seen as a distance metric or pseudo-metric.\n\n\\item Let \\textbf{Fin-sFuzz} be the sub-category of bounded fuzzy simplicial sets. Let \\textbf{FinEPMet} be the sub-category of finite extended-pseudo-metric spaces. \n\n\\end{itemize}\n\n\\begin{theorem}[{\\citep[Theorem 1]{mcinnes2018umap}}]\nThe functors \\textbf{FinReal}: \\textbf{Fin-sFuzz} $\\rightarrow$ \\textbf{FinEPMet} and \\textbf{FinSing}: \\textbf{FinEPMet} $\\rightarrow$ \\textbf{Fin-sFuzz} form an adjunction with \\textbf{FinReal} and \\textbf{FinSing} as the left and right adjoints. \n\\end{theorem}\n\\begin{proof}\nProof is available in {\\citep[Appendix B]{mcinnes2018umap}}. \n\\end{proof}\n\nThe above theorem shows that we can convert an extended-pseudo-metric space to a fuzzy simplicial set and vice versa. In other words, we have a fuzzy simplicial representation of data space. \nHence, we have the following corollary. \n\n\\begin{corollary}[Fuzzy Topological Representation {\\citep[Definition 9]{mcinnes2018umap}}]\nConsider a dataset $\\mathcal{X} := \\{\\ensuremath\\boldsymbol{x}_i \\in \\mathbb{R}^d\\}_{i=1}^n$ lying on an underlying manifold $\\mathcal{M}$. Let $\\{(\\mathcal{X}, d_i)\\}_{i=1}^n$ be a family of extended-pseudo-metric spaces with common carrier set $\\mathcal{X}$ such that:\n\\begin{align}\nd_i(\\ensuremath\\boldsymbol{x}_j, \\ensuremath\\boldsymbol{x}_l) := \n\\left\\{\n    \\begin{array}{ll}\n        d_\\mathcal{M}(\\ensuremath\\boldsymbol{x}_j, \\ensuremath\\boldsymbol{x}_l) - \\rho_i & \\mbox{if } i=j \\text{ or } i=l, \\\\\n        \\infty & \\mbox{Otherwise},\n    \\end{array}\n\\right.\n\\end{align}\nwhere $d_\\mathcal{M}(.,.)$ is the geodesic (shortest) distance on manifold and $\\rho_i$ is the distance to the nearest neighbor of $\\ensuremath\\boldsymbol{x}_i$ (see Eq. (\\ref{equation_umap_rho})). \nThe \\textit{fuzzy topological representation} of dataset $\\mathcal{X}$ is:\n\\begin{align*}\n\\bigcup_{i=1}^n \\textbf{FinSing}((\\mathcal{X}, d_i)),\n\\end{align*}\nwhere $\\bigcup$ is the fuzzy set union. \n\\end{corollary}\n\nUMAP creates a fuzzy topological representation for the high-dimensional dataset. Then, it initializes a low-dimensional embedding of dataset and creates a fuzzy topological representation for the low-dimensional embedding of dataset. Then, it tries to modify the low-dimensional embedding of dataset in a way that the fuzzy topological representation of embedding becomes similar to the fuzzy topological representation of high-dimensional data. A measure of difference of two fuzzy topological representations $(\\mathcal{A},\\mu_1)$ and $(\\mathcal{A},\\mu_2)$ is their cross-entropy defined as \\cite{mcinnes2018umap}:\n\\begin{align}\\label{equation_fuzzy_cross_entropy}\n&c\\big((\\mathcal{A},\\mu_1), (\\mathcal{A},\\mu_2)\\big) := \\nonumber\\\\\n&\\sum_{a \\in \\mathcal{A}} \\Big(\\mu_1(a) \\ln\\Big(\\frac{\\mu_1(a)}{\\mu_2(a)}\\Big) + (1-\\mu_1(a)) \\ln\\Big(\\frac{1-\\mu_1(a)}{1-\\mu_2(a)}\\Big)\\Big).\n\\end{align}\nUMAP minimizes this cross-entropy by changing the embedding iteratively. Hence, it uses cross-entropy as its cost function, i.e., Eq. (\\ref{equation_umap_cost}).  \n\n\\section{Neighbor Embedding: Comparison with t-SNE and LargeVis}\\label{section_compare_tSNE_LargeVis}\n\nUMAP has a connection with t-SNE \\cite{maaten2008visualizing,ghojogh2020stochastic} and LargeVis \\cite{tang2016visualizing}.\nThis connection is explained in {\\citep[Appendix C]{mcinnes2018umap}}. \nIn fact, all UMAP, t-SNE, and LargeVis are neighbor embedding methods in which attractive and repulsive forces are used \\cite{bohm2020unifying}. As was explained before, for every point considered as anchor, attractive forces are used for pushing neighbor (also called positive) points to anchor and repulsive forces are used for pulling non-neighbor (also called negative) points away from the anchor points. Note that the ideas of triplet and contrastive losses as well as Fisher discriminant analysis are the same \\cite{ghojogh2020fisher}. \nAn empirical comparison of UMAP and t-SNE is also available in \\cite{repke2021robust}. \n\n\\textbf{-- Comparison of probabilities:}\nIn t-SNE, the probabilities in the input and embedding spaces are \\cite{ghojogh2020stochastic}:\n\\begin{align}\n&p_{j|i} := \\frac{\\exp\\big(\\!-\\frac{\\|\\ensuremath\\boldsymbol{x}_i - \\ensuremath\\boldsymbol{x}_j\\|_2}{\\sigma_i}\\big)}{\\sum_{k=1, k \\neq i}^n \\exp\\big(\\!-\\frac{\\|\\ensuremath\\boldsymbol{x}_i - \\ensuremath\\boldsymbol{x}_k\\|_2}{\\sigma_i}\\big)}, \\label{equation_tSNE_p_Directional} \\\\\n&p_{ij} := \\frac{p_{j|i} + p_{i|j}}{2n}, \\label{equation_tSNE_p} \\\\\n&q_{ij} := \\frac{(1+\\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^2)^{-1}}{\\sum_{k=1}^n \\sum_{l=1, l \\neq k}^{n} (1+\\|\\ensuremath\\boldsymbol{y}_k - \\ensuremath\\boldsymbol{y}_l\\|_2^2)^{-1}}, \\label{equation_tSNE_q}\n\\end{align}\nwhere $p_{i|i} = 0, \\forall i$.\nThe $p_{j|i}$ probabilities can be computed for $k$NN graph where $p_{j|i}$ is set to zero for non-neighbor points in the $k$NN graph.\nLargeVis uses the same $p_{ij}$ probabilities as t-SNE but approximates the $k$NN to compute it very fast and become more efficient. In LargeVis, the probability in the embedding space is:\n\\begin{align}\n&q_{ij} := (1+\\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^2)^{-1}. \\label{equation_LargeVis_q}\n\\end{align}\n\nComparing Eqs. (\\ref{equation_UMAP_p_Directional}) and (\\ref{equation_tSNE_p_Directional}) shows that UMAP, t-SNE, and LargeVis all use Gaussian or RBF kernel for probabilities in the input space.\nComparing Eqs. (\\ref{equation_UMAP_p}) and (\\ref{equation_tSNE_p}) shows that UMAP and t-SNE/LargeVis use different approaches for symmetrizing the probabilities in the input space. \nComparing Eqs. (\\ref{equation_UMAP_q}), (\\ref{equation_tSNE_q}), and (\\ref{equation_LargeVis_q}) shows that, in contrast to t-SNE, UMAP and LargeVis do not normalize the probabilities in the embedding space by all pairs of points. This advantage makes UMAP much faster than t-SNE and also makes it more suitable for mini-batch optimization in deep learning (this will be explained more in Section \\ref{section_parametric_UMAP}). \nComparing Eqs. (\\ref{equation_UMAP_q}), (\\ref{equation_tSNE_q}), and (\\ref{equation_LargeVis_q}) also shows that UMAP, t-SNE, and LargeVis all use Cauchy distribution for probabilities in the embedding space. In fact if we set $a=b=1$ in Eq. (\\ref{equation_UMAP_q}), it is exactly the same as Eq. (\\ref{equation_tSNE_q}) up to the scale of normalization. \n\n\\textbf{-- Comparison of cost functions:}\nThe cost function in t-SNE, to be minimized, is the KL-divergence \\cite{kullback1951information} of the probabilities in the input and embedding spaces:\n\\begin{equation}\\label{equation_tSNE_cost}\n\\begin{aligned}\nc_4 &:= \\sum_{i=1}^n \\sum_{j=1, j \\neq i}^n p_{ij} \\ln(\\frac{p_{ij}}{q_{ij}}) \\\\\n&= \\sum_{i=1}^n \\sum_{j=1, j \\neq i}^n \\big( p_{ij} \\ln(p_{ij}) - p_{ij} \\ln(q_{ij}) \\big),\n\\end{aligned}\n\\end{equation}\nwhere Eqs. (\\ref{equation_tSNE_p}) and (\\ref{equation_tSNE_q}) are used. \nThe cost function of LargeVis, to be minimized, is a negative likelihood function stated below:\n\\begin{equation}\\label{equation_LargeVis_cost}\n\\begin{aligned}\nc_5 &:= -\\sum_{i=1}^n \\sum_{j=1, j \\neq i}^n \\big(p_{ij} \\ln(q_{ij}) + \\lambda \\ln(1 - q_{ij}) \\big),\n\\end{aligned}\n\\end{equation}\nwhere $\\lambda$ is the regularization parameter and Eqs. (\\ref{equation_tSNE_p}) and (\\ref{equation_LargeVis_q}) are used. \nComparing Eqs. (\\ref{equation_umap_cost2}), (\\ref{equation_tSNE_cost}), and (\\ref{equation_LargeVis_cost}) shows that UMAP, t-SNE, and LargeVis have similar, but not exactly equal, cost functions. \nThe first term in all these cost functions is responsible for the attractive forces and the second term is for the repulsive forces; hence, they can all be considered as neighbor embedding methods \\cite{bohm2020unifying}. \n\n\\section{Discussion on Repulsive Forces and Negative Sampling in the UMAP's Cost Function}\\label{section_discussion_repulsive_forces}\n\n\\subsection{UMAP's Emphasis on Repulsive Forces}\n\nThe gradient of UMAP's cost function, Eq. (\\ref{equation_umap_cost2_withAttractiveAndRepulsiveForces}), in epoch $\\nu$ is denoted by $(\\partial c_2 / \\partial \\ensuremath\\boldsymbol{y}_i) |_\\nu$ and it can be stated as \\cite{damrich2021umap}:\n\\begin{align}\\label{equation_umap_cost_epoch}\n\\frac{\\partial c_2}{\\partial \\ensuremath\\boldsymbol{y}_i}\\bigg|_\\nu = \\sum_{j=1}^n \\Big( \\mathbb{I}_{ij}^\\nu\\, \\frac{\\partial c_{i,j}^a}{\\partial \\ensuremath\\boldsymbol{y}_i} + \\mathbb{I}_{ji}^\\nu\\, \\frac{\\partial c_{j,i}^a}{\\partial \\ensuremath\\boldsymbol{y}_i} + \\mathbb{I}_{ij}^\\nu\\, \\sum_{l=1}^n \\mathbb{I}_{ijl}^\\nu\\, \\frac{\\partial c_{i,l}^r}{\\partial \\ensuremath\\boldsymbol{y}_i} \\Big),\n\\end{align}\nwhere $c_{i,j}^a$ and $c_{i,j}^r$ are defined in Eqs. (\\ref{equation_umap_cost2_attractive}) and (\\ref{equation_umap_cost2_attractive}), respectively, and $\\mathbb{I}_{ij}^\\nu$ is a binary random variable which is one if the points $\\ensuremath\\boldsymbol{y}_i$ and $\\ensuremath\\boldsymbol{y}_j$ are randomly selected in epoch $\\nu$ and otherwise it is zero. Also, $\\mathbb{I}_{ijl}^\\nu$ is a binary random variable which is one if the point $\\ensuremath\\boldsymbol{y}_l$ is one of the negative samples which is randomly sampled for the pair $\\ensuremath\\boldsymbol{y}_i$ and $\\ensuremath\\boldsymbol{y}_j$ in epoch $\\nu$ and otherwise it is zero. \nRecall that the original UMAP does not update the embedding of negative sample itself so we do not have a term for that update in this gradient. \n\nEq. (\\ref{equation_umap_cost_epoch}) is only for one epoch. \nThe expectation of Eq. (\\ref{equation_umap_cost_epoch}) over all epochs is \\cite{damrich2021umap}:\n\\begin{align}\n&\\mathbb{E}\\Big[\\frac{\\partial c_2}{\\partial \\ensuremath\\boldsymbol{y}_i}\\Big|_\\nu\\Big] \\nonumber\\\\\n&= \\mathbb{E}\\Big[\\sum_{j=1}^n \\Big( \\mathbb{I}_{ij}^\\nu\\, \\frac{\\partial c_{i,j}^a}{\\partial \\ensuremath\\boldsymbol{y}_i} + \\mathbb{I}_{ji}^\\nu\\, \\frac{\\partial c_{j,i}^a}{\\partial \\ensuremath\\boldsymbol{y}_i} + \\mathbb{I}_{ij}^\\nu\\, \\sum_{l=1}^n \\mathbb{I}_{ijl}^\\nu\\, \\frac{\\partial c_{i,l}^r}{\\partial \\ensuremath\\boldsymbol{y}_i} \\Big)\\Big] \\nonumber\\\\\n&\\overset{(a)}{=} \\sum_{j=1}^n \\Big( \\mathbb{E}[\\mathbb{I}_{ij}^\\nu]\\, \\frac{\\partial c_{i,j}^a}{\\partial \\ensuremath\\boldsymbol{y}_i} + \\mathbb{E}[\\mathbb{I}_{ji}^\\nu]\\, \\frac{\\partial c_{j,i}^a}{\\partial \\ensuremath\\boldsymbol{y}_i}\\Big) \\nonumber\\\\\n&~~~~~~~+ \\sum_{j=1}^n \\sum_{l=1}^n \\mathbb{E}[\\mathbb{I}_{ij}^\\nu\\, \\mathbb{I}_{ijl}^\\nu]\\, \\frac{\\partial c_{i,l}^r}{\\partial \\ensuremath\\boldsymbol{y}_i} \\nonumber\\\\\n&\\overset{(b)}{=} \\sum_{j=1}^n \\Big( p_{ij} \\frac{\\partial c_{i,j}^a}{\\partial \\ensuremath\\boldsymbol{y}_i} + p_{ji} \\frac{\\partial c_{j,i}^a}{\\partial \\ensuremath\\boldsymbol{y}_i}\\Big) + \\sum_{j=1}^n \\sum_{l=1}^n p_{ij} \\frac{m}{n} \\frac{\\partial c_{i,l}^r}{\\partial \\ensuremath\\boldsymbol{y}_i} \\nonumber\n\\end{align}\n\\begin{align}\n&\\overset{(c)}{=} \\sum_{j=1}^n \\Big( p_{ij} \\frac{\\partial c_{i,j}^a}{\\partial \\ensuremath\\boldsymbol{y}_i} + p_{ji} \\frac{\\partial c_{j,i}^a}{\\partial \\ensuremath\\boldsymbol{y}_i}\\Big) + \\frac{m}{n} \\underbrace{\\sum_{j=1}^n p_{ij}}_{=\\, d_i} \\sum_{l=1}^n \\frac{\\partial c_{i,l}^r}{\\partial \\ensuremath\\boldsymbol{y}_i} \\nonumber\\\\\n&\\overset{(d)}{=} \\sum_{j=1}^n \\Big( p_{ij} \\frac{\\partial c_{i,j}^a}{\\partial \\ensuremath\\boldsymbol{y}_i} + p_{ji} \\frac{\\partial c_{j,i}^a}{\\partial \\ensuremath\\boldsymbol{y}_i}\\Big) + \\frac{d_i m}{n} \\sum_{j=1}^n \\frac{\\partial c_{i,j}^r}{\\partial \\ensuremath\\boldsymbol{y}_i} \\nonumber\\\\\n&\\overset{(e)}{=} 2 \\sum_{j=1}^n \\Big( p_{ij} \\frac{\\partial c_{i,j}^a}{\\partial \\ensuremath\\boldsymbol{y}_i} + \\frac{d_i m}{2n} \\frac{\\partial c_{i,j}^r}{\\partial \\ensuremath\\boldsymbol{y}_i} \\Big), \\label{equation_umap_gradient_cost_epoch}\n\\end{align}\nwhere $(a)$ is because expectation is a linear operator, $(b)$ is because $\\mathbb{E}[\\mathbb{I}_{ij}^\\nu] = p_{ij}$ and $\\mathbb{E}[\\mathbb{I}_{ij}^\\nu\\, \\mathbb{I}_{ijl}^\\nu] = \\mathbb{E}[\\mathbb{I}_{ijl}^\\nu\\, |\\, \\mathbb{I}_{ij}^\\nu] \\times \\mathbb{E}[\\mathbb{I}_{ij}^\\nu] = \\frac{m}{n} \\times p_{ij}$, $(c)$ is because we define the degree of the $i$-th point (node) in the $k$NN graph as $d_i := \\sum_{j=1}^n p_{ij}$, $(d)$ is because we change the dummy variable $l$ to $j$ in the last summation, and $(e)$ is because $p_{ij} = p_{ji}$ and $c_{i,j}^a = c_{j,i}^a$ are symmetric. \n\nOn the other hand, according to Eq. (\\ref{equation_umap_cost2_withAttractiveAndRepulsiveForces_2}), the gradient of UMAP's cost function, Eq. (\\ref{equation_umap_cost2}), can be stated as \\cite{damrich2021umap}:\n\\begin{align}\\label{equation_umap_gradient_withAttractiveRepulsiveDerivatives}\n\\frac{\\partial c_2}{\\partial \\ensuremath\\boldsymbol{y}_i} = 2\\sum_{j=1}^n \\Big(p_{ij}\\, \\frac{\\partial c^a_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_i} + (1 - p_{ij})\\, \\frac{\\partial c^r_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_i}\\Big).\n\\end{align}\nEq. (\\ref{equation_umap_gradient_withAttractiveRepulsiveDerivatives}) is the gradient of the original UMAP's loss function while Eq. (\\ref{equation_umap_gradient_cost_epoch}) is the expected gradient of UMAP's loss function.\nComparing Eqs. (\\ref{equation_umap_gradient_cost_epoch}) and (\\ref{equation_umap_gradient_withAttractiveRepulsiveDerivatives}) shows that the original UMAP puts more emphasis on negative samples (or repulsive forces) compared to the expected UMAP's loss function because for a negative sample we have $1-p_{ij} \\approx 1$ while $d_i m /n \\approx 0$ because $m \\ll n$. Therefore, UMAP is mistakenly putting more emphasis on negative sampling (or repulsive forces) than required \\cite{damrich2021umap}. This has also been empirically investigated in \\cite{bohm2020unifying} that negative sampling (or repulsive forces) in UMAP has more weight than required. \n\n\\subsection{UMAP's Effective Cost Function}\n\nThe original UMAP does not update the embedding of negative samples themselves. If we also update them, i.e. we perform line \\ref{algorithm_umap_update_negativeSample} in Algorithm \\ref{algorithm_umap}, the gradient of UMAP's cost function, Eq. (\\ref{equation_umap_cost2_withAttractiveAndRepulsiveForces}), in epoch $\\nu$ can be stated as \\cite{damrich2021umap}:\n\\begin{align}\\label{equation_umap_cost_epoch_withNegativeSampleUpdate}\n\\frac{\\partial c_2}{\\partial \\ensuremath\\boldsymbol{y}_i}\\bigg|_\\nu = &\\sum_{j=1}^n \\Big( \\mathbb{I}_{ij}^\\nu\\, \\frac{\\partial c_{i,j}^a}{\\partial \\ensuremath\\boldsymbol{y}_i} + \\mathbb{I}_{ji}^\\nu\\, \\frac{\\partial c_{j,i}^a}{\\partial \\ensuremath\\boldsymbol{y}_i} \\nonumber\\\\\n&+ \\mathbb{I}_{ij}^\\nu\\, \\sum_{l=1}^n \\mathbb{I}_{ijl}^\\nu\\, \\frac{\\partial c_{i,l}^r}{\\partial \\ensuremath\\boldsymbol{y}_i} + \\sum_{k=1}^n \\mathbb{I}_{jk}^\\nu\\, \\mathbb{I}_{jki}^\\nu\\, \\frac{\\partial c_{j,i}^r}{\\partial \\ensuremath\\boldsymbol{y}_i} \\Big).\n\\end{align}\nIf we do reverse engineering to find the cost function from its gradient, the cost function at epoch $\\nu$ becomes:\n\\begin{align}\nc_2 \\big|_\\nu := \\sum_{i=1}^n \\sum_{j=1, j\\neq i}^n \\Big( \\mathbb{I}_{ij}^\\nu\\, c_{i,j}^a + \\sum_{l=1}^n \\mathbb{I}_{ij}^\\nu\\, \\mathbb{I}_{ijl}^\\nu\\, c_{i,l}^r \\Big).\n\\end{align}\nThis is the cost at one epoch. The expectation of this cost over all epochs is \\cite{damrich2021umap}:\n\\begin{align}\nc_2 &= \\mathbb{E}[c_2 \\big|_\\nu] \\nonumber\\\\\n&= \\sum_{i=1}^n \\sum_{j=1, j\\neq i}^n \\Big( \\mathbb{E}[\\mathbb{I}_{ij}^\\nu]\\, c_{i,j}^a + \\sum_{l=1}^n \\mathbb{E}[\\mathbb{I}_{ij}^\\nu\\, \\mathbb{I}_{ijl}^\\nu]\\, c_{i,l}^r \\Big) \\nonumber\\\\\n&= \\sum_{i=1}^n \\sum_{j=1, j\\neq i}^n \\Big( \\mathbb{E}[\\mathbb{I}_{ij}^\\nu]\\, c_{i,j}^a \\Big) \\nonumber\\\\\n&~~~~~~~~~~~~~~ + \\sum_{i=1}^n \\sum_{j=1, j\\neq i}^n  \\sum_{l=1}^n \\Big( \\mathbb{E}[\\mathbb{I}_{ij}^\\nu\\, \\mathbb{I}_{ijl}^\\nu]\\, c_{i,l}^r \\Big) \\nonumber\\\\\n&\\overset{(a)}{=} \\sum_{i=1}^n \\sum_{j=1, j\\neq i}^n \\Big( p_{ij}\\, c_{i,j}^a \\Big) + \\sum_{i=1}^n \\sum_{j=1, j\\neq i}^n  \\sum_{l=1}^n \\Big( \\frac{m}{n} p_{ij}\\, c_{i,l}^r \\Big) \\nonumber\\\\\n&= \\sum_{i=1}^n \\sum_{j=1, j\\neq i}^n \\Big( p_{ij}\\, c_{i,j}^a \\Big) \\nonumber\\\\\n&~~~~~~~~~~~~~~ + \\frac{m}{n} \\Big(\\sum_{i=1}^n\\underbrace{\\sum_{j=1}^n p_{ij}}_{=d_i} + \\sum_{j=1}^n\\underbrace{\\sum_{i=1}^n p_{ji}}_{=d_j} \\Big) \\sum_{l=1}^n c_{i,l}^r \\nonumber\\\\\n&\\overset{(b)}{=} \\sum_{i=1}^n \\sum_{j=1,j \\neq i}^n \\Big( p_{ij}\\, c_{i,j}^a \\Big) + \\sum_{i=1}^n \\sum_{j=i+1}^n \\frac{(d_i+d_j) m}{n} c_{i,j}^r \\nonumber\\\\\n&\\overset{(c)}{=} 2 \\sum_{i=1}^n \\sum_{j=i+1}^n \\Big( p_{ij}\\, c_{i,j}^a + \\frac{(d_i+d_j) m}{2n} c_{i,j}^r \\Big), \\label{equation_umap_effective_loss}\n\\end{align}\nwhere $(a)$ is because $\\mathbb{E}[\\mathbb{I}_{ij}^\\nu] = p_{ij}$ and $\\mathbb{E}[\\mathbb{I}_{ij}^\\nu\\, \\mathbb{I}_{ijl}^\\nu] = \\mathbb{E}[\\mathbb{I}_{ijl}^\\nu\\, |\\, \\mathbb{I}_{ij}^\\nu] \\times \\mathbb{E}[\\mathbb{I}_{ij}^\\nu] = \\frac{m}{n} \\times p_{ij}$, $(b)$ is because we change the dummy variable $l$ to $j$ in the last summation, and $(c)$ is because of symmetry of terms in the fist summations with respect to $i$ and $j$. \nEq. (\\ref{equation_umap_effective_loss}) can be considered as UMAP's effective cost function \\cite{damrich2021umap} because it also updates the embedding of negative samples by line \\ref{algorithm_umap_update_negativeSample} in Algorithm \\ref{algorithm_umap}. \n\nComparing UMAP's cost, Eq. (\\ref{equation_umap_cost2_withAttractiveAndRepulsiveForces_2}), with UMAP's effective cost, Eq. (\\ref{equation_umap_effective_loss}), shows that the weight of negative sample (or repulsive forces) should be $\\frac{(d_i+d_j) m}{2n}$ rather than $(1-p_{ij})$ if we also update the embeddings of negative samples. As we have $m \\ll n$ and $p_{ij}$ is small for negative samples, this weight is much less than the weight in the original UMAP. \n\n\\section{DensMAP for Density-Preserving Embedding}\\label{section_DensMAP}\n\nAs was explained before, UMAP uses a binary search for the scale  of each point, $\\sigma_i$, to satisfy Eq. (\\ref{equation_umap_sigma}), so as t-SNE \\cite{maaten2008visualizing} which has a similar search for its scale using entropy as perplexity. The search makes the neighborhoods of various points behave similarly so UMAP and t-SNE both assume that points are uniformly distributed on an underlying low-dimensional manifold. Hence, UMAP ignores the density of data around every point by canceling the effect of density with binary search for scales of points. DensMAP \\cite{narayan2020density} regularizes the cost function of UMAP to take into account and add back the information of density around each point. It is shown empirically that this consideration of density information results in better embedding \\cite{narayan2020density} although we will have more computation for calculation of the regularization term. \n\nIf the neighbors of a point are very close to it, that region is dense for that point. Therefore, a measure of local density can be the local radius defined as the expected (average) distances from neighbors. We denote the local densities in the input and embedding spaces by:\n\\begin{align}\n& R_p(\\ensuremath\\boldsymbol{x}_i) := \\mathbb{E}_{j\\sim p}\\big[\\|\\ensuremath\\boldsymbol{x}_i - \\ensuremath\\boldsymbol{x}_j\\|_2^2\\big] = \\frac{\\sum_{j=1}^n p_{ij} \\|\\ensuremath\\boldsymbol{x}_i - \\ensuremath\\boldsymbol{x}_j\\|_2^2}{\\sum_{j=1}^n \\|\\ensuremath\\boldsymbol{x}_i - \\ensuremath\\boldsymbol{x}_j\\|_2^2}, \\\\\n& R_q(\\ensuremath\\boldsymbol{y}_i) := \\mathbb{E}_{j\\sim q}\\big[\\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^2\\big] = \\frac{\\sum_{j=1}^n q_{ij} \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^2}{\\sum_{j=1}^n \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^2}.\n\\end{align}\nAs the volume of points is proportional to the powers of radius (e.g., notice that the volume of three dimensional sphere is proportional to radius to the power three), the relation of local densities in the input and embedding spaces can be:\n\\begin{align}\nR_q(\\ensuremath\\boldsymbol{y}_i) = \\alpha\\, \\big(R_p(\\ensuremath\\boldsymbol{x}_i)\\big)^\\beta \\implies r_q^i = \\beta\\, r_p^i + \\gamma,\n\\end{align}\nwhere $r_q^i := \\ln(R_q(\\ensuremath\\boldsymbol{y}_i))$, $r_p^i := \\ln(R_p(\\ensuremath\\boldsymbol{y}_i))$, and $\\gamma := \\ln(\\alpha)$. Therefore, the relation of logarithms of the local densities should be affine dependence. A measure of linear (or affine) dependence is correlation so we use the correlation of logarithms of local densities:\n\\begin{align}\n\\text{Corr}(r_q,r_p) := \\frac{\\text{Cov}(r_q, r_p)}{\\sqrt{\\text{Var}(r_q) \\text{Var}(r_p)}},\n\\end{align}\nwhere the covariance and variance of densities are:\n\\begin{align*}\n& \\text{Cov}(r_q, r_p) := \\frac{1}{n-1} \\sum_{i=1}^n \\Big[(r_q^i - \\mu_q) (r_p^i - \\mu_p) \\Big] \\\\\n& \\text{Var}(r_q) := \\frac{1}{n-1} \\sum_{i=1}^n (r_q^i - \\mu_q)^2, \n\\end{align*}\nwhere $\\mu_q := (1/n) \\sum_{j=1}^n r_q^j$, $\\mu_p := (1/n) \\sum_{j=1}^n r_p^j$, and $\\text{Var}(r_p)$ is defined similarly. \nThe cost function of densMAP, to be minimized, is the UMAP's cost function regularized by maximization of the correlation of local densities \\cite{narayan2020density}:\n\\begin{align}\nc_6 := c_2 - \\lambda\\, \\text{Corr}(r_q,r_p),\n\\end{align}\nwhere $\\lambda$ is the regularization parameter which weights the correlation compared to the UMAP's original cost. \nThe gradient of $c_2$ is the gradient of original UMAP discussed before. The gradient of the correlation term is \\cite{narayan2020density}:\n\\begin{align}\n\\frac{\\partial c_6}{\\partial \\ensuremath\\boldsymbol{y}_i} = \\sum_{i=1}^n \\sum_{j=1, j \\neq i}^n \\frac{\\partial \\text{Corr}(r_q,r_p)}{\\partial d_{ij}^2} (y_i - y_j), \n\\end{align}\nwhere $d_{ij}^2 := \\|\\ensuremath\\boldsymbol{y}_i - \\ensuremath\\boldsymbol{y}_j\\|_2^2$ and:\n\\begin{align*}\n& \\frac{\\partial \\text{Corr}(r_q,r_p)}{\\partial d_{ij}^2} = \\frac{1}{(n-1) \\text{Var}(r_q)^{(3/2)}} \\times \\\\\n&~~~~~~~ \\bigg[\\text{Var}(r_q)\\big(r_p^i \\frac{\\partial r_q^i}{\\partial d_{ij}^2} + r_p^j \\frac{\\partial r_q^j}{\\partial d_{ij}^2}\\big) \\\\\n&~~~~~~~ - \\text{Cov}(r_q, r_p)\\big((r_q^i - \\mu_q) \\frac{\\partial r_q^i}{\\partial d_{ij}^2} + (r_p^j - \\mu_q) \\frac{\\partial r_q^j}{\\partial d_{ij}^2}\\big)\\bigg],\n\\end{align*}\nand:\n\\begin{align*}\n& \\frac{\\partial r_q^i}{\\partial d_{ij}^2} = (1 + a d_{ij}^{2b})^{-2} \\Big[ab d_{ij}^{2(b-1)} + e^{-r_q^i} (1 + a(1-b) d_{ij}^2)\\Big].\n\\end{align*}\nProofs of these derivatives are available in {\\citep[Supplementary Note 2]{narayan2020density}}. As in UMAP, DensMAP uses stochastic gradient descent for optimization. Except for the cost function, the algorithm of DensMAP is the same as UMAP. \n\n\\SetAlCapSkip{0.5em}\n\\IncMargin{0.8em}\n\\begin{algorithm2e}[!t]\n\\DontPrintSemicolon\n    \\textbf{Input}: Training data $\\{\\ensuremath\\boldsymbol{x}_i\\}_{i=1}^n$, learning rate $\\eta$\\;\n    Construct $k$NN graph\\;\n    Initialize weights $\\theta$ of network randomly\\;\n    Initialize $\\{\\ensuremath\\boldsymbol{y}_i\\}_{i=1}^n \\gets \\{f_{\\theta}(\\ensuremath\\boldsymbol{x}_i)\\}_{i=1}^n$\\;\n    Calculate $p_{ij}$ and $q_{ij}$ for $\\forall i,j \\in \\{1, \\dots, n\\}$ by Eqs. (\\ref{equation_UMAP_p}) and (\\ref{equation_UMAP_q})\\;\n    // make batches:\\;\n    \\For{$s$ from $1$ to $\\lfloor n/b \\rfloor$}{\n        $\\mathcal{B}_s \\gets \\{\\ensuremath\\boldsymbol{x}_{(s-1)b+1}, \\dots, \\ensuremath\\boldsymbol{x}_{sb}\\}$\\;\n        // We denote $\\mathcal{B}_s = \\{\\ensuremath\\boldsymbol{x}_1^{(s)}, \\dots, \\ensuremath\\boldsymbol{x}_b^{(s)}\\}$\\;\n    }\n    $\\nu \\gets 0$\\;\n    \\While{not converged}{\n        $\\nu \\gets \\nu + 1$ \\quad // epoch index\\; \n        // optimize over every mini-batch:\\;\n        \\For{$s$ from $1$ to $\\lfloor n/b \\rfloor$}{\n            $\\{\\ensuremath\\boldsymbol{y}_i^{(s)}\\}_{i=1}^b \\gets \\{f_{\\theta}(\\ensuremath\\boldsymbol{x}_i^{(s)})\\}_{i=1}^b$\\;\n            $c^a_s \\gets 0, c^r_s \\gets 0$\\;\n            \\For{$i$ from $1$ to $b$}{\n                \\For{$j$ from $1$ to $b$}{\n                    $u \\sim U(0,1)$\\;\n                    \\If{$u \\leq p_{ij}$}{\n                        $q_{ij} = (1 + a \\|\\ensuremath\\boldsymbol{y}_i^{(s)} - \\ensuremath\\boldsymbol{y}_j^{(s)}\\|_2^{2b})^{-1}$\\;\n                        $c^a_s = c^a_s +(- \\ln(q_{ij}))$\\;\n                        \\For{$m$ iterations}{\n                            $l \\sim U\\{1, \\dots, b\\}$\\;\n                            $q_{il} = (1 + a \\|\\ensuremath\\boldsymbol{y}_i^{(s)} - \\ensuremath\\boldsymbol{y}_l^{(s)}\\|_2^{2b})^{-1}$\\;\n                            $c^r_s = c^r_s +(- \\ln(1-q_{il}))$\\;\n                        }\n                    }\n                }\n            }\n            $\\theta \\gets$ backpropagate with loss $(c^a_s + c^r_s)$\\;\n        }\n    }\n    \\textbf{Return} $\\{\\ensuremath\\boldsymbol{y}_i\\}_{i=1}^n \\gets \\{f_{\\theta}(\\ensuremath\\boldsymbol{x}_i)\\}_{i=1}^n$\\;\n\\caption{Parametric UMAP algorithm}\\label{algorithm_parametric_umap}\n\\end{algorithm2e}\n\\DecMargin{0.8em}\n\n\\section{Parametric UMAP for Embedding by Deep Learning}\\label{section_parametric_UMAP}\n\nDimensionality reduction algorithms can have their parametric version in which the cost function of the algorithm is used as the loss function of a neural network and the parameters (i.e., weights) of network are trained by backpropagating the error of loss function. \nInspired by parametric t-SNE \\cite{van2009learning}, we can have parametric UMAP \\cite{sainburg2020parametric}. Parametric UMAP uses UMAP's cost function, Eq. (\\ref{equation_umap_cost}), as the loss function of a neural network for deep learning. It optimizes this cost in mini-batches rather than on the whole dataset; therefore, it can be used for embedding large datasets. \nAlso, because of nonlinearity of neural networks, the parametric UMAP can handle highly nonlinear data better than UMAP. \nNote that the learnable parameters in UMAP are the embedding of points but the learnable parameters in the parametric UMAP are the weights of neural network so that the embedding is obtained by network.\nAn advantage of parametric UMAP over parametric t-SNE is that the probability of UMAP in the embedding space, Eq. (\\ref{equation_UMAP_q}), does not have a normalization factor; hence, there is no need to normalize over the whole dataset. This makes UMAP easy to be used in deep learning. \n \n\nThe algorithm of parametric UMAP is shown in Algorithm \\ref{algorithm_parametric_umap}. As this algorithm shows, we make mini-batches of data points where $\\mathcal{B}_s$ denotes the $s$-th batch. In every epoch, we iterate over mini-batches and for every mini-batch, we iterate twice over the points of batch to have anchors and positive points. We optimize the cost function of UMAP over the batch and not the entire data. We denote the attractive and repulsive cost functions by $c_s^a$ and $c_s^r$, respectively. \nAccording to Eq. (\\ref{equation_umap_cost2_withAttractiveAndRepulsiveForces}), the loss function of neural network in parametric UMAP should be $(c_s^a + c_s^r)$. Backpropagating this loss function trains the parameters $\\theta$ of the neural network denoted by $f_\\theta(.)$. After training, the embeddings are obtained as $\\{f_\\theta(\\ensuremath\\boldsymbol{x}_i)\\}_{i=1}^n$.\nNote that as was discussed in Section \\ref{section_supervised_semisupervised_umap}, one can combine the UMAP's cost function with cross-entropy loss or triplet loss to have supervised or semi-supervised embedding. Moreover, combining UMAP's cost function with reconstruction loss can train an autoencoder for embedding in its middle code layer.\n\n\n\n\n\\SetAlCapSkip{0.5em}\n\\IncMargin{0.8em}\n\\begin{algorithm2e}[!t]\n\\DontPrintSemicolon\n    \\textbf{Input}: New batch of training data $\\ensuremath\\boldsymbol{X}_\\text{new} = \\{\\ensuremath\\boldsymbol{x}_i\\}_{i=1}^n$\\;\n    $\\ensuremath\\boldsymbol{X}_\\text{new}, \\ensuremath\\boldsymbol{X}_\\text{updated} \\gets$ Update $k$NN graph by PANENE method\\;\n    \\uIf{it is initial batch}{\n        Initialize $\\{\\ensuremath\\boldsymbol{y}_i\\}_{i=1}^n$ by Laplacian eigenmap\\;\n    }\n    \\Else{\n        \\For{each new point $\\ensuremath\\boldsymbol{x}_i$ in $\\ensuremath\\boldsymbol{X}_\\text{new}$}{\n            Find nearest neighbor to previously accumulated data\\;\n            Initialize $\\ensuremath\\boldsymbol{y}_i$ to the embedding of the nearest neighbor point plus Gaussian noise\\;\n        }\n    }\n    \\For{each new or updated point $\\ensuremath\\boldsymbol{x}_i$ in $\\ensuremath\\boldsymbol{X}_\\text{new}$ or $\\ensuremath\\boldsymbol{X}_\\text{updated}$}{\n        Calculate/update $\\rho_i$ and $\\sigma_i$ by Eqs. (\\ref{equation_umap_rho}) and (\\ref{equation_umap_sigma})\\;\n        Calculate $p_{ij}$ and $q_{ij}$ for $\\forall j$ by Eqs. (\\ref{equation_UMAP_p}) and (\\ref{equation_UMAP_q})\\;\n    }\n    \\While{not converged}{\n        \\For{each new or updated point $\\ensuremath\\boldsymbol{x}_i$ in $\\ensuremath\\boldsymbol{X}_\\text{new}$ or $\\ensuremath\\boldsymbol{X}_\\text{updated}$}{\n            \\For{$j$ from $1$ to $n$}{\n                $u \\sim U(0,1)$\\;\n                \\If{$u \\leq p_{ij}$}{\n                    $\\ensuremath\\boldsymbol{y}_i \\gets \\ensuremath\\boldsymbol{y}_i - \\eta \\frac{\\partial c^a_{i,j}}{\\partial \\ensuremath\\boldsymbol{y}_i}$\\;\n                    \\For{$m$ iterations}{\n                        $l \\sim U\\{1, \\dots, n\\}$\\;\n                        $\\ensuremath\\boldsymbol{y}_i \\gets \\ensuremath\\boldsymbol{y}_i - \\eta \\frac{\\partial c^r_{i,l}}{\\partial \\ensuremath\\boldsymbol{y}_i}$\\;\n                    }\n                }\n            }\n        }\n    }\n    \\textbf{Return} embedding $\\{\\ensuremath\\boldsymbol{y}_i\\}$ for the new and updated points\\;\n\\caption{Progressive UMAP algorithm}\\label{algorithm_umap_progressive}\n\\end{algorithm2e}\n\\DecMargin{0.8em}\n\n\n\\section{Progressive UMAP for Streaming and Out-of-sample Data}\\label{section_progressive_UMAP}\n\nThe original UMAP does not support out-of-sample (test) data embedding. Progressive UMAP \\cite{ko2020progressive} can embed out-of-sample data. It can also be used for embedding streaming (online) data. Although UMAP is generally faster than t-SNE, it takes some noticeable time to embed big data. Progressive UMAP can be used to embed some portion of data and then complete the embedding by embedding the rest of data as streaming data. \n\nThe algorithm of progressive UMAP is shown in Algorithm \\ref{algorithm_umap_progressive}. It uses PANENE \\cite{jo2018panene} for constructing a streaming $k$NN graph. PANENE uses randomized kd-trees \\cite{muja2009fast} for approximating and updating $k$NN graph. When a new batch of data, $\\ensuremath\\boldsymbol{X}_\\text{new}$, is arrived, some of data points are affected in $k$NN graph because their neighbors are changed or they become new neighbors of some points or they are no longer neighbors of some points. We denote the affected points by $\\ensuremath\\boldsymbol{X}_\\text{updated}$. If $\\ensuremath\\boldsymbol{X}_\\text{new}$ is the first batch of data, we embed data by Laplacian eigenmap, exactly as the original UMAP does. If the coming batch is not the first batch, we find the nearest neighbor of every new point among the previously accumulated data. Then, we set the initial embedding of every new point as the embedding of its nearest neighbor point added with some Gaussian noise. \nAs in the original UMAP, for every new or updated point, we calculate or update $\\rho_i$ and $\\sigma_i$ by Eqs. (\\ref{equation_umap_rho}) and (\\ref{equation_umap_sigma}).\nFor every new point $\\ensuremath\\boldsymbol{x}_i$, we also calculate $p_{ij}$ and $q_{ij}$ by Eqs. (\\ref{equation_UMAP_p}) and (\\ref{equation_UMAP_q}), where $j$ indexes all existing and new points. Then, for every new or updated point, we update the embedding of point by gradient descent where $m$ negative samples are used as in the original UMAP. The paper of progressive UMAP has not mentioned updating the embedding of neighbor (positive) points which we have in the original UMAP. \n\n\n\n\\section{Conclusion}\\label{section_conclusion}\n\nThis was a tutorial paper on UMAP and its variants. We explained the details of UMAP and derived its gradients. We explained the theory of UMAP's cost function by algebraic topology and fuzzy category theory. We compared UMAP with t-SNE and LargeVis, discussed the repulsive forces and negative sampling, and introduced DensMAP, parametric UMAP, and progressive UMAP.  \nFor brevity, some other algorithms such as using genetic programming with UMAP \\cite{schofield2021using} was not explained in this paper. \n\n\n\n\n\n\n\n", "meta": {"timestamp": "2021-09-07T02:35:43", "yymm": "2109", "arxiv_id": "2109.02508", "language": "en", "url": "https://arxiv.org/abs/2109.02508"}}
{"text": "\\section{Introduction}\n\\label{sec: intro}\n\nThe purpose of this paper is to characterise simplicity of twisted C*-algebras arising from continuous $2$-cocycles on Deaconu--Renault groupoids of actions of $\\mathbb{N}^k$ on second-countable locally compact Hausdorff spaces. The study of twisted C*-algebras associated to continuous groupoid $2$-cocycles dates back to Renault's seminal work \\cite{Renault1980}. They serve both as a very flexible C*-algebraic framework for modelling dynamical systems, and as a source of tractable models for classifiable C*-algebras \\cite{RT1985, aHKS2011, BCSS2016, Li2020}. So it is important to be able to determine when a given twisted groupoid C*-algebra is simple; but this is in general a complicated question.\n\nDeaconu--Renault groupoids encode actions of submonoids of abelian groups by local homeomorphisms of locally compact Hausdorff spaces. In hindsight, the first example of such a groupoid was the one associated to the one-sided full shift on $n$ letters, introduced by Renault in \\cite{Renault1980} as a model for the Cuntz algebra. However groupoids of this type for generic local homeomorphisms (that is, actions of $\\mathbb{N}$) were first studied by Deaconu \\cite{Deaconu1995}, and have come to be known as (rank-$1$) Deaconu--Renault groupoids. Shortly afterwards they were used as models for graph C*-algebras in \\cite{KPRR1997, KPR1998}, and later still, Yeend \\cite{Yeend2007JOT} showed that rank-$1$ Deaconu--Renault groupoids provide models for the topological-graph C*-algebras of Katsura \\cite{Katsura2004TAMS}.\n\nFor the dual reasons that most of the key examples studied had been related to $0$-dimensional spaces, and that $\\mathbb{N}$ embeds in $\\mathbb{Z}$, which has trivial cohomology, no work was done on twisted C*-algebras associated to Deaconu--Renault groupoids for many years. However, in 2000, Kumjian and Pask \\cite{KP2000} introduced higher-rank graphs (or $k$-graphs) and demonstrated that the associated C*-algebras can be described as the C*-algebras of Deaconu--Renault groupoids of actions of $\\mathbb{N}^k$. This led to the development \\cite{KPS2012JFA, KPS2013JMAA, KPS2015TAMS} of twisted $k$-graph C*-algebras. Kumjian, Pask, and Sims showed that from a $2$-cocycle on a $k$-graph, one can construct a $2$-cocycle on the associated Deaconu--Renault groupoid so that the twisted C*-algebras coincide, and they used this model to characterise simplicity of twisted $k$-graph C*-algebras \\cite{KPS2016JNG}, as well as to describe applications of this characterisation to the study of crossed products of graph algebras by quasi-free actions.\n\nHere we build substantially on elements of the analysis of \\cite{KPS2016JNG} to describe precisely when the twisted C*-algebra of a Deaconu--Renault groupoid for an action of $\\mathbb{N}^k$ by local homeomorphisms is simple (\\cref{thm: simplicity characterisation}). To demonstrate the applicability of our main theorem, we use this result to investigate simplicity of crossed products of C*-algebras associated to rank-$1$ Deaconu--Renault groupoids by actions of $\\mathbb{Z}$ induced by $\\mathbb{T}$-valued $1$-cocycles (\\cref{thm: CP simple}), and we specialise to the Deaconu--Renault groupoids of topological graphs to characterise simplicity of crossed products of topological-graph C*-algebras by quasi-free automorphisms (\\cref{cor: topgraph exact,cor: topgraph checkable}).\n\nThe paper is organised as follows. In \\cref{sec: background} we establish background and notation. In \\cref{sec: isotropy} we describe the periodicity group $P_T$ of a minimal action $T$ of $\\mathbb{N}^k$ on a second-countable locally compact Hausdorff space $X$, and we show that the interior of the isotropy of $\\GG_T$ is isomorphic to the group bundle $X \\times P_T$. In \\cref{sec: cohomology} we show that every $2$-cocycle on $\\GG_T$ is cohomologous to one whose restriction to $X \\times P_T$ is determined by a fixed bicharacter $\\omega$ of $P_T$ that vanishes on its own centre $Z_\\omega$, and we use this to give a concrete description of the spectral action $\\theta$ of $\\GG_T/\\II_T$ on $X \\times \\widehat{Z}_\\omega$. Then in \\cref{sec: simplicity} we state and prove our main theorem. We finish in \\cref{sec: application} by describing an application to crossed products of rank-$1$ Deaconu--Renault groupoid C*-algebras by automorphisms induced by continuous $1$-cocycles. We provide two appendices---one on group cohomology and one on twisted group C*-algebras---to provide a handy reference to some key results on these two topics that we need in the body of the paper, and have found difficult to locate explicitly in the literature.\n\n\n\\section{Background}\n\\label{sec: background}\n\nIn this section we present the necessary background on Hausdorff \\'etale groupoids, $2$-cocycles, twisted groupoid C*-algebras, and Deaconu--Renault groupoids.\n\nWe refer to a topological groupoid $\\mathcal{G}$ with a locally compact Hausdorff topology under which multiplication and inversion are continuous as a \\hl{Hausdorff groupoid}. We write $\\GG^{(0)}$ for the unit space of $\\mathcal{G}$, and $\\GG^{(2)}$ for the set of composable pairs in $\\mathcal{G}$. Given subsets $A, B \\subseteq \\mathcal{G}$, we write $AB \\coloneqq \\{ \\alpha\\beta : (\\alpha, \\beta) \\in (A \\times B) \\cap \\GG^{(2)} \\}$ and $A^{-1} \\coloneqq \\{ \\alpha^{-1} : \\alpha \\in A \\}$, and for $\\gamma \\in \\mathcal{G}$, we write $\\gamma A \\coloneqq \\{\\gamma\\} A$ and $A \\gamma \\coloneqq A \\{\\gamma\\}$. We say that $\\mathcal{G}$ is \\hl{\\'etale} if the range and source maps $r,s\\colon \\mathcal{G} \\to \\GG^{(0)}$ are local homeomorphisms. We call a subset $B$ of $\\mathcal{G}$ a \\hl{bisection} if $B$ is contained in an open subset $U$ of $\\mathcal{G}$ such that $r\\restr{U}$ and $s\\restr{U}$ are homeomorphisms onto open subsets of $\\GG^{(0)}$. Every second-countable Hausdorff \\'etale groupoid has a countable basis of open bisections. For each $x \\in \\GG^{(0)}$, we define $\\mathcal{G}_x \\coloneqq s^{-1}(x)$ and $\\mathcal{G}^x \\coloneqq r^{-1}(x)$; and $\\mathcal{G}_x^x \\coloneqq \\mathcal{G}_x \\cap \\mathcal{G}^x$. We say that $\\mathcal{G}$ is \\hl{minimal} if $r(\\mathcal{G}_x)$ is dense in $\\GG^{(0)}$ for every $x \\in \\GG^{(0)}$. The \\hl{isotropy subgroupoid} of $\\mathcal{G}$ is the groupoid $\\operatorname{Iso}(\\mathcal{G}) \\coloneqq \\bigcup_{x \\in \\GG^{(0)}} \\, \\mathcal{G}_x^x = \\{ \\gamma \\in \\mathcal{G} : r(\\gamma) = s(\\gamma) \\}$. The interior $\\mathcal{I}$ of the isotropy of a Hausdorff \\'etale groupoid $\\mathcal{G}$ is itself a Hausdorff \\'etale groupoid with unit space $\\mathcal{I}^{(0)} = \\GG^{(0)}$. We say that $\\mathcal{G}$ is \\hl{effective} if $\\mathcal{I} = \\GG^{(0)}$, and we say that $\\mathcal{G}$ is \\hl{topologically principal} if $\\{ x \\in \\GG^{(0)} : \\mathcal{G}_x^x = \\{x\\} \\}$ is dense in $\\GG^{(0)}$. By \\cite[Lemma~3.1]{BCFS2014}, every topologically principal Hausdorff \\'etale groupoid is effective, and every effective second-countable Hausdorff \\'etale groupoid is topologically principal.\n\nThe following definition of a groupoid action comes from \\cite[Definition~1.60]{Goehle2009}.\n\n\\begin{definition} \\label{def: groupoid action}\nSuppose that $\\mathcal{G}$ is a topological groupoid and $X$ is a topological space. We say that \\hl{$\\mathcal{G}$ acts continuously on (the left of) $X$}, and that $X$ is a \\hl{continuous (left) $\\mathcal{G}$-space}, if there is a continuous surjective map $R\\colon X \\to \\GG^{(0)}$ and a continuous map $\\theta\\colon (\\gamma,x) \\mapsto \\gamma \\cdot x$ from $\\mathcal{G} \\star X \\coloneqq \\{ (\\gamma,x) \\in \\mathcal{G} \\times X : s(\\gamma) = R(x) \\}$ to $X$, satisfying\n\\begin{enumerate}[label=(A\\arabic*)]\n\\item \\label{item: composing groupoid actions} if $(\\alpha,\\beta) \\in \\GG^{(2)}$ and $(\\beta,x) \\in \\mathcal{G} \\star X$, then $(\\alpha\\beta,x),\\, (\\alpha,\\, \\beta \\cdot x) \\in \\mathcal{G} \\star X$, and we have $\\alpha \\cdot (\\beta \\cdot x) = (\\alpha\\beta) \\cdot x$; and\n\\item \\label{item: action of groupoid unit} for all $x \\in X$, we have $(R(x),x) \\in \\mathcal{G} \\star X$, and $R(x) \\cdot x = x$.\n\\end{enumerate}\nWe refer to the map $\\theta$ as a \\hl{continuous (left) action} of $\\mathcal{G}$ on $X$. For each $x \\in X$, the \\hl{orbit} of $x$ under $\\theta$ is the set\n\\[\n[x]_{\\theta} \\coloneqq \\{ \\gamma \\cdot x : (\\gamma, x) \\in \\mathcal{G} \\star X \\}.\n\\]\n\\end{definition}\n\nWe now recall the relevant cohomology theory for groupoids from \\cite[Section~I.1]{Renault1980}. See \\cref{sec_appendix: group chohomology} for the relevant cohomology theory for groups.\n\n\\begin{definition} \\label{def: groupoid cohomology}\nLet $\\mathcal{G}$ be a topological groupoid, and let $A$ be a topological abelian group with identity $e_A$.\n\\begin{enumerate}[label=(\\roman*), ref={\\cref{def: groupoid cohomology}(\\roman*)}]\n\\item A \\hl{continuous $A$-valued $1$-cochain} on $\\mathcal{G}$ is a continuous map $b\\colon \\mathcal{G} \\to A$. We say that $b$ is \\hl{normalised} if $b(r(\\gamma)) = b(s(\\gamma)) = e_A$ for all $\\gamma \\in \\mathcal{G}$.\n\\item A \\hl{continuous $A$-valued $1$-cocycle} on $\\mathcal{G}$ is a continuous $1$-cochain $c\\colon \\mathcal{G} \\to A$ satisfying $c(\\alpha\\beta) = c(\\alpha) c(\\beta)$ for all $(\\alpha,\\beta) \\in \\GG^{(2)}$.\n\\item A \\hl{continuous $A$-valued $2$-cocycle} on $\\mathcal{G}$ is a continuous map $\\sigma\\colon \\GG^{(2)} \\to A$ that satisfies the \\hl{$2$-cocycle identity}: $\\sigma(\\alpha,\\beta) \\, \\sigma(\\alpha\\beta,\\gamma) = \\sigma(\\alpha,\\beta\\gamma) \\, \\sigma(\\beta,\\gamma)$ for all $\\alpha, \\beta, \\gamma \\in \\mathcal{G}$ such that $s(\\alpha) = r(\\beta)$ and $s(\\beta) = r(\\gamma)$, and is \\hl{normalised}, in the sense that $\\sigma(r(\\gamma),\\gamma) = \\sigma(\\gamma,s(\\gamma)) = e_A$ for all $\\gamma \\in \\mathcal{G}$. We write $Z^2(\\mathcal{G},A)$ for the group of continuous $A$-valued $2$-cocycles on $\\mathcal{G}$.\n\\item \\label{item: 2-coboundary} The \\hl{continuous $2$-coboundary} associated to a continuous normalised $A$-valued $1$-cochain $b\\colon \\mathcal{G} \\to A$ is the map $\\delta^1 b \\colon \\GG^{(2)} \\to A$ given by $\\delta^1 b(\\alpha,\\beta) \\coloneqq b(\\alpha) \\, b(\\beta) \\, b(\\alpha\\beta)^{-1}$.\n\\item We say that two continuous $2$-cocycles $\\sigma,\\tau\\colon \\GG^{(2)} \\to A$ are \\hl{cohomologous} if there exists a continuous normalised $1$-cochain such that $\\delta^1 b(\\alpha,\\beta) = \\sigma(\\alpha,\\beta)^{-1} \\, \\tau(\\alpha,\\beta)$ for all $(\\alpha,\\beta) \\in \\GG^{(2)}$.\n\\end{enumerate}\n\\end{definition}\n\nWe write $\\mathbb{T}$ for the multiplicative group of complex numbers of modulus $1$. Suppose that $\\mathcal{G}$ is a Hausdorff groupoid and take $\\sigma \\in Z^2(\\mathcal{G},\\mathbb{T})$. Let $\\mathcal{G} \\times_\\sigma \\mathbb{T}$ be the set $\\mathcal{G} \\times \\mathbb{T}$ endowed with the product topology, and equipped with the multiplication operation \\begin{equation} \\label{eqn: twisted groupoid multiplication}\n(\\alpha,w)(\\beta,z) \\coloneqq \\big(\\alpha\\beta, \\, \\sigma(\\alpha,\\beta) wz\\big),\n\\end{equation}\ndefined for all $(\\alpha,\\beta) \\in \\GG^{(2)}$ and $w, z \\in \\mathbb{T}$, and the inversion operation \\begin{equation} \\label{eqn: twisted groupoid inversion}\n(\\alpha,w)^{-1} \\coloneqq \\big(\\alpha^{-1}, \\, \\overline{\\sigma(\\alpha,\\alpha^{-1})} \\overline{w}\\big),\n\\end{equation}\ndefined for all $(\\alpha,w) \\in \\mathcal{G} \\times \\mathbb{T}$. Then $\\mathcal{G} \\times_\\sigma \\mathbb{T}$ is a Hausdorff groupoid.\n\nWe now recall Renault's construction of the full twisted groupoid C*-algebra $C^*(\\mathcal{G},\\sigma)$ associated to a Hausdorff \\'etale groupoid $\\mathcal{G}$ and a continuous $\\mathbb{T}$-valued $2$-cocycle $\\sigma$ on $\\mathcal{G}$. Note that Renault gives this construction for groupoids that are not necessarily \\'etale, but we specialise to the \\'etale case since we will primarily be dealing with Deaconu--Renault groupoids, which are \\'etale. Renault also defines reduced twisted groupoid C*-algebras, but we will only be working with amenable groupoids, and in this setting, the full and reduced C*-algebras coincide. Let $C_c(\\mathcal{G},\\sigma)$ denote the complex vector space of continuous compactly supported complex-valued functions on $\\mathcal{G}$, equipped with multiplication given by the \\hl{twisted convolution} formula\n\\[\n(f * g)(\\gamma) \\coloneqq \\sum_{\\substack{(\\alpha,\\beta) \\in \\GG^{(2)}, \\\\ \\alpha\\beta = \\gamma}} \\sigma(\\alpha,\\beta) \\, f(\\alpha) \\, g(\\beta) = \\sum_{\\zeta \\in \\mathcal{G}^{r(\\gamma)}} \\sigma(\\zeta, \\zeta^{-1}\\gamma) \\, f(\\zeta) \\, g(\\zeta^{-1}\\gamma),\n\\]\nand involution given by\n\\[\nf^*(\\gamma) \\coloneqq \\overline{\\sigma(\\gamma,\\gamma^{-1})} \\, \\overline{f(\\gamma^{-1})}.\n\\]\nThen $C_c(\\mathcal{G},\\sigma)$ is a $*$-algebra. We write $fg$ for the twisted convolution product $f * g$ when the intended meaning is clear. The \\hl{full twisted groupoid C*-algebra} $C^*(\\mathcal{G},\\sigma)$ is the completion of $C_c(\\mathcal{G},\\sigma)$ with respect to the \\hl{full C*-norm}, which is given by\n\\[\n\\ensuremath{\\lVert} f \\ensuremath{\\rVert} \\coloneqq \\sup\\big\\{ \\ensuremath{\\lVert} \\pi(f) \\ensuremath{\\rVert} : \\pi \\text{ is a $*$-representation of } C_c(\\mathcal{G},\\sigma) \\big\\}.\n\\]\n\nGiven a locally compact Hausdorff space $Y$ and a function $f \\in C_c(Y)$, we define the \\hl{open support} of $f$ to be the set $\\operatorname{osupp}(f) \\coloneqq f^{-1}(\\mathbb{C} {\\setminus} \\{0\\})$, and the \\hl{support} of $f$ to be the set $\\operatorname{supp}(f) \\coloneqq \\overline{\\operatorname{osupp}(f)}$.\n\nWe now recall the definition of the Deaconu--Renault groupoid associated to an action of $\\mathbb{N}^k$ by local homeomorphisms. Details appear in \\cite[Proposition~3.1]{SW2016}.\n\nFix $k \\in \\mathbb{N} {\\setminus} \\{0\\}$. Let $T\\colon n \\mapsto T^n$ be an action of $\\mathbb{N}^k$ on a locally compact Hausdorff space $X$ by local homeomorphisms. We call the pair $(X,T)$ a \\hl{rank-$k$ Deaconu--Renault system}. Define\n\\[\n\\GG_T \\coloneqq \\{ (x,m-n,y) \\in X \\times \\mathbb{Z}^k \\times X \\,:\\, m, n \\in \\mathbb{N}^k, \\, T^m(x) = T^n(y) \\},\n\\]\nand\n\\[\n\\GG_T^{(2)} \\coloneqq \\{ ((x,m,y), (w,n,z)) \\in \\GG_T \\times \\GG_T \\,:\\, y = w \\}.\n\\]\nIf $((x,m,y), (y,n,z)) \\in \\GG_T^{(2)}$, then $(x,m+n,z), (y,-m,x) \\in \\GG_T$. We define multiplication from $\\GG_T^{(2)}$ to $\\GG_T$ by $(x,m,y)(y,n,z) \\coloneqq (x,m+n,z)$, and inversion on $\\GG_T$ by $(x,m,y)^{-1} \\coloneqq (y,-m,x)$. Then $\\GG_T$ is a groupoid, called a \\hl{Deaconu--Renault groupoid}. The unit space of $\\GG_T$ is $\\GG_T^{(0)} = \\{ (x,0,x) : x \\in X \\}$, and we identify it with $X$. The range and source maps of $\\GG_T$ are given by $r(x,m,y) \\coloneqq x$ and $s(x,m,y) \\coloneqq y$. For open sets $U, V \\subseteq X$ and for $m, n \\in \\mathbb{N}^k$, we define\n\\[\nZ(U,m,n,V) \\coloneqq \\{ (x,m-n,y) \\,:\\, x \\in U, \\, y \\in V,\\, \\text{and } T^m(x) = T^n(y) \\}.\n\\]\nThe collection $\\{ Z(U,m,n,V) : U, V \\subseteq X \\text{ are open, and } m, n \\in \\mathbb{N}^k \\}$ is a basis for a locally compact Hausdorff topology on $\\GG_T$. The sets $Z(U,m,n,V)$ such that $T^m\\restr{U}$ and $T^n\\restr{V}$ are homeomorphisms onto their ranges and $T^m(U) = T^n(V)$ form a basis for the same topology. Under this topology, $\\GG_T$ is a locally compact Hausdorff \\'etale groupoid. If $X$ is second-countable, then $\\GG_T$ is also second-countable.\n\n\\begin{lemma} \\label{lemma: 1-cocycle c}\nLet $(X,T)$ be a rank-$k$ Deaconu--Renault system. The map $c\\colon (x,n,y) \\mapsto n$ is a continuous $\\mathbb{Z}^k$-valued $1$-cocycle on $\\GG_T$, and for each $x \\in X$, the restriction of $c$ to $(\\GG_T)_x^x$ is injective.\n\\end{lemma}\n\n\\begin{proof}\nFix $\\alpha = (x, p, y)$ and $\\beta = (y,q,z) \\in \\GG_T$. Then $c(\\alpha\\beta) = p+q = c(\\alpha) + c(\\beta)$, and so $c$ is a $1$-cocycle. Since each $c\\restr{Z(U, m, n, V)}$ is constant, $c$ is locally constant and hence continuous.\n\\end{proof}\n\n\\begin{definition}\nLet $(X,T)$ be a rank-$k$ Deaconu--Renault system. The \\hl{orbit} under $T$ of $x \\in X$ is\n\\[\n\\Orb[T]{x} \\coloneqq \\bigcup_{m,n \\in \\mathbb{N}^k} (T^m)^{-1}\\big(T^n(x)\\big) = \\{ y \\in X : T^m(y) = T^n(x) \\text{ for some } m, n \\in \\mathbb{N}^k \\}.\n\\]\nWe say that $(X,T)$ is \\hl{minimal} if $\\Orb[T]{x}$ is dense in $X$ for each $x \\in X$. We frequently just write $\\Orb{x}$ for $\\Orb[T]{x}$.\n\\end{definition}\n\n\\begin{remark} \\label{rem: minimality}\nWe have $\\Orb{x} = r(s^{-1}(x)) \\subseteq \\GG_T^{(0)}$, and so $\\GG_T$ is minimal if and only if $(X,T)$ is minimal.\n\\end{remark}\n\n\\begin{remark}\nBy \\cite[Lemma~3.5]{SW2016}, every Deaconu--Renault groupoid is amenable, and so we can discuss the twisted C*-algebra associated to a Deaconu--Renault groupoid and a continuous $2$-cocycle without any ambiguity as to whether we mean the full or reduced C*-algebra.\n\\end{remark}\n\n\\begin{remark}\nThe C*-algebras studied here are related to previous work. Suppose that $\\Lambda$ is a proper, source-free topological $k$-graph with infinite-path space $\\Lambda^\\infty$ (as defined in \\cite[Section~3]{AB2018}). For each $n \\in \\mathbb{N}^k$, let $T^n\\colon \\Lambda^\\infty \\to \\Lambda^\\infty$ be the shift map. Then $(\\Lambda^\\infty,T)$ is a rank-$k$ Deaconu--Renault system, and the associated Deaconu--Renault groupoid $\\mathcal{G}_\\Lambda \\coloneqq \\GG_T$ is called the \\hl{boundary-path groupoid} of the topological $k$-graph. The twisted C*-algebras $C^*(\\mathcal{G}_\\Lambda,\\sigma)$ associated to continuous $2$-cocycles $\\sigma \\in Z^2(\\mathcal{G}_\\Lambda, \\mathbb{T})$ on proper, source-free topological $k$-graphs generalise the twisted C*-algebras of discrete $k$-graphs studied in \\cite{KPS2012JFA, KPS2013JMAA, KPS2015TAMS, KPS2016JNG}, and are studied in the first-named author's PhD thesis \\cite{Armstrong2019}. In \\cite{AB2018}, the first- and second-named authors study an alternative notion of a twisted C*-algebra of a topological $k$-graph associated to a continuous $2$-cocycle on the topological $k$-graph itself, which is constructed using a product system of Hilbert bimodules. In the case where $\\Lambda$ is a discrete $k$-graph, it is known (see \\cite[Theorem~7.2.2]{Armstrong2014}) that these two constructions give the same C*-algebra, but in the more general topological setting, the relationship is unknown.\n\\end{remark}\n\n\n\\section{The interior of the isotropy of a Deaconu--Renault groupoid}\n\\label{sec: isotropy}\n\nIn this section we introduce the periodicity group $P_T$ of a minimal Deaconu--Renault groupoid $(X,T)$, and we show that the interior $\\II_T$ of the isotropy of $\\GG_T$ can be identified with $X \\times P_T$.\n\n\\begin{definition} \\label{def: PT}\nLet $(X,T)$ be a rank-$k$ Deaconu--Renault system. For each nonempty precompact open set $U \\subseteq X$, we define\n\\[\nP_T(U) \\coloneqq \\{ m - n \\,:\\, m, n \\in \\mathbb{N}^k, \\text{ and } T^m\\restr{\\overline{U}} = T^n\\restr{\\overline{U}} \\text{ is injective} \\}.\n\\]\nWe define\n\\[\nP_T \\coloneqq \\bigcup_{\\varnothing \\ne U \\subseteq X \\text{ precompact open}} P_T(U).\n\\]\n\\end{definition}\n\n\\begin{remark}\nWhen $k = 1$, the set $P_T(U)$ is related to the group $\\Stabess{x}$ from \\cite[Page~29]{CRST2021}. Specifically, $\\Stabess{x}$ contains $P_T(U)$ for any precompact open set $U$ containing $x$; but also, since $\\Stabess{x}$ is a subgroup of $\\mathbb{Z}^k$, and hence finitely generated, it is not too hard to check that there is an open cover of $X$ by sets $U$ such that $P_T(U) = \\Stabess{x}$ for each $x$ in $U$.\n\\end{remark}\n\nIn addition to being needed for our own arguments, our next result, \\cref{prop: IT characterisation}, plugs a gap in the literature---it is mentioned without proof in \\cite[Page~30]{CRST2021}.\n\n\n\\begin{prop} \\label{prop: PT characterisation}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system. Then\n\\[\nP_T = \\{ p \\in \\mathbb{Z}^k : (x,p,x) \\in \\GG_T \\text{ for all } x \\in X \\},\n\\]\nand $P_T$ is a subgroup of $\\mathbb{Z}^k$.\n\\end{prop}\n\nIn order to prove \\cref{prop: PT characterisation}, we need the following lemma.\n\n\\begin{lemma} \\label{lemma: open nbhd when T^m(y) ne T^n(y)}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system. Suppose that $m, n \\in \\mathbb{N}^k$ and $y \\in X$ satisfy $T^m(y) \\ne T^n(y)$. Then there exists an open neighbourhood $W \\subseteq X$ of $y$ such that $T^m\\restr{W}$ and $T^n\\restr{W}$ are injective and $T^m(W) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} T^n(W) = \\varnothing$.\n\\end{lemma}\n\n\\begin{proof}\nSince $X$ is Hausdorff, we can choose open neighbourhoods $U \\subseteq X$ of $T^m(y)$ and $V \\subseteq X$ of $T^n(y)$ such that $U \\cap V = \\varnothing$. Define $A \\coloneqq (T^m)^{-1}(U) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} (T^n)^{-1}(V)$. Then $y \\in A$. Since $T^m$ and $T^n$ are local homeomorphisms, there is an open neighbourhood $W \\subseteq A$ of $y$ such that $T^m\\restr{W}$ and $T^n\\restr{W}$ are injective, and we have $T^m(W) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} T^n(W) \\subseteq U \\cap V = \\varnothing$.\n\\end{proof}\n\n\\begin{proof}[Proof of \\cref{prop: PT characterisation}]\nFix $p \\in P_T$. Then there exist $m, n \\in \\mathbb{N}^k$ and a nonempty open set $U \\subseteq X$ such that $p = m - n$, and $T^m\\restr{U} = T^n\\restr{U}$ is injective. Fix $z \\in X$. We claim that $(z,p,z) \\in \\GG_T$. Since $\\Orb{z}$ is dense in $X$, we have $U \\cap \\Orb{z} \\ne \\varnothing$, and so there exist $y \\in U$ and $a,b \\in \\mathbb{N}^k$ such that $T^a(y) = T^b(z)$. Thus,\n\\[\nT^{b+m}(z) = T^m(T^b(z)) = T^m(T^a(y)) = T^a(T^m(y)),\n\\]\nand\n\\[\nT^{b+n}(z) = T^n(T^b(z)) = T^n(T^a(y)) = T^a(T^n(y)).\n\\]\nSince $y \\in U$, we have $T^m(y) = T^n(y)$, and hence $T^{b+m}(z) = T^{b+n}(z)$. Therefore, $(z,p,z) = \\big(z, (b+m) - (b+n), z\\big) \\in \\GG_T$, and so\n\\[\nP_T \\subseteq \\{ p \\in \\mathbb{Z}^k : (x,p,x) \\in \\GG_T \\text{ for all } x \\in X \\}.\n\\]\n\nWe now show that $\\mathbb{Z}^k \\!\\setminus\\! P_T \\subseteq \\{ p \\in \\mathbb{Z}^k : (x, p, x) \\notin \\GG_T \\text{ for some } x \\in X \\}$. To see this, fix $p \\in \\mathbb{Z}^k \\!\\setminus\\! P_T$. Let $(m_i,n_i)_{i=1}^\\infty$ be an enumeration of $\\{(m,n) \\in \\mathbb{N}^k \\times \\mathbb{N}^k \\,:\\, m - n = p\\}$. We must find $x \\in X$ such that $T^{m_i}(x) \\ne T^{n_i}(x)$ for all $i \\ge 1$. We claim that there exist nonempty precompact open subsets $V_0, V_1, V_2, \\dotsc$ of $X$ satisfying\n\\begin{enumerate}[label=(\\arabic*)]\n\\item \\label{item: Vthing1} $\\overline{V_i} \\subseteq V_{i-1}$ for all $i \\ge 1$,\n\\item \\label{item: Vthing2} $T^{m_i}\\restr{\\overline{V_{i-1}}}$ and $T^{n_i}\\restr{\\overline{V_{i-1}}}$ are injective for all $i \\ge 1$, and\n\\item \\label{item: Vthing3} $T^{m_i}\\big(\\overline{V_i}\\big) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} T^{n_i}\\big(\\overline{V_i}\\big) = \\varnothing$ for all $i \\ge 1$.\n\\end{enumerate}\nTo start, let $V_0$ be a nonempty precompact open subset of $X$ such that $T^{m_1}\\restr{\\overline{V_0}}$ and $T^{n_1}\\restr{\\overline{V_0}}$ are injective. Now fix $i \\ge 1$ and suppose that $V_0, \\dotsc, V_{i-1}$ satisfy \\cref{item: Vthing1}--\\cref{item: Vthing3}. Since $m_i - n_i = p \\notin P_T$, we have $T^{m_i}\\restr{V_{i-1}} \\ne T^{n_i}\\restr{V_{i-1}}$, and so there exists $y \\in V_{i-1}$ such that $T^{m_i}(y) \\ne T^{n_i}(y)$. Thus, by \\cref{lemma: open nbhd when T^m(y) ne T^n(y)}, there exists an open neighbourhood $W \\subseteq V_{i-1}$ of $y$ such that $T^{m_i}(W) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} T^{n_i}(W) = \\varnothing$. Since $X$ is locally compact and Hausdorff and $T^{m_{i+1}}$ and $T^{n_{i+1}}$ are local homeomorphisms, there is an open neighbourhood $V_i$ of $y$ such that $\\overline{V_i} \\subseteq W$ and $T^{m_{i+1}}\\restr{\\overline{V_i}}$ and $T^{n_{i+1}}\\restr{\\overline{V_i}}$ are injective. So induction gives the desired sets $V_i$. Each $V_i$ is contained in the compact set $\\overline{V_0}$, and so the descending intersection $\\bigcap_{i=1}^\\infty \\overline{V_i}$ is nonempty. Any $x \\in \\bigcap_{i=1}^\\infty \\overline{V_i}$ satisfies $T^{m_i}(x) \\ne T^{n_i}(x)$ for all $i \\ge 1$.\n\nWe conclude by showing that $P_T$ is a subgroup of $\\mathbb{Z}^k$. For all $x \\in X$, we have $(x,0,x) \\in \\GG_T^{(0)} \\subseteq \\GG_T$, and so $0 \\in P_T$. Suppose that $p, q \\in P_T$. For all $x \\in X$, we have $(x,p,x), (x,q,x) \\in \\GG_T$, and hence $(x,p-q,x) = (x,p,x)(x,q,x)^{-1} \\in \\GG_T$. Thus $p - q \\in P_T$, and so $P_T$ is a subgroup of $\\mathbb{Z}^k$.\n\\end{proof}\n\nGiven a rank-$k$ Deaconu--Renault system $(X,T)$, we write $\\II_T$ for the topological interior of $\\operatorname{Iso}(\\GG_T)$. Since $\\GG_T$ is a locally compact Hausdorff \\'etale groupoid, so is $\\II_T$. From this point forward, we will assume that $X$ is second-countable (and hence so are $\\GG_T$ and $\\II_T$). We know from \\cite[Lemma~3.5]{SW2016} that $\\GG_T$ is amenable, and hence \\cite[Proposition~5.1.1]{ADR2000} implies that $\\II_T$ is amenable.\n\n\\begin{prop} \\label{prop: IT characterisation}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Let $P_T$ be as in \\cref{def: PT}. Then\n\\[\n\\II_T = \\{ (x,p,x) : x \\in X, \\, p \\in P_T \\} \\cong X \\times P_T.\n\\]\n\\end{prop}\n\n\\begin{proof}\nFor $\\subseteq$, fix $\\gamma \\in \\II_T$. Let $c\\colon \\GG_T \\to \\mathbb{Z}^k$ be the continuous $1$-cocycle defined in \\cref{lemma: 1-cocycle c}. Let $p \\coloneqq c(\\gamma)$ so that $\\gamma = (x,p,x)$ for some $x \\in X$. We claim that $p \\in P_T$. By \\cref{rem: minimality}, $\\GG_T$ is minimal, and hence \\cite[Proposition~2.1]{KPS2016JNG} implies that for all $y \\in X$,\n\\[\nc\\big(\\II_T \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} (\\GG_T)_y^y\\big) = c\\big(\\II_T \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} (\\GG_T)_x^x\\big),\n\\]\nand thus\n\\[\np = c(x,p,x) \\in c\\big(\\II_T \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} (\\GG_T)_x^x\\big) = c\\big(\\II_T \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} (\\GG_T)_y^y\\big).\n\\]\nSo \\cref{prop: PT characterisation} gives $p \\in P_T$.\n\nFor $\\supseteq$, fix $x \\in X$ and $p \\in P_T$. By the definition of $P_T$, there exist $m, n \\in \\mathbb{N}^k$ and a nonempty precompact open set $U \\subseteq X$ such that $p = m-n$ and $T^m\\restr{\\overline{U}} = T^n\\restr{\\overline{U}}$ is injective. This injectivity forces $Z(U,m,n,U) = \\{ (y,p,y) : y \\in U \\} \\subseteq \\II_T$. Fix $y \\in U$. Then $(y,p,y) \\in \\II_T$, and so \\cite[Proposition~2.1]{KPS2016JNG} implies that\n\\[\np = c(y,p,y) \\in c\\big(\\II_T \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} (\\GG_T)_y^y\\big) = c\\big(\\II_T \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} (\\GG_T)_x^x\\big),\n\\]\nand hence $(x,p,x) \\in \\II_T$.\n\\end{proof}\n\n\\begin{remark}\n\\Cref{prop: IT characterisation} is related to the sets $\\Sigma_X$ and $H(T)$ of \\cite{SW2016} as follows. Let $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. In the notation of \\cite[Section~3]{SW2016}, suppose that $\\Sigma = \\Sigma_X$. Then $T$ is an irreducible action of $\\mathbb{N}^k$ on $X$, and \\cite[Proposition~3.10]{SW2016} implies that\n\\[\n\\II_T = \\{ (x,p,x) : x \\in X, \\, p \\in H(T) \\} \\cong X \\times H(T).\n\\]\nThus \\cref{prop: IT characterisation} implies that $P_T = H(T)$.\n\\end{remark}\n\nWe now present two corollaries of \\cref{prop: IT characterisation}.\n\n\\begin{cor} \\label{cor: GT effective iff PT trivial}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Then $\\GG_T$ is effective if and only if $P_T = \\{0\\}$.\n\\end{cor}\n\n\\begin{proof}\nBy \\cref{prop: IT characterisation}, $\\II_T = \\{ (x,p,x) : x \\in X, \\, p \\in P_T \\}$. Hence\n\\[\n\\GG_T \\text{ is effective} \\iff \\II_T = \\GG_T^{(0)} \\iff P_T = \\{0\\}. \\qedhere\n\\]\n\\end{proof}\n\n\\begin{cor}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Let $c\\colon \\GG_T \\to \\mathbb{Z}^k$ be as in \\cref{lemma: 1-cocycle c}. Then for each $p \\in P_T$, we have $(c\\restr{\\II_T})^{-1}(p) = \\{(x, p, x) : x \\in X\\}$, and $\\left\\{ c\\restr{\\II_T}^{-1}(p) : p \\in P_T \\right\\}$ is a collection of mutually disjoint clopen bisections whose union is $\\II_T$.\n\\end{cor}\n\n\\begin{proof}\nFix $p \\in P_T$. Since $c$ is continuous and $\\mathbb{Z}^k$ is discrete, $c\\restr{\\II_T}^{-1}(p)$ is clopen. Fix $x \\in X$. If $\\alpha, \\beta \\in c\\restr{\\II_T}^{-1}(p)$ and $r(\\alpha) = x = r(\\beta)$, then $s(\\alpha) = x = s(\\beta)$ because $\\alpha, \\beta \\in \\operatorname{Iso}(\\GG_T)$, and hence $\\alpha = (x,p,x) = \\beta$. So $r\\restr{c\\restr{\\II_T}^{-1}(p)}$ is injective, and a similar argument shows that $s\\restr{c\\restr{\\II_T}^{-1}(p)}$ is also injective. Hence $c\\restr{\\II_T}^{-1}(p)$ is a bisection. By \\cref{prop: IT characterisation}, we have $c(\\II_T) = P_T$, and the result follows.\n\\end{proof}\n\nWe now prove that if $\\GG_T$ is minimal, then we can form the quotient groupoid $\\GG_T/\\II_T$.\n\n\\begin{prop} \\label{prop: HT quotient groupoid}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Then $\\II_T$ is a closed subgroupoid of $\\GG_T$ and acts freely and properly on $\\GG_T$ by right-multiplication. The set $\\HH_T \\coloneqq \\GG_T / \\II_T$ is a locally compact Hausdorff \\'etale groupoid, with multiplication given by $[\\alpha][\\beta] \\coloneqq [\\alpha\\beta]$ for $(\\alpha,\\beta) \\in \\GG_T^{(2)}$, inversion given by $[\\gamma]^{-1} \\coloneqq [\\gamma^{-1}]$ for $\\gamma \\in \\GG_T$, and range and source maps given by $r([\\gamma]) = [r(\\gamma)]$ and $s([\\gamma]) = [s(\\gamma)]$.\n\\end{prop}\n\n\\begin{proof}\nTogether, \\cref{rem: minimality,lemma: 1-cocycle c} allow us to apply \\cite[Proposition~2.1]{KPS2016JNG} to see that $\\II_T$ is a closed subgroupoid of $\\GG_T$. Therefore, \\cite[Proposition~2.5(d)]{SW2016} implies that $\\HH_T$ is a locally compact Hausdorff \\'etale groupoid under the given operations.\n\\end{proof}\n\nWe conclude this section with two technical lemmas that we use in the proof of our characterisation of simplicity of $C^*(\\GG_T,\\sigma)$ in \\cref{sec: simplicity}.\n\n\\begin{lemma} \\label{lemma: IT U/V bar}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Let $c \\colon \\GG_T \\to \\mathbb{Z}^k$ be as in \\cref{lemma: 1-cocycle c}. Fix $m, n \\in \\mathbb{Z}^k$, and let $U$ and $V$ be precompact open bisections of $\\GG_T$ such that $U \\subseteq c^{-1}(m)$ and $V \\subseteq c^{-1}(n)$. Then $\\overline{\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V} \\subseteq \\II_T \\overline{U}$ and $\\overline{\\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U} \\subseteq \\II_T \\overline{V}$.\n\\end{lemma}\n\n\\begin{proof}\nDefine $K \\coloneqq r(\\overline{U}) \\times \\{n - m\\} \\times r(\\overline{U})$ and $W \\coloneqq K \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} \\II_T$. Since $r$ is continuous, $K$ is compact, and hence closed. Since $\\II_T$ is closed by \\cref{prop: HT quotient groupoid}, $W$ is closed, and hence is a compact subset of $K$ and of $\\II_T$. We claim that $\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V \\subseteq W \\overline{U}$. For this, suppose that $\\gamma \\in \\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V$. Then there exist $\\xi \\in \\II_T$ and $\\eta \\in U \\subseteq c^{-1}(m)$ such that $\\gamma = \\xi \\eta \\in V \\subseteq c^{-1}(n)$. Hence $\\xi = \\gamma \\eta^{-1} \\subseteq c^{-1}(n - m)$. We also have $r(\\xi) = s(\\xi) = r(\\eta) \\in r(\\overline{U})$, and so $\\xi \\in K \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} \\II_T = W$. Hence $\\gamma = \\xi \\eta \\in W \\overline{U}$, and so $\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V \\subseteq W \\overline{U}$. Since $W$ and $\\overline{U}$ are compact, $W \\overline{U}$ is compact, and hence closed. Thus $\\overline{\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V} \\subseteq \\II_T \\overline{U}$. A symmetric argument shows that $\\overline{\\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U} \\subseteq \\II_T \\overline{V}$.\n\\end{proof}\n\n\\begin{lemma} \\label{lemma: s(gamma) in IT U cap V}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Let $c \\colon \\GG_T \\to \\mathbb{Z}^k$ be as in \\cref{lemma: 1-cocycle c}. Fix $m, n \\in \\mathbb{Z}^k$, and let $U$ and $V$ be precompact open bisections of $\\GG_T$ such that $U \\subseteq c^{-1}(m)$ and $V \\subseteq c^{-1}(n)$. Then $s(\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V) = s(\\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U)$. Moreover, if $(V^{-1} \\II_T U) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} \\II_T \\ne \\varnothing$, then $n - m \\in P_T$, and for each $\\gamma \\in (V^{-1} \\II_T U) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} \\II_T$, we have $s(\\gamma) \\in s(\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V)$.\n\\end{lemma}\n\n\\begin{proof}\nWe first show that $s(\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V) = s(\\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U)$. By symmetry, it suffices to show that $s(\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V) \\subseteq s(\\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U)$. Suppose that $x \\in s(\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V)$. Then there exist $\\zeta \\in \\II_T$ and $\\eta \\in U$ such that $\\zeta \\eta \\in V$ and $x = s(\\zeta \\eta) = s(\\eta)$. Since $\\zeta^{-1} \\in \\II_T$, we have $\\eta = \\zeta^{-1} (\\zeta \\eta) \\in \\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U$, and hence $x = s(\\eta) \\in s(\\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U)$. Thus $s(\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V) \\subseteq s(\\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U)$, as required.\n\nFor the second statement, suppose that $\\gamma \\in (V^{-1} \\II_T U) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} \\II_T$. Then there exist $\\alpha \\in U$, $\\beta \\in V$, and $\\xi \\in \\II_T$ such that $\\gamma = \\beta^{-1} \\xi \\alpha$, and hence $c(\\gamma) = -n + c(\\xi) + m$. Since $\\gamma, \\xi \\in \\II_T$, we have $c(\\gamma), c(\\xi) \\in P_T$ by \\cref{prop: IT characterisation}, and hence $n - m = c(\\xi) - c(\\gamma) \\in P_T$, because $P_T$ is a group by \\cref{prop: PT characterisation}. Since $\\gamma \\in \\II_T$, we have $s(\\beta) = r(\\gamma) = s(\\gamma) = r(\\alpha^{-1})$, and hence $(\\beta,\\alpha^{-1}) \\in \\GG_T^{(2)}$. Since $\\xi \\in \\II_T$, we have $r(\\beta\\alpha^{-1}) = s(\\beta^{-1}) = r(\\xi) = s(\\xi) = r(\\alpha) = s(\\beta\\alpha^{-1})$. We also have $c(\\beta\\alpha^{-1}) = n - m \\in P_T$, and thus \\cref{prop: IT characterisation} implies that $\\beta\\alpha^{-1} \\in \\II_T$. Hence $\\beta = (\\beta\\alpha^{-1})\\alpha \\in \\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V$, and so $s(\\gamma) = r(\\gamma) = s(\\beta) \\in s(\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V)$.\n\\end{proof}\n\n\n\\section{Cohomology of Deaconu--Renault groupoids}\n\\label{sec: cohomology}\n\nIn this section we show that every continuous $\\mathbb{T}$-valued $2$-cocycle on a minimal Deaconu--Renault groupoid $\\GG_T$ is cohomologous to a continuous $\\mathbb{T}$-valued $2$-cocycle $\\sigma$ on $\\GG_T$ that is constant on $\\II_T$ (in the sense of \\cref{def: constant on IT}). We also introduce the spectral action $\\theta$ of $\\HH_T \\coloneqq \\GG_T / \\II_T$, analogous to \\cite[Lemma~3.6]{KPS2016JNG}.\n\n\\begin{definition} \\label{def: constant on IT}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Suppose that $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. We say that $\\sigma$ is \\hl{constant on $\\II_T$} if\n\\[\n\\sigma\\big((x,m,x), (x,n,x)\\big) = \\sigma\\big((y,m,y),(y,n,y)\\big) \\quad \\text{ for all } x, y \\in X \\text{ and } m, n \\in P_T.\n\\]\nIf $\\omega \\in Z^2(P_T,\\mathbb{T})$ is the $2$-cocycle satisfying $\\sigma\\big((x,m,x), (x,n,x)\\big) = \\omega(m,n)$ for all $x \\in X$ and $m, n \\in P_T$, then we say that $\\sigma$ is \\hl{$\\omega$-constant on $\\II_T$}, and we write $\\sigma\\bigrestr{\\II_T^{(2)}} = 1_X \\times \\omega$.\n\\end{definition}\n\nRecall that if $A$ is a countable discrete abelian group, then a \\emph{bicharacter} of $A$ is a function $\\omega\\colon A \\times A \\to \\mathbb{T}$ such that for each fixed $a \\in A$, the maps $\\omega(\\cdot,a)\\colon A \\to \\mathbb{T}$ and $\\omega(a,\\cdot)\\colon A \\to \\mathbb{T}$ are group homomorphisms. (See \\cref{sec_appendix: group chohomology} for some additional background on bicharacters and cohomology of groups.) The following proposition and the lemmas used in its proof are extensions of cohomological results from \\cite[Section~3]{KPS2016JNG} about boundary-path groupoids of cofinal, row-finite, source-free $k$-graphs to the more general setting of Deaconu--Renault groupoids. \\Cref{prop: DR cohomologous vanishing bicharacter} is a generalisation of \\cite[Proposition~3.1]{KPS2016JNG}, but we have adapted it slightly to prove that the bicharacter $\\omega \\in Z^2(P_T,\\mathbb{T})$ can be chosen in such a way that it vanishes on its centre $Z_\\omega$ in each coordinate (in the sense that $\\omega(Z_\\omega,P_T) \\,\\cup\\, \\omega(P_T,Z_\\omega) = \\{1\\}$; see \\cref{def: centre of cocycle}), and hence descends to a bicharacter $\\tilde{\\omega} \\in Z^2(P_T/Z_\\omega,\\mathbb{T})$.\n\n\\begin{prop} \\label{prop: DR cohomologous vanishing bicharacter}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Suppose that $\\rho \\in Z^2(\\GG_T,\\mathbb{T})$. For each $x \\in X$, define $\\rho_x\\colon P_T \\times P_T \\to \\mathbb{T}$ by\n\\[\n\\rho_x(m,n) \\coloneqq \\rho\\big((x,m,x),(x,n,x)\\big).\n\\]\nThen $\\rho_x \\in Z^2(P_T,\\mathbb{T})$. There exists a bicharacter $\\omega \\in Z^2(P_T,\\mathbb{T})$ such that $\\omega$ vanishes on $Z_\\omega$ in each coordinate, and $\\omega$ is cohomologous to $\\rho_x$ for every $x \\in X$. For any such bicharacter $\\omega$, there exists $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$ such that $\\sigma$ is cohomologous to $\\rho$ and is $\\omega$-constant on $\\II_T$ (in the sense of \\cref{def: constant on IT}), and there exists a bicharacter $\\tilde{\\omega} \\in Z^2(P_T/Z_\\omega,\\mathbb{T})$ such that\n\\[\n\\tilde{\\omega}(p+Z_\\omega,q+Z_\\omega) = \\omega(p,q) \\ \\text{ for all } p, q \\in P_T.\n\\]\n\\end{prop}\n\nIn order to prove \\cref{prop: DR cohomologous vanishing bicharacter}, we need the following two results. The first of these results is an extension of \\cite[Lemma~3.2]{KPS2016JNG} to the setting of Deaconu--Renault groupoids.\n\n\\begin{lemma} \\label{lemma: twistchar_gamma^sigma}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. For each $x \\in X$, define $\\sigma_x\\colon\nP_T \\times P_T \\to \\mathbb{T}$ by\n\\[\n\\sigma_x(m,n) \\coloneqq \\sigma\\big((x,m,x),(x,n,x)\\big).\n\\]\nThen $\\sigma_x \\in Z^2(P_T,\\mathbb{T})$. For $\\gamma \\in \\GG_T$ and $y \\coloneqq s(\\gamma) \\in X$, define $\\ensuremath{\\tau}_\\gamma^\\sigma\\colon P_T \\to \\mathbb{T}$ by\n\\begin{equation} \\label{eqn: twistchar_gamma^sigma(p)}\n\\ensuremath{\\tau}_\\gamma^\\sigma(p) \\coloneqq \\sigma\\big(\\gamma,(y,p,y)\\big) \\, \\sigma\\big(\\gamma(y,p,y),\\gamma^{-1}\\big) \\, \\overline{\\sigma(\\gamma,\\gamma^{-1})}.\n\\end{equation}\n\\begin{enumerate}[label=(\\alph*)]\n\\item \\label{item: x to eval sigma_x cts} For all $m, n \\in P_T$, the map $x \\mapsto \\sigma_x(m,n)$ from $X$ to $\\mathbb{T}$ is continuous.\n\\item \\label{item: twistchar_.^sigma cts} For each $p \\in P_T$, the map $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma(p)$ from $\\GG_T$ to $\\mathbb{T}$ is continuous.\n\\item \\label{item: twistchar_.^sigma in conjugation} Fix $\\gamma = (x,m,y) \\in \\GG_T$, $p \\in P_T$, and $w, z \\in \\mathbb{T}$. Under the multiplication and inversion operations on $\\GG_T \\times_\\sigma \\mathbb{T}$ (as defined in \\cref{eqn: twisted groupoid multiplication,eqn: twisted groupoid inversion}), we have\n\\begin{equation} \\label{eqn: chi conjugation}\n(\\gamma, w) \\big((y,p,y), z\\big) (\\gamma, w)^{-1} = \\big((x,p,x), \\, \\ensuremath{\\tau}_\\gamma^\\sigma(p) z\\big).\n\\end{equation}\n\\item \\label{item: twistchar_gamma^sigma(p+q)} For all $\\gamma \\in \\GG_T$ and $p, q \\in P_T$, we have\n\\begin{equation} \\label{eqn: twistchar_gamma^sigma(p+q)}\n\\ensuremath{\\tau}_\\gamma^\\sigma(p+q) = \\sigma_{r(\\gamma)}(p,q)\\, \\overline{\\sigma_{s(\\gamma)}(p,q)}\\, \\ensuremath{\\tau}_\\gamma^\\sigma(p) \\, \\ensuremath{\\tau}_\\gamma^\\sigma(q).\n\\end{equation}\n\\item \\label{item: twistchar_.^sigma 1-cocycle} If $\\omega$ is a bicharacter of $P_T$ such that $\\sigma$ is $\\omega$-constant on $\\II_T$, then $\\sigma_x = \\sigma_y$ for all $x, y \\in X$, and $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma$ is a continuous $\\widehat{P}_T$-valued $1$-cocycle on $\\GG_T$.\n\\end{enumerate}\n\\end{lemma}\n\n\\begin{proof}\nRoutine calculations show that since $\\sigma$ is normalised and satisfies the $2$-cocycle identity, we have $\\sigma_x \\in Z^2(P_T,\\mathbb{T})$ for each $x \\in X$.\n\nFor part~\\cref{item: x to eval sigma_x cts}, note that for each $m, n \\in P_T$, the map $x \\mapsto \\sigma_x(m,n)$ is the composition of the continuous maps $x \\mapsto \\!\\big((x,m,x),(x,n,x)\\big)$ and $\\sigma$.\n\nFor part~\\cref{item: twistchar_.^sigma cts}, fix $p \\in P_T$. For $\\gamma \\in \\GG_T$,\n\\[\n\\ensuremath{\\tau}_\\gamma^\\sigma(p) \\,=\\, \\sigma\\big(\\gamma,(s(\\gamma),p,s(\\gamma))\\big) \\, \\sigma\\big(\\gamma \\, (s(\\gamma),p,s(\\gamma)),\\gamma^{-1}\\big) \\, \\overline{\\sigma\\big(\\gamma,\\gamma^{-1}\\big)}.\n\\]\nThus the map $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma(p)$ from $\\GG_T$ to $\\mathbb{T}$ is continuous because it is a product of continuous functions.\n\nFor part~\\cref{item: twistchar_.^sigma in conjugation}, fix $\\gamma = (x,m,y) \\in \\GG_T$, $p \\in P_T$, and $w, z \\in \\mathbb{T}$. We have\n\\[\n\\gamma (y,p,y) \\gamma^{-1} = (x,m,y)(y,p,y)(y,-m,x) = (x,p,x),\n\\]\nand hence\n\\begin{align*}\n(\\gamma, w) \\big((y,p,y), z\\big) (\\gamma, w)^{-1} &= \\big(\\gamma(y,p,y),\\, \\sigma(\\gamma,(y,p,y)) \\, wz\\big) \\big(\\gamma^{-1}, \\, \\overline{\\sigma(\\gamma,\\gamma^{-1})} \\, \\overline{w}\\big) \\\\\n&= \\big(\\gamma(y,p,y)\\gamma^{-1}, \\, \\sigma(\\gamma(y,p,y), \\gamma^{-1}) \\, \\sigma(\\gamma,(y,p,y)) \\, \\overline{\\sigma(\\gamma,\\gamma^{-1})} \\, z\\big) \\\\\n&= \\big((x,p,x), \\, \\ensuremath{\\tau}_\\gamma^\\sigma(p) z\\big).\n\\end{align*}\n\nFor part~\\cref{item: twistchar_gamma^sigma(p+q)}, fix $\\gamma = (x,m,y) \\in \\GG_T$ and $p, q \\in P_T$. For all $z \\in \\mathbb{T}$, we have\n\\begin{align*}\n\\big((y,p,y),1\\big) \\big((y,q,y),z\\big) &= \\big((y,p,y)(y,q,y), \\, \\sigma\\big((y,p,y),(y,q,y)\\big) z\\big) \\\\\n&= \\big((y,p+q,y), \\, \\sigma_y(p,q) z\\big),\n\\end{align*}\nand so, taking $z = \\overline{\\sigma_y(p,q)}$, we see that\n\\begin{equation} \\label{eqn: y p plus q y}\n\\big((y,p,y), 1\\big) \\big((y,q,y),\\, \\overline{\\sigma_y(p,q)}\\big) = \\big((y,p+q,y), 1\\big).\n\\end{equation}\nTogether, \\cref{eqn: chi conjugation,eqn: y p plus q y} imply that\n\\begin{align*}\n\\big((x,p+q,x), \\, \\ensuremath{\\tau}_\\gamma^\\sigma(p+q)\\big) &= (\\gamma,1) \\, \\big((y,p+q,y), 1\\big) \\, (\\gamma,1)^{-1} \\\\\n&= (\\gamma,1) \\, \\big((y,p,y), 1\\big) \\big((y,q,y), \\, \\overline{\\sigma_y(p,q)}\\big) \\, (\\gamma,1)^{-1} \\\\\n&= (\\gamma,1) \\, \\big((y,p,y), 1\\big) \\, (\\gamma,1)^{-1} (\\gamma,1) \\, \\big((y,q,y), \\, \\overline{\\sigma_y(p,q)}\\big) \\, (\\gamma,1)^{-1} \\\\\n&= \\big((x,p,x), \\, \\ensuremath{\\tau}_\\gamma^\\sigma(p)\\big) \\, \\big((x,q,x), \\, \\ensuremath{\\tau}_\\gamma^\\sigma(q) \\, \\overline{\\sigma_y(p,q)}\\big) \\\\\n&= \\big((x,p+q,x), \\, \\sigma_x(p,q) \\, \\overline{\\sigma_y(p,q)} \\, \\ensuremath{\\tau}_\\gamma^\\sigma(p) \\, \\ensuremath{\\tau}_\\gamma^\\sigma(q)\\big),\n\\end{align*}\nand hence\n\\[\n\\ensuremath{\\tau}_\\gamma^\\sigma(p+q) = \\sigma_{r(\\gamma)}(p,q) \\, \\overline{\\sigma_{s(\\gamma)}(p,q)} \\, \\ensuremath{\\tau}_\\gamma^\\sigma(p) \\, \\ensuremath{\\tau}_\\gamma^\\sigma(q).\n\\]\n\nFor part~\\cref{item: twistchar_.^sigma 1-cocycle}, since $\\sigma$ is $\\omega$-constant on $\\II_T$, for all $x, y \\in X$ and $p, q \\in P_T$, we have\n\\[\n\\sigma_x(p,q) = \\sigma\\big((x,p,x),(x,q,x)\\big) = \\omega(p,q) = \\sigma\\big((y,p,y),(y,q,y)\\big) = \\sigma_y(p,q).\n\\]\nSo $\\sigma_x = \\sigma_y$, and for each $\\gamma \\in \\GG_T$, \\cref{eqn: twistchar_gamma^sigma(p+q)} reduces to\n\\[\n\\ensuremath{\\tau}_\\gamma^\\sigma(p+q) = \\ensuremath{\\tau}_\\gamma^\\sigma(p)\\, \\ensuremath{\\tau}_\\gamma^\\sigma(q).\n\\]\nThus $\\ensuremath{\\tau}_\\gamma^\\sigma\\colon P_T \\to \\mathbb{T}$ is a homomorphism, and so\n$\\ensuremath{\\tau}_\\gamma^\\sigma \\in \\widehat{P}_T$ for each $\\gamma \\in \\GG_T$.\n\nWe now show that the map $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma$ is multiplicative. Fix $\\alpha = (x,m,u),\\, \\beta = (u,n,y) \\in \\GG_T$, and $p \\in P_T$. Using \\cref{eqn: chi conjugation}, we compute\n\\begin{align*}\n\\big((x,p,x), \\, \\ensuremath{\\tau}_\\alpha^\\sigma(p) \\, \\ensuremath{\\tau}_{\\beta}^\\sigma(p)\\big) &= (\\alpha,1) \\, \\big((u,p,u),\\, \\ensuremath{\\tau}_{\\beta}^\\sigma(p)\\big) \\, (\\alpha,1)^{-1} \\\\\n&= (\\alpha,1) (\\beta,1) \\, \\big((y,p,y), 1\\big) \\, (\\beta,1)^{-1} (\\alpha,1)^{-1} \\\\\n&= \\big(\\alpha\\beta, \\, \\sigma(\\alpha,\\beta)\\big) \\big((y,p,y), 1\\big) \\big(\\alpha\\beta, \\, \\sigma(\\alpha,\\beta)\\big)^{-1} \\\\\n&= \\big((x,p,x), \\, \\ensuremath{\\tau}_{\\alpha\\beta}^\\sigma(p)\\big).\n\\end{align*}\nHence $\\ensuremath{\\tau}_\\alpha^\\sigma\\, \\ensuremath{\\tau}_{\\beta}^\\sigma = \\ensuremath{\\tau}_{\\alpha\\beta}^\\sigma$, and so $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma$ is a $\\widehat{P}_T$-valued $1$-cocycle on $\\GG_T$.\n\nWe conclude by showing that the map $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma$ is continuous. Fix a finite subset $F \\subseteq P_T$ and an open subset $U \\subseteq \\mathbb{T}$. The set\n\\[\nS_{\\widehat{P}_T}(F,U) = \\{ \\phi \\in \\widehat{P}_T \\,:\\, \\phi(F) \\subseteq U \\}\n\\]\nis a typical subbasis element for the compact-open topology on $\\widehat{P}_T$, and so it suffices to show that $\\big\\{ \\gamma \\in \\GG_T : \\ensuremath{\\tau}_\\gamma^\\sigma \\in S_{\\widehat{P}_T}(F,U) \\big\\}$ is an open subset of $\\GG_T$. We have\n\\[\n\\big\\{ \\gamma \\in \\GG_T : \\ensuremath{\\tau}_\\gamma^\\sigma(F) \\subseteq U \\big\\} = \\bigcap_{p \\in F} \\big\\{\\gamma \\in \\GG_T : \\ensuremath{\\tau}_\\gamma^\\sigma(p) \\in U \\big\\},\n\\]\nwhich is open by part~\\cref{item: twistchar_.^sigma cts}.\n\\end{proof}\n\nThe following lemma is an extension of \\cite[Lemma~3.3]{KPS2016JNG} to the setting of Deaconu--Renault groupoids.\n\n\\begin{lemma} \\label{lemma: [sigma_x]=[sigma_y]}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. As in \\cref{lemma: twistchar_gamma^sigma}, for each $x \\in X$, define $\\sigma_x \\in Z^2(P_T,\\mathbb{T})$ by\n\\[\n\\sigma_x(m,n) \\coloneqq \\sigma\\big((x,m,x),(x,n,x)\\big).\n\\]\nThen the cohomology class of $\\sigma_x$ does not depend on $x$.\n\\end{lemma}\n\n\\begin{proof}\nBy \\cref{thm: when cohomologous}, it suffices to show that $\\sigma_x\\sigma_x^* = \\sigma_y\\sigma_y^*$ for all $x,y \\in X$. By \\cref{prop: PT characterisation}, $P_T$ is a subgroup of the finitely generated free abelian group $\\mathbb{Z}^k$, and so $P_T \\cong \\mathbb{Z}^l$ for some $l \\le k$. Fix free abelian generators $g_1, \\dotsc, g_l$ of $P_T$. Since each $\\sigma_x\\sigma_x^*$ is a bicharacter (by \\cref{thm: when cohomologous}), it suffices to show that $(\\sigma_x\\sigma_x^*)(g_i,g_j) = (\\sigma_y\\sigma_y^*)(g_i,g_j)$ for all $i,j \\in \\{1,\\dotsc,l\\}$ and $x,y \\in X$. To see this, we first show that $\\sigma_{r(\\gamma)}\\sigma_{r(\\gamma)}^*(g_i,g_j) = \\sigma_{s(\\gamma)}\\sigma_{s(\\gamma)}^*(g_i,g_j)$, for all $\\gamma \\in \\GG_T$ and $i,j \\in \\{1,\\dotsc,l\\}$.\n\nLet $P_T \\times_\\sigma \\mathbb{T} \\coloneqq P_T \\times \\mathbb{T}$ be the semidirect product group, which is equal to $P_T \\times \\mathbb{T}$ as a set, but has group operation\n\\[\n(p,w)(q,z) = \\big(p + q, \\, \\sigma(p,q) \\, w \\, z\\big).\n\\]\nDefine $i_\\sigma : \\mathbb{T} \\to P_T \\times_\\sigma \\mathbb{T}$ by $i_\\sigma(z) = (0, z)$ and $q_\\sigma : P_T \\times_\\sigma \\mathbb{T} \\to P_T$ by $q_\\sigma(p,z) = p$. Consider the bijection $M\\colon H^2(P_T,\\mathbb{T}) \\to \\operatorname{Ext}(P_T,\\mathbb{T})$ of \\cref{thm: H^2 and Ext} that maps the cohomology class of a $2$-cocycle $\\sigma \\in Z^2(P_T,\\mathbb{T})$ to the congruence class of the central extension\n\\[\n1 \\to \\mathbb{T} \\xrightarrow{i_\\sigma} P_T \\times_\\sigma \\mathbb{T} \\xrightarrow{q_\\sigma} P_T \\to 0.\n\\]\nFix $\\gamma \\in \\GG_T$. We aim to prove that $\\sigma_{s(\\gamma)}$ and $\\sigma_{r(\\gamma)}$ are cohomologous by showing that their cohomology classes have the same image under $M$. So we must find a homomorphism\n\\[\n\\varphi_\\gamma\\colon P_T \\times_{\\sigma_{s(\\gamma)}} \\mathbb{T} \\to P_T \\times_{\\sigma_{r(\\gamma)}} \\mathbb{T}\n\\]\nthat makes the diagram\n\\begin{center}\n\\vspace{-3ex}\n\\begin{equation} \\label{diagram: r and s class extensions}\n\\begin{tikzcd}\n{} & {P_T \\times_{\\sigma_{s(\\gamma)}} \\mathbb{T}} \\arrow{dd}{\\varphi_\\gamma} \\arrow{dr}{q_{\\sigma_{s(\\gamma)}}} & \\\\\n\\mathbb{T} \\arrow{ur}{i_{\\sigma_{s(\\gamma)}}} \\arrow{dr}{i_{\\sigma_{r(\\gamma)}}} && {P_T} \\\\\n& {P_T \\times_{\\sigma_{r(\\gamma)}} \\mathbb{T}} \\arrow{ur}{q_{\\sigma_{r(\\gamma)}}} & {}\n\\end{tikzcd}\n\\end{equation}\n\\end{center}\ncommute. Let $\\ensuremath{\\tau}_\\gamma^\\sigma\\colon P_T \\to \\mathbb{T}$ be the map of \\cref{lemma: twistchar_gamma^sigma}, and define $\\varphi_\\gamma \\colon P_T \\times_{\\sigma_{s(\\gamma)}} \\mathbb{T} \\to P_T \\times_{\\sigma_{r(\\gamma)}} \\mathbb{T}$ by $\\varphi_\\gamma(m,z) \\coloneqq \\left(m, \\, \\ensuremath{\\tau}_\\gamma^\\sigma(m) z\\right)$. Fix $(m,z), (n,w) \\in P_T \\times_{\\sigma_{s(\\gamma)}} \\mathbb{T}$. Recalling from \\cref{lemma: twistchar_gamma^sigma}\\cref{item: twistchar_gamma^sigma(p+q)} that\n\\[\n\\ensuremath{\\tau}_\\gamma^\\sigma(m+n) = \\sigma_{r(\\gamma)}(m,n)\\, \\overline{\\sigma_{s(\\gamma)}(m,n)}\\, \\ensuremath{\\tau}_\\gamma^\\sigma(m)\\, \\ensuremath{\\tau}_\\gamma^\\sigma(n),\n\\]\nwe obtain\n\\begin{align*}\n\\varphi_\\gamma\\big((m,z)(n,w)\\big) &= \\varphi_\\gamma\\big(m+n,\\,\\sigma_{s(\\gamma)}(m,n) \\, zw\\big) \\\\\n&= \\big(m+n, \\, \\ensuremath{\\tau}_\\gamma^\\sigma(m+n) \\, \\sigma_{s(\\gamma)}(m,n) \\, zw\\big) \\\\\n&= \\big(m+n, \\, \\sigma_{r(\\gamma)}(m,n) \\, \\ensuremath{\\tau}_\\gamma^\\sigma(m) \\, \\ensuremath{\\tau}_\\gamma^\\sigma(n) \\, zw\\big) \\\\\n&= \\big(m, \\, \\ensuremath{\\tau}_\\gamma^\\sigma(m) z\\big) \\big(n, \\, \\ensuremath{\\tau}_\\gamma^\\sigma(n) w\\big) \\\\\n&= \\varphi_\\gamma(m,z) \\, \\varphi_\\gamma(n,w),\n\\end{align*}\nand thus $\\varphi_\\gamma$ is a homomorphism. Since $\\sigma$ is normalised, the formula~\\labelcref{eqn: twistchar_gamma^sigma(p)} from \\cref{lemma: twistchar_gamma^sigma} gives $\\ensuremath{\\tau}_\\gamma^\\sigma(0) = 1$, and it follows that the diagram~\\labelcref{diagram: r and s class extensions} commutes. Therefore, $\\sigma_{r(\\gamma)}$ is cohomologous to $\\sigma_{s(\\gamma)}$, and so \\cref{thm: when cohomologous} implies that\n\\begin{equation} \\label{eqn: cocycles equivalent on generators}\n\\big(\\sigma_{r(\\gamma)}\\sigma_{r(\\gamma)}^*\\big)(g_i,g_j) = \\big(\\sigma_{s(\\gamma)}\\sigma_{s(\\gamma)}^*\\big)(g_i,g_j) \\quad \\text{for all } i, j \\in \\{1,\\dotsc,l\\},\n\\end{equation}\nas claimed.\n\nNow fix $x,y \\in X$. Since $(X,T)$ is minimal, there is a sequence $(\\gamma_n)_{n\\in\\mathbb{N}}$ in $\\GG_T$ such that $s(\\gamma_n) = x$ for all $n \\in \\mathbb{N}$, and $r(\\gamma_n) \\to y$ as $n \\to \\infty$. Fix $i,j \\in \\{1,\\dotsc,l\\}$. By \\cref{lemma: twistchar_gamma^sigma}\\cref{item: x to eval sigma_x cts}, the map $u \\mapsto \\sigma_u(g_i,g_j)$ is continuous, and hence the map $u \\mapsto \\big(\\sigma_u\\sigma_u^*\\big)(g_i,g_j)$ is continuous. So $(\\sigma_y \\sigma^*_y)(g_i, g_j) = \\lim_{n \\to \\infty} (\\sigma_{r(\\gamma_n)} \\sigma^*_{r(\\gamma_n)})(g_i, g_j)$. \\Cref{eqn: cocycles equivalent on generators} gives $(\\sigma_{r(\\gamma_n)} \\sigma^*_{r(\\gamma_n)})(g_i, g_j) = (\\sigma_x \\sigma^*_x)(g_i, g_j)$ for each $n \\in \\mathbb{N}$, and so $\\big(\\sigma_y\\sigma_y^*\\big)(g_i,g_j) = \\big(\\sigma_x\\sigma_x^*\\big)(g_i,g_j)$.\n\\end{proof}\n\n\\begin{proof}[Proof of \\cref{prop: DR cohomologous vanishing bicharacter}]\n\\Cref{lemma: [sigma_x]=[sigma_y]} shows that $\\rho_x$ is a $\\mathbb{T}$-valued $2$-cocycle on $P_T$ whose cohomology class is independent of $x$. So there exists a $2$-cocycle $\\omega \\in Z^2(P_T,\\mathbb{T})$ whose cohomology class agrees with that of each $\\rho_x$. By \\cref{thm: cohomologous vanishing bicharacter}, we may assume that $\\omega$ is a bicharacter that vanishes on $Z_\\omega$ in each coordinate, and that there is a bicharacter $\\tilde{\\omega} \\in Z^2(P_T/Z_\\omega,\\mathbb{T})$ such that\n\\[\n\\tilde{\\omega}(p+Z_\\omega,q+Z_\\omega) = \\omega(p,q) \\quad \\text{for all } p, q \\in P_T.\n\\]\n\nWe now construct $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$ such that $\\sigma$ is cohomologous to $\\rho$, and $\\sigma$ is $\\omega$-constant on $\\II_T$. For each $x \\in X$, the $2$-cocycles $\\rho_x$ and $\\omega$ are cohomologous, and so the map $\\tilde{c}_x\\colon P_T \\times P_T \\to \\mathbb{T}$ defined by\n\\[\n\\tilde{c}_x(p,q) \\coloneqq \\overline{\\omega(p,q)} \\rho_x(p,q)\n\\]\nis a $2$-coboundary on $P_T$. Since $P_T$ is a subgroup of $\\mathbb{Z}^k$ (by \\cref{prop: PT characterisation}), there is an integer $l \\in \\{1,\\dotsc,k\\}$ such that $P_T \\cong \\mathbb{Z}^l$. Fix free abelian generators $g_1, \\dotsc, g_l$ for $P_T$. For $m \\in P_T$, let $m_1, \\dotsc, m_l$ be the unique integers such that $m = \\sum_{i=1}^l m_i g_i$. For each $i \\in \\{1, \\dotsc, l\\}$, we write $\\langle g_j : j \\le i \\rangle$ for the group generated by the set $\\{ g_j : 1 \\le j \\le i \\}$. We claim that there are maps $b_x\\colon P_T \\to \\mathbb{T}$, indexed by $x \\in X$, such that $x \\mapsto b_x(m)$ is continuous for each $m \\in P_T$, and for each $i \\in \\{1,\\dotsc,l\\}$, we have\n\\begin{equation} \\label{eqn: b_x inductive}\nb_x(m) \\, \\overline{b_x(m+g_i)} = \\tilde{c}_x(g_i,m), \\quad \\text{whenever } m \\in \\langle g_j : j \\le i \\rangle.\n\\end{equation}\nTo see this, for each $x \\in X$ define $b_x(0) \\coloneqq 1 \\in \\mathbb{T}$. The map $x \\mapsto b_x(0)$ is trivially continuous. Fix $i \\in \\{1,\\dotsc,l\\}$. Suppose inductively that the maps $b_x$ have been defined on $\\langle g_j : j < i \\rangle$, and that $x \\mapsto b_x(m)$ is continuous for each $m \\in \\langle g_j : j < i \\rangle$. To extend $b_x$ to $\\langle g_j : j \\le i \\rangle$, first observe that $b_x(m)$ is already defined when $m = \\sum_{j=1}^i m_j g_j$ and $m_i = 0$. Now suppose inductively that $b_x(m)$ is defined and $x \\mapsto b_x(m)$ is continuous whenever $\\ensuremath{\\lvert} m_i \\ensuremath{\\rvert} \\le a$ for some $a \\in \\mathbb{N}$, and that $b_x$ satisfies \\cref{eqn: b_x inductive} whenever $\\ensuremath{\\lvert} m_i \\ensuremath{\\rvert}, \\, \\ensuremath{\\lvert} m_i + 1 \\ensuremath{\\rvert} \\le a$. Fix $m \\in \\langle g_j : j \\le i \\rangle$ such that $\\ensuremath{\\lvert} m_i \\ensuremath{\\rvert} = a + 1$. Define\n\\[\nb_x(m) \\coloneqq\n\\begin{cases}\nb_x(m-g_i) \\, \\overline{\\tilde{c}_x(g_i,m-g_i)} & \\text{if } m_i > 0 \\\\\nb_x(m+g_i) \\, \\tilde{c}_x(g_i,m) & \\text{if } m_i < 0.\n\\end{cases}\n\\]\nSince \\cref{lemma: twistchar_gamma^sigma}\\cref{item: x to eval sigma_x cts} implies that the maps $x \\mapsto \\tilde{c}_x(p,q)$ are continuous for all $p, q \\in P_T$, the inductive hypothesis guarantees that $x \\mapsto b_x(m)$ is continuous. Moreover, rearranging each of the cases in the definition of $b_x(m)$ shows that \\cref{eqn: b_x inductive} is satisfied. So the claim follows by induction.\n\nRecall the coboundary map $\\delta^1$ of \\cref{item: 2-coboundary}. We claim that $\\delta^1 b_x = \\tilde{c}_x$. To see this, first choose a normalised $1$-cochain $\\tilde{b}_x\\colon P_T \\to \\mathbb{T}$ such that $\\delta^1 \\tilde{b}_x = \\tilde{c}_x$. (This is possible because $\\tilde{c}_x$ is a $2$-coboundary on $P_T$.) Define $a_x\\colon P_T \\to \\mathbb{T}$ by\n\\[\na_x(m) \\coloneqq \\prod_{i=1}^l \\overline{\\tilde{b}_x(g_i)^{m_i}}.\n\\]\nA straightforward calculation shows that $a_x$ is a $1$-cocycle, and so $\\delta^1 a_x$ is trivial. Hence $\\delta^1(a_x\\tilde{b}_x) = \\delta^1 \\tilde{b}_x = \\tilde{c}_x$. Putting $m = 0$ in \\cref{eqn: b_x inductive}, we see that for each $i \\in \\{1,\\dotsc,l\\}$, $b_x(g_i) = 1$. Hence\n\\[\n(a_x \\tilde{b}_x)(0) = \\Bigg(\\prod_{i=1}^l \\overline{\\tilde{b}_x(g_i)^0}\\Bigg) \\tilde{b}_x(0) = 1 = b_x(0),\n\\]\nand for each $i \\in \\{1, \\dotsc, l\\}$,\n\\[\n(a_x \\tilde{b}_x)(g_i) = \\Bigg(\\prod_{\\substack{j=1, \\\\ j \\ne i}}^l \\overline{\\tilde{b}_x(g_j)^0}\\Bigg)\\!\\left(\\overline{\\tilde{b}_x(g_i)^1}\\right)\\tilde{b}_x(g_i) = 1 = b_x(g_i).\n\\]\nThus, for all $i \\in \\{1,\\dotsc,l\\}$ and $m \\in \\langle g_j : j \\le i \\rangle$, we have\n\\begin{align*}\n(a_x \\tilde{b}_x)(m) \\, \\overline{(a_x \\tilde{b}_x)(m+g_i)} \\,&=\\, (a_x \\tilde{b}_x)(g_i) \\, (a_x \\tilde{b}_x)(m) \\, \\overline{(a_x \\tilde{b}_x)(g_i+m)} \\\\\n&=\\, \\delta^1(a_x \\tilde{b}_x)(g_i,m) \\\\\n&=\\, \\tilde{c}_x(g_i,m) \\\\\n&=\\, b_x(m) \\, \\overline{b_x(m+g_i)}.\n\\end{align*}\nSo $b_x$ and $a_x \\tilde{b}_x$ both map $0$ and each generator $g_i$ to $1$, and they also both satisfy \\cref{eqn: b_x inductive}. Hence $a_x \\tilde{b}_x = b_x$, and thus $\\delta^1 b_x = \\delta^1(a_x \\tilde{b}_x) = \\tilde{c}_x$, as claimed.\n\nSince the maps $(x,p,x) \\mapsto x$ and $x \\mapsto b_x(p)$ are both continuous for each fixed $p \\in P_T$, the map $\\tilde{b}\\colon \\II_T \\to \\mathbb{T}$ given by $\\tilde{b}(x,p,x) \\coloneqq b_x(p)$ is a continuous $1$-cochain on $\\II_T$. We extend $\\tilde{b}$ to a map $b\\colon \\GG_T \\to \\mathbb{T}$ by setting $b(\\gamma) \\coloneqq 1$ for all $\\gamma \\in \\GG_T {\\setminus} \\II_T$. Since $\\II_T$ is a clopen subset of $\\GG_T$ (by \\cref{prop: HT quotient groupoid}), this map $b$ is a continuous $1$-cochain on $\\GG_T$. We have $b(x,0,x) = b_x(0) = 1$ for all $x \\in X$, and so $b$ is normalised. Thus the map $\\delta^1 b\\colon \\GG_T^{(2)} \\to \\mathbb{T}$ given by $\\delta^1 b(\\alpha,\\beta) \\coloneqq b(\\alpha) \\, b(\\beta) \\, b(\\alpha\\beta)^{-1}$ is a continuous $2$-coboundary on $\\GG_T$. Define $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$ by $\\sigma(\\alpha,\\beta) \\coloneqq \\rho(\\alpha,\\beta) \\, \\overline{\\delta^1 b(\\alpha,\\beta)}$. Since $\\sigma$ and $\\rho$ differ by the $2$-coboundary $\\delta^1 b$, they are cohomologous, and so \\cite[Proposition~II.1.2]{Renault1980} implies that $C^*(\\GG_T,\\rho) \\cong C^*(\\GG_T,\\sigma)$. Finally, fix $x \\in X$ and $p, q \\in P_T$. Since $\\delta^1 b_x = \\tilde{c}_x = \\rho_x \\, \\omega$, we have\n\\[\n\\sigma\\big((x,p,x),(x,q,x)\\big) = \\rho_x(p,q) \\, \\overline{\\delta^1 b_x(p,q)} = \\omega(p,q),\n\\]\nand so $\\sigma$ is $\\omega$-constant on $\\II_T$.\n\\end{proof}\n\nThe following result is an extension of \\cite[Lemma~3.6]{KPS2016JNG} to the setting of Deaconu--Renault groupoids.\n\n\\begin{prop} \\label{prop: spectral action}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Suppose that $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$, and that $\\omega \\in Z^2(P_T,\\mathbb{T})$ is a bicharacter that vanishes on $Z_\\omega$ in each coordinate such that $\\sigma$ is $\\omega$-constant on $\\II_T$, as in \\cref{prop: DR cohomologous vanishing bicharacter}. Let $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma$ be the continuous $\\widehat{P}_T$-valued $1$-cocycle on $\\GG_T$ defined in \\cref{lemma: twistchar_gamma^sigma}\\cref{item: twistchar_.^sigma 1-cocycle}. For all $\\gamma \\in \\II_T$ and $p \\in Z_\\omega$, we have $\\ensuremath{\\tau}_\\gamma^\\sigma(p) = 1$. Let $\\HH_T = \\GG_T / \\II_T$ be the quotient groupoid of \\cref{prop: HT quotient groupoid}. There is a continuous $\\widehat{Z}_\\omega$-valued $1$-cocycle $[\\gamma] \\mapsto \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma$ on $\\HH_T$ such that $\\tilde{\\ensuremath{\\tau}}^\\sigma_{[\\gamma]}(p) = \\ensuremath{\\tau}_\\gamma^\\sigma(p)$ for all $\\gamma \\in \\GG_T$ and $p \\in Z_\\omega$. There is a continuous action $\\theta$ of $\\HH_T$ on $X \\times \\widehat{Z}_\\omega$ such that\n\\[\n\\theta_{[\\gamma]}\\big(s(\\gamma),\\, \\chi\\big) = \\big(r(\\gamma),\\, \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma \\, \\chi\\big) \\ \\text{ for all $\\gamma \\in \\GG_T$ and $\\chi \\in \\widehat{Z}_\\omega$}.\n\\]\n\\end{prop}\n\nWe call the action $\\theta$ of \\cref{prop: spectral action} the \\hl{spectral action} associated to $(T,\\sigma)$. We denote the orbit of $(x,\\chi) \\in X \\times \\widehat{Z}_\\omega$ under $\\theta$ by $[x, \\chi]_\\theta$.\n\n\\begin{proof}[{Proof of \\cref{prop: spectral action}}]\nFix $\\gamma \\in \\II_T$. \\Cref{prop: IT characterisation} implies that there exist $y \\in X$ and $m \\in P_T$ such that $\\gamma = (y,m,y)$. We claim that $\\ensuremath{\\tau}_\\gamma^\\sigma(Z_\\omega) = \\{1\\}$. Fix $p \\in Z_\\omega$. Using the formula~\\labelcref{eqn: twistchar_gamma^sigma(p)} from \\cref{lemma: twistchar_gamma^sigma}, and that $\\omega$ is a bicharacter satisfying $\\sigma\\bigrestr{\\II_T^{(2)}} = 1_X \\times \\omega$, and that $\\omega\\omega^*$ is an antisymmetric bicharacter, we see that\n\\begin{align*}\n\\ensuremath{\\tau}_\\gamma^\\sigma(p) &= \\sigma\\big(\\gamma,(y,p,y)\\big) \\, \\sigma\\big(\\gamma(y,p,y),\\gamma^{-1}\\big) \\, \\overline{\\sigma\\big(\\gamma,\\gamma^{-1}\\big)} \\\\\n&= \\sigma\\big((y,m,y),(y,p,y)\\big) \\, \\sigma\\big((y,m+p,y),(y,-m,y)\\big) \\, \\overline{\\sigma\\big((y,m,y),(y,-m,y)\\big)} \\\\\n&= \\omega(m,p)\\, \\omega(m+p,-m)\\, \\overline{\\omega(m,-m)} \\\\\n&= \\omega(m,p)\\, \\overline{\\omega(p,m)} \\\\\n&= \\overline{(\\omega\\omega^*)(p,m)},\n\\end{align*}\nwhich is $1$ because $p \\in Z_\\omega$. Thus $\\ensuremath{\\tau}_\\gamma^\\sigma(Z_\\omega) = \\{1\\}$, as claimed.\n\nFor any $\\gamma \\in \\GG_T$, we have $\\ensuremath{\\tau}_\\gamma^\\sigma \\in \\widehat{P}_T$ by \\cref{lemma: twistchar_gamma^sigma}\\cref{item: twistchar_.^sigma 1-cocycle}, and so $\\ensuremath{\\tau}_\\gamma^\\sigma\\restr{Z_\\omega} \\in \\widehat{Z}_\\omega$. Suppose that $\\alpha,\\beta \\in \\GG_T$ satisfy $[\\alpha] = [\\beta]$. Then $\\eta = \\beta^{-1}\\alpha \\in \\II_T$ satisfies $\\alpha = \\beta\\eta$. For $p \\in Z_\\omega$, we have $\\ensuremath{\\tau}_\\eta^\\sigma(p) = 1$, and thus, since $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma$ is a $1$-cocycle,\n\\[\n\\ensuremath{\\tau}_\\alpha^\\sigma(p) = \\ensuremath{\\tau}_{\\beta\\eta}^\\sigma(p) = \\ensuremath{\\tau}_\\beta^\\sigma(p) \\, \\ensuremath{\\tau}_\\eta^\\sigma(p) = \\ensuremath{\\tau}_\\beta^\\sigma(p).\n\\]\nTherefore, there is a map $[\\gamma] \\mapsto \\tilde{\\ensuremath{\\tau}}^\\sigma_{[\\gamma]}$ from $\\HH_T$ to $\\widehat{Z}_\\omega$ such that $\\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma(p) = \\ensuremath{\\tau}_\\gamma^\\sigma(p)$ for all $\\gamma \\in \\GG_T$ and $p \\in Z_\\omega$. For $\\alpha, \\beta \\in \\GG_T$ and $p \\in Z_\\omega$,\n\\[\n\\tilde{\\ensuremath{\\tau}}_{[\\alpha][\\beta]}^\\sigma(p) = \\tilde{\\ensuremath{\\tau}}_{[\\alpha\\beta]}^\\sigma(p) = \\ensuremath{\\tau}_{\\alpha\\beta}^\\sigma(p) = \\ensuremath{\\tau}_\\alpha^\\sigma(p) \\, \\ensuremath{\\tau}_\\beta^\\sigma(p) = \\tilde{\\ensuremath{\\tau}}_{[\\alpha]}^\\sigma(p) \\, \\tilde{\\ensuremath{\\tau}}_{[\\beta]}^\\sigma(p),\n\\]\nand so $\\tilde{\\ensuremath{\\tau}}_{[\\alpha][\\beta]}^\\sigma = \\tilde{\\ensuremath{\\tau}}_{[\\alpha]}^\\sigma \\, \\tilde{\\ensuremath{\\tau}}_{[\\beta]}^\\sigma$. Thus $[\\gamma] \\mapsto \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma$ is a $\\widehat{Z}_\\omega$-valued $1$-cocycle on $\\HH_T$.\n\nWe claim that $[\\gamma] \\mapsto \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma$ is continuous on $\\HH_T$. Fix a finite subset $F \\subseteq Z_\\omega$ and an open subset $U \\subseteq \\mathbb{T}$, so that $S_{\\widehat{Z}_\\omega}(F,U) \\coloneqq \\{ \\chi \\in \\widehat{Z}_\\omega : \\chi(F) \\subseteq U \\}$ is a typical subbasis element for the topology on $\\widehat{Z}_\\omega$. It suffices to show that $\\big\\{ [\\gamma] \\in \\HH_T : \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma \\in S_{\\widehat{Z}_\\omega}(F,U) \\big\\}$ is open in $\\HH_T$. Since $F$ is finite,\n\\[\nS_{\\widehat{P}_T}(F,U) \\coloneqq \\{ \\chi \\in \\widehat{P}_T \\,:\\, \\chi(F) \\subseteq U \\}\n\\]\nis open in $\\widehat{P}_T$. By \\cref{lemma: twistchar_gamma^sigma}\\cref{item: twistchar_.^sigma 1-cocycle}, the map $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma$ is continuous on $\\GG_T$, and hence $\\{\\gamma \\in \\GG_T : \\ensuremath{\\tau}_\\gamma^\\sigma \\in S_{\\widehat{P}_T}(F,U)\\}$ is open in $\\GG_T$. Let $\\pi_T\\colon \\GG_T \\to \\HH_T$ denote the quotient map $\\gamma \\mapsto [\\gamma]$. Then $\\pi_T^{-1}\\big(\\big\\{ [\\gamma] : \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma \\in S_{\\widehat{Z}_\\omega}(F,U) \\big\\}\\big) = \\{ \\gamma \\in \\GG_T : \\ensuremath{\\tau}_\\gamma^\\sigma(F) \\subseteq U \\}$ is open. Thus, by the definition of the quotient topology, $\\big\\{ [\\gamma] : \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma \\in S_{\\widehat{Z}_\\omega}(F,U) \\big\\}$ is open in $\\HH_T$.\n\nIt remains to show that $\\theta$ is a continuous action of $\\HH_T$ on $X \\times \\widehat{Z}_\\omega$. For $\\alpha, \\beta \\in \\GG_T$ such that $[\\alpha] = [\\beta]$, we have $\\alpha\\beta^{-1} \\in \\II_T$, and hence $r(\\alpha) = r(\\beta)$ and $s(\\alpha) = s(\\beta)$. Define $R\\colon X \\times \\widehat{Z}_\\omega\\, \\to\\, \\HH_T^{(0)}$ by $R(x,\\chi) \\coloneqq [x]$. Then $R$ is continuous and surjective. Recall from \\cref{def: groupoid action} that the fibred product $\\HH_T \\star (X \\times \\widehat{Z}_\\omega)$ is defined by\n\\[\n\\HH_T \\star (X \\times \\widehat{Z}_\\omega) = \\big\\{ \\big([\\gamma],\\, (x,\\chi)\\big) \\,:\\, x \\in X, \\, \\chi \\in \\widehat{Z}_\\omega,\\, \\gamma \\in (\\GG_T)_x \\big\\}.\n\\]\nSince $[\\gamma] \\mapsto \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma$ is a continuous map from $\\HH_T$ to $\\widehat{Z}_\\omega$ and $r\\colon \\HH_T \\to X$ is continuous, the map $\\big([\\gamma],\\, (s(\\gamma),\\, \\chi)\\big) \\mapsto \\theta_{[\\gamma]}\\big(s(\\gamma),\\, \\chi\\big) = \\big(r(\\gamma),\\, \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma \\, \\chi\\big)$ from $\\HH_T \\star (X \\times \\widehat{Z}_\\omega)$ to $X \\times \\widehat{Z}_\\omega$ is continuous. To see that $\\theta$ is an action, we must show that conditions \\cref{item: composing groupoid actions,item: action of groupoid unit} of \\cref{def: groupoid action} are satisfied.\n\nFor \\cref{item: composing groupoid actions}, fix $(x,\\chi) \\in X \\times \\widehat{Z}_\\omega$ and $\\big([\\alpha],[\\beta]\\big) \\in \\HH_T^{(2)}$ such that $\\big([\\beta],\\, (x,\\chi)\\big) \\in \\HH_T \\star (X \\times \\widehat{Z}_\\omega)$. Then $s([\\alpha]) = r([\\beta])$, and $s([\\beta]) = R(x,\\chi) = [x]$. Hence $s([\\alpha][\\beta]) = s([\\beta]) = R(x,\\chi)$, and so $\\big([\\alpha][\\beta],\\, (x,\\chi)\\big) \\in \\HH_T \\star (X \\times \\widehat{Z}_\\omega)$. Since $s(\\beta) = x$,\n\\[\n\\theta_{[\\beta]}(x,\\chi) = \\theta_{[\\beta]}\\big(s(\\beta),\\chi\\big) = \\big(r(\\beta),\\, \\tilde{\\ensuremath{\\tau}}_{[\\beta]}^\\sigma \\, \\chi\\big).\n\\]\nThus\n\\[\nR\\big(\\theta_{[\\beta]}(x,\\chi)\\big) = R\\big(r(\\beta),\\, \\tilde{\\ensuremath{\\tau}}_{[\\beta]}^\\sigma \\, \\chi\\big) = [r(\\beta)] = r([\\beta]) = s([\\alpha]),\n\\]\nand so $\\big([\\alpha], \\, \\theta_{[\\beta]}(x,\\chi)\\big) \\in \\HH_T \\star (X \\times \\widehat{Z}_\\omega)$. Finally, since $[\\gamma] \\mapsto \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma$ is a $\\widehat{Z}_\\omega$-valued $1$-cocycle on $\\HH_T$, we have $\\tilde{\\ensuremath{\\tau}}_{[\\alpha]}^\\sigma \\, \\tilde{\\ensuremath{\\tau}}_{[\\beta]}^\\sigma = \\tilde{\\ensuremath{\\tau}}_{[\\alpha][\\beta]}^\\sigma = \\tilde{\\ensuremath{\\tau}}_{[\\alpha\\beta]}^\\sigma$, and hence\n\\begin{align*}\n\\theta_{[\\alpha]}\\big(\\theta_{[\\beta]}(x,\\chi)\\big) &= \\theta_{[\\alpha]}\\big(\\theta_{[\\beta]}(s(\\beta),\\, \\chi)\\big) = \\theta_{[\\alpha]}\\big(r(\\beta), \\, \\tilde{\\ensuremath{\\tau}}_{[\\beta]}^\\sigma \\, \\chi\\big) = \\theta_{[\\alpha]}\\big(s(\\alpha), \\, \\tilde{\\ensuremath{\\tau}}_{[\\beta]}^\\sigma \\, \\chi\\big) \\\\\n&= \\big(r(\\alpha),\\, \\tilde{\\ensuremath{\\tau}}_{[\\alpha]}^\\sigma \\, (\\tilde{\\ensuremath{\\tau}}_{[\\beta]}^\\sigma \\, \\chi)\\big) = \\big(r(\\alpha\\beta), \\, \\tilde{\\ensuremath{\\tau}}_{[\\alpha\\beta]}^\\sigma \\, \\chi\\big) = \\theta_{[\\alpha\\beta]}\\big(s(\\alpha\\beta), \\, \\chi\\big) = \\theta_{[\\alpha][\\beta]}(x,\\chi).\n\\end{align*}\nThus, \\cref{item: composing groupoid actions} is satisfied.\n\nFor \\cref{item: action of groupoid unit}, fix $(x,\\chi) \\in X \\times \\widehat{Z}_\\omega$. Then $s(R(x,\\chi)) = s([x]) = [x] = R(x,\\chi)$, and so $\\big(R(x,\\chi), (x,\\chi)\\big) \\in \\HH_T \\star (X \\times \\widehat{Z}_\\omega)$. Since $x \\in \\II_T$, we have $\\ensuremath{\\tau}_x^\\sigma(Z_\\omega) = \\{1\\}$. Thus, for all $p \\in Z_\\omega$, we have $\\tilde{\\ensuremath{\\tau}}_{[x]}^\\sigma(p) = \\ensuremath{\\tau}_x^\\sigma(p) = 1$, and so $\\tilde{\\ensuremath{\\tau}}_{[x]}^\\sigma \\, \\chi = \\chi$. Hence\n\\[\n\\theta_{R(x,\\chi)}(x,\\chi) = \\theta_{[x]}\\big(s(x), \\, \\chi\\big) = \\big(r(x), \\, \\tilde{\\ensuremath{\\tau}}_{[x]}^\\sigma \\, \\chi\\big) = (x,\\chi). \\qedhere\n\\]\n\\end{proof}\n\n\n\\section{Realising \\texorpdfstring{$C^*(\\II_T,\\sigma)$}{C*(IT,sigma)} as an induced algebra}\n\\label{sec: induced algebra}\n\nIn this section we realise the twisted C*-algebra associated to the interior $\\II_T$ of the isotropy of a Deaconu--Renault groupoid $\\GG_T$ and a continuous $2$-cocycle $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$ as an induced algebra. We then describe the ideals of this induced algebra. We begin by introducing a spanning set $\\mathcal{B}_T$ for $C_c(\\GG_T)$ and then giving a tensor-product decomposition of $C^*(\\II_T,\\sigma)$.\n\n\\begin{lemma} \\label{lemma: spanning set BT}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Let $c\\colon \\GG_T \\to \\mathbb{Z}^k$ be as in \\cref{lemma: 1-cocycle c}. Let\n\\[\n\\mathcal{B}_T \\coloneqq \\left\\{ f \\in C_c(\\GG_T) \\,:\\, \\operatorname{supp}(f) \\text{ is a bisection contained in } c^{-1}(n), \\text{ for some } n \\in \\mathbb{Z}^k \\right\\}.\n\\]\nThen $C_c(\\GG_T) = \\operatorname{span} \\mathcal{B}_T$.\n\\end{lemma}\n\n\\begin{proof}\nFix $f \\in C_c(\\GG_T)$. Since $\\operatorname{supp}(f)$ is compact, there is a finite set $\\mathcal{F}$ of precompact open bisections that cover $\\operatorname{supp}(f)$. Since each $U \\in \\mathcal{F}$ is precompact, there are only finitely many $n \\in \\mathbb{Z}^k$ such that $U \\cap c^{-1}(n) \\ne \\varnothing$. Since each $c^{-1}(n) \\cap U$ is open, it is a precompact open bisection, so we can assume that $c$ is constant on each $U \\in \\mathcal{F}$. Now, as in the proof of \\cite[Lemma~9.1.3]{Sims2020}, fix a partition of unity $\\{ g_U : U \\in \\mathcal{F} \\}$ on $\\operatorname{supp}(f)$ subordinate to $\\mathcal{F}$. By the Tietze extension theorem, each $g_U$ extends to an element $\\tilde{g}_U$ of $C_c(\\GG_T)$. Now the pointwise products $f_U \\coloneqq \\tilde{g}_U \\cdot f$ satisfy $\\operatorname{supp}(f_U) \\subseteq U$, and $\\sum_{U \\in \\mathcal{F}} f_U = f$.\n\\end{proof}\n\n\\begin{lemma} \\label{lemma: h cdot 1_p span}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. For each $h \\in C_c(X)$ and $p \\in P_T$, define $h \\cdot 1_p\\colon \\II_T \\to \\mathbb{C}$ by\n\\[\n(h \\cdot 1_p)(x,m,x) \\coloneqq \\delta_{p,m} \\, h(x).\n\\]\nThen $h \\cdot 1_p \\in C_c(\\II_T)$ for each $p \\in P_T$, and $C_c(\\II_T) = \\operatorname{span}\\{ h \\cdot 1_p : h \\in C_c(X), \\, p \\in P_T \\}$.\n\\end{lemma}\n\n\\begin{proof}\nFor each $h \\in C_c(X)$ and $p \\in P_T$, we have\n\\[\n\\operatorname{osupp}(h \\cdot 1_p) = \\big(\\!\\operatorname{osupp}(h) \\times \\{p\\} \\times \\operatorname{osupp}(h)\\big) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} \\II_T,\n\\]\nand hence $h \\cdot 1_p \\in C_c(\\II_T)$. Fix $f \\in C_c(\\II_T)$. Since $\\operatorname{supp}(f)$ is compact, there is a finite set $F \\subseteq P_T$ such that $\\operatorname{supp}(f) \\subseteq \\bigcup_{p \\in F} \\, c\\restr{\\II_T}^{-1}(p)$. For $p \\in F$, define $h_p\\colon X \\to \\mathbb{C}$ by $h_p(x) \\coloneqq f(x,p,x)$. Then $\\operatorname{osupp}(h_p) = r\\big(c\\restr{\\II_T}^{-1}(p) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} \\operatorname{osupp}(f)\\big)$, and hence $h_p \\in C_c(X)$. Moreover, $f = \\sum_{p \\in F} h_p \\cdot 1_p$.\n\\end{proof}\n\n\\begin{prop} \\label{prop: tensor prod decomp}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Suppose that $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$, and that $\\omega \\in Z^2(P_T,\\mathbb{T})$ is a bicharacter that vanishes on $Z_\\omega$ in each coordinate and satisfies $\\sigma\\bigrestr{\\II_T^{(2)}} = 1_X \\times \\omega$, as in \\cref{prop: DR cohomologous vanishing bicharacter}. Let $\\{ u_p : p \\in P_T \\}$ be the canonical family of generating unitaries for the twisted group C*-algebra $C^*(P_T,\\omega)$. There is an isomorphism $\\Upsilon\\colon C^*(\\II_T,\\sigma) \\to C_0(X) \\otimes C^*(P_T,\\omega)$ such that $\\Upsilon(h \\cdot 1_p) = h \\otimes u_p$ for all $h \\in C_c(X)$ and $p \\in P_T$.\n\\end{prop}\n\n\\begin{proof}\nThe argument used to prove \\cite[Lemma~4.1]{KPS2016JNG} works here---for more detail and an alternative approach to proving injectivity, see \\cite[Proposition~8.1.3]{Armstrong2019}.\n\\end{proof}\n\nBefore stating the next theorem, we recall the following facts relating to twisted group C*-algebras. Define $B \\coloneqq P_T / Z_\\omega$. There is a right action of $\\widehat{B}$ on $\\widehat{P}_T$ such that\n\\[\n(\\phi \\cdot \\chi)(p) = \\phi(p) \\, \\chi(p + Z_\\omega) \\quad \\text{for all } \\phi \\in \\widehat{P}_T, \\, \\chi \\in \\widehat{B}, \\text{ and } p \\in P_T.\n\\]\nThis action induces a continuous, free, proper, right action of $\\widehat{B}$ on $X \\times \\widehat{P}_T$ given by $(x,\\phi) \\cdot \\chi \\coloneqq (x, \\phi \\cdot \\chi)$. By \\cite[Theorem~4.40]{Folland2016}, the map $\\phi \\cdot \\widehat{B} \\mapsto \\phi\\restr{Z_\\omega}$ is an isomorphism $\\widehat{P}_T / \\widehat{B} \\cong \\widehat{Z}_\\omega$. Thus $\\phi \\mapsto \\phi\\restr{Z_\\omega}$ is a quotient map from $\\widehat{P}_T$ to $\\widehat{Z}_\\omega$, and so \\cite[Theorem~3.3.17]{Engelking1989} implies that $Q\\colon (x,\\phi) \\mapsto (x,\\phi\\restr{Z_\\omega})$ is a quotient map from $X \\times \\widehat{P}_T$ to $X \\times \\widehat{Z}_\\omega$.\n\nLet $\\{ U_{p+Z_\\omega} : p+Z_\\omega \\in B \\}$ be the canonical family of generating unitaries for the twisted group C*-algebra $C^*(B,\\tilde{\\omega})$. By the universal property of $C^*(B,\\tilde{\\omega})$, there is a strongly continuous action $\\beta^B$ of $\\widehat{B}$ on $C^*(B,\\tilde{\\omega})$ such that\n\\[\n\\beta_\\chi^B(U_{p+Z_\\omega}) = \\chi(p+Z_\\omega) \\, U_{p+Z_\\omega} \\quad \\text{for all } \\chi \\in \\widehat{B} \\text{ and } p \\in P_T.\n\\]\n(See \\cite[Theorem~4.3.1]{Armstrong2019} for proofs of the existence of these two actions of $\\widehat{B}$.) Recall from \\cref{def: induced algebra} the definition of the induced algebra $ \\operatorname{Ind}_{\\widehat{B}}^{X \\times \\widehat{P}_T}\\!\\big(C^*(B,\\tilde{\\omega}), \\, \\beta^B\\big)$ associated to the dynamical system $\\big(C^*(B,\\tilde{\\omega}), \\widehat{B}, \\beta^B\\big)$.\n\n\\begin{thm} \\label{thm: twisted isotropy induced algebra}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$, and let $\\omega \\in Z^2(P_T,\\mathbb{T})$ and $\\tilde{\\omega} \\in Z^2(P_T/Z_\\omega,\\mathbb{T})$ be bicharacters chosen as in \\cref{prop: DR cohomologous vanishing bicharacter}. Define\n\\begin{align*}\n\\mathcal{X}_T^\\omega \\coloneqq& \\operatorname{Ind}_{\\widehat{B}}^{X \\times \\widehat{P}_T}\\!\\big(C^*(B,\\tilde{\\omega}), \\, \\beta^B\\big) \\\\\n=&\\, \\Bigg\\{ f \\in C_0\\big(X \\times \\widehat{P}_T, \\, C^*(B,\\tilde{\\omega})\\big) \\ : \\ \\begin{tabular}{@{}p{\\textwidth-23.5em}@{}}\\setstretch{1.35}$f(x, \\phi \\cdot \\chi) = \\big(\\beta_\\chi^B\\big)^{-1}\\big(f(x,\\phi)\\big)$ for all $(x,\\phi) \\in X \\times \\widehat{P}_T$ and $\\chi \\in \\widehat{B}$\\end{tabular} \\Bigg\\}.\n\\end{align*}\nThere is an isomorphism $\\psi_T\\colon C^*(\\II_T,\\sigma) \\to \\mathcal{X}_T^\\omega$ such that\n\\[\n\\psi_T(h \\cdot 1_p)(x,\\phi) = h(x) \\, \\overline{\\phi(p)} \\, U_{p+Z_\\omega}\n\\]\nfor all $h \\in C_c(X)$, $p \\in P_T$, and $(x,\\phi) \\in X \\times \\widehat{P}_T$.\n\\end{thm}\n\n\\begin{proof}\nDefine $\\mathcal{Y}_T^\\omega\\coloneqq \\operatorname{Ind}_{\\widehat{B}}^{\\widehat{P}_T}\\!\\big(C^*(B,\\tilde{\\omega}), \\, \\beta^B\\big)$. Recall from \\cref{prop: tensor prod decomp,thm: twisted group C* induced algebra} the definitions of the isomorphisms\n\\[\n\\Upsilon\\colon C^*(\\II_T,\\sigma) \\to C_0(X) \\otimes C^*(P_T,\\omega) \\quad \\text{ and } \\quad \\Omega\\colon C^*(P_T,\\omega) \\to \\mathcal{Y}_T^\\omega.\n\\]\nBy \\cite[Propositions~B.13~and~B.16]{RW1998}, there is an isomorphism\n\\[\n\\Gamma\\colon C_0(X) \\otimes C^*(P_T,\\omega) \\to C_0\\big(X, \\, \\mathcal{Y}_T^\\omega\\big)\n\\]\nsuch that $\\Gamma(f \\otimes a)(x) = f(x) \\, \\Omega(a)$ for all $f \\in C_0(X)$, $a \\in C^*(P_T,\\omega)$, and $x \\in X$. Hence\n\\begin{equation} \\label{eqn: Gamma of Upsilon}\n\\Gamma\\big(\\Upsilon(h \\cdot 1_p)\\big)(x) = \\Gamma(h \\otimes u_p)(x) = h(x) \\, \\Omega(u_p),\n\\end{equation}\nfor all $h \\in C_0(X)$, $p \\in P_T$, and $x \\in X$. Applications of \\cite[Propositions~B.13, B.15(b), and B.16, and Corollary~B.17]{RW1998} show that there is an isomorphism\n\\[\n\\Lambda\\colon C_0\\big(X, \\, C\\big(\\widehat{P}_T, \\, C^*(B,\\tilde{\\omega})\\big)\\big) \\to C_0\\big(X \\times \\widehat{P}_T, \\, C^*(B,\\tilde{\\omega})\\big)\n\\]\ngiven by $\\Lambda(g)(x,\\phi) = g(x)(\\phi)$. (See the proof of \\cite[Proposition~8.2.2]{Armstrong2019} for details.) We claim that for each $g \\in C_0\\big(X, \\, C\\big(\\widehat{P}_T, \\, C^*(B,\\tilde{\\omega})\\big)\\big)$,\n\\begin{equation} \\label[claim]{claim: induced algebras coincide}\n\\Lambda(g) \\in \\mathcal{X}_{T,\\omega} \\quad \\text{ if and only if } \\quad g(x) \\in \\mathcal{Y}_{T,\\omega} \\text{ for all } x \\in X.\n\\end{equation}\nTo see this, fix $g \\in C_0\\big(X, \\, C\\big(\\widehat{P}_T, \\, C^*(B,\\tilde{\\omega})\\big)\\big)$. For all $x \\in X$, $\\phi \\in \\widehat{P}_T$, and $\\chi \\in \\widehat{B}$, we have\n\\[\n\\Lambda(g)(x, \\phi \\cdot \\chi) = g(x)(\\phi \\cdot \\chi) \\quad \\text{ and } \\quad \\big(\\beta_\\chi^B\\big)^{-1}\\big(\\Lambda(g)(x,\\phi)\\big) = \\big(\\beta_\\chi^B\\big)^{-1}\\big(g(x)(\\phi)\\big),\n\\]\nand hence\n\\[\n\\Lambda(g)(x, \\phi \\cdot \\chi) = \\big(\\beta_\\chi^B\\big)^{-1}\\big(\\Lambda(g)(x,\\phi)\\big) \\quad \\text{ if and only if } \\quad g(x)(\\phi \\cdot \\chi) = \\big(\\beta_\\chi^B\\big)^{-1}\\big(g(x)(\\phi)\\big).\n\\]\nIt is now clear from the definitions of $\\mathcal{X}_{T,\\omega}$ and $\\mathcal{Y}_{T,\\omega}$ that \\cref{claim: induced algebras coincide} holds. Therefore, $\\Lambda$ restricts to an isomorphism $\\widetilde{\\Lambda}\\colon C_0\\big(X, \\mathcal{Y}_{T,\\omega}\\big) \\to \\mathcal{X}_{T,\\omega}$, and so\n\\[\n\\psi_T \\coloneqq \\widetilde{\\Lambda} \\circ \\Gamma \\circ \\Upsilon \\colon C^*(\\II_T,\\sigma) \\to \\mathcal{X}_{T,\\omega}\n\\]\nis an isomorphism. Using \\cref{eqn: Gamma of Upsilon} and the definitions of $\\widetilde{\\Lambda}$ and $\\Omega$, we see that for all $h \\in C_c(X)$, $p \\in P_T$, and $(x,\\phi) \\in X \\times \\widehat{P}_T$,\n\\[\n\\psi_T(h \\cdot 1_p)(x,\\phi) = \\Gamma\\big(\\Upsilon(h \\cdot 1_p)\\big)(x)(\\phi) = h(x) \\, \\Omega(u_p)(\\phi) = h(x) \\, \\overline{\\phi(p)} \\, U_{p+Z_\\omega}. \\qedhere\n\\]\n\\end{proof}\n\nWe now give a useful description of the ideals of the induced algebra $\\mathcal{X}_T^\\omega$.\n\n\\begin{prop} \\label{prop: ideals of twisted isotropy induced algebra}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$, and let $\\omega \\in Z^2(P_T,\\mathbb{T})$ and $\\tilde{\\omega} \\in Z^2(P_T/Z_\\omega,\\mathbb{T})$ be bicharacters chosen as in \\cref{prop: DR cohomologous vanishing bicharacter}. Define $\\mathcal{X}_T^\\omega \\coloneqq \\operatorname{Ind}_{\\widehat{B}}^{X \\times \\widehat{P}_T}\\!\\big(C^*(B,\\tilde{\\omega}), \\, \\beta^B\\big)$. If $I$ is an ideal of $\\mathcal{X}_T^\\omega$, then\n\\[\nK_I \\coloneqq \\{ (x,\\phi) \\in X \\times \\widehat{P}_T : f(x,\\phi) = 0 \\text{ for all } f \\in I \\}\n\\]\nis a closed subset of $X \\times \\widehat{P}_T$, and we have\n\\[\nI = \\{ f \\in \\mathcal{X}_T^\\omega : f\\restr{K_I} \\equiv 0 \\}.\n\\]\n\\end{prop}\n\nIn order to prove \\cref{prop: ideals of twisted isotropy induced algebra}, we need the following lemma.\n\n\\begin{lemma}[{\\cite[Proposition~8.2.4]{Armstrong2019}}] \\label{lemma: simple twisted group C*-algebra}\nLet $G$ be a countable discrete abelian group with identity $e$, and let $\\varsigma \\in Z^2(G,\\mathbb{T})$ be a bicharacter. Let $\\{ u_g : g \\in G \\}$ be the canonical family of generating unitaries for the twisted group C*-algebra $C^*(G,\\varsigma)$. Suppose that for all $g \\in G$, we have $(\\varsigma\\varsigma^*)\\big(\\{g\\} \\times G\\big) = \\{1\\}$ if and only if $g = e$. Then $C^*(G,\\varsigma)$ is a simple C*-algebra with a unique trace $\\tau_e\\colon C^*(G,\\varsigma) \\to \\mathbb{C}$, which satisfies $\\tau_e(u_g) = \\delta_{g,e}$ for all $g \\in G$.\n\\end{lemma}\n\n\\begin{proof}\nA routine argument shows that there is a state $\\tau_e\\colon C^*(G,\\varsigma) \\to \\mathbb{C}$ satisfying $\\tau_e(u_g) = \\delta_{g,e}$ for all $g \\in G$. For all $g, h \\in G$, we have\n\\[\n\\tau_e(u_g u_h) = \\varsigma(g,h) \\, \\tau_e(u_{g+h}) = \\varsigma(g,h) \\, \\delta_{g+h,e} = \\varsigma(g,-g) \\, \\delta_{h,-g} = \\varsigma(-g,g) \\, \\delta_{h,-g} = \\tau_e(u_h u_g),\n\\]\nand it follows from bilinearity and continuity of multiplication that $\\tau_e$ is a trace.\n\nFor uniqueness, suppose that $\\tau\\colon C^*(G,\\varsigma) \\to \\mathbb{C}$ is a trace. Then $\\tau(u_e) = 1 = \\tau_e(u_e)$. We claim that $\\tau(u_g) = 0 = \\tau_e(u_g)$ for all $g \\in G {\\setminus} \\{e\\}$. To see this, suppose that $\\tau(u_g) \\ne 0$ for some $g \\in G$. We must show that $g = e$. Fix $h \\in G$. Since $\\tau$ is a trace, we have\n\\[\n\\varsigma(g-h,h) \\, \\tau(u_g) = \\varsigma(g-h,h) \\, \\tau(u_{(g-h)+h}) = \\tau(u_{g-h} u_h) = \\tau(u_h u_{g-h}) = \\varsigma(h,g-h) \\, \\tau(u_g).\n\\]\nSince $\\tau(u_g) \\ne 0$ by assumption, it follows that $\\varsigma(g-h,h) = \\varsigma(h,g-h)$. Thus, since $\\varsigma$ is a bicharacter, we have\n\\[\n\\varsigma(g,h) \\, \\varsigma(-h,h) = \\varsigma(g-h,h) = \\varsigma(h,g-h) = \\varsigma(h,-h) \\, \\varsigma(h,g).\n\\]\nSince $\\varsigma(-h,h) = \\overline{\\varsigma(h,h)} = \\varsigma(h,-h)$, we obtain $\\varsigma(g,h) = \\varsigma(h,g)$. Hence $(\\varsigma\\varsigma^*)(g,h) = \\varsigma(g,h) \\, \\overline{\\varsigma(h,g)} = 1$, and so $(\\varsigma\\varsigma^*)\\big(\\{g\\} \\times G\\big) = \\{1\\}$. Therefore, $g = e$ by our hypothesis, and it follows that $\\tau = \\tau_e$.\n\nNow, since $G$ is abelian and $C^*(G,\\varsigma)$ has a unique trace, \\cite[Corollary~2.3]{BO2016} implies that $C^*(G,\\varsigma)$ is simple.\n\\end{proof}\n\n\\begin{proof}[Proof of \\cref{prop: ideals of twisted isotropy induced algebra}]\nWe have $K_I = \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}}_{f \\in I} \\, f^{-1}(0)$, which is closed because each $f \\in I$ is continuous. It is well known that if $C^*(B,\\tilde{\\omega})$ is simple, then $I = \\{ f \\in \\mathcal{X}_T^\\omega : f\\restr{K_I} \\equiv 0 \\}$ (see \\cite[Proposition~4.2.1]{Armstrong2019} for a proof). We will use \\cref{lemma: simple twisted group C*-algebra} to show that $C^*(B,\\tilde{\\omega})$ is simple. Fix $p \\in P_T$. \\Cref{prop: DR cohomologous vanishing bicharacter} implies that for all $q \\in P_T$, we have\n\\begin{equation} \\label{eqn: omega omega star}\n(\\tilde{\\omega}\\tilde{\\omega}^*)(p + Z_\\omega, q + Z_\\omega) = \\omega(p,q) \\, \\overline{\\omega(q,p)} = (\\omega\\omega^*)(p,q).\n\\end{equation}\nBy the definition of $Z_\\omega$, we have $p \\in Z_\\omega$ if and only if $(\\omega\\omega^*)(p,q) = 1$ for all $q \\in P_T$. Thus, \\cref{eqn: omega omega star} implies that $p + Z_\\omega$ is the identity element of $B$ if and only if $(\\tilde{\\omega}\\tilde{\\omega}^*)\\big(\\{p + Z_\\omega\\} \\times B\\big) = \\{1\\}$, and so \\cref{lemma: simple twisted group C*-algebra} implies that $C^*(B,\\tilde{\\omega})$ is simple.\n\\end{proof}\n\n\n\\section{Simplicity of twisted \\texorpdfstring{C*-algebras}{C*-algebras} of \\texorpdfstring{Deaconu--Renault}{Deaconu-Renault} groupoids}\n\\label{sec: simplicity}\n\nIn this section we characterise simplicity of twisted C*-algebras of Deaconu--Renault groupoids in terms of the underlying data, using the spectral action defined in \\cref{prop: spectral action}.\n\n\\begin{thm} \\label{thm: simplicity characterisation}\nLet $(X,T)$ be a rank-$k$ Deaconu--Renault system such that $X$ is second-countable. Fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$.\n\\begin{enumerate}[label=(\\alph*)]\n\\item \\label{item: main simple implies minimal} If $(X,T)$ is not minimal, then $C^*(\\GG_T,\\sigma)$ is not simple.\n\\item \\label{item: main minimal implies simple} Suppose that $(X,T)$ is minimal. Let $\\omega \\in Z^2(P_T,\\mathbb{T})$ and $\\tilde{\\omega} \\in Z^2(P_T/Z_\\omega,\\mathbb{T})$ be bicharacters chosen as in \\cref{prop: DR cohomologous vanishing bicharacter}. Let $\\theta$ be the spectral action associated to $(T,\\sigma)$ as in \\cref{prop: spectral action}. Then $C^*(\\GG_T,\\sigma)$ is simple if and only if $\\theta$ is minimal.\n\\end{enumerate}\n\\end{thm}\n\n\\begin{proof}[Proof of \\cref{thm: simplicity characterisation}\\cref{item: main simple implies minimal}]\nThis follows from \\cite[Corollary~4.9]{Renault1991} applied to the groupoid dynamical system $\\big(\\GG_T, \\GG_T \\times_\\sigma \\mathbb{T}, C_0(\\GG_T^{(0)})\\big)$, but it is easy to provide a short direct proof. Since $(X,T)$ is not minimal, there exists $x \\in X$ such that $\\overline{\\Orb{x}}$ is a proper closed invariant set. Let $\\mathcal{H} \\coloneqq \\GG_T\\restr{\\overline{\\Orb{x}}} = \\{ \\gamma \\in \\GG_T : s(\\gamma) \\in \\overline{\\Orb{x}} \\}$, and let $\\tau$ be the restriction of $\\sigma$ to $\\mathcal{H}^{(2)}$. Then the restriction map $f \\mapsto f\\restr{\\mathcal{H}}$ is a $*$-homomorphism from $C_c(\\GG_T,\\sigma)$ to $C^*(\\mathcal{H},\\tau)$, and so it extends to a homomorphism $R\\colon C^*(\\GG_T,\\sigma) \\to C^*(\\mathcal{H},\\tau)$. Since $\\ker(R) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} C_0(\\GG_T^{(0)}) = C_0(X {\\setminus} \\overline{\\Orb{x}})$ is neither $\\{0\\}$ nor all of $C_0(\\GG_T^{(0)})$, we see that $\\ker(R)$ is a nonzero proper ideal of $C^*(\\GG_T,\\sigma)$.\n\\end{proof}\n\nTo prove part~\\cref{item: main minimal implies simple} of \\cref{thm: simplicity characterisation}, we need several preliminary results. Let $\\omega \\in Z^2(P_T,\\mathbb{T})$ and $\\tilde{\\omega} \\in Z^2(P_T/Z_\\omega,\\mathbb{T})$ be bicharacters chosen as in \\cref{prop: DR cohomologous vanishing bicharacter}. Define $B \\coloneqq P_T/Z_\\omega$, and recall from \\cref{thm: twisted isotropy induced algebra} the definition of the isomorphism\n\\[\n\\psi_T\\colon C^*(\\II_T,\\sigma) \\to \\mathcal{X}_T^\\omega = \\operatorname{Ind}_{\\widehat{B}}^{X \\times \\widehat{P}_T}\\!\\big(C^*(B,\\tilde{\\omega}), \\, \\beta^B\\big).\n\\]\nLet $\\iota\\colon C^*(\\II_T,\\sigma) \\to C^*(\\GG_T,\\sigma)$ be the homomorphism of \\cite[Proposition~6.1]{Armstrong2021}, so\n\\[\n\\iota(f)(\\gamma) = \\begin{cases}\nf(\\gamma) & \\ \\text{if } \\gamma \\in \\II_T \\\\\n0 & \\ \\text{if } \\gamma \\notin \\II_T\n\\end{cases} \\quad \\text{ for all } f \\in C_c(\\II_T,\\sigma) \\text{ and } \\gamma \\in \\GG_T.\n\\]\nSince $\\II_T$ is amenable (by \\cite[Lemma~3.5]{SW2016} and \\cite[Proposition~5.1.1]{ADR2000}), $\\iota$ is injective by \\cite[Proposition~6.1]{Armstrong2021}. Define $M_T^\\sigma \\coloneqq \\iota\\big(C^*(\\II_T,\\sigma)\\big) \\subseteq C^*(\\GG_T,\\sigma)$.\n\nWe begin by showing that there is a bounded linear map on $M_T^\\sigma$ given by conjugation in $C^*(\\GG_T,\\sigma)$ by a fixed element of $C_c(\\GG_T,[0,1])$ that is supported on a bisection.\n\n\\begin{lemma} \\label{lemma: conjugation map}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable, and fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. Let $U$ be an open bisection of $\\GG_T$. Suppose that $g \\in C_c(\\GG_T,[0,1])$ satisfies $\\operatorname{supp}(g) \\subseteq U$. For all $f \\in C_c(\\II_T,\\sigma)$, we have $g^*\\iota(f)g \\in \\iota\\big(C_c(\\II_T,\\sigma)\\big)$. There is a linear contraction $\\Xi_g \\colon M_T^\\sigma \\to M_T^\\sigma$ given by $\\Xi_g(a) \\coloneqq g^*ag$.\n\\end{lemma}\n\n\\begin{proof}\nFix $f \\in C_c(\\II_T,\\sigma)$. Since $U$ is a bisection containing $\\operatorname{supp}(g)$, we have\n\\[\n\\operatorname{supp}\\!\\big(g^*\\iota(f)g\\big) \\subseteq U^{-1} \\, \\II_T \\, U \\subseteq \\II_T,\n\\]\nand hence $g^*\\iota(f)g \\in \\iota\\big(C_c(\\II_T,\\sigma)\\big)$. Since $g$ has range in $[0,1]$ and is supported on a bisection, $\\ensuremath{\\lVert} g \\ensuremath{\\rVert} = \\ensuremath{\\lVert} g \\ensuremath{\\rVert}_\\infty \\le 1$, and thus\n\\[\n\\ensuremath{\\lVert} g^* \\iota(f) g \\ensuremath{\\rVert} \\le \\ensuremath{\\lVert} g^* \\ensuremath{\\rVert} \\, \\ensuremath{\\lVert} \\iota(f) \\ensuremath{\\rVert} \\, \\ensuremath{\\lVert} g \\ensuremath{\\rVert} \\le \\ensuremath{\\lVert} \\iota(f) \\ensuremath{\\rVert}.\n\\]\nTherefore, $\\iota(f) \\mapsto g^* \\iota(f) g$ extends to a linear contraction $\\Xi_g\\colon M_T^\\sigma \\to M_T^\\sigma$.\n\\end{proof}\n\nIn the next lemma we introduce a bounded linear map $\\Theta_{U,g}$ on the induced algebra $\\mathcal{X}_{T,\\omega}$ that is reminiscent of the spectral action $\\theta$ associated to the pair $(T,\\sigma)$. This map $\\Theta_{U,g}$ is defined in terms of a fixed element $g$ of $C_c(\\GG_T,[0,1])$ that is supported on an open bisection $U$ of $\\GG_T$, and as we show in \\cref{prop: properties of Theta}\\cref{item: Theta is conjugation}, it simply amounts to conjugation of elements of $M_T^\\sigma \\cong \\mathcal{X}_{T,\\omega}$ by $g$.\n\n\\begin{lemma} \\label{lemma: Theta is a BLM}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable, and fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. Let $U$ be an open bisection of $\\GG_T$. Suppose that $g \\in C_c(\\GG_T,\\sigma)$ satisfies $\\operatorname{supp}(g) \\subseteq U$ and that $g(U) \\subseteq [0,1]$. For each $x \\in s(U)$, let $\\alpha_{U,x}$ denote the unique element of $U$ with source $x$. Let $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma$ be the continuous $\\widehat{P}_T$-valued $1$-cocycle of \\cref{lemma: twistchar_gamma^sigma}\\cref{item: twistchar_.^sigma 1-cocycle}. For $f \\in \\mathcal{X}_T^\\omega$, define $\\Theta_{U,g}(f)\\colon X \\times \\widehat{P}_T \\to C^*(B,\\tilde{\\omega})$ by\n\\[\n\\Theta_{U,g}(f)(x,\\phi) \\coloneqq\n\\begin{cases}\n\\ensuremath{\\lvert} g(\\alpha_{U,x}) \\ensuremath{\\rvert}^2 \\, f\\big(r(\\alpha_{U,x}),\\,\\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma\\,\\phi\\big) &\\quad \\text{if } x \\in s(U) \\\\\n0 &\\quad \\text{if } x \\notin s(U).\n\\end{cases}\n\\]\nThen $\\Theta_{U,g}(f) \\in \\mathcal{X}_T^\\omega$, and $\\Theta_{U,g}\\colon \\mathcal{X}_T^\\omega \\to \\mathcal{X}_T^\\omega$ is a bounded linear map.\n\\end{lemma}\n\n\\begin{proof}\nFix $f \\in \\mathcal{X}_T^\\omega$. Then\n\\begin{equation} \\label{eqn: induced algebra equation}\nf(x, \\phi \\cdot \\chi) = \\big(\\beta_\\chi^B\\big)^{-1}\\big(f(x,\\phi)\\big) \\ \\text{ for all } (x,\\phi) \\in X \\times \\widehat{P}_T \\text{ and } \\chi \\in \\widehat{B}.\n\\end{equation}\nWe first show that $\\Theta_{U,g}(f) \\in C_0\\big(X \\times \\widehat{P}_T, \\, C^*(B,\\tilde{\\omega})\\big)$. The map $\\Theta_{U,g}(f)$ is continuous because $x \\mapsto \\alpha_{U, x}$ is continuous. We have $\\operatorname{supp}(\\Theta_{U,g}(f)) \\subseteq s(\\operatorname{supp}(g)) \\times \\widehat{P}_T$, and so $\\Theta_{U,g}(f)$ has compact support. Hence $\\Theta_{U,g}(f) \\in C_0\\big(X \\times \\widehat{P}_T, \\, C^*(B,\\tilde{\\omega})\\big)$. We must show that $\\Theta_{U,g}(f)$ satisfies \\cref{eqn: induced algebra equation}. Fix $(x,\\phi) \\in X \\times \\widehat{P}_T$ and $\\chi \\in \\widehat{B}$. If $x \\notin s(U)$, then\n\\[\n\\Theta_{U,g}(f)(x, \\phi \\cdot \\chi) = 0 = (\\beta_\\chi^B)^{-1}(0) = \\big(\\beta_\\chi^B\\big)^{-1}\\big(\\Theta_{U,g}(f)(x,\\phi)\\big).\n\\]\nSuppose that $x \\in s(U)$. Since $f \\in \\mathcal{X}_T^\\omega$, \\cref{eqn: induced algebra equation} implies that\n\\begin{align*}\n\\Theta_{U,g}(f)(x, \\phi \\cdot \\chi) &= \\ensuremath{\\lvert} g(\\alpha_{U,x}) \\ensuremath{\\rvert}^2 \\, f\\big(r(\\alpha_{U,x}),\\,(\\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma\\,\\phi) \\cdot \\chi\\big) \\\\\n&= \\ensuremath{\\lvert} g(\\alpha_{U,x}) \\ensuremath{\\rvert}^2 \\, \\big(\\beta_\\chi^B\\big)^{-1}\\big(f\\big(r(\\alpha_{U,x}),\\,\\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma\\,\\phi\\big)\\big)\n= \\big(\\beta_\\chi^B\\big)^{-1}\\big(\\Theta_{U,g}(f)(x,\\phi)\\big).\n\\end{align*}\nTherefore, $\\Theta_{U,g}(f) \\in \\mathcal{X}_T^\\omega$. Since the range of $g$ is contained in $[0,1]$, routine calculations show that $\\Theta_{U,g}\\colon \\mathcal{X}_T^\\omega \\to \\mathcal{X}_T^\\omega$ is a bounded linear map.\n\\end{proof}\n\nIn the next lemma we show that the set of functions of the form $\\iota(h \\cdot 1_p)$ (as defined in \\cref{lemma: h cdot 1_p span}) is invariant under conjugation in $C^*(\\GG_T,\\sigma)$ by a fixed element of $C_c(\\GG_T,[0,1])$ that is supported on a bisection.\n\n\\begin{lemma} \\label{lemma: conjugating h.1_p}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable, and fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. Let $U$ be an open bisection of $\\GG_T$. Suppose that $g \\in C_c(\\GG_T,\\sigma)$ satisfies $\\operatorname{supp}(g) \\subseteq U$ and that $g(U) \\subseteq [0,1]$. For each $x \\in s(U)$, let $\\alpha_{U,x}$ denote the unique element of $U$ with source $x$. Let $\\Xi_g\\colon M_T^\\sigma \\to M_T^\\sigma$ and $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma$ be as in \\cref{lemma: conjugation map,lemma: twistchar_gamma^sigma}\\cref{item: twistchar_.^sigma 1-cocycle}. For each $x \\in s(U)$, let $\\alpha_{U,x}$ denote the unique element of $U$ with source $x$. Fix $h \\in C_c(X)$ and $p \\in P_T$, and define $H_{g,p}\\colon X \\to \\mathbb{C}$ by\n\\[\nH_{g,p}(x) \\coloneqq\n\\begin{cases}\n\\ensuremath{\\lvert} g(\\alpha_{U,x}) \\ensuremath{\\rvert}^2 \\, \\overline{\\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma(p)} \\, h(r(\\alpha_{U,x})) &\\quad \\text{if } x \\in s(U) \\\\\n0 &\\quad \\text{if } x \\notin s(U).\n\\end{cases}\n\\]\nThen $H_{g,p} \\in C_c(X)$, and we have $\\,\\Xi_g(\\iota(h \\cdot 1_p)) = \\iota(H_{g,p} \\cdot 1_p)$.\n\\end{lemma}\n\n\\begin{proof}\nSince $x \\mapsto \\alpha_{U,x}$ is continuous on $s(U)$, and since $\\gamma \\mapsto \\ensuremath{\\tau}_\\gamma^\\sigma(p)$ is continuous by \\cref{lemma: twistchar_gamma^sigma}\\cref{item: twistchar_.^sigma cts}, the map $H_{g,p}$ is continuous. Since $\\operatorname{supp}(H_{g,p}) \\subseteq s(\\operatorname{supp}(g))$, we have $H_{g,p} \\in C_c(X)$.\n\nBy \\cref{lemma: conjugation map}, we have $\\Xi_g(\\iota(h \\cdot 1_p)) \\in \\iota\\big(C_c(\\II_T,\\sigma)\\big)$. Thus, for all $\\gamma \\in \\GG_T {\\setminus} \\II_T$, we have\n\\[\n\\Xi_g(\\iota(h \\cdot 1_p))(\\gamma) = 0 = \\iota(H_{g,p} \\cdot 1_p)(\\gamma).\n\\]\nSuppose that $\\gamma \\in \\II_T$. Then by \\cref{prop: IT characterisation}, there exist $x \\in X$ and $m \\in P_T$ such that $\\gamma = (x,m,x)$. We have\n\\[\n\\operatorname{supp}\\!\\big(\\Xi_g(\\iota(h \\cdot 1_p))\\big) \\subseteq \\operatorname{supp}(g^*) \\, \\operatorname{supp}(\\iota(h \\cdot 1_p)) \\, \\operatorname{supp}(g) \\subseteq U^{-1} \\, \\II_T \\, U.\n\\]\nThus, if $x \\notin s(U)$, then $\\gamma \\notin \\operatorname{supp}\\!\\big(\\Xi_g(\\iota(h \\cdot 1_p))\\big)$ and $H_{g,p}(x) = 0$, and hence\n\\[\n\\Xi_g(\\iota(h \\cdot 1_p))(x,m,x) = 0 = \\iota(H_{g,p} \\cdot 1_p)(x,m,x).\n\\]\nSuppose that $x \\in s(U)$. Since $g$ is supported on the bisection $U$,\n\\begin{align*}\n\\Xi_g(\\iota(h \\cdot 1_p))(x,m,x) \\,&=\\, \\ensuremath{\\tau}_{(\\alpha_{U,x})^{-1}}^\\sigma(m) \\ g^*(\\alpha_{U,x}) \\ \\iota(h \\cdot 1_p)\\big(r(\\alpha_{U,x}),m,r(\\alpha_{U,x})\\big) \\ g(\\alpha_{U,x}) \\\\\n&=\\, \\ensuremath{\\lvert} g(\\alpha_{U,x}) \\ensuremath{\\rvert}^2 \\, \\overline{\\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma(m)} \\, \\delta_{p,m} \\, h(r(\\alpha_{U,x})) \\\\\n&=\\, \\delta_{p,m} \\, H_{g,p}(x) \\\\\n&=\\, \\iota(H_{g,p} \\cdot 1_p)(x,m,x).\n\\end{align*}\nTherefore, $\\,\\Xi_g(\\iota(h \\cdot 1_p)) = \\iota(H_{g,p} \\cdot 1_p)$.\n\\end{proof}\n\nIn the following proposition we describe exactly how the map $\\Theta_{U,g}$ defined in \\cref{lemma: Theta is a BLM}. relates to the conjugation map $\\Xi_g$ defined in \\cref{lemma: conjugation map}. We also show that ideals of $\\mathcal{X}_{T,\\omega}$ induced by ideals of $C^*(\\GG_T,\\sigma)$ are invariant under $\\Theta_{U,g}$, which is a key result used in the proof of \\cref{thm: simplicity characterisation}\\cref{item: main minimal implies simple}.\n\n\\begin{prop} \\label{prop: properties of Theta}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable, and fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. Let $U$ be an open bisection of $\\GG_T$. Suppose that $g \\in C_c(\\GG_T,[0,1])$ satisfies $\\operatorname{supp}(g) \\subseteq U$. Recall the definitions of the bounded linear maps $\\Xi_g\\colon M_T^\\sigma \\to M_T^\\sigma$ from \\cref{lemma: conjugation map} and $\\Theta_{U,g}\\colon \\mathcal{X}_T^\\omega \\to \\mathcal{X}_T^\\omega$ from \\cref{lemma: Theta is a BLM}.\n\\begin{enumerate}[label=(\\alph*)]\n\\item \\label{item: Theta is conjugation} For all $a \\in C^*(\\II_T,\\sigma)$, we have\n\\[\n\\Theta_{U,g}(\\psi_T(a)) = \\psi_T\\big(\\iota^{-1}\\big(\\Xi_g(\\iota(a))\\big)\\big).\n\\]\n\\item \\label{item: induced algebra ideals are invariant under Theta} Suppose that $I$ is an ideal of $C^*(\\GG_T,\\sigma)$, and that $J$ is an ideal of $C^*(\\II_T,\\sigma)$ such that $\\iota(J) = I \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} M_T^\\sigma$. Then the ideal $\\psi_T(J)$ is invariant under $\\Theta_{U,g}$.\n\\end{enumerate}\n\\end{prop}\n\n\\begin{proof}\nFor part~\\cref{item: Theta is conjugation}, fix $h \\in C_c(X)$ and $p \\in P_T$. Since all the maps involved are bounded and linear, \\cref{lemma: h cdot 1_p span} implies that it suffices to show that\n\\[\n\\Theta_{U,g}(\\psi_T(h \\cdot 1_p)) = \\psi_T\\big(\\iota^{-1}\\big(\\Xi_g(\\iota(h \\cdot 1_p))\\big)\\big).\n\\]\nRecall from \\cref{lemma: conjugating h.1_p} that there is a function $H_{g,p} \\in C_c(X)$ given by\n\\[\nH_{g,p}(x) =\n\\begin{cases}\n\\ensuremath{\\lvert} g(\\alpha_{U,x}) \\ensuremath{\\rvert}^2 \\, \\overline{\\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma(p)} \\, h(r(\\alpha_{U,x})) &\\quad \\text{if } x \\in s(U) \\\\\n0 &\\quad \\text{if } x \\notin s(U),\n\\end{cases}\n\\]\nwhich satisfies $\\,\\Xi_g(\\iota(h \\cdot 1_p)) = \\iota(H_{g,p} \\cdot 1_p)$. Thus, for all $(x,\\phi) \\in X \\times P_T$, we have\n\\begin{align*}\n\\Theta_{U,g}(\\psi_T(h \\cdot 1_p))(x,\\phi) \\,&=\\, \\begin{cases}\n\\ensuremath{\\lvert} g(\\alpha_{U,x}) \\ensuremath{\\rvert}^2 \\, \\psi_T(h \\cdot 1_p)\\big(r(\\alpha_{U,x}),\\,\\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma\\,\\phi\\big) &\\quad \\text{if } x \\in s(U) \\\\\n0 &\\quad \\text{if } x \\notin s(U)\n\\end{cases} \\\\\n&= \\, \\begin{cases}\n\\ensuremath{\\lvert} g(\\alpha_{U,x}) \\ensuremath{\\rvert}^2 \\, h(r(\\alpha_{U,x})) \\, \\overline{\\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma(p)} \\, \\overline{\\phi(p)} \\, U_{p+Z_\\omega} &\\quad \\text{if } x \\in s(U) \\\\\n0 &\\quad \\text{if } x \\notin s(U)\n\\end{cases} \\\\\n&=\\, H_{g,p}(x) \\, \\overline{\\phi(p)} \\, U_{p+Z_\\omega} \\\\\n&=\\, \\psi_T(H_{g,p} \\cdot 1_p)(x,\\phi) \\\\\n&=\\, \\psi_T\\big(\\iota^{-1}\\big(\\Xi_g(\\iota(h \\cdot 1_p))\\big)\\big)(x,\\phi).\n\\end{align*}\n\nFor part~\\cref{item: induced algebra ideals are invariant under Theta}, fix $a \\in J$. Then $\\iota(a) \\in I \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} M_T^\\sigma$. Since $I$ is an ideal of $C^*(\\GG_T,\\sigma)$ and the range of $\\Xi_g$ is contained in $M_T^\\sigma$, we have $\\Xi_g(\\iota(a)) = g^* \\iota(a) g \\in I \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} M_T^\\sigma = \\iota(J)$, and so $\\iota^{-1}\\big(\\Xi_g(\\iota(a))\\big) \\in J$. Hence part~\\cref{item: Theta is conjugation} implies that\n\\[\n\\Theta_{U,g}(\\psi_T(a)) = \\psi_T\\big(\\iota^{-1}\\big(\\Xi_g(\\iota(a))\\big)\\big) \\in \\psi_T(J),\n\\]\nand thus $\\Theta_{U,g}(\\psi_T(J)) \\subseteq \\psi_T(J)$.\n\\end{proof}\n\nWe now use \\cref{prop: properties of Theta}\\cref{item: induced algebra ideals are invariant under Theta} to show that the closed subsets of $X \\times \\widehat{P}_T$ characterising the ideals of the induced algebra $\\mathcal{X}_{T,\\omega}$ are invariant under the spectral action $\\theta$ associated to the pair $(T,\\sigma)$.\n\n\\begin{prop} \\label{prop: Q(K_J) is closed and invariant under theta}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable, and fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. Suppose that $I$ is an ideal of $C^*(\\GG_T,\\sigma)$, and that $J$ is an ideal of $C^*(\\II_T,\\sigma)$ such that $\\iota(J) = I \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} M_T^\\sigma$. Define\n\\[\nK_J \\coloneqq \\{ (x,\\phi) \\in X \\times \\widehat{P}_T \\,:\\, f(x,\\phi) = 0 \\text{ for all } f \\in \\psi_T(J) \\}.\n\\]\nLet $Q\\colon X \\times \\widehat{P}_T \\to X \\times \\widehat{Z}_\\omega$ be the quotient map $(x,\\phi) \\mapsto (x,\\phi\\restr{Z_\\omega})$. Then $Q^{-1}(Q(K_J)) = K_J$, and $Q(K_J)$ is closed and invariant under the spectral action $\\theta$ of \\cref{prop: spectral action}.\n\\end{prop}\n\n\\begin{proof}\nWe trivially have $K_J \\subseteq Q^{-1}\\!\\left(Q(K_J)\\right)$. We must show that $Q^{-1}\\!\\left(Q(K_J)\\right) \\subseteq K_J$. Fix $(x,\\phi) \\in Q^{-1}\\!\\left(Q(K_J)\\right)$. Then $(x,\\phi\\restr{Z_\\omega}) = Q(x,\\phi) \\in Q(K_J)$, and so there exists $(y,\\rho) \\in K_J$ such that $(x,\\phi\\restr{Z_\\omega}) = Q(y,\\rho) = (y,\\rho\\restr{Z_\\omega})$. We have $x = y$ and $\\phi\\restr{Z_\\omega} = \\rho\\restr{Z_\\omega}$, and hence \\cite[Theorem~4.40]{Folland2016} implies that $\\phi \\cdot \\widehat{B} = \\rho \\cdot \\widehat{B}$. So there exists $\\chi \\in \\widehat{B}$ such that $\\phi \\cdot \\chi = \\rho \\cdot 1_{\\widehat{B}} = \\rho$. Since $(x, \\phi \\cdot \\chi) = (y,\\rho) \\in K_J$, we have $f(x, \\phi \\cdot \\chi) = 0$ for all $f \\in \\psi_T(J)$. Thus, since $\\psi_T(J) \\subseteq \\mathcal{X}_T^\\omega$, we have $f(x,\\phi) = \\beta_\\chi^B\\!\\left(f(x, \\phi \\cdot \\chi)\\right) = 0$ for all $f \\in \\psi_T(J)$. Hence $(x,\\phi) \\in K_J$, and so $Q^{-1}\\!\\left(Q(K_J)\\right) = K_J$. Since $Q$ is a quotient map, \\cite[Proposition~2.4.3]{Engelking1989} implies that $C \\subseteq X \\times \\widehat{Z}_\\omega$ is closed if and only if $Q^{-1}(C) \\subseteq X \\times \\widehat{P}_T$ is closed. Since $Q^{-1}(Q(K_J)) = K_J$ is closed in $X \\times \\widehat{P}_T$ (by \\cref{prop: ideals of twisted isotropy induced algebra}), we deduce that $Q(K_J)$ is closed.\n\nWe now show that $Q(K_J)$ is invariant under $\\theta$. Fix $(x,\\zeta) \\in Q(K_J)$ and $\\gamma \\in (\\GG_T)_x$. Then there exists $\\phi \\in \\widehat{P}_T$ such that $(x,\\phi) \\in K_J$ and $\\phi\\restr{Z_\\omega} = \\zeta$. We must show that $\\theta_{[\\gamma]}(x,\\zeta) \\in Q(K_J)$. \\Cref{prop: spectral action} implies that $\\ensuremath{\\tau}_\\gamma^\\sigma\\restr{Z_\\omega} = \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma$, and so\n\\[\n\\theta_{[\\gamma]}(x,\\zeta) = \\big(r(\\gamma),\\, \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma \\, \\zeta \\big) = Q\\!\\left(r(\\gamma),\\, \\ensuremath{\\tau}_\\gamma^\\sigma \\, \\phi\\right)\\!.\n\\]\nHence it suffices to show that $\\left(r(\\gamma),\\, \\ensuremath{\\tau}_\\gamma^\\sigma \\, \\phi\\right) \\in K_J$. Fix $f \\in \\psi_T(J)$. We must show that $f\\!\\left(r(\\gamma),\\, \\ensuremath{\\tau}_\\gamma^\\sigma \\, \\phi\\right) = 0$. Let $U \\subseteq \\GG_T$ be an open bisection containing $\\gamma$. By Urysohn's lemma there exists $g \\in C_c(\\GG_T,[0,1])$ such that $\\operatorname{supp}(g) \\subseteq U$ and $g(\\gamma) = 1$. Let $\\Theta_{U,g}\\colon \\mathcal{X}_T^\\omega \\to \\mathcal{X}_T^\\omega$ be as in \\cref{lemma: Theta is a BLM}. Since $s\\restr{U}^{-1}(x) = \\gamma$ and $g(\\gamma) = 1$,\n\\begin{equation} \\label{eqn: theta is Theta}\n\\Theta_{U,g}(f)(x,\\phi) = f\\!\\left(r(\\gamma), \\, \\ensuremath{\\tau}_\\gamma^\\sigma \\, \\phi\\right)\\!.\n\\end{equation}\nSince $f \\in \\psi_T(J)$, \\cref{prop: properties of Theta}\\cref{item: induced algebra ideals are invariant under Theta} implies that $\\Theta_{U,g}(f) \\in \\psi_T(J)$. Since $\\psi_T(J)$ is an ideal of $\\mathcal{X}_T^\\omega$, \\cref{prop: ideals of twisted isotropy induced algebra} implies that\n\\[\n\\psi_T(J) = \\{ f \\in \\mathcal{X}_T^\\omega : f\\restr{K_J} \\equiv 0 \\}.\n\\]\nThus, since $(x,\\phi) \\in K_J$ and $\\Theta_{U,g}(f) \\in \\psi_T(J)$, we have\n\\begin{equation} \\label{eqn: Theta x phi zero}\n\\Theta_{U,g}(f)(x,\\phi) = 0.\n\\end{equation}\nTogether, \\cref{eqn: theta is Theta,eqn: Theta x phi zero} imply that $f\\!\\left(r(\\gamma), \\, \\ensuremath{\\tau}_\\gamma^\\sigma \\, \\phi\\right) = 0$, as required.\n\\end{proof}\n\nWe now prove several technical results that we use in the proof of \\cref{thm: simplicity characterisation}\\cref{item: main minimal implies simple} to show that when the spectral action $\\theta$ is not minimal, the twisted groupoid C*-algebra $C^*(\\GG_T,\\sigma)$ is not simple. We first show that, given an element $(x,\\phi) \\in X \\times \\widehat{P}_T$ with non-dense orbit under $\\theta$, there is a nonzero element of $\\mathcal{X}_{T,\\omega} \\cong M_T^\\sigma$ that is supported off the orbit of $(x,\\phi)$.\n\n\\begin{lemma} \\label{lemma: supp outside orbit}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable, and fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. Let $Q\\colon X \\times \\widehat{P}_T \\to X \\times \\widehat{Z}_\\omega$ be the quotient map $(x,\\phi) \\mapsto (x,\\phi\\restr{Z_\\omega})$. Suppose that $(x,\\phi) \\in X \\times \\widehat{P}_T$ satisfies $\\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta} \\ne X \\times \\widehat{Z}_\\omega$. Then $Q^{-1}\\big(\\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta}\\big)$ is a proper closed subset of $X \\times \\widehat{P}_T$, and there exists $f \\in M_T^\\sigma {\\setminus} \\{0\\}$ such that\n\\[\n\\operatorname{supp}\\!\\big((\\psi_T \\circ \\iota^{-1})(f)\\big) \\subseteq (X \\times \\widehat{P}_T) \\setminus Q^{-1}\\big(\\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta}\\big).\n\\]\n\\end{lemma}\n\n\\begin{proof}\nLet $C_{(x,\\phi)} \\coloneqq Q^{-1}\\big(\\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta}\\big)$. Since $\\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta} \\ne X \\times \\widehat{Z}_\\omega$ and $Q$ is surjective, $C_{(x,\\phi)} \\ne X \\times \\widehat{P}_T$. Since $Q$ is continuous, $C_{(x,\\phi)}$ is closed. By Urysohn's lemma there exists $h \\in C_c\\big(X \\times \\widehat{P}_T, [0,1]\\big) \\setminus \\{0\\}$ such that $\\operatorname{supp}(h) \\subseteq (X \\times \\widehat{P}_T) \\setminus C_{(x,\\phi)}$. Define $g\\colon X \\times \\widehat{P}_T \\to C^*(B,\\tilde{\\omega})$ by\n\\[\ng(y,\\rho) \\,\\coloneqq\\, \\int_{\\widehat{B}} \\, h(y,\\rho\\cdot\\chi) \\, \\beta^B_\\chi(U_{0+Z_\\omega}) \\, \\d\\chi \\,=\\, \\int_{\\widehat{B}} \\, h(y,\\rho\\cdot\\chi) \\, U_{0+Z_\\omega} \\, \\d\\chi.\n\\]\nBy \\cite[Lemma~6.17]{RW1998}, we have $g \\in \\mathcal{X}_T^\\omega$. Since $h \\ne 0$ and $h(y,\\rho) \\ge 0$ for all $(y,\\rho) \\in \\operatorname{supp}(h)$, we have $g \\ne 0$. We claim that $\\operatorname{supp}(g) \\subseteq (X \\times \\widehat{P}_T) \\setminus C_{(x,\\phi)}$. Fix $(y,\\rho) \\in C_{(x,\\phi)}$. Then $Q(y,\\rho) \\in \\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta}$. It suffices to show that $g(y,\\rho) = 0$. Fix $\\chi \\in \\widehat{B}$. For all $m \\in Z_\\omega$, we have $\\chi(m + Z_\\omega) = 1$, and hence $(\\rho \\cdot \\chi)(m) = \\rho(m) \\, \\chi(m+Z_\\omega) = \\rho(m)$. Thus\n\\[\nQ(y, \\rho \\cdot \\chi) = \\big(y, \\, (\\rho \\cdot \\chi)\\restr{Z_\\omega}\\big) = \\big(y, \\rho\\restr{Z_\\omega}\\big) = Q(y,\\rho) \\in \\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta},\n\\]\nand hence $(y, \\rho \\cdot \\chi) \\in C_{(x,\\phi)}$. Since $\\operatorname{supp}(h) \\subseteq (X \\times \\widehat{P}_T) \\setminus C_{(x,\\phi)}$, we have $h(y, \\rho \\cdot \\chi) = 0$ for all $\\chi \\in \\widehat{B}$, and therefore,\n\\[\ng(y,\\rho) = \\int_{\\widehat{B}} h(y,\\rho\\cdot\\chi) \\, U_{0+Z_\\omega} \\, d\\chi = 0.\n\\]\nDefine $f \\coloneqq (\\iota \\circ \\psi_T^{-1})(g) \\in M_T^\\sigma$. Since $g \\ne 0$ and $\\iota \\circ \\psi_T^{-1}$ is injective, we have $f \\ne 0$. Since $(\\psi_T \\circ \\iota^{-1})(f) = g$, we have\n\\[\n\\operatorname{supp}\\!\\big((\\psi_T \\circ \\iota^{-1})(f)\\big) = \\operatorname{supp}(g) \\subseteq (X \\times \\widehat{P}_T) \\setminus C_{(x,\\phi)}. \\qedhere\n\\]\n\\end{proof}\n\nRecall from \\cite[Lemma~6.3(b)]{Armstrong2021} that since $\\II_T$ is closed in $\\GG_T$ (by \\cref{prop: HT quotient groupoid}) and amenable, there is a conditional expectation $\\Phi\\colon C^*(\\GG_T,\\sigma) \\to M_T^\\sigma$ satisfying $\\Phi \\circ \\iota = \\iota$ and $\\Phi(f) = \\iota(f\\restr{\\II_T})$ for all $f \\in C_c(\\GG_T,\\sigma)$.\n\n\\begin{lemma} \\label{lemma: conditional expectation and conjugation}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable, and fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. Recall from \\cref{lemma: spanning set BT} the definition of the spanning set $\\mathcal{B}_T$ for $C_c(\\GG_T,\\sigma)$. Given $a, b \\in \\mathcal{B}_T$ and $f \\in M_T^\\sigma$, there exist $p, q, g \\in \\mathcal{B}_T$ such that $gq^*, pg^* \\in \\iota\\big(C_c(\\II_T,\\sigma)\\big)$, the range of $g$ is contained in $[0,1]$, and the map $\\Xi_g$ of \\cref{lemma: conjugation map} satisfies $\\Phi(b^*fa) = \\Xi_g(gq^*fpg^*)$.\n\\end{lemma}\n\n\\begin{proof}\nDefine $U \\coloneqq \\operatorname{osupp}(a)$ and $V \\coloneqq \\operatorname{osupp}(b)$. Since $a, b \\in \\mathcal{B}_T$, both $\\overline{U}$ and $\\overline{V}$ are compact bisections, and there exist $m, n \\in \\mathbb{Z}^k$ such that $\\overline{U} \\subseteq c^{-1}(m)$ and $\\overline{V} \\subseteq c^{-1}(n)$. Define $X\n\\coloneqq \\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U$ and $Y \\coloneqq \\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V$. Define $p,q\\colon \\GG_T \\to \\mathbb{C}$ by\n\\[\np(\\gamma) \\coloneqq \\begin{cases}\na(\\gamma) & \\text{if } \\gamma \\in \\overline{X} \\\\\n0 & \\text{if } \\gamma \\notin \\overline{X},\n\\end{cases}\n\\qquad \\text{and} \\qquad\nq(\\gamma) \\coloneqq \\begin{cases}\nb(\\gamma) & \\text{if } \\gamma \\in \\overline{Y} \\\\\n0 & \\text{if } \\gamma \\notin \\overline{Y}.\n\\end{cases}\n\\]\nSince $\\operatorname{supp}(p) \\subseteq \\overline{X} \\subseteq \\overline{U}$ and $\\operatorname{supp}(q) \\subseteq \\overline{Y} \\subseteq \\overline{V}$, we have $p, q \\in \\mathcal{B}_T$. Let $W$ be an open bisection of $\\GG_T$ such that $\\overline{V} \\subseteq W \\subseteq c^{-1}(n)$. By Urysohn's lemma there exists $g \\in C_c(\\GG_T,[0,1])$ such that $\\operatorname{supp}(g) \\subseteq W$ and $g\\restr{\\overline{V}} \\equiv 1$. Then $g \\in \\mathcal{B}_T$, and $gq^*, pg^* \\in C_c(\\GG_T,\\sigma)$. We claim that $gq^*, pg^* \\in \\iota\\big(C_c(\\II_T,\\sigma)\\big)$. To see this, it suffices to show that $\\operatorname{osupp}(gq^*) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cup}}} \\operatorname{osupp}(pg^*) \\subseteq \\II_T$. Since $q$ and $g$ are\nsupported on bisections, we have\n\\[\n\\operatorname{osupp}(gq^*) = \\operatorname{osupp}(g) (\\operatorname{osupp}(q))^{-1} \\subseteq W V^{-1} \\subseteq W W^{-1} = r(W) \\subseteq \\II_T.\n\\]\nBy \\cref{lemma: IT U/V bar}, $\\overline{X} = \\overline{\\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U} \\subseteq \\II_T \\overline{V} \\subseteq \\II_T W$, and since $p$ and $g$ are supported on bisections, we deduce that\n\\[\n\\operatorname{osupp}(pg^*) = \\operatorname{osupp}(p) (\\operatorname{osupp}(g))^{-1} \\subseteq \\overline{X} W^{-1} \\subseteq \\II_T W W^{-1} = \\II_T \\, r(W) \\subseteq \\II_T.\n\\]\nTherefore, $gq^*, pg^* \\in \\iota\\big(C_c(\\II_T,\\sigma)\\big)$, and \\cref{lemma: conjugation map} implies that\n\\begin{equation} \\label{eqn: osupp Xi_g IT}\n\\operatorname{osupp}\\!\\big(\\Xi_g(g q^* f p g^*)\\big) \\subseteq \\II_T.\n\\end{equation}\nWe conclude by showing that $\\Phi(b^*fa) = \\Xi_g(gq^*fpg^*)$. Since $\\iota$, $\\Phi$, and $\\Xi_g$ are bounded linear maps, \\cref{lemma: h cdot 1_p span} implies that it suffices to consider $f = \\iota(h \\cdot 1_p)$ for some $h \\in C_c(X)$ and $p \\in P_T$. Define $D \\coloneqq \\operatorname{osupp}(f) \\subseteq c\\restr{\\II_T}^{-1}(p)$. Then $\\operatorname{osupp}(g^*g) \\subseteq s(W)$, and so\n\\begin{equation} \\label{eqn: osupp(Xi_g(gq^*fpg^*))}\n\\operatorname{osupp}\\!\\big(\\Xi_g(g q^* f p g^*)\\big) = \\operatorname{osupp}(g^* g q^* f p g^* g) \\subseteq s(W) (V^{-1} D U) s(W) \\subseteq V^{-1} D U.\n\\end{equation}\nTogether, \\cref{eqn: osupp Xi_g IT,eqn: osupp(Xi_g(gq^*fpg^*))} imply that\n\\[\n\\operatorname{osupp}\\!\\big(\\Xi_g(g q^* f p g^*)\\big) \\subseteq (V^{-1} D U) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} \\II_T = \\operatorname{osupp}(b^*fa) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} \\II_T = \\operatorname{osupp}\\!\\big(\\Phi(b^*fa)\\big).\n\\]\nThus, if $\\Phi(b^*fa)(\\gamma) = 0$ for some $\\gamma \\in \\GG_T$, then $\\Xi_g(gq^*fpg^*)(\\gamma) = 0$. Suppose that $\\gamma \\in \\GG_T$ satisfies $\\Phi(b^*fa)(\\gamma) \\ne 0$. Then $\\gamma \\in \\II_T$, and \\cref{eqn: osupp(Xi_g(gq^*fpg^*))} implies that there exist $\\alpha \\in U$, $\\beta \\in V$, and $\\xi \\in D \\subseteq \\II_T$ such that $\\gamma = \\beta^{-1} \\xi \\alpha \\in \\II_T$. A routine calculation gives\n\\begin{equation} \\label{eqn: Phi(b^*fa) expression}\n\\Phi(b^*fa)(\\gamma) = (b^*fa)(\\beta^{-1}\\xi\\alpha) = \\sigma(\\beta^{-1}\\xi,\\alpha) \\, \\sigma(\\beta^{-1},\\xi) \\, \\overline{\\sigma(\\beta^{-1},\\beta)} \\, \\overline{b(\\beta)} \\, f(\\xi) \\, a(\\alpha).\n\\end{equation}\nDefine $y \\coloneqq s(\\gamma)$. Since $\\gamma \\in \\II_T$, we have $s(\\beta) = r(\\gamma) = y$. Since $\\beta \\in V$ and $g\\restr{\\overline{V}} \\equiv 1$,\n\\begin{equation} \\label{eqn: g^*g is 1}\n(g^*g)(y) = (g^*g)(s(\\beta)) = \\ensuremath{\\lvert} g(\\beta) \\ensuremath{\\rvert}^2 = 1.\n\\end{equation}\nA routine calculation using \\cref{eqn: g^*g is 1} and that $\\sigma$ is normalised gives\n\\begin{align*}\n\\Xi_g(gq^*fpg^*)(\\gamma) &= (g^* g q^* f p g^* g)(y \\gamma y) \\\\\n&= \\sigma(y, \\gamma y) \\, \\sigma(\\gamma, y) \\, (g^*g)(y) \\, (q^* f p)(\\gamma) \\, (g^*g)(y) \\\\\n&= (q^*fp)(\\beta^{-1} \\xi \\alpha) \\\\\n&= \\sigma(\\beta^{-1}\\xi,\\alpha) \\, \\sigma(\\beta^{-1},\\xi) \\, \\overline{\\sigma(\\beta^{-1},\\beta)} \\, \\overline{q(\\beta)} \\, f(\\xi) \\, p(\\alpha). \\numberthis \\label{eqn: Xi_g(g q^* f p g^*) expression}\n\\end{align*}\nWe claim that $p(\\alpha) = a(\\alpha)$ and $q(\\beta) = b(\\beta)$. Since $\\gamma \\in (V^{-1} \\II_T U) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} \\II_T$, \\cref{lemma: s(gamma) in IT U cap V} implies that\n\\[\ny = s(\\gamma) \\in s(\\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V) = s(\\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U).\n\\]\nSo there exist $\\eta \\in \\II_T V \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} U = X \\subseteq U$ and $\\zeta \\in \\II_T U \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} V = Y \\subseteq V$ such that $s(\\eta) = y = s(\\zeta)$. Since $s\\restr{U}$ and $s\\restr{V}$ are homeomorphisms onto their ranges and\n\\[\ns(\\eta) = s(\\alpha) = y = s(\\beta) = s(\\zeta),\n\\]\nwe deduce that $\\alpha = \\eta \\in X$ and $\\beta = \\zeta \\in Y$. Hence $p(\\alpha) = a(\\alpha)$ and $q(\\beta) = b(\\beta)$. Together, \\cref{eqn: Phi(b^*fa) expression,eqn: Xi_g(g q^* f p g^*) expression} now give\n\\[\n\\Phi(b^*fa)(\\gamma) = \\sigma(\\beta^{-1}\\xi,\\alpha) \\, \\sigma(\\beta^{-1},\\xi) \\, \\overline{\\sigma(\\beta^{-1},\\beta)} \\, \\overline{b(\\beta)} \\, f(\\xi) \\, a(\\alpha) = \\Xi_g(gq^*fpg^*)(\\gamma). \\qedhere\n\\]\n\\end{proof}\n\n\\begin{prop} \\label{prop: conjugation supp outside orbit}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable, and fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. Let $Q\\colon X \\times \\widehat{P}_T \\to X \\times \\widehat{Z}_\\omega$ be the quotient map $(x,\\phi) \\mapsto (x,\\phi\\restr{Z_\\omega})$, and let $\\Phi\\colon C^*(\\GG_T,\\sigma) \\to M_T^\\sigma$ be the conditional expectation of \\cite[Lemma~6.3(b)]{Armstrong2021} that extends restriction of functions to $\\II_T$. Fix $(x,\\phi) \\in X \\times \\widehat{P}_T$. Suppose that $f \\in M_T^\\sigma$ satisfies\n\\[\n\\operatorname{supp}\\!\\big((\\psi_T \\circ \\iota^{-1})(f)\\big) \\subseteq (X \\times \\widehat{P}_T) \\setminus Q^{-1}\\big(\\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta}\\big).\n\\]\nThen for all $a, b \\in C^*(\\GG_T,\\sigma)$, we have\n\\[\n\\big(\\psi_T \\circ \\iota^{-1} \\circ \\Phi \\big)(b^*fa)(x,\\phi) = 0.\n\\]\n\\end{prop}\n\n\\begin{proof}\nLet $\\operatorname{ev}_{(x,\\phi)}\\colon \\mathcal{X}_T^\\omega \\to C^*(B,\\tilde{\\omega})$ denote the evaluation map $f \\mapsto f(x,\\phi)$. Recall from \\cref{lemma: spanning set BT} the definition of the spanning set $\\mathcal{B}_T$ for $C_c(\\GG_T,\\sigma)$. Let $C_{(x,\\phi)} \\coloneqq Q^{-1}\\big(\\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta}\\big)$. Fix $a, b \\in \\mathcal{B}_T$, and suppose that $f \\in M_T^\\sigma$ satisfies\n\\[\n\\operatorname{supp}\\!\\big((\\psi_T \\circ \\iota^{-1})(f)\\big) \\subseteq (X \\times \\widehat{P}_T) \\setminus C_{(x,\\phi)}.\n\\]\nSince $\\operatorname{ev}_{(x,\\phi)}$, $\\psi_T$, $\\iota^{-1}$, and $\\Phi$ are all bounded linear maps, it suffices to show that\n\\[\n\\big(\\operatorname{ev}_{(x,\\phi)} \\circ \\, \\psi_T \\circ \\iota^{-1} \\circ \\Phi \\big)(b^*fa) = 0.\n\\]\nLet $\\Xi_g$ be the bounded linear map defined in \\cref{lemma: conjugation map}. By \\cref{lemma: conditional expectation and conjugation} there exist $p, q, g \\in \\mathcal{B}_T$ such that $gq^*, pg^* \\in \\iota\\big(C_c(\\II_T,\\sigma)\\big)$, the range of $g$ is contained in $[0,1]$, and\n\\begin{equation} \\label{eqn: Phi(b^*fa) = Xi_g(g q^* f p g^*)}\n\\Phi(b^*fa) = \\Xi_g(gq^*fpg^*).\n\\end{equation}\nLet $U$ be an open bisection of $\\GG_T$ containing $\\operatorname{supp}(g)$. For $y \\in s(U)$, let $\\alpha_{U,y}$ denote the unique element of $U$ with source $y$. Define $h_q \\coloneqq \\psi_T\\big(\\iota^{-1}(gq^*)\\big)$ and $h_p \\coloneqq \\psi_T\\big(\\iota^{-1}(pg^*)\\big)$. Then\n\\begin{equation} \\label{eqn: psi_T iota inverse homomorphism}\n\\psi_T\\big(\\iota^{-1}(g q^* f p g^*)\\big) = h_q \\, \\psi_T\\big(\\iota^{-1}(f)\\big) \\, h_p.\n\\end{equation}\nBy \\cref{prop: properties of Theta}\\cref{item: Theta is conjugation},\n\\begin{equation} \\label{eqn: theta is conjugation of g q^* f p g^*}\n\\psi_T\\big(\\iota^{-1}\\big(\\Xi_g(g q^* f p g^*)\\big)\\big) = \\Theta_{U,g}\\big(\\psi_T\\big(\\iota^{-1}(g q^* f p g^*)\\big)\\big).\n\\end{equation}\nTogether, \\cref{eqn: Phi(b^*fa) = Xi_g(g q^* f p g^*),eqn: theta is conjugation of g q^* f p g^*} imply that\n\\begin{align*}\n\\big(\\!\\operatorname{ev}_{(x,\\phi)} \\circ \\, \\psi_T \\,\\circ\\, & \\iota^{-1} \\circ \\Phi\\big)(b^*fa) \\\\\n&= \\psi_T\\big(\\iota^{-1}\\big(\\Xi_g(g q^* f p g^*)\\big)\\big)(x,\\phi) \\\\\n&= \\Theta_{U,g}\\big(\\psi_T\\big(\\iota^{-1}(g q^* f p g^*)\\big)\\big)(x,\\phi) \\\\\n&= \\begin{cases}\n\\ensuremath{\\lvert} g(\\alpha_{U,x}) \\ensuremath{\\rvert}^2 \\ \\psi_T\\big(\\iota^{-1}(g q^* f p g^*)\\big)\\big(r(\\alpha_{U,x}), \\, \\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma \\, \\phi\\big) & \\quad \\text{if } x \\in s(U) \\\\\n0 & \\quad \\text{if } x \\notin s(U). \\\\\n\\end{cases}\n\\end{align*}\nThus, to see that $\\big(\\!\\operatorname{ev}_{(x,\\phi)} \\circ\\, \\psi_T \\circ \\iota^{-1} \\circ \\Phi\\big)(b^*fa) = 0$, it suffices to show that if $x \\in s(U)$, then\n\\[\n\\psi_T\\big(\\iota^{-1}(g q^* f p g^*)\\big) \\big(r(\\alpha_{U,x}), \\, \\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma \\, \\phi\\big) = 0.\n\\]\nIf $x \\in s(U)$, then\n\\[\nQ\\big(r(\\alpha_{U,x}),\\, \\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma \\, \\phi\\big) = \\big(r(\\alpha_{U,x}), \\, \\tilde{\\ensuremath{\\tau}}_{[\\alpha_{U,x}]}^\\sigma \\, \\phi\\restr{Z_\\omega}\\big) = \\theta_{[\\alpha_{U,x}]}(x,\\phi\\restr{Z_\\omega}),\n\\]\nand hence\n\\[\n\\big(r(\\alpha_{U,x}), \\, \\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma \\, \\phi\\big) \\in Q^{-1}\\big({\\theta_{[\\alpha_{U,x}]}(x,\\phi\\restr{Z_\\omega})}\\big) \\subseteq C_{(x,\\phi)}.\n\\]\nSince $\\operatorname{supp}\\!\\big((\\psi_T \\circ \\iota^{-1})(f)\\big) \\subseteq (X \\times \\widehat{P}_T) \\setminus C_{(x,\\phi)}$, we obtain $\\psi_T\\big(\\iota^{-1}(f)\\big) \\big(r(\\alpha_{U,x}), \\, \\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma \\, \\phi\\big) = 0$. Combining this with \\cref{eqn: psi_T iota inverse homomorphism} gives\n\\[\n\\psi_T\\big(\\iota^{-1}(g q^* f p g^*)\\big) \\big(r(\\alpha_{U,x}), \\, \\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma \\, \\phi\\big) = \\big(h_q \\, \\psi_T\\big(\\iota^{-1}(f)\\big) \\, h_p\\big) \\big(r(\\alpha_{U,x}), \\, \\ensuremath{\\tau}_{\\alpha_{U,x}}^\\sigma \\, \\phi\\big) = 0. \\qedhere\n\\]\n\\end{proof}\n\nWe now construct a state $\\kappa_{(x,\\phi)}$ of $C^*(\\GG_T,\\sigma)$ defined in terms of a fixed element $(x,\\phi) \\in X \\times \\widehat{P}_T$. In the proof of \\cref{thm: simplicity characterisation}\\cref{item: main minimal implies simple}, we show that if $(x,\\phi)$ has non-dense orbit under $\\theta$, then the GNS representation associated to $\\kappa_{(x,\\phi)}$ is nonzero and has nontrivial kernel, and thus $C^*(\\GG_T,\\sigma)$ is simple.\n\n\\begin{lemma} \\label{lemma: state kappa}\nLet $(X,T)$ be a minimal rank-$k$ Deaconu--Renault system such that $X$ is second-countable, and fix $\\sigma \\in Z^2(\\GG_T,\\mathbb{T})$. Fix $(x,\\phi) \\in X \\times \\widehat{P}_T$. Let $\\Phi\\colon C^*(\\GG_T,\\sigma) \\to M_T^\\sigma$ be the conditional expectation of \\cite[Lemma~6.3(b)]{Armstrong2021} that extends restriction of functions to $\\II_T$, and let $\\operatorname{ev}_{(x,\\phi)}\\colon \\mathcal{X}_T^\\omega \\to C^*(B,\\tilde{\\omega})$ be the evaluation map $f \\mapsto f(x,\\phi)$. Let $\\ensuremath{\\operatorname{Tr}_T^\\omega}$ denote the canonical trace on $C^*(B,\\tilde{\\omega})$ (as defined in \\cref{lemma: simple twisted group C*-algebra}). Let\n\\[\n\\kappa_{(x,\\phi)} \\coloneqq \\ensuremath{\\operatorname{Tr}_T^\\omega} \\circ \\operatorname{ev}_{(x,\\phi)} \\circ\\, \\psi_T \\circ \\iota^{-1} \\circ \\Phi\\colon C^*(\\GG_T,\\sigma) \\to \\mathbb{C}.\n\\]\nFor all $h \\in C_c(X)$ such that $h(x) = 1$, we have $\\kappa_{(x,\\phi)}\\big(\\iota(h \\cdot 1_0)\\big) = 1$. Moreover, $\\kappa_{(x, \\phi)}$ is a state of $C^*(\\GG_T,\\sigma)$.\n\\end{lemma}\n\n\\begin{proof}\nSuppose that $h \\in C_c(X)$ satisfies $h(x) = 1$. Since $\\Phi \\circ \\iota = \\iota$, we have\n\\[\n\\big(\\!\\operatorname{ev}_{(x,\\phi)} \\circ\\, \\psi_T \\circ \\iota^{-1} \\circ \\Phi\\big) \\big(\\iota(h \\cdot 1_0)\\big) = \\psi_T(h \\cdot 1_0)(x,\\phi) = h(x) \\, \\overline{\\phi(0)} \\, U_{0+Z_\\omega} = U_{0+Z_\\omega},\n\\]\nand hence\n\\begin{equation} \\label{eqn: kappa h is 1}\n\\kappa_{(x,\\phi)}\\big(\\iota(h \\cdot 1_0)\\big) = \\ensuremath{\\operatorname{Tr}_T^\\omega}(U_{0+Z_\\omega}) = 1.\n\\end{equation}\nSince $\\Phi$, $\\iota^{-1}$, $\\psi_T$, $\\operatorname{ev}_{(x,\\phi)}$, and $\\ensuremath{\\operatorname{Tr}_T^\\omega}$ are all positive norm-decreasing linear maps, $\\kappa_{(x,\\phi)}$ is a positive linear functional, and $\\ensuremath{\\lVert} \\kappa_{(x,\\phi)} \\ensuremath{\\rVert} \\le 1$. By Urysohn's lemma there exists $h \\in C_c(X)$ such that $h(x) = 1$. Then \\cref{eqn: kappa h is 1} implies that $\\ensuremath{\\lVert} \\kappa_{(x,\\phi)} \\ensuremath{\\rVert} \\ge 1$, and thus $\\kappa_{(x,\\phi)}$ is a state of $C^*(\\GG_T,\\sigma)$.\n\\end{proof}\n\nWe conclude this section by proving \\cref{thm: simplicity characterisation}\\cref{item: main minimal implies simple}, which says that if $(X,T)$ is minimal, then $C^*(\\GG_T,\\sigma)$ is simple if and only if the spectral action $\\theta$ is minimal.\n\n\\begin{proof}[Proof of \\cref{thm: simplicity characterisation}\\cref{item: main minimal implies simple}]\nSuppose that $\\theta$ is minimal. Let $I$ be a nontrivial ideal of $C^*(\\GG_T,\\sigma)$. Then there exists a non-injective homomorphism $\\Psi$ of $C^*(\\GG_T,\\sigma)$ such that $I = \\ker(\\Psi)$. By \\cite[Theorem~6.4]{Armstrong2021}, $J \\coloneqq \\ker(\\Psi \\circ \\iota)$ is a nontrivial ideal of $C^*(\\II_T,\\sigma)$. We have\n\\[\n\\iota(J) \\,=\\, \\{ \\iota(a) : a \\in C^*(\\II_T,\\sigma),\\, \\Psi(\\iota(a)) = 0 \\} \\,=\\, \\{ b \\in M_T^\\sigma : \\Psi(b) = 0 \\} \\,=\\, I \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}} M_T^\\sigma \\,\\subseteq\\, I.\n\\]\nThus, to see that $C^*(\\GG_T,\\sigma)$ is simple, it suffices to show that $J = C^*(\\II_T,\\sigma)$, because then $\\iota(C_0(X)) \\subseteq \\iota(J) \\subseteq I$, and (as argued in \\cite[Theorem~5.3.13]{Armstrong2019}) \\cite[Proposition~3.18]{Exel2008} implies that $I =\nC^*(\\GG_T,\\sigma)$. Define\n\\[\nK_J \\coloneqq \\{ (x,\\phi) \\in X \\times \\widehat{P}_T \\,:\\, f(x,\\phi) = 0 \\text{ for all } f \\in \\psi_T(J) \\}.\n\\]\nSince $\\psi_T(J)$ is an ideal of $\\mathcal{X}_T^\\omega$, \\cref{prop: ideals of twisted isotropy induced algebra} implies that $K_J$ is a closed subset of $X \\times \\widehat{P}_T$, and\n\\[\n\\psi_T(J) = \\{f \\in \\mathcal{X}_T^\\omega : f\\restr{K_J} \\equiv 0 \\}.\n\\]\nLet $Q_T\\colon X \\times \\widehat{P}_T \\to X \\times \\widehat{Z}_\\omega$ be the quotient map $(x,\\phi) \\mapsto (x,\\phi\\restr{Z_\\omega})$. Suppose that $Q(K_J)$ is nonempty, and fix $(x,\\zeta) \\in Q(K_J)$. By \\cref{prop: Q(K_J) is closed and invariant under theta}, $Q(K_J)$ is closed and invariant under $\\theta$, and hence\n\\[\n\\overline{[x,\\zeta]_\\theta} = \\overline{\\{ \\theta_{[\\gamma]}(x,\\zeta) : \\gamma \\in (\\GG_T)_x \\}} \\subseteq Q(K_J).\n\\]\nSince $\\theta$ is minimal by assumption, $Q(K_J) = X \\times \\widehat{Z}_\\omega$. Thus, \\cref{prop: Q(K_J) is closed and invariant under theta} implies that\n\\[\nK_J = Q^{-1}\\!\\left(Q(K_J)\\right) = Q^{-1}\\big(X \\times \\widehat{Z}_\\omega\\big) = X \\times \\widehat{P}_T.\n\\]\nHence\n\\[\n\\psi_T(J) = \\{f \\in \\mathcal{X}_T^\\omega : f\\restr{K_J} \\equiv 0 \\} = \\{0\\},\n\\]\nwhich contradicts that $J$ is nontrivial, because $\\psi_T$ is injective. Therefore, $Q(K_J) = \\varnothing$, forcing $K_J = \\varnothing$, and hence $\\psi_T(J) = \\mathcal{X}_T^\\omega$. Since $\\psi_T$ is an isomorphism, $J = C^*(\\II_T,\\sigma)$, and hence $C^*(\\GG_T,\\sigma)$ is simple.\n\nFor the converse, we prove the contrapositive. Suppose that $\\theta$ is not minimal. Then there exists $(x,\\phi) \\in X \\times \\widehat{P}_T$ such that\n\\[\n\\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta} = \\overline{\\{ \\theta_{[\\gamma]}(x,\\phi\\restr{Z_\\omega}) : \\gamma \\in (\\GG_T)_x \\}} \\ne X \\times \\widehat{Z}_\\omega.\n\\]\nLet\n\\[\n\\kappa_{(x,\\phi)} \\coloneqq \\ensuremath{\\operatorname{Tr}_T^\\omega} \\circ \\operatorname{ev}_{(x,\\phi)} \\circ\\, \\psi_T \\circ \\iota^{-1} \\circ \\Phi\\colon C^*(\\GG_T,\\sigma) \\to \\mathbb{C}\n\\]\nbe the state of $C^*(\\GG_T,\\sigma)$ defined in \\cref{lemma: state kappa}. Let $\\kappa \\coloneqq \\kappa_{(x, \\phi)}$, let\n\\[\nN_\\kappa \\coloneqq \\{ f \\in C^*(\\GG_T,\\sigma) : \\kappa(f^*f) = 0 \\}\n\\]\nbe the null space for $\\kappa$, and let $\\pi_\\kappa\\colon C^*(\\GG_T,\\sigma) \\to B(\\mathcal{H}_\\kappa)$ be the GNS representation associated to $\\kappa$. To see that $C^*(\\GG_T,\\sigma)$ is not simple, it suffices to prove that\n\\[\n\\{0\\} \\ne \\ker(\\pi_\\kappa) \\ne C^*(\\GG_T,\\sigma).\n\\]\nSince $\\kappa \\ne 0$, we have $\\mathcal{H}_\\kappa \\ne \\{0\\}$. So since $\\pi_\\kappa$ is nondegenerate, $\\ker(\\pi_\\kappa) \\ne C^*(\\GG_T, \\sigma)$. We now show that $\\ker(\\pi_\\kappa) \\ne \\{0\\}$. Define $C_{(x,\\phi)} \\coloneqq Q^{-1}\\big(\\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta}\\big)$. Since $\\overline{[x, \\phi\\restr{Z_\\omega}]_\\theta} \\ne X \\times \\widehat{Z}_\\omega$, \\cref{lemma: supp outside orbit} shows that $C_{(x,\\phi)}$ is a proper closed subset of $X \\times \\widehat{P}_T$, and there exists $f \\in M_T^\\sigma {\\setminus} \\{0\\}$ such that\n\\[\n\\operatorname{supp}\\!\\big((\\psi_T \\circ \\iota^{-1})(f)\\big) \\subseteq (X \\times \\widehat{P}_T) \\setminus C_{(x,\\phi)}.\n\\]\nFix $a, b \\in C^*(\\GG_T,\\sigma)$. To see that $\\pi_\\kappa(f) = 0$, it suffices to show that\n\\[\n\\big(\\pi_\\kappa(f)(a + N_k) \\mid b + N_k\\big) = 0.\n\\]\nSince $\\pi_\\kappa$ is the GNS representation associated to $\\kappa$, we have\n\\[\n\\big(\\pi_\\kappa(f)(a + N_k) \\mid b + N_k\\big) = \\big(fa + N_k \\mid b + N_k\\big) = \\kappa(b^*fa).\n\\]\nBy \\cref{prop: conjugation supp outside orbit}, we have $(\\psi_T \\circ \\iota^{-1} \\circ \\Phi)(b^*fa)(x,\\phi) = 0$, and hence\n\\[\n\\kappa(b^*fa) = \\big(\\!\\ensuremath{\\operatorname{Tr}_T^\\omega} \\circ \\operatorname{ev}_{(x,\\phi)} \\circ\\, \\psi_T \\circ \\iota^{-1} \\circ \\Phi\\big)(b^*fa) = \\ensuremath{\\operatorname{Tr}_T^\\omega}(0) = 0.\n\\]\nHence $\\big(\\pi_\\kappa(f)(a + N_k) \\mid b + N_k\\big) = 0$, giving $\\ker(\\pi_\\kappa) \\ne \\{0\\}$.\n\\end{proof}\n\n\\begin{remark}\nIf $X$ is the infinite-path space of a cofinal, row-finite $k$-graph with no sources, and each $T^n$ is the degree-$n$ shift map, then \\cref{thm: simplicity characterisation} coincides with the simplicity characterisation given in \\cite[Corollary~4.8]{KPS2016JNG}.\n\\end{remark}\n\n\\begin{remark}\nTheorem~5.1 of \\cite{BCFS2014} shows that $C^*(\\GG_T)$ is simple if and only if $\\GG_T$ is minimal and effective. We claim that \\cite[Theorem~5.1]{BCFS2014} is equivalent to \\cref{thm: simplicity characterisation} when $\\sigma$ is trivial. In this case, $\\omega$ and each $\\tilde{\\ensuremath{\\tau}}_\\gamma^\\sigma$ are also trivial, and $Z_\\omega = P_T$. So \\cref{thm: simplicity characterisation} says that $C^*(\\GG_T,\\sigma) = C^*(\\GG_T)$ is simple if and only if the set\n\\[\n[x,\\phi]_\\theta = \\left\\{ \\big(r(\\gamma),\\, \\tilde{\\ensuremath{\\tau}}_{[\\gamma]}^\\sigma \\, \\phi\\big) : \\gamma \\in (\\GG_T)_x \\right\\} = r\\big((\\GG_T)_x\\big) \\times \\{\\phi\\}\n\\]\nis dense in $X \\times \\widehat{P}_T$ for all $(x,\\phi) \\in X \\times \\widehat{P}_T$. Since $r\\big((\\GG_T)_x\\big)$ is dense in $X$ and $X \\times \\{\\phi\\}$ is closed, we deduce that $\\theta$ is minimal if and only if $P_T = \\{0\\}$. By \\cref{cor: GT effective iff PT trivial}, this occurs precisely when $\\GG_T$ is effective.\n\\end{remark}\n\n\n\\section{An application to some crossed products by \\texorpdfstring{$\\mathbb{Z}$}{the integers}}\n\\label{sec: application}\n\nIn this section we apply our theorem to characterise simplicity of crossed products of C*-algebras of rank-$1$ Deaconu--Renault groupoids arising from continuous $\\mathbb{T}$-valued functions on the underlying spaces. We then specialise this to the analogue of quasi-free actions on topological-graph C*-algebras.\n\nTo set up our example, we will need the following folklore result about multipliers of the C*-algebras of Hausdorff \\'etale groupoids. We write $C_b(Y)$ for the set of continuous, bounded, complex-valued functions on a locally compact Hausdorff space $Y$.\n\n\\begin{lemma} \\label{lemma: bisection multipliers}\nLet $\\mathcal{G}$ be a Hausdorff \\'etale groupoid, and fix $\\sigma \\in Z^2(\\mathcal{G},\\mathbb{T})$. Suppose that $B \\subseteq \\mathcal{G}$ is a clopen bisection of $\\mathcal{G}$ such that $s(B)$ and $r(B)$ are closed, and fix $f \\in C_b(B)$. For $g \\in C_c(\\mathcal{G},\\sigma)$, the convolution product $f * g$ given by $(f * g)(\\gamma) = \\sum_{\\alpha \\in \\mathcal{G}^{r(\\gamma)}} \\sigma(\\alpha,\\alpha^{-1}\\gamma) \\, f(\\alpha) \\, g(\\alpha^{-1}\\gamma)$ belongs to $C_c(\\mathcal{G},\\sigma)$. There is a multiplier $M_f$ of $C^*(\\mathcal{G},\\sigma)$ such that for $g \\in C_c(\\mathcal{G},\\sigma)$ we have $M_f(g) = f * g$. If $s(B) = r(B) = \\GG^{(0)}$ and $f(B) \\subseteq \\mathbb{T}$, then $M_f$ is a unitary multiplier of $C^*(\\mathcal{G},\\sigma)$. If $B_1$ and $B_2$ are two clopen bisections such that $r(B_i)$ and $s(B_i)$ are closed for each $i$, and $f_i \\in C_b(B_i)$ for each $i$, then the convolution product $f_1 * f_2$ belongs to $C_b(B_1 B_2)$, and we have $M_{f_1} \\circ M_{f_2} = M_{f_1 * f_2}$;\nlikewise, $f_1^* \\in C_b(B_1^{-1})$ and $M^*_{f_1} = M_{f_1^*}$.\n\\end{lemma}\n\n\\begin{proof}\nFix $g \\in C_c(\\mathcal{G},\\sigma)$. Since $\\operatorname{supp}(g)$ is compact, its image $r(\\operatorname{supp}(g))$ under the continuous range map is also compact. Fix any compactly supported function $h \\in C_c\\big(\\GG^{(0)},[0,1]\\big)$ such that $h\\restr{r(\\operatorname{supp}(g))} \\equiv 1$. Then $f * g = f * (h * g) = (f * h) * g$. The function $f * h$ is given by $(f * h)(\\gamma) = f(\\gamma) h(s(\\gamma))$, and since $\\operatorname{supp}(f * h) \\subseteq s\\restr{B}^{-1}\\big(\\!\\operatorname{supp}(h) \\cap s(B)\\big) \\subseteq B$, it follows that $f * h \\in C_c(\\mathcal{G},\\sigma)$. So $f * g = (f * h) * g \\in C_c(\\mathcal{G},\\sigma)$.\n\nUsing the same function $h$ as above, we see that\n\\[\n\\ensuremath{\\lVert} f * g \\ensuremath{\\rVert}^2 = \\ensuremath{\\lVert} ((f * h) * g)^* * ((f * h) * g) \\ensuremath{\\rVert} = \\ensuremath{\\lVert} g^* * (h * f^* * f * h) * g \\ensuremath{\\rVert}.\n\\]\nIn $C^*(\\mathcal{G},\\sigma)$, we have $g^* * (h * f^* * f * h) * g \\le \\ensuremath{\\lVert} h * f^* * f * h\\ensuremath{\\rVert}_\\infty \\, g^*g \\le \\ensuremath{\\lVert} f \\ensuremath{\\rVert}_\\infty^2 \\, g^*g$, and so we deduce that $\\ensuremath{\\lVert} f * g \\ensuremath{\\rVert} \\le \\ensuremath{\\lVert} f \\ensuremath{\\rVert}_\\infty \\ensuremath{\\lVert} g \\ensuremath{\\rVert}$. So the map $g \\mapsto f * g$ on $C_c(\\mathcal{G},\\sigma)$ extends to a bounded linear map $M_f$ of norm at most $\\ensuremath{\\lVert} f \\ensuremath{\\rVert}_\\infty$ on $C^*(\\mathcal{G},\\sigma)$. Defining $f^*\\colon B^{-1} \\to \\mathbb{C}$ by $f^*(\\gamma) = \\overline{\\sigma(\\gamma^{-1}, \\gamma)} \\overline{f(\\gamma^{-1})}$ as usual, associativity of multiplication shows that for $g,h \\in C_c(\\mathcal{G},\\sigma)$, we have $M_f(g)^* * h = g^* * f^* * h = g^* * M_{f^*}(h)$. Thus $M_f$ is adjointable with respect to the standard inner product on $C^*(\\mathcal{G},\\sigma)$, with adjoint $M_{f^*}$. So $M_f$ is a multiplier, as claimed. If $s(B) = r(B) = \\GG^{(0)}$ and $f(B) \\subseteq \\mathbb{T}$, then for $g \\in C_c(\\mathcal{G},\\sigma)$ we have $M^*_f(M_f(g)) = f^* * f * g = 1_{\\GG^{(0)}} * g = g$, and similarly, $M_f(M^*_f(g)) = g$. So continuity gives $M^*_f M_f = M_f M^*_f = 1_{\\mathcal{M}(C^*(\\mathcal{G},\\sigma))}$, and thus $M_f$ is a unitary.\n\nFor the final statement, we already saw that $M_{f^*} = M^*_f$ for all $f$, so we just have to establish the multiplicativity. If $B_1$ and $B_2$ are clopen bisections and $f_i \\in C_b(B_i)$, then $B_1B_2$ is an open bisection because multiplication is open. To see that $B_1B_2$ is also closed, suppose that $\\gamma_i \\to \\gamma$ and each $\\gamma_i \\in B_1B_2$. Then each $\\gamma_i$ can be written as $\\alpha_i\\beta_i$ with each $\\alpha_i$ in $B_1$ and each $\\beta_i$ in $B_2$. Since $\\gamma_i \\to \\gamma$, we have $r(\\alpha_i) = r(\\gamma_i) \\to r(\\gamma)$, and then since $r\\restr{B_1}$ is a homeomorphism, $\\alpha_i \\to \\alpha$ for some $\\alpha \\in B_1$. Similarly (using $s$ in place of $r$), we have $\\beta_i \\to \\beta$ for some $\\beta \\in B_2$. Since each $s(\\alpha_i) = r(\\beta_i)$, continuity gives $s(\\alpha) = r(\\beta)$, and since $\\alpha_i\\beta_i = \\gamma_i \\to \\gamma$, continuity also gives $\\alpha\\beta = \\gamma$. So $\\gamma \\in B_1B_2$. The convolution formula shows that $\\operatorname{supp}(f_1 * f_2) \\subseteq B_1 B_2$ and that $\\ensuremath{\\lVert} f_1 * f_2 \\ensuremath{\\rVert}_\\infty \\le \\ensuremath{\\lVert} f_1 \\ensuremath{\\rVert}_\\infty \\ensuremath{\\lVert} f_2 \\ensuremath{\\rVert}_\\infty$. For $g \\in C_c(\\mathcal{G},\\sigma)$ we have $M_{f_1}(M_{f_2}(g)) = f_1 * (f_2 * g) = (f_1 * f_2) * g = M_{f_1 * f_2}(g)$, and then continuity gives $M_{f_1}M_{f_2} = M_{f_1*f_2}$.\n\\end{proof}\n\n\\begin{lemma} \\label{lemma: the CP}\nLet $(X,T)$ be a rank-$1$ Deaconu--Renault system such that $X$ is second-countable, and let $h\\colon X \\to \\mathbb{T}$ be a continuous function.\n\\begin{enumerate}[label=(\\alph*)]\n\\item \\label{item: eventually constant} For each $(x, p, y) \\in \\GG_T \\subseteq X \\times \\mathbb{Z} \\times X$, the sequence\n\\[\n\\Big(\\prod^N_{i=0} h(T^i(x)) \\prod^{N-p}_{i=0} \\overline{h(T^i(y))}\\Big)^\\infty_{N = \\ensuremath{\\lvert} p \\ensuremath{\\rvert}}\n\\]\nis eventually constant.\n\\item \\label{item: continuous 1-cocycle} There is a continuous $1$-cocycle $\\widetilde{h}\\colon \\GG_T \\to \\mathbb{T}$ such that\n\\[\n\\widetilde{h}(x, p, y) = \\prod^N_{i=0} h(T^i(x)) \\prod^{N-p}_{i=0} \\overline{h(T^i(y))}\n\\]\nfor large $N \\in \\mathbb{N}$.\n\\item \\label{item: action exists} There is an action $\\alpha^h\\colon \\mathbb{Z} \\to \\operatorname{Aut}(C^*(\\GG_T))$ such that $\\alpha^h_n(f)(\\gamma) = \\widetilde{h}(\\gamma)^n f(\\gamma)$ for all $f \\in C_c(\\GG_T)$ and $\\gamma \\in \\GG_T$.\n\\item \\label{item: 2-cocycle} There is a continuous $2$-cocycle $c_h\\colon (\\GG_T \\times \\mathbb{Z})^{(2)} \\to \\mathbb{T}$ given by\n\\[\nc_h((\\alpha, m),(\\beta, n)) \\coloneqq \\widetilde{h}(\\beta)^m,\n\\]\nand there is an isomorphism $\\phi\\colon C^*(\\GG_T) \\rtimes_{\\alpha^h} \\mathbb{Z} \\to C^*(\\GG_T \\times \\mathbb{Z}, c_h)$ such that\n\\[\n\\phi\\big(i_{C^*(\\GG_T)}(f) \\, i_\\mathbb{Z}(n)\\big)(\\gamma,p) = \\delta_{n,p} f(\\gamma).\n\\]\n\\end{enumerate}\n\\end{lemma}\n\n\\begin{proof}\n\\Cref{item: eventually constant} Fix $(x, p, y) \\in \\GG_T$. By definition there exist $m,n \\in \\mathbb{N}$ such that $m-n = p$ and $T^m(x) = T^n(y)$. Let $N \\coloneqq \\max\\{m, n\\}$. For $M \\ge N$, we have $T^{M+1}(x) = T^{M - m + 1}(T^m(x)) = T^{M - m + 1}(T^n(y)) = T^{M+1 - p}(y)$. Hence\n\\[\n\\prod^{M+1}_{i=0} h(T^i(x)) \\prod^{M+1-p}_{i=0} \\overline{h(T^i(y))} = \\prod^M_{i=0} h(T^i(x)) \\prod^{M-p}_{i=0} \\overline{h(T^i(y))}.\n\\]\nNow a simple induction proves~\\cref{item: eventually constant}.\n\n\\Cref{item: continuous 1-cocycle} For fixed $N \\in \\mathbb{N}$, the map $x \\mapsto \\prod^N_{i=0} h(T^i(x))$ is continuous, and it follows that $\\widetilde{h}$ is continuous on each basic open set $Z(U, m, n, V)$. So $\\widetilde{h}$ is continuous. To see that $\\widetilde{h}$ is a $1$-cocycle, fix $((x, p, y), (y, q, z)) \\in \\GG_T^{(2)}$. Then for sufficiently large $M, N \\in \\mathbb{N}$, we have\n\\[\n\\widetilde{h}(x, p, y) \\, \\widetilde{h}(y, q, z)\n= \\prod^{M}_{i=0} h(T^i(x)) \\, \\prod^{M-p}_{i=0} \\overline{h(T^i(y))} \\, \\prod^{N}_{i=0} h(T^i(y)) \\, \\prod^{N-q}_{i=0} \\overline{h(T^i(z))}.\n\\]\nIn particular, by choosing $M$ large enough, we can take $N = M-p$, so that $N - q = M - (p+q)$, and then $\\prod^{M-p}_{i=0} \\overline{h(T^i(y))} \\prod^{N}_{i=0} h(T^i(y)) = 1$, giving\n\\[\n\\widetilde{h}(x, p, y) \\, \\widetilde{h}(y, q, z) = \\prod^{M}_{i=0} h(T^i(x)) \\prod^{M-(p+q)}_{i=0} \\overline{h(T^i(z))} = \\widetilde{h}(x, p+q, z).\n\\]\nThus $\\widetilde{h}$ is a $1$-cocycle.\n\n\\Cref{item: action exists} It is standard that a continuous $1$-cocycle on $\\GG_T$ determines an automorphism of its C*-algebra: the given formula for $\\alpha^h_n$ determines a linear transformation of $C_c(\\GG_T)$. For $f,g \\in C_c(\\GG_T)$ and $\\gamma \\in \\GG_T$, using that $\\widetilde{h}$ is a $1$-cocycle at the third equality, we calculate\n\\begin{align*}\n\\big(\\alpha^h_n(f)*\\alpha^h_n(g)\\big)(\\gamma) &= \\sum_{\\beta \\in \\GG_T^{r(\\gamma)}} \\alpha^h_n(f)(\\beta) \\, \\alpha^h_n(g)(\\beta^{-1}\\gamma) \\\\\n&= \\sum_{\\beta \\in \\GG_T^{r(\\gamma)}} \\widetilde{h}(\\beta)^n \\, \\widetilde{h}(\\beta^{-1}\\gamma)^n \\, f(\\beta) \\, g(\\beta^{-1}\\gamma) \\\\\n&= \\widetilde{h}(\\gamma)^n \\sum_{\\beta \\in \\GG_T^{r(\\gamma)}} f(\\beta) \\, g(\\beta^{-1}\\gamma) = \\alpha^h_n(f * g)(\\gamma).\n\\end{align*}\nSimilarly, we have $\\alpha^h_n(f)^*(\\gamma) = \\overline{\\widetilde{h}(\\gamma^{-1})^n f(\\gamma^{-1})} = \\widetilde{h}(\\gamma)^n \\overline{f(\\gamma^{-1})} = \\alpha^h_n(f^*)(\\gamma)$. Hence $\\alpha^h_n$ is a $*$-homomorphism from $C_c(\\GG_T)$ to $C_c(\\GG_T) \\subseteq C^*(\\GG_T)$. The universal property of $C^*(\\GG_T)$ then implies that $\\alpha^h_n$ extends to a homomorphism $C^*(\\GG_T) \\to C^*(\\GG_T)$, and since $\\alpha^{h}_{-n}$ is an inverse for it on the dense subspace $C_c(\\GG_T)$, we see that $\\alpha^h$ is an automorphism. The map $n \\mapsto \\alpha^h_n$ is a homomorphism because $\\alpha^h_n(\\alpha^h_m(f))(\\gamma) = \\widetilde{h}(\\gamma)^n \\, \\widetilde{h}(\\gamma)^m f(\\gamma) = \\alpha^h_{n+m}(f)(\\gamma)$.\n\n\\Cref{item: 2-cocycle} The map $c_h$ is continuous because $\\widetilde{h}$ is continuous. To see that $c_h$ satisfies the $2$-cocycle identity, fix a composable triple $\\big((\\alpha, m), (\\beta, n), (\\gamma, p)\\big)$ and calculate\n\\begin{align*}\nc_h\\big((\\alpha, m), (\\beta, n)\\big) \\, c_h\\big((\\alpha, m)(\\beta, n), (\\gamma, p)\\big) &= \\widetilde{h}(\\beta)^m \\, \\widetilde{h}(\\gamma)^{m+n} = \\widetilde{h}(\\beta\\gamma)^m \\, \\widetilde{h}(\\gamma)^n \\\\\n&= c_h\\big((\\alpha,m),(\\beta, n)(\\gamma, p)\\big) \\, c_h\\big((\\beta, n), (\\gamma, p)\\big).\n\\end{align*}\nTo see that $c_h$ is normalised, observe that since $\\widetilde{h}$ is a $1$-cocycle, it vanishes on $\\GG_T^{(0)}$, and so $c_h\\big((r(\\gamma), 0), (\\gamma, n)\\big) = \\widetilde{h}(\\gamma)^0 = 1 = \\widetilde{h}(s(\\gamma))^n = c_h\\big((\\gamma, n), (s(\\gamma), 0)\\big)$.\n\nFor the final statement, first note that for $n \\in \\mathbb{Z}$, the set $\\GG_T^{(0)} \\times \\{n\\}$ is a clopen bisection of $\\GG_T \\times \\mathbb{Z}$ with range and source equal to $(\\GG_T \\times \\mathbb{Z})^{(0)}$. Hence \\cref{lemma: bisection multipliers} shows that there is a unitary multiplier $U_n$ of $C^*(\\GG_T \\times \\mathbb{Z}, c_h)$ that acts on $C_c(\\GG_T \\times \\mathbb{Z}, c_h)$ by convolution with the characteristic function $1_{\\GG_T^{(0)} \\times \\{n\\}}$. Since $\\widetilde{h}$ vanishes on $\\GG_T^{(0)}$, the final statement of \\cref{lemma: bisection multipliers} shows that $n \\mapsto U_n$ is a unitary representation of $\\mathbb{Z}$.\n\nSince $\\GG_T \\times \\{0\\}$ is isomorphic to $\\GG_T$ and $c_h$ is trivial on $\\GG_T \\times \\{0\\}$, the universal property of $C^*(\\GG_T)$ yields a homomorphism $\\pi\\colon C^*(\\GG_T) \\to C^*(\\GG_T \\times \\mathbb{Z}, c_h)$ such that $\\pi(f)(\\gamma,m) = \\delta_{m,0} f(\\gamma)$ for all $f \\in C_c(\\GG_T)$ and $(\\gamma,m) \\in \\GG_T \\times \\mathbb{Z}$. We claim that $\\pi$ is nondegenerate. To see this, fix $g \\in C_c(\\GG_T \\times \\mathbb{Z}, c_h)$, and use Urysohn's lemma to choose $f \\in C_c(\\GG_T)$ such that $\\operatorname{supp}(f) \\subseteq \\GG_T^{(0)}$ and $f\\restr{\\pi_1(r(\\operatorname{supp}(g)))} \\equiv 1$, where $\\pi_1$ is the projection of $\\GG_T \\times \\mathbb{Z}$ onto the first coordinate. A routine calculation shows that $\\pi(f)g = g$, and hence $\\pi$ is nondegenerate.\n\nWe claim that $(\\pi,U)$ is a covariant representation of $(C^*(\\GG_T), \\mathbb{Z}, \\alpha^h)$. To see this, fix $f \\in C_c(\\GG_T)$ and $n \\in \\mathbb{Z}$. For all $(\\gamma, m) \\in \\GG_T \\times \\mathbb{Z}$, we have\n\\begin{align*}\n(U_n \\, \\pi(f) \\, &U^*_n)(\\gamma,m) \\\\\n&= (U_n \\, \\pi(f) \\, U_{-n})(\\gamma,m) \\\\\n&= \\sum_{(\\alpha,p)(\\beta,q)(\\lambda,l) = (\\gamma,m)} \\widetilde{h}(\\beta)^p \\, \\widetilde{h}(\\lambda)^{p+q} \\, 1_{\\GG_T^{(0)} \\times \\{n\\}}(\\alpha,p) \\, \\pi(f)(\\beta,q) \\, 1_{\\GG_T^{(0)} \\times \\{-n\\}}(\\lambda,l).\n\\end{align*}\nIf $(\\alpha,p)(\\beta,q)(\\lambda,l) = (\\gamma,m)$ contributes a nonzero term, then $(\\alpha, p) \\in \\GG_T^{(0)} \\times \\{n\\}$ and $(\\lambda, l) \\in \\GG_T^{(0)} \\times \\{-n\\}$; thus $(\\alpha, p) = (r(\\gamma), n)$ and $(\\lambda, l) = (s(\\gamma), -n)$, and hence $(\\beta, q) = (\\gamma, m)$. So we obtain\n\\[\n(U_n \\, \\pi(f) \\, U^*_n)(\\gamma, m) = \\widetilde{h}(\\gamma)^n \\, \\pi(f)(\\gamma,m) = \\delta_{m,0} \\, \\widetilde{h}(\\gamma)^n f(\\gamma) = \\pi(\\alpha^h_n(f))(\\gamma, m).\n\\]\nTherefore, $(\\pi,U)$ is a nondegenerate covariant representation of $(C^*(\\GG_T), \\mathbb{Z}, \\alpha^h)$, and so the universal property of the crossed product gives a homomorphism $\\phi\\colon C^*(\\GG_T) \\rtimes_{\\alpha^h} \\mathbb{Z} \\to C^*(\\GG_T \\times \\mathbb{Z}, c_h)$ such that $\\phi(i_{C^*(\\GG_T)}(f) \\, i_\\mathbb{Z}(n))(\\gamma,p) = (\\pi(f) \\, U_n)(\\gamma, p) = \\delta_{n,p} f(\\gamma)$.\n\nTo see that $\\phi$ is injective, it suffices by \\cite[Theorem~4.5]{Katsura2004TAMS} to show that $\\pi$ is injective and that there is a strongly continuous action $\\beta$ of $\\mathbb{T}$ on $C^*(\\GG_T \\times \\mathbb{Z}, c_h)$ such that for each $z \\in \\mathbb{T}$, we have $\\beta_z(\\pi(f)) = \\pi(f)$ for all $f \\in C_c(\\GG_T)$, and the extension $\\overline{\\beta}_z$ of $\\beta_z$ to the multiplier algebra $\\mathcal{M}(C^*(\\GG_T \\times \\mathbb{Z}, c_h))$ satisfies $\\overline{\\beta}_z(U_n) = z^n U_n$ for all $n \\in \\mathbb{Z}$.\n\nThe homomorphism $\\pi$ is clearly injective on $C_0(\\GG_T^{(0)})$ and intertwines the canonical expectations $C^*(\\GG_T) \\to C_0(\\GG_T^{(0)})$ and $C^*(\\GG_T \\times \\mathbb{Z}, c_h) \\to C_0((\\GG_T \\times \\mathbb{Z})^{(0)})$ extending restriction of $C_c$-functions to $\\GG_T^{(0)}$ and $(\\GG_T \\times \\mathbb{Z})^{(0)}$, respectively. Since $\\GG_T$ is amenable \\cite[Lemma~3.5]{SW2016}, the expectation $C^*(\\GG_T) \\to C_0(\\GG_T^{(0)})$ is faithful, and so a standard argument (see, for example, \\cite[Lemma~3.14]{SWW2014}) shows that $\\pi$ is injective.\n\nSo we just need to construct the action $\\beta$. For $z \\in \\mathbb{T}$, the map $\\beta_z\\colon C_c(\\GG_T \\times \\mathbb{Z}, c_h) \\to C^*(\\GG_T \\times \\mathbb{Z}, c_h)$ given by $\\beta_z(f)(\\gamma, n) = z^n f(\\gamma, n)$ is a $*$-homomorphism, so the universal property of $C^*(\\GG_T \\times \\mathbb{Z}, c_h)$ implies that it extends to an endomorphism $\\beta_z$ of $C^*(\\GG_T \\times \\mathbb{Z}, c_h)$. Since $\\beta_{\\overline{z}} \\circ \\beta_z$ is the identity on $C_c(\\GG_T \\times \\mathbb{Z}, c_h)$, we deduce that $\\beta_z$ is an automorphism, and since $\\beta_{z} \\circ \\beta_w$ agrees with $\\beta_{zw}$ on $C_c(\\GG_T \\times \\mathbb{Z}, c_h)$ we see that $z \\mapsto \\beta_z$ is a homomorphism. For $f \\in C_c(\\GG_T \\times \\{n\\})$, the map $z \\mapsto \\beta_z(f)$ is clearly continuous. So an $\\frac{\\varepsilon}{3}$-argument shows that $\\beta$ is a strongly continuous action of $\\mathbb{T}$.\n\nWe claim that the extension of each $\\beta_z$ to the multiplier algebra $\\mathcal{M}(C^*(\\GG_T \\times \\mathbb{Z}, c_h))$ satisfies $\\overline{\\beta}_z(U_n) = z^n U_n$ for each $n \\in \\mathbb{Z}$. To see this, fix $n \\in \\mathbb{Z}$ and an increasing sequence $K_i \\subseteq \\GG_T^{(0)}$ of compact sets with $\\bigcup_{i \\in \\mathbb{N}} K_i = \\GG_T^{(0)}$, and for each $i \\in \\mathbb{N}$, fix $h_i \\in C_c\\big(\\GG_T^{(0)} \\times \\{n\\}, [0,1]\\big)$ such that $h_i\\restr{K_i \\times \\{n\\}} \\equiv 1$. For $f \\in C_c(\\GG_T \\times \\mathbb{Z})$ there exists $N \\in \\mathbb{N}$ large enough so that $r(\\operatorname{supp}(f)) \\subseteq K_N \\times \\{0\\}$, and then $h_i * f = U_n f$ for all $i \\ge N$. So the $h_i$ converge strictly to $U_n$, and since $\\beta_z(h_i) = z^n h_i$ for all $z \\in \\mathbb{T}$, this establishes the claim.\n\nIt remains only to prove that $\\phi$ is surjective. For this, fix an open bisection $B$ of $\\GG_T \\times \\mathbb{Z}$ and distinct points $\\beta,\\gamma \\in B$. Then $B = B' \\times \\{n\\}$ for some open bisection $B'$ of $\\GG_T$ and some $n \\in \\mathbb{Z}$, and so $\\beta = (\\beta', n)$ and $\\gamma = (\\gamma', n)$ for distinct $\\beta', \\gamma' \\in \\GG_T$. Fix $f \\in C_c(\\GG_T)$ such that $\\operatorname{supp}(f) \\subseteq B'$, $f(\\beta') = 1$, and $f(\\gamma') = 0$. Then $\\phi(i_{C^*(\\GG_T)}(f) \\, i_\\mathbb{Z}(n))(\\beta', n) = (\\pi(f) \\, U_n)(\\beta', n) = 1$ and $\\phi(i_{C^*(\\GG_T)}(f) \\, i_\\mathbb{Z}(n))(\\gamma', n) = (\\pi(f) \\, U_n)(\\gamma', n) = 0$. So \\cite[Corollary~9.3.5]{Sims2020} shows that $\\phi$ is surjective.\n\\end{proof}\n\n\\begin{thm} \\label{thm: CP simple}\nLet $(X,T)$ be a rank-$1$ Deaconu--Renault system such that $X$ is second-countable, and let $h\\colon X \\to \\mathbb{T}$ be a continuous function. Let $\\widetilde{h}\\colon \\GG_T \\to \\mathbb{T}$ be the $1$-cocycle of \\cref{lemma: the CP}\\cref{item: continuous 1-cocycle}, and let $\\alpha^h\\colon \\mathbb{Z} \\to \\operatorname{Aut}(C^*(\\GG_T))$ be the action of \\cref{lemma: the CP}\\cref{item: action exists}. Write $\\rho$ for the action of $\\GG_T$ on $X \\times \\mathbb{T}$ given by $\\rho_\\gamma(s(\\gamma), z) \\coloneqq (r(\\gamma), \\widetilde{h}(\\gamma) z)$. Suppose that $X$ is an uncountable space. Then the crossed product $C^*(\\GG_T) \\rtimes_{\\alpha^h} \\mathbb{Z}$ is simple if and only if $\\rho$ is minimal.\n\\end{thm}\n\nIn order to prove \\cref{thm: CP simple}, we need the following lemma.\n\n\\begin{lemma} \\label{lemma: X uncountable GT top principal}\nLet $(X,T)$ be a minimal rank-$1$ Deaconu--Renault system such that $X$ is second-countable. If $X$ is uncountable, then $\\GG_T$ is topologically principal.\n\\end{lemma}\n\n\\begin{proof}\nSince $\\GG_T$ is second-countable, it suffices by \\cite[Lemma~3.1]{BCFS2014} to show that $\\II_T = \\GG_T^{(0)}$. To see this, we suppose that $\\II_T \\ne \\GG_T^{(0)}$ and derive a contradiction. Recall from \\cref{prop: IT characterisation} that $\\II_T = \\{ (x, p, x) : p \\in P_T \\}$. Since $\\II_T$ is nontrivial, there exists $p \\in \\mathbb{Z} {\\setminus} \\{0\\}$ such that $(x,p,x) \\in \\GG_T$ for all $x$. By definition of the topology on $\\GG_T$, it follows that for each $x \\in X$ there is an open neighbourhood $U$ of $x$ and a pair $m > n \\in \\mathbb{N}$ such that $T^m(x) = T^n(x)$ for all $x \\in U$. Since the pairs $m > n \\in \\mathbb{N}$ are countable and $X$ is not countable, that $\\GG_T$ is second-countable implies that there exist $x, U, m, n$ as above so that $U$ is not countable. Since $X$ is second-countable and $T^n$ is a local homeomorphism, $(T^n)^{-1}(x)$ is countable for every $x \\in X$, and so $V = T^n(U)$ is an uncountable open set and $p = m - n > 0$ satisfies $T^p(x) = x$ for all $x \\in V$. Fix $x \\in V$. Since $V$ is uncountable, there exists $y \\in V$ such that\n\\begin{equation} \\label[condition]{cond: for contradiction}\nT^q(x) \\ne y \\ \\text{ for all } q \\in \\mathbb{N}.\n\\end{equation}\nSince $\\GG_T$ is minimal, there is a sequence $(z_i, m_i, x)^\\infty_{i=1}$ in $\\GG_T$ such that $z_i \\to y$. Write each $m_i = a_i - b_i$ with $a_i, b_i \\ge 0$ so that $T^{a_i}(z_i) = T^{b_i}(x)$. For each $i \\in \\mathbb{N}$, there exists $k > 0$ such that $kp > b_i$; and then $T^{a_i + (kp-b_i)}(z_i) = T^{kp}(x) = x$. So we can assume that each $m_i > 0$ and that $T^{m_i}(z_i) = x$ for all $i \\in \\mathbb{N}$. By passing to a subsequence, we may assume that each $m_i - m_1$ is divisible by $p$. Fix $l > 0$ such that $lp > m_1$, let $d \\coloneqq lp - m_1$, let $z \\coloneqq T^d(x)$, and let $n_i \\coloneqq m_i + d$ for all $i \\in \\mathbb{N}$. Then $T^{n_i}(z_i) = z$ for all $i \\in \\mathbb{N}$, and each $n_i$ is divisible by $p$. Since $z_i \\to y$, we eventually have $z_i \\in V$, and so we eventually have $z = T^{n_i}(z_i) = z_i$. But this forces $y = z = T^d(x)$, which contradicts \\cref{cond: for contradiction}. Thus $\\II_T$ is trivial, as claimed.\n\\end{proof}\n\n\\begin{proof}[Proof of \\cref{thm: CP simple}]\nBy \\cref{lemma: the CP}\\cref{item: 2-cocycle}, the crossed product $C^*(\\GG_T) \\rtimes_{\\alpha^h} \\mathbb{Z}$ is isomorphic to the twisted groupoid C*-algebra $C^*(\\GG_T \\times \\mathbb{Z}, c_h)$, to which we aim to apply \\cref{thm: simplicity characterisation}. For this, observe first that if $\\overline{T}$ is the action of $\\mathbb{N}^2$ given by $\\overline{T}^{(m,n)} = T^m$, then $\\GG_T \\times \\mathbb{Z} \\cong \\mathcal{G}_{\\overline{T}}$\n\nFirst suppose that $(X,T)$ is not minimal. Then $\\mathcal{G}_{\\overline{T}}$ is also not minimal, and the action $\\rho$ is not minimal. So $(X,\\overline{T})$ is not minimal, and hence \\cref{thm: simplicity characterisation}\\cref{item: main simple implies minimal} implies that $C^*(\\GG_T) \\rtimes_{\\alpha^h} \\mathbb{Z} \\cong C^*(\\mathcal{G}_{\\overline{T}},c_h)$ is not simple. So it suffices to prove the result when $(X,T)$ is minimal.\n\nNow suppose that $(X,T)$ is minimal. Since $X$ is uncountable, \\cref{lemma: X uncountable GT top principal} implies that $\\II_T = \\GG_T^{(0)}$. The isomorphism $\\GG_T \\times \\mathbb{Z} \\to \\mathcal{G}_{\\overline{T}}$ is given by $((x, m, y), n) \\mapsto (x, (m,n), y)$. So the interior $\\mathcal{I}_{\\overline{T}}$ of the isotropy of $\\mathcal{G}_{\\overline{T}}$ is precisely $\\{ (x, (m,n), x) : (x,m,x) \\in \\II_T, \\, n \\in \\mathbb{Z} \\}$. So the preceding paragraph implies that $\\mathcal{I}_{\\overline{T}} = \\{ (x, (0,n), x) : x \\in X, n \\in \\mathbb{Z} \\}$. The isomorphism $\\GG_T \\times \\mathbb{Z} \\to \\mathcal{G}_{\\overline{T}}$ intertwines $c_h$ with the $2$-cocycle $\\sigma \\in Z^2(\\mathcal{G}_{\\overline{T}},\\mathbb{T})$ given by $\\sigma((x, (m,n), y), (y, (p, q), z)) \\coloneqq \\widetilde{h}(y,p,z)^n$. The restriction of this $\\sigma$ to $\\mathcal{I}_{\\overline{T}}^{(2)}$ satisfies\n\\[\n\\sigma\\big((x, (0,m), x), (x, (0,n), x)\\big) = \\widetilde{h}(x, 0, x)^m = 1.\n\\]\nHence $\\sigma$ is $\\omega$-constant on $\\mathcal{I}_{\\overline{T}}$ with $\\omega = 1$. It follows that $Z_\\omega = P_{\\overline{T}} = \\{0\\} \\times \\mathbb{Z} \\cong \\mathbb{Z}$.\n\nThe $\\widehat{\\mathbb{Z}}$-valued $1$-cocycle $\\ensuremath{\\tau}^\\sigma$ obtained from \\cref{lemma: twistchar_gamma^sigma}\\cref{item: twistchar_.^sigma 1-cocycle} satisfies\n\\begin{align*}\n\\ensuremath{\\tau}_{(x, (m,n), y)}^\\sigma(p) &= \\sigma\\big((x, (m,n), y), (y, (0,p), y)\\big) \\\\\n&\\qquad \\cdot \\sigma\\big((x, (m,n+p), y), (y, (-m,-n), x)\\big) \\\\\n&\\qquad \\cdot \\overline{\\sigma\\big((x, (m,n), y), (y, (-m,-n), x)\\big)} \\\\\n&= \\widetilde{h}(y,0,y)^n \\, \\widetilde{h}(y, -m, x)^{n+p} \\, \\overline{\\widetilde{h}(y, -m, x)^n} \\\\\n&= \\widetilde{h}(y, -m, x)^p.\n\\end{align*}\nSo the isomorphism $\\chi \\mapsto \\chi(1)$ from $\\widehat{\\mathbb{Z}}$ to $\\mathbb{T}$ carries $\\ensuremath{\\tau}_{(x, (m,n),y)}^\\sigma$ to $\\widetilde{h}(y, -m, x) \\in \\mathbb{T}$.\n\nWe have $\\mathcal{H}_{\\overline{T}} \\coloneqq \\mathcal{G}_{\\overline{T}}/\\mathcal{I}_{\\overline{T}} \\cong (\\GG_T \\times \\mathbb{Z})/(\\GG_T^{(0)} \\times \\mathbb{Z}) \\cong \\GG_T$ and the isomorphism is the map $[(x, (m,n), y)] \\mapsto (x, m, y)$. So the spectral action $\\theta$ of $\\mathcal{H}_{\\overline{T}}$ on $X \\times \\widehat{Z}_\\omega$ of \\cref{prop: spectral action} is identified with the action of $\\GG_T$ on $X \\times \\mathbb{T}$ given by $\\theta_{(x, m, y)}(y, z) \\coloneqq (x, \\widetilde{h}(y, -m, x) z)$, which is precisely the action $\\rho$. So \\cref{thm: simplicity characterisation} shows that $C^*(\\mathcal{G}_{\\overline{T}}, c_h)$ is simple if and only if $\\rho$ is minimal.\n\\end{proof}\n\nIn the following result, we write $t$ and $o$ for the terminus (range) and origin (source) map in a topological graph, so as to avoid confusion with the range and source maps $r$ and $s$ in the associated groupoid. We write $X(E)$ for the graph correspondence associated to a topological graph $E = (E^0, E^1, t, o)$, and we write $\\mathcal{O}_{X(E)}$ for the associated Cuntz--Pimsner algebra. We write $(j_{C_0(E^0)}, \\, j_{X(E)})$ for the universal Cuntz--Pimsner-covariant representation of $X(E)$ that generates $\\mathcal{O}_{X(E)}$. See \\cite{Katsura2004TAMS, Yeend2006CM, LPS2014} for background on topological graphs and their C*-algebras.\n\n\\begin{cor} \\label{cor: topgraph exact}\nLet $E = (E^0, E^1, t, o)$ be a second-countable topological graph such that the terminus map $t\\colon E^1 \\to E^0$ is proper and surjective, and the infinite-path space $E^\\infty$ is uncountable. Suppose that $\\ell\\colon E^1 \\to \\mathbb{T}$ is a continuous function. There is an action $\\beta^\\ell\\colon \\mathbb{Z} \\curvearrowright C^*(E)$ such that $\\beta^\\ell_n(j_{X(E)}(\\xi)) = j_{X(E)}(\\ell^n \\cdot \\xi)$ for all $\\xi \\in C_c(E^1)$. Extend $\\ell$ to a continuous function $\\ell\\colon E^* \\to \\mathbb{T}$ by defining $\\ell(e_1 \\dotsb e_n) \\coloneqq \\prod^n_{i=1} \\ell(e_i)$ and $\\ell\\restr{E^0} \\equiv 1$, and let $T\\colon E^\\infty \\to E^\\infty$ be the shift map $T(x_1 x_2 x_3 \\dotsb) = x_2 x_3 \\dotsb$. Then $C^*(E) \\rtimes_{\\beta^\\ell} \\mathbb{Z}$ is simple if and only if for every infinite path $x \\in E^\\infty$, the set\n\\begin{equation} \\label{eqn: topgraph orbit}\n\\big\\{ \\big(\\lambda T^n(x), \\, \\ell(\\lambda) \\overline{\\ell(x(0,n))}\\big) : n \\in \\mathbb{N}, \\, \\lambda \\in E^* t(T^n(x)) \\big\\}\n\\end{equation}\nis dense in $E^\\infty \\times \\mathbb{T}$.\n\\end{cor}\n\n\\begin{proof}\nThe map $\\xi \\mapsto \\ell \\cdot \\xi$ is a unitary operator $U_\\ell$ on the graph correspondence $X(E)$. If $\\xi \\in C_c(E^1)$ is a positive-valued function such that $o$ is injective on $\\operatorname{supp}(\\xi)$, then a quick calculation shows that conjugation by $U_\\ell$ fixes the rank-$1$ operator $\\Theta_{\\xi, \\xi}$. Using this, it is routine to see that if $(\\psi,\\pi)$ is a covariant Toeplitz representation of $E$ as in \\cite[Definitions 2.2~and~2.10]{LPS2014}, then so is $(\\psi \\circ U_\\ell, \\pi)$. So the universal property of $C^*(E) \\cong \\mathcal{O}_{X(E)}$ described by \\cite[Theorem~2.13]{LPS2014} yields a unique automorphism $\\beta^\\ell$ that fixes $j_{C_0(E^0)}(C_0(E^0))$ and satisfies $\\beta^\\ell(j_{X(E)}(\\xi)) = j_{X(E)}(\\ell \\cdot \\xi)$ for $\\xi \\in C_c(E^1)$. The formula $\\beta^\\ell_n \\coloneqq (\\beta^\\ell)^n$ then gives the desired action.\\footnote{We could also appeal to the fourth paragraph of \\cite[Page~462]{LN2004}.}\n\nSince $t\\colon E^1 \\to E^0$ is proper, \\cite[Propositions~3.11~and~3.16]{AB2018} show that $E^\\infty$ is a locally compact Hausdorff space and $T$ is a local homeomorphism. By \\cite[Theorem~5.2]{Yeend2006CM}, there is an isomorphism $\\phi\\colon C^*(E) \\to C^*(\\GG_T)$ such that $\\phi(j_{C_0(E^0)}(f))(x,m,y) = \\delta_{x,y} \\, \\delta_{m,0} \\, f(t(x))$ for $f \\in C_0(E^0)$ and $\\phi(j_{X(E)}(\\xi))(x,m,y) = \\delta_{T(x), y} \\, \\delta_{m,1} \\, \\xi(x_1)$ for $\\xi \\in C_c(E^1)$.\n\nDefine $h\\colon E^\\infty \\to \\mathbb{T}$ by $h(x) \\coloneqq \\ell(x_1)$. Then $h$ is continuous. Let $\\alpha^h \\in \\operatorname{Aut}(C^*(\\GG_T))$ be the automorphism $\\alpha^h_1$ of \\cref{lemma: the CP}\\cref{item: action exists}. A routine calculation shows that $\\alpha^h \\circ \\phi$ agrees with $\\phi \\circ \\beta^\\ell$ on $j_{C_0(E^0)}(C_0(E^0)) \\mathbin{\\scalebox{1.2}{\\ensuremath{\\cup}}} j_{X(E)}(C_c(E^1))$, and hence the uniqueness of the automorphism $\\beta^\\ell$ discussed in the first paragraph shows that $\\alpha^h \\circ \\phi = \\phi \\circ \\beta^\\ell$. It therefore suffices to show that $C^*(\\GG_T) \\rtimes_{\\alpha^h} \\mathbb{Z}$ is simple if and only if the set described in \\cref{eqn: topgraph orbit} is dense for each $x \\in E^\\infty$.\n\nFix $x \\in E^\\infty$. We claim that the set described in \\cref{eqn: topgraph orbit} is precisely the orbit of $(x,1)$ under the action $\\rho$ of \\cref{thm: CP simple}. We have\n\\[\n(\\GG_T)_x = \\left\\{ (\\lambda T^n(x), \\ensuremath{\\lvert} \\lambda \\ensuremath{\\rvert} - n, x) \\,:\\, n \\in \\mathbb{N}, \\, \\lambda \\in E^* t(T^n(x)) \\right\\},\n\\]\nand so for each $\\gamma \\in (\\GG_T)_x$, we have\n\\begin{equation} \\label{eqn: rho_gamma orbit}\n\\rho_\\gamma(x,1) = \\big(\\lambda T^n(x), \\, \\widetilde{h} (\\lambda T^n(x), \\ensuremath{\\lvert} \\lambda \\ensuremath{\\rvert} - n, x)\\big),\n\\end{equation}\nfor some $n \\in \\mathbb{N}$ and $\\lambda \\in E^* t(T^n(x))$. Direct calculation shows that\n\\begin{equation} \\label{eqn: tildeh vs ell}\n\\widetilde{h}(\\mu x, \\ensuremath{\\lvert} \\mu \\ensuremath{\\rvert} - \\ensuremath{\\lvert} \\nu \\ensuremath{\\rvert}, \\nu x) = \\ell(\\mu)\\overline{\\ell(\\nu)},\n\\end{equation}\nfor all $x \\in E^\\infty$ and $\\mu,\\nu \\in E^* t(x)$. Together, \\cref{eqn: rho_gamma orbit,eqn: tildeh vs ell} imply that the set described in \\cref{eqn: topgraph orbit} is the orbit of $(x,1)$ under $\\rho$. Since $\\rho$ commutes with the action of $\\mathbb{T}$ on $E^\\infty \\times \\mathbb{T}$ by translation in the second coordinate, the orbit of $(x, 1)$ is dense if and only if the orbit of $(x, z)$ is dense for every $z \\in \\mathbb{T}$. That is, the set described in \\cref{eqn: topgraph orbit} is dense for each $x \\in E^\\infty$ if and only if every $\\rho$-orbit is dense. So the result follows from \\cref{thm: CP simple}.\n\\end{proof}\n\nTo conclude, for the class of topological graphs appearing in \\cref{cor: topgraph exact}, we give a sufficient condition phrased purely in terms of the graph without reference to the shift map on its infinite-path space, for simplicity of the crossed product described there. (The hypothesis that $E^\\infty$ is uncountable is quite weak, and follows from a number of elementary conditions on the graph: for example, that $E^0$ is uncountable, or that $E$ has at least one vertex that supports at least two distinct cycles.)\n\n\\begin{cor} \\label{cor: topgraph checkable}\nLet $E = (E^0, E^1, t, o)$ be a second-countable topological graph such that the terminus map $t\\colon E^1 \\to E^0$ is proper and surjective, and the infinite-path space $E^\\infty$ is uncountable. Let $\\ell\\colon E^1 \\to \\mathbb{T}$ be a continuous function. Extend $\\ell$ to $E^*$ by defining $\\ell(e_1 \\dotsb e_n) \\coloneqq \\prod^n_{i=1} \\ell(e_i)$ and $\\ell\\restr{E^0} \\equiv 1$. For each $v \\in E^0$, define\n\\[\n\\ForwardOrbit{v} \\coloneqq \\bigcup_{\\mu \\in E^*v} (t(\\mu), \\ell(\\mu)) \\subseteq E^0 \\times \\mathbb{T}.\n\\]\nIf $\\ForwardOrbit{v}$ is dense in $E^0 \\times \\mathbb{T}$ for each $v \\in E^0$, then the crossed product $C^*(E) \\rtimes_{\\beta^\\ell} \\mathbb{Z}$ of \\cref{cor: topgraph exact} is simple.\n\\end{cor}\n\n\\begin{proof}\nSuppose that $\\ForwardOrbit{v}$ is dense in $E^0 \\times \\mathbb{T}$ for each $v \\in E^0$. We aim to invoke \\cref{thm: CP simple}. Fix $(x, w), (y, z) \\in E^\\infty \\times \\mathbb{T}$. Recall from \\cite[Proposition~3.11~and~Lemma~3.13]{AB2018} that for $n \\in \\mathbb{N}$ and an open neighbourhood $U \\subseteq E^n$ of $y(0,n)$ such that $o\\restr{U}$ is injective, the set $Z(U) = \\{ y' \\in E^\\infty : y'(0,n) \\in U \\}$ is a basic open neighbourhood of $y$. Let $d$ be the metric on $\\mathbb{T}$ induced by the usual metric on $\\mathbb{R}$ via the local homeomorphism $t \\mapsto e^{it}$ from $\\mathbb{R}$ to $\\mathbb{T}$. Let $\\rho$ be the action of $\\GG_T$ on $E^\\infty \\times \\mathbb{T}$ from \\cref{thm: CP simple}. It suffices to fix a neighbourhood $U$ as above and an $\\varepsilon > 0$ and show that there exists $\\gamma \\in (\\GG_T)_x$ such that $\\rho_\\gamma(x,w) \\in Z(U) \\times B_d(z; \\varepsilon)$. Let $\\mu_y \\coloneqq y(0,n) \\in U$. Since $\\ell$ is continuous, by shrinking $U$ if necessary, we may assume that\n\\begin{equation} \\label[condition]{cond: U small}\nd\\big(\\ell(\\mu), \\ell(\\mu_y)\\big) < \\frac{\\varepsilon}{2} \\quad \\text{ for all } \\mu \\in U.\n\\end{equation}\nSince $o\\colon E^1 \\to E^0$ is a local homeomorphism, it is an open map, and so $o(U)$ is open. Since $\\ForwardOrbit{t(x)}$ is dense in $E^0 \\times \\mathbb{T}$, we can find $\\lambda \\in E^* t(x)$ such that\n\\[\n(t(\\lambda), \\ell(\\lambda)) \\in o(U) \\times B_d\\big(z\\overline{\\ell(\\mu_y)}\\overline{w}; \\, \\tfrac{\\varepsilon}{2}\\big).\n\\]\nLet $\\mu_{t(\\lambda)}$ be the unique element of $U$ such that $o(\\mu_{t(\\lambda)}) = t(\\lambda)$. Since $d$ is rotation-invariant, \\cref{cond: U small} implies that\n\\begin{equation} \\label{eqn: distance between mu terms}\nd\\big(\\ell(\\mu_{t(\\lambda)} \\lambda) w, \\ell(\\mu_y) \\ell(\\lambda) w\\big) = d\\big(\\ell(\\mu_{t(\\lambda)}) \\ell(\\lambda) w, \\ell(\\mu_y) \\ell(\\lambda) w\\big) = d\\big(\\ell(\\mu_{t(\\lambda)}), \\ell(\\mu_y)\\big) < \\frac{\\varepsilon}{2}.\n\\end{equation}\nMoreover, since $d$ is rotation-invariant and $\\ell(\\lambda) \\in B_d\\big(z\\overline{\\ell(\\mu_y)}\\overline{w}; \\, \\frac{\\varepsilon}{2}\\big)$, we have\n\\begin{equation} \\label{eqn: distance to z}\nd\\big(\\ell(\\mu_y) \\ell(\\lambda) w, z\\big) = d\\big(\\ell(\\lambda), z \\overline{\\ell(\\mu_y)} \\overline{w}\\big) < \\frac{\\varepsilon}{2}.\n\\end{equation}\nTogether, \\cref{eqn: distance between mu terms,eqn: distance to z} imply that\n\\begin{equation} \\label{eqn: distance to z less than epsilon}\nd\\big(\\ell(\\mu_{t(\\lambda)} \\lambda) w, z\\big) \\le d\\big(\\ell(\\mu_{t(\\lambda)} \\lambda) w, \\ell(\\mu_y) \\ell(\\lambda) w\\big) + d\\big(\\ell(\\mu_y) \\ell(\\lambda) w, z\\big) < \\frac{\\varepsilon}{2} + \\frac{\\varepsilon}{2} = \\varepsilon.\n\\end{equation}\nNow \\cref{eqn: tildeh vs ell,eqn: distance to z less than epsilon} imply that\n\\[\n\\rho_{(\\mu_{t(\\lambda)} \\lambda x, n + \\ensuremath{\\lvert} \\lambda \\ensuremath{\\rvert}, x)}(x,w) = \\big(\\mu_{t(\\lambda)} \\lambda x, \\, \\ell(\\mu_{t(\\lambda)} \\lambda) w\\big) \\in Z(U) \\times B_d(z; \\varepsilon),\n\\]\nas required.\n\\end{proof}\n\n\n\n\\section{\\@ifstar{\\starsection}{\\nostarsection}}\n\\newcommand\\sectionspace{\\vspace{0.5ex}}\n\\newcommand\\nostarsection[1]{\\sectionspace\\origsection{#1}\\sectionspace}\n\\newcommand\\starsection[1]{\\sectionspace\\origsection*{#1}\\sectionspace}\n\\makeatother\n\n\\newcommand{\\creflastconjunction}{, and\\nobreakspace}\n\n\\setlist[enumerate]{font=\\normalfont}\n\\crefname{enumi}{}{}\n\n\\newcommand\\numberthis{\\addtocounter{equation}{1}\\tag{\\theequation}}\n\n\\numberwithin{equation}{section}\n\\crefname{equation}{Equation}{Equations}\n\n\\crefname{condition}{Condition}{Conditions}\n\\creflabelformat{condition}{#2(#1)#3}\n\n\\crefname{claim}{Claim}{Claims}\n\\creflabelformat{claim}{#2(#1)#3}\n\n\\newtheorem{theorem}{Theorem}[section]\n\n\\newtheorem{thm}[theorem]{Theorem}\n\\crefname{thm}{Theorem}{Theorems}\n\n\\newtheorem{lemma}[theorem]{Lemma}\n\\crefname{lemma}{Lemma}{Lemmas}\n\n\\newtheorem{prop}[theorem]{Proposition}\n\\crefname{prop}{Proposition}{Propositions}\n\n\\newtheorem{cor}[theorem]{Corollary}\n\\crefname{cor}{Corollary}{Corollaries}\n\n\\theoremstyle{definition}\n\n\\newtheorem{definition}[theorem]{Definition}\n\\crefname{definition}{Definition}{Definitions}\n\n\\theoremstyle{remark}\n\n\\newtheorem{remark}[theorem]{Remark}\n\\crefname{remark}{Remark}{Remarks}\n\n\\newcommand{\\hl}[1]{\\textcolor{magenta}{\\emph{#1}}}\n\\newcommand{\\mathbb{C}}{\\mathbb{C}}\n\\newcommand{\\mathbb{N}}{\\mathbb{N}}\n\\newcommand{\\mathbb{R}}{\\mathbb{R}}\n\\newcommand{\\mathbb{T}}{\\mathbb{T}}\n\\newcommand{\\mathbb{Z}}{\\mathbb{Z}}\n\\renewcommand{\\AA}{\\mathcal{A}}\n\\newcommand{\\mathcal{F}}{\\mathcal{F}}\n\\newcommand{\\mathcal{G}}{\\mathcal{G}}\n\\newcommand{\\GG^{(0)}}{\\mathcal{G}^{(0)}}\n\\newcommand{\\GG^{(2)}}{\\mathcal{G}^{(2)}}\n\\newcommand{\\mathcal{H}}{\\mathcal{H}}\n\\newcommand{\\mathcal{I}}{\\mathcal{I}}\n\\newcommand{\\mathcal{M}}{\\mathcal{M}}\n\\newcommand{\\mathcal{O}}{\\mathcal{O}}\n\\newcommand{\\mathcal{X}}{\\mathcal{X}}\n\\newcommand{\\mathcal{Y}}{\\mathcal{Y}}\n\\renewcommand{\\d}{\\mathrm{d}}\n\\newcommand{\\mathcal{B}_T}{\\mathcal{B}_T}\n\\newcommand{\\GG_T}{\\mathcal{G}_T}\n\\newcommand{\\GG_T^{(0)}}{\\mathcal{G}_T^{(0)}}\n\\newcommand{\\GG_T^{(2)}}{\\mathcal{G}_T^{(2)}}\n\\newcommand{\\HH_T}{\\mathcal{H}_T}\n\\newcommand{\\HH_T^{(0)}}{\\mathcal{H}_T^{(0)}}\n\\newcommand{\\HH_T^{(2)}}{\\mathcal{H}_T^{(2)}}\n\\newcommand{\\II_T}{\\mathcal{I}_T}\n\\newcommand{\\II_T^{(0)}}{\\mathcal{I}_T^{(0)}}\n\\newcommand{\\II_T^{(2)}}{\\mathcal{I}_T^{(2)}}\n\\newcommand{P_T}{P_T}\n\\newcommand{\\widehat{P}_T}{\\widehat{P}_T}\n\\newcommand{\\operatorname{span}}{\\operatorname{span}}\n\\newcommand{\\overline{\\vecspan}}{\\overline{\\operatorname{span}}}\n\\newcommand{\\operatorname{supp}}{\\operatorname{supp}}\n\\newcommand{\\operatorname{osupp}}{\\operatorname{osupp}}\n\\newcommand{\\operatorname{ev}}{\\operatorname{ev}}\n\\newcommand{\\operatorname{id}}{\\operatorname{id}}\n\\newcommand{\\operatorname{Aut}}{\\operatorname{Aut}}\n\\newcommand{\\operatorname{Ext}}{\\operatorname{Ext}}\n\\newcommand{\\ForwardOrbit}[1]{\\operatorname{Orb}^+(#1)}\n\\newcommand{\\operatorname{Ind}}{\\operatorname{Ind}}\n\\newcommand{\\operatorname{Iso}}{\\operatorname{Iso}}\n\\newcommand{\\Orb}[2][{}]{[#2]_{#1}}\n\\newcommand{\\operatorname{Prim}}{\\operatorname{Prim}}\n\\newcommand{\\Stabess}[1]{\\operatorname{Stab}^{\\operatorname{ess}}(#1)}\n\\newcommand{\\ensuremath{\\lvert}}{\\ensuremath{\\lvert}}\n\\newcommand{\\ensuremath{\\rvert}}{\\ensuremath{\\rvert}}\n\\newcommand{\\ensuremath{\\lVert}}{\\ensuremath{\\lVert}}\n\\newcommand{\\ensuremath{\\rVert}}{\\ensuremath{\\rVert}}\n\\newcommand{\\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}}}{\\mathbin{\\scalebox{1.2}{\\ensuremath{\\cap}}}}\n\\newcommand{\\mathbin{\\scalebox{1.2}{\\ensuremath{\\cup}}}}{\\mathbin{\\scalebox{1.2}{\\ensuremath{\\cup}}}}\n\\newcommand{\\ensuremath{{}_s{\\times}_r\\,}}{\\ensuremath{{}_s{\\times}_r\\,}}\n\\newcommand{\\restr}[1]{\\ensuremath{\\vert_{#1}}}\n\\newcommand{\\bigrestr}[1]{\\ensuremath{\\big\\vert_{#1}}}\n\\newcommand{\\ensuremath{\\operatorname{Tr}_T^\\omega}}{\\ensuremath{\\operatorname{Tr}_T^\\omega}}\n\\newcommand{\\ensuremath{\\tau}}{\\ensuremath{\\tau}}\n", "meta": {"timestamp": "2021-09-28T02:32:22", "yymm": "2109", "arxiv_id": "2109.02583", "language": "en", "url": "https://arxiv.org/abs/2109.02583"}}
{"text": "\\section{Introduction}\nIn this paper, we focus on the node-level feedback for Independent Cascade (IC) Models \\citep{kempe2003maximizing} and present a detailed analysis.  These models have been adopted widely for investigating complex networks such as social networks. \n\nIn an IC model on a network graph, all nodes are either \\emph{active} or \\emph{inactive}. A node remains active once it is activated. Influence spreads over the network through a sequence of infinitely many discrete time steps.  A set of nodes called a \\emph{seed set} may be activated at time step $0$. At time step $t+1$, nodes that are activated at time $t$ attempt to activate their inactive neighbors independently with particular probabilities. A node that is activated at $t$ can only make at most one attempt to activate a neighbor through an edge. This attempt occurs at time step $t+1$. If the activation succeeds, the neighbor becomes active and will attempt to activate its own neighbors at time step $t+2$. If the activation fails, this neighbor will never be activated again by this node. The IC model captures the spread of influence from a seed set over the complex network.  The spread of influence terminates when all  nodes have been activated, or all nodes have used their attempt to activate their followers.   \n\nWe consider $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, a directed (undirected) graph.  Let $\\mathcal{V}$ be the set of all nodes and $\\mathcal{E}$ be the collection of all directed (undirected) edges. \nThe IC model can be seen as equivalent to a \\emph{coin flip} process \\citep{kempe2003maximizing}. To be specific, the seed set $S$ is activated at the beginning of diffusion. Then,  a Bernoulli random variable $\\textbf{y}(e) \\sim \\text{Bern}(p_{e})$ is independently sampled on each edge $e\\in \\mathcal{E}$ with probability $p_{e}$. If we consider the subgraph consisting of edges with positive flips ($\\textbf{y}(e) = 1 $) along, a node $v \\in \\mathcal{V} \\setminus S$ is activated during this diffusion process if it is connected with any node $u \\in S$ through a directed path within this subgraph. \n\nWhen the influence probabilities are known, the offline solution to IC problems has been well studied \\citep{kempe2003maximizing}. However, in most real-world scenarios, the influence probabilities are not readily available and must be learnt. \nLearning the diffusion probabilities is challenging. Online Influence Maximization (OIM) considers the scenario where an agent is actively selecting seed sets, observing diffusion outcomes, and learning the influence probabilities simultaneously in multiple learning rounds. In the literature, most works assume the access to the realizations of edge activations during the diffusion process \\citep{wen2017online,OIM}, called \\emph{edge-level feedback}. In this case, for any activated node, the agent has explicit information about which neighbor triggerred this activation. Edge-level feedback is widely used in investigating social networks but turns out to be unsuitable for some important applications where edge-level information is unavailable. \nWe focus on \\emph{node-level feedback} models  \\citep{goyal2010learning,saito2008prediction,netrapalli2012learning,vaswani2015influence}, in contrast to edge-level feedback,  where the agent can only observe the status of nodes rather than edges. \n\nNode-level feedback models are common in real-world applications because of the lack of edge-level information. \nFor instance, consider the spread of a rumor on a social network. People might search for information on the network, then pass this information on offline or over private channels, rather than visibly over the network. To study the spread of rumors over the social network, we might only be able to find when and what a person searched, instead of precisely who influenced each person to conduct such a search.\n\n \nAnother example arises in  online streaming platforms, such as Netflix. A high quality movie prompts a user to  watch other films with similar characteristics. Nevertheless, due to time constraints, the user might not watch another movie immediately, even if the user is interested.  \n When the user watches it a few days later, all the related movies previously watched by the user might be attributed an influence in triggering the current selection.  Ultimately, only node-level information is available. \n\nDespite the importance of node-level feedback models when investigating the spread of influence, it is challenging to develop an analytical framework even to estimate influence probabilities \nin these models. To illustrate the difficulty, suppose a set of parent nodes $\\mathcal{B}$ attempt to activate the same follower node $v$.\nUnder the IC model, the aggregated activation probability can be expressed as\n$$\np(\\mathcal{B},v) = 1- \\prod_{u\\in \\mathcal{B}} ( 1- p_{u,v}).\n$$\nUnfortunately, the resulting aggregated probability is nonconvex for most of the existing family of parametric functions, including both the linear and exponential families \\citep{mccullagh2019generalized}. By using the log-transformation $z_{u,v} = -\\log(1-p_{u,v})$, \n\\cite{vaswani2015influence} established a framework  under which the likelihood function becomes convex and the maximum likelihood estimator could be solved numerically by conducting first-order gradient descent. \nNevertheless, this transformation does not belong to any of the above families, so that no theoretical support can be established naturally for the performance of the estimators using the existing methods established in \\cite{abbasi2011improved,li2017provably}. \n\n  The few available existing studies of node-level feedback have focused on developing heuristics, such as Partial Credits \\citep{goyal2010learning} or Expectation Maximization (EM) \\citep{saito2008prediction}, to obtain estimators for the true activation probabilities. Apart from them, \\cite{vaswani2015influence} proposed a gradient-descent algorithm to find the maximum likelihood estimator. However, the theoretical performance of the maximum likelihood estimator is still lacking investigation.  In particular, a confidence-region guarantee of the maximum likelihood estimator has not been established. \n  \nMoreover, under node-level feedback, when more than one edge attempt to activate the same follower node,  \n  we could only observe their combined effect rather than the outcome on every edge.  For this reason, in some extreme cases where multiple edges sharing a common follower tend to be activated together, classical learning algorithms such as UCB and Thompson Sampling can only find an accurate estimation of the aggregated probabilities,  rather than that of every single edge.  Meanwhile, to obtain an approximate solution, existing algorithms require the influence probabilities on every edge as inputs rather than the aggregated probabilities. Thus, well-designed learning algorithms that \n  use node-level feedback instead of edge-level information are required for such models. \n\n\nIn this paper, for an edge $e\\in \\mathcal{E}$ with feature $x_e$, we assume that the influence probability takes the form $p_e = 1- \\exp(-x_e^\\top \\theta^*)$.  This form was first  proposed in \\cite{netrapalli2012learning} and is widely adopted in node-level learning \\citep{vaswani2015influence}. In this scenario, the likelihood function remains concave so that a global maximizer can be obtained.  \n\n\\textbf{Contributions and Paper Organization.} \nOur contributions to node-level online learning are two-fold. \nFirst, given node-level observations, we are the first to show how to construct a confidence region around the maximum likelihood estimator.  This problem has not previously been tackled, because of the nonlinearity and other complex intrinsic mathematical structures of the resulting  likelihood function. \n\nThen, using the confidence region result, we propose an online learning algorithm that  learns influence probabilities efficiently and selects seed sets simultaneously. We prove that our algorithm achieves an $\\tilde \\mathcal{O}(\\sqrt{T})$ scaled cumulative regret, which matches the one established in edge-level feedback models \\citep{wen2017online} despite having less information available.  This is also the first problem-independent regret bound for node-level IC problems.\n\nThe rest of this paper is organized into four sections. In Section \\ref{sec:review}, we review the literature related to IM. In Section \\ref{sec:confidence_region}, we construct a confidence region around the maximum likelihood estimator with node-level observations. In Section \\ref{sec:node_learning}, we propose an online learning algorithm and establish an upper bound for the worst-case theoretical regret. We conclude this paper in Section \\ref{sec:conclusion}. \n\n\\section{Literature Reviews}\\label{sec:review}\nIndependent Cascade (IC) and Linear Threshold (LT) \\citep{kempe2003maximizing} are two models for Influence Maximization (IM) that are widely studied. Classical IM problems consider the spread of influence over a network, where a unit reward is obtained when a node is influenced. Under a cardinality constraint, IM aims at finding a seed set $S$ so that the expected reward or the expected number of influenced nodes is maximized. Although it is NP-hard to the optimal seed $S$, under both IC and LT with cardinality constraints, \\cite{kempe2003maximizing} showed that the objective function is monotone and sub-modular with respect to $S$. Thus, $(1-1/e)$-approximation solutions could be efficiently found by greedy algorithms for both IC and LT models \\citep{Nemhauser1978}. In addition, variants of IM models have been considered by \\cite{Bharathi2007competitive,He2016RobustIM,He2018RobustIM,Chen2016RobustIM} to handle more complex real-world scenarios. \n\nIn scenarios where the influence probabilities are unknown, OIM bandits \\citep{valko2016bandits} frameworks have been proposed, where an agent needs to interact with the network actively and update the agent's estimator based on observations. Based on the observed information, OIM bandits can be classified as edge-level feedback \\citep{wen2017online,OIM} where the states of both influenced edges and nodes are observed; or node-level feedback \\citep{goyal2010learning,saito2008prediction,netrapalli2012learning,vaswani2015influence}  where only the states of nodes can be observed. Node-level feedback IC provides access to much less information compared with edge-level models.  Up to now, it has been challenging to estimate influence probabilities based on node-level observations accurately. Heuristic approaches such as Partial Credit \\citep{goyal2010learning} and Expectation Maximization \\citep{saito2008prediction} have been proposed to obtain an estimator of influence probabilities. Maximum Likelihood Estimation (MLE) has also been applied to obtain an estimator \\citep{vaswani2015influence}, but no confidence region have been established for these estimators. Several fundamental problems under node-level feedback, such as the derivation of confidence region for the maximum likelihood estimator, and the design of efficient learning algorithms, have remained largely unexplored. \n\nCompared to an IC model, in an LT model, the activation of each component $u$ contributes a non-negative weight $w_{u,v}$ to its neighbor $v$. For a specific node $v$, once the sum of all contributions $\\sum w_{u,v}$ exceeds a uniformly drawn threshold $\\delta_v \\in [0,1]$, the node becomes active. Thus, under an LT model, only node-level information can be observed. \n \\cite{vaswanilearning} developed a gradient-descent approach to obtain the maximum likelihood estimator. \n Based on the linear sum property, \\cite{yang2019online} estimated the  influence probabilities by Ordinary Least Square (OLS) and analyzed its theoretical performance. They further developed an exploration-exploitation algorithm to learn the probabilities and select seed sets. \\cite{li2020online} established a more potent approximation oracle to the offline LT solution and designed an UCB-type online learning algorithm. \nHowever, when multiple nodes $u \\in \\mathcal{B}$ attempt to influence a specific node $v$ simultaneously, unlike the simple linear sum $ \\sum_{u \\in \\mathcal{B}} w_{u,v}$ in LT, the aggregated influence probability in IC models takes a complicated non-linear form $p(\\mathcal{B},v) = 1- \\prod_{u \\in \\mathcal{B}} (1- p_{u,v})$ so that OLS is not applicable. MLE approaches have been proposed to deal with the non-linearity \\citep{vaswani2015influence} but no confidence-region guarantees have yet been established. We are the first to provide a confidence-region guarantee on the maximum likelihood estimator, design online learning algorithm, and provide $\\tilde \\mathcal{O}(\\sqrt{T})$ regret under node-level feedback IC models. \n \n \nFurthermore, OIM falls in the Combinatorial Multi-Armed Bandit (CMAB) regime, where each arm is a combination of multiple selected items.\n CMAB has found widespread applications in the real-world, such as assortment optimization \\citep{agrawal2019mnl,agrawal2017thompson,oh2021multinomial}, and product ranking \\citep{chen2020revenue}. Most CMAB models assume \\emph{semi-bandit} feedback, where the agent can observe the outcome of every item within the selected arm \\citep{gai2012combinatorial,chen2013combinatorial,kveton2015tight,chen2016combinatorial,Wang2017ImprovingRB}, or \\emph{partial-feedback}, where the agent can only observe a subset of the selected items \\citep{kveton2015combinatorial,combes2015combinatorial,katariya2016dcm,zong2016cascading,cheung2019thompson}. \n \nUnder the  online adversarial setting, more generic models have been studied by \n\\cite{mannor2011bandits, audibert2014regret}. \nIn addition, OIM is closely related to Cascading Bandits, which tracks back to \\cite{kveton2015cascading}, and could be treated as a special case of IC model with a chain-type graph topology.\n\n \n \n \n\n\n\n\n\\section{Confidence Ellipsoid under Node-level Feedback}\\label{sec:confidence_region}\nDue to the non-convex structure of the aggregated probability, the influence probabilities are hard to estimate under node-level feedback. \nThis section starts from the MLE and converts the non-convex log-likelihood into a convex one by log-transformation. After such a transformation, \nthe resulting likelihood function does not belong to any of the existing families of distributions that have been studied,  and several inherent technical challenges arise when analyzing the maximum likelihood estimator's performance. We develop a novel approach to tackle each of these challenges and build up a framework to investigate maximum likelihood estimator performance under node-level feedback.\n\n\n\n\\subsection{Maximum Likelihood Estimation}\\label{sec:MLE}\nHere, we analyze the MLE of the influence probabilities following the framework set up by \\cite{netrapalli2012learning,vaswani2015influence}. \nFor any node $v$, let $\\cN^{in}(v) = \\{u: (u, v) \\in \\mathcal{E}\\}$ be the collection of its parent nodes. Consider a diffusion process, node $v$ is \\emph{observed} at time step $t$ if any of its parent nodes are activated at time step $t-1$, and \\emph{unobserved} otherwise. \nUnder a node-level feedback IC model,\nat a given time step $t$, let $\\mathcal{B} \\subset \\cN^{in}(v)$ be the collection of parent nodes of $v$ activated at time step $t-1$, every node $u \\in \\mathcal{B}$\nattempts to activate $v$ simultaneously and independently, with only the status of $v$ being observable. In particular, we can  only see whether $v$ remains inactive or not. Note that when $\\mathcal{B} = \\emptyset$, there can be no attempt to activate $v$. \n\nIn contrast to an edge-level-feedback model, where we know the activation status of every single edge, much information is lost in the node-level-feedback model. Specifically,\nif $v$ remains inactive, none of the nodes $u \\in \\mathcal{B}$ successfully activated it.  If $v$ becomes active, we do not know precisely which node led to this activation. Thus, we can only analyze the diffusion process by seeing the combined effect of the node set $\\mathcal{B}$. \nWe denote by $p_{\\min}: = \\min_{e \\in \\mathcal{E}} p(e) >0$ the minimum activation probability over all edges in $\\mathcal{G}$, and denote by $\\| \\cdot \\|$ the Euclidean-norm $\\| \\cdot \\|_2$ unless otherwise specified.\n\n\nWe consider the log-transformation and define by $m(z) := 1-\\exp(-z)$ for ease of notation. For every edge $e = (u,v) \\in \\mathcal{E}$ with activation probability $p_{u,v} = 1-\\exp(-z_{u,v})$, we assume that there exists an unknown vector $\\theta^* \\in \\mathbb{R}^d$ such that $ z_{u,v}= x_{u,v}^\\top \\theta^*$, where $x_{u,v}$ is the feature this edge. \nEquivalently, we have $ p_e = 1- \\exp(- z_e) = 1- \\exp(-x_e^\\top \\theta^*)$. \n This form of influence probability generalizes the framework set up in \\cite{netrapalli2012learning,vaswani2015influence}. Meanwhile, when the influence probabilities are small, it is easy to see that $p_e = 1-\\exp(-x_e^\\top\\theta^*)\\approx x_e^\\top \\theta^*$ so that it is a good approximation of the linear influence probability $p_e = x_e^\\top\\theta^*$ considered in \\cite{wen2017online}. \n\n\n\nWe index every edge $e \\in \\mathcal{E}$ as $e_1, \\cdots, e_{|\\mathcal{E}|}$. Note that in the \\emph{tabular} case where edge features are unknown, we denote by $d_k$ the in-degree of the follower node that edge $e_k$ is pointing to,\nand then set $x_{e_k} = (\\underbrace{0,0,\\cdots,0}_{k-1\\,\\,0' s},1/d_k,0,\\cdots, 0) \\in \\mathbb{R}^{|\\mathcal{E}|}$. \nThat is, the $k$-th components of $x_{e_k}$ is $1/d_k$ and other components are zero.\nFor any node set $\\mathcal{B}$, let $x(\\mathcal{B},v) := \\sum_{u \\in \\mathcal{B}} x_{u,v}$, by setting edge features in this default way,  it ensures that \n$\\| x(\\mathcal{B},v) \\|\\leq \\sum_{u \\in \\mathcal{B}} \\| x_{u,v}\\| \\leq 1$. Then, we can derive a tractable maximum likelihood estimator for the influence probabilities of every edge in the absence of any given feature information on the edges. \n\n\n\nGiven any node set $\\mathcal{B}$, we have \n\\begin{align*}\np(\\mathcal{B},v) :=  1-\\prod_{u \\in \\mathcal{B}} \\Big ( 1- m(x_{u,v}^\\top \\theta^*) \\Big ) = 1- \\exp \\Big (-\\sum_{u \\in \\mathcal{B}} x_{u,v}^\\top \\theta^* \\Big)  = m \\Big (x(\\mathcal{B},v)^\\top \\theta^* \\Big).\n\\end{align*}\nThus, the effect of node set $\\mathcal{B}$ can be equivalently expressed as that of a \\emph{hyper-edge} with feature $x(\\mathcal{B},v)$. For ease of presentation, we adopt this hyper-edge notation throughout this paper.  \n\nGiven the observations of $n$ hyper-edges $x_1,\\cdots, x_n$, we denote by $Y_1,\\cdots, Y_n$ their corresponding realizations, where $Y_i = 1$ if $x_i$ successfully activates its follower node, and $Y_i = 0$ otherwise. The likelihood function for the sequence of events can be written as: \n$$\nL_n(\\theta;Y_1,\\cdots, Y_n) = \\prod_{i=1}^n m(x_i^\\top\\theta)^{Y_i} \\Big (1- m(x_i^\\top\\theta)\\Big )^{1- Y_i},\n$$\nwith the log-likelihood function being \n\\begin{equation}\\label{eq:log-likelihood}\nl_n(\\theta;Y_1,\\cdots, Y_n) = \\sum_{i=1}^n \\Big ( Y_i \\log m(x_i^\\top\\theta) +(1- Y_i) \\log (1- m(x_i^\\top\\theta))\\Big ).\n\\end{equation}\nTaking derivatives with respect to $\\theta$, we have\n\\begin{equation}\\label{eq:gradient}\n    \\nabla l_n(\\theta;Y_1,\\cdots, Y_n) = \\sum_{i=1}^n x_i (\\frac{Y_i}{m(x_i^\\top\\theta)} - 1),\n\\end{equation}\nand \n\\begin{equation}\\label{eq:hessian}\n    \\nabla^2 l_n(\\theta;Y_1,\\cdots, Y_n) = -\\sum_{i=1}^n x_i x_i^\\top \\frac{Y_i \\exp(-x_i^\\top\\theta)}{m^2(x_i^\\top\\theta)}.\n\\end{equation}\n Here, the Hessian is negative semi-definite, which implies the concavity of the log-likelihood function. Hence, first-order methods such as gradient ascent lead to convergence of the maximum likelihood estimators.  We adopt the gradient ascent method to derive the maximum likelihood estimator of diffusion probabilities.\n\n\\subsection{Assumptions and Preliminaries}\nLearning the diffusion probabilities for an IC model is challenging, especially when the agent can only make observations at the node level. Existing works have focused on developing heuristics to obtain an estimator for the true activation probability \\citep{goyal2010learning,saito2008prediction,vaswani2015influence}.\nFurthermore, because of the nonlinear aggregated influence probability that IC models preserve, the OLS approach adopted in LT models cannot be applied.  A new method for learning the diffusion probabilities must be developed instead.\n\n\n\n\n Throughout this paper, we impose the following normalization assumption on all possible combinations of hyper-edges. \n\\begin{assumption}\\label{assumption:feature_norm}\nFor every $v \\in \\mathcal{V}$ and every $\\mathcal{B} \\subset \\mathcal{N}^{in}(v)$, $\\| \\sum_{u\\in \\mathcal{B}} x_{u,v}\\|\\leq 1$. \n\\end{assumption}\nNote that this assumption can always be satisfied by re-scaling on the feature space, and also holds under our setting of edge features in Section \\ref{sec:MLE} for the tabular case in the absence of feature information.\nLet $\\mathbb{B}_1(\\theta^*) := \\{\\theta: \\| \\theta - \\theta^*\\| \\leq 1 \\}$ denote the unit ball around $\\theta^*$.\nLet $\\kappa > 0 $ be the minimum value of $\\exp(-x^\\top\\theta)/m^2(x^\\top\\theta)$ over the unit ball $\\mathbb{B}_1(\\theta^*)  := \\{\\theta: \\| \\theta - \\theta^*\\| \\leq 1 \\}$ around $\\theta^*$ for any $x \\in \\mathbb{R}^{d}$ such that $\\|x \\|\\leq 1$.  That is, \n\\begin{equation}\\label{def:kappa}\n  \\kappa:= \\inf \\Big \\{ \\frac{\\exp(-x^\\top\\theta)}{m^2(x^\\top\\theta)} \\Big | \\|\\theta - \\theta^* \\|\\leq 1 , x\\in \\mathbb{R}^d,\\| x\\|\\leq 1 \\Big \\}.  \n\\end{equation}\nNote that $\\kappa >0$ holds by the fact that $\\exp(-x^\\top\\theta)/m^2(x^\\top\\theta) >0$ for all $\\| \\theta \\| < \\infty$ and $\\|x \\|\\leq 1$. Then for all hyper-edge $e$ with feature $x_e$, under Assumption \\ref{assumption:feature_norm} that $\\|x_e \\| \\leq 1$, it is easy to see $\\exp(-x_e^\\top\\theta)/m^2(x_e^\\top\\theta) \\geq \\kappa$.\n\nRecall the log-likelihood \\eqref{eq:log-likelihood}, its first-order derivative \\eqref{eq:gradient}, and Hessian \\eqref{eq:hessian}. \nClearly, in the first-order derivative \\eqref{eq:gradient}, a node provides information about the influence probabilities as long as it is observed. Nevertheless, one can also see that only data from hyper-edges that successfully activate their follower nodes are included in the Hessian \\eqref{eq:gradient}.\nLet $\\hat \\theta_n$ be the maximum likelihood estimator to \\eqref{eq:log-likelihood},\nto better investigate its performance, we\ndenote by\n\\begin{equation}\\label{def:V}\n \\mathbf V_n := \\sum_{i=1}^n x_i x_i^\\top Y_i,\\   \\text{ and } \\ \\mathbf M_n := \\sum_{i=1}^n x_i x_i^\\top. \n\\end{equation}\nHere, $\\mathbf V_n$ and $\\mathbf M_n$ are two positive semi-definite matrices containing successful activations only, and all activations, respectively, so that both are essential in establishing confidence regions as we will discuss presently.\n\n\\textbf{Importance of $\\mathbf V_n$.}\nFirst, for $\\theta \\in \\mathbb{B}_1(\\theta^*) $, we have \n$ - \\nabla^2 l(\\theta)  \\succeq \\sum_{i=1}^n x_i x_i^\\top Y_i \\kappa =  \\kappa \\mathbf V_n$. Thus, it is possible to provide a guarantee of the local concavity of the log-likelihood function \\eqref{eq:log-likelihood} around the estimator $\\hat \\theta_n$ in terms of $\\mathbf V_n$.  The convexity, in turns, determines the ``size'' of the confidence region around the estimator.\n\nTo do this, for any vector $z \\in \\mathbb{R}^d$ and symmetric matrix $V \\in \\mathbb{R}^{d \\times d}$, we define a $V$-norm  as $\\|z \\|_{V} = \\sqrt{z^\\top V z}$.  Based on the form of the Hessian derived \\eqref{eq:hessian}, it is natural to assess the difference $\\hat \\theta_n - \\theta^*$ by whether or not $\\theta^*$ falls in the region $\\mathcal{C}_n = \\{\\theta: \\|\\theta - \\hat \\theta_n\\|_{\\mathbf V_n}  \\leq c_n \\}$ with a certain probability.  By the eigenvalue decomposition, it is easy to see that $\\mathcal{C}_n$ is contained in an Euclidean ball with radius $r = c_n/\\sqrt{\\lambda_{\\min}(\\mathbf V_n)}$, namely $\\mathbb{B}_r(\\hat \\theta_n) = \\{ \\theta : \\|\\theta - \\hat \\theta_n\\|^2 \\leq c_n^2/\\lambda_{\\min}(\\mathbf V_n)\\}$.  Instead of using this standard ball, we find it convenient to use the $\\mathbf V_n$-norm ball.  The latter is an ellipsoid and provides better estimation in the directions with larger eigenvalues.\n\n\n\\textbf{Importance of $\\mathbf M_n$.}\nConsidering the $\\mathbf V_n$ distance alone is insufficient to measure $\\hat \\theta_n - \\theta^*$ accurately. By definition \\eqref{def:V}, $\\mathbf V_n = \\sum_{i=1}^n x_i x_i^\\top Y_i$ is the collection of feature information from successful edges, which involves randomness due to the cascade process. To construct a $\\mathbf V_n$-ball as the confidence ellipsoid, we must investigate the relationship between $\\mathbf V_n$ and its expectation. Consider the expectation $\\mathbb{E}[\\mathbf V_n]$, we have $\\mathbb{E}[\\mathbf V_n] = \\mathbb{E} [ \\sum_{i=1}^n x_i x_i^\\top Y_i] = \\sum_{i=1}^n x_i x_i^\\top  p_i \\succeq p_{\\min} \\sum_{i=1}^n x_i x_i^\\top = p_{\\min} \\mathbf M_n$,\n with $p_{\\min}: = \\min_{e \\in \\mathcal{E}} p(e)$ being the minimal activation probability over all edges. We conclude that $\\mathbb{E}[\\mathbf V_n] \\succeq p_{\\min} \\mathbf M_n$. In this way, $\\mathbf M_n$ is linked with the expectation of $\\mathbf V_n$.\n \n\n\\textbf{Bridge connecting $\\mathbf M_n$ and $\\mathbf V_n$.}\nAs stated,\ninvestigating the relationship between $\\mathbf V_n$ and $\\mathbf M_n$ is crucial to our confidence region analysis. In most of the existing literature, such as \\cite{li2017provably}, it has not been necessary to explore this relationship.  The reason is that in past models, $\\mathbf V_n$ is defined as $\\mathbf V_n := \\sum_{i=1}^n x_i x_i^\\top$, unlike the IC model where $\\mathbf V_n = \\sum_{i=1}^n x_i x_i^\\top Y_i$. Given the observed edges $x_1,\\cdots, x_n$, in their models, $\\mathbf V_n$ is a deterministic quantity so that $\\mathbf V_n = \\mathbb{E}[\\mathbf V_n]$.  Thus it is unnecessary to explore $\\mathbf V_n$ and its expectation, making their analysis much less challenging.\n\nTo relate $\\mathbf M_n$ and $\\mathbf V_n$, we consider the following Semi-Definite Program (SDP): \n\\begin{equation} \\label{prob:sdp_rho}\n        \\rho_n^* : = \\max \\{ \\rho | \\mathbf V_n - \\rho \\mathbf M_n \\succeq 0 \\},\n\\end{equation}\nin order to obtain $\\mathbf M_n \\succeq \\mathbf V_n \\succeq \\rho_n^* \\mathbf M_n $. With such a relationship, $\\lambda_{\\min}(\\mathbf M_n)$ and $\\rho_n^* \\lambda_{\\min}(\\mathbf M_n)$ serve as the upper and lower bounds for $\\lambda_{\\min}(\\mathbf V_n)$, respectively, which play substantial roles in the forthcoming analysis. \n\n\\textbf{Sub-Gaussian randomness in edge realizations.}\nFinally, for ease of notation, we denote by $\\epsilon_i = Y_i - m(x_i^\\top\\theta^*)$. That is, \n\\begin{equation*}\n\\epsilon_i = \\begin{cases}\n1- m(x_i^\\top\\theta^*) & \\text{ with prob. } m(x_i^\\top\\theta^*), \\\\\n-m(x_i^\\top\\theta^*) & \\text{ with prob. } 1- m(x_i^\\top\\theta^*).\n\\end{cases}\n\\end{equation*}\nIt is easy to see that $\\epsilon_i$ is a sub-Gaussian random variable such that $ \\mathbb{E}[ \\epsilon_i] = 0$. In particular, for any $s > 0$, we have \n\\begin{equation}\\label{eq:subGaussian}\n    \\mathbb{E}[\\exp(s \\epsilon_i)] \\leq \\exp(\\frac{s^2}{2}).\n\\end{equation}\n\n\\subsection{Confidence Ellipsoid}\nIn the rest of this section, we show how to construct a confidence ball around the maximum likelihood estimator $\\hat \\theta_n$. Building up confidence balls for maximum likelihood estimators has drawn many researchers' attention because of its widespread application in various practical problems. \nWhen the observations are independent and identically distributed, Fisher information has been considered to be one of the most efficient tools to provide a lower bound on an unbiased estimator's variance, which further leads to a confidence region with a rather succinct form \\citep{Fisher001OA}. When the observations are dependent, which is the case for most stochastic bandit problems, it becomes hard to analyze the estimator's performance.\n \\cite{abbasi2011improved} considered the linear stochastic bandit problem and addressed the dependent observation issue\nusing a martingale approach.  They constructed a sharp confidence ball in all directions of the feature space. \\cite{li2017provably} adopted generalized linear models (GLM) and extended the linear results to an  exponential family of functions in logistic and probit regression. However, the likelihood function for node-level IC assumes a more complex structure than that found in all existing works, so that we can apply none of these approaches directly to this case. \nThe performance of the estimator $\\hat \\theta_n$ is therefore worthy of further study here.\n\n\\textbf{Comparisons with closest works.}\nLet $\\mu(x_i^\\top\\theta)$ be any non-negative function representing the activation probability for a random variable with feature $x_i$ (similar to $m(x_i^\\top\\theta)$ here).  \\cite{li2017provably}, the closest to our work,  considered a log-likelihood function whose derivative takes the form \n\\begin{equation*}\n    \\nabla l_n(\\theta) = \\sum_{i=1}^n x_i \\Big (\\mu(x_i^\\top\\theta) - Y_i \\Big ) = \\sum_{i=1}^n x_i \\Big (\\mu(x_i^\\top\\theta) -\\mu(x_i^\\top\\theta^*) -\\epsilon_i \\Big ).\n\\end{equation*}\nOur work differs from that of \\cite{li2017provably} in three respects, which we will outline here.  \n\\begin{itemize}\n    \\item \nFirst, the Hessian in the case considered by \\cite{li2017provably} is  $\\nabla^2 l_n(\\theta)  = \\sum_{i=1}^n x_i x_i^\\top \\dot{\\mu}(x_i^\\top\\theta)$, which implies $\\mathbf V_n = \\mathbf M_n$. Thus, as we mentioned earlier, they do not need to investigate the relationship between $\\mathbf V_n$ and $\\mathbf M_n$, which significantly simplifies their analysis.  \n\n\\item Secondly, let $G_n(\\theta) :=  \\sum_{i=1}^n x_i \\Big (\\mu(x_i^\\top\\theta) - \\mu(x_i^\\top\\theta^*) \\Big )$. \nClearly, $G_n(\\theta^*) = 0$ and $G_n( \\hat  \\theta_n) = \\sum_{i=1}^n \\epsilon_ix_i$. Using the fact that $G_n(\\hat  \\theta_n)$ is a sum of independent bounded sub-Gaussian random vectors, \\cite{li2017provably} established  that $\\theta^*$ falls in the ellipsoid $(\\theta -  \\hat  \\theta_n)^\\top\\mathbf M_n (\\theta - \\hat  \\theta_n) \\leq c^2$ with a certain probability, where $\\mathbf M_n = \\sum_{i=1}^n x_i x_i^\\top$ and $c>0$ is a constant.\nHowever, the above method does not directly apply to our case. If we follow that approach, after decomposing $Y_i = m(x_i^\\top\\theta^*) + \\epsilon$, we have $\\nabla l_n(\\theta) = \\sum_{i=1}^n x_i (\\frac{m(x_i^\\top\\theta^*) + \\epsilon_i}{m(x_i^\\top\\theta)} - 1)$. \nConsider the corresponding link function $ G_n(\\theta) = \\sum_{i=1}^n x_i (\\frac{m(x_i^\\top  \\theta^*)}{m(x_i^\\top\\theta)} - 1)$.  We obtain $G_n(\\theta^*) = 0$ and $G_n( \\hat  \\theta_n) = \\sum_{i=1}^n x_i \\epsilon_i/m(x_i^\\top\\hat   \\theta_n)$. It is important to note that $\\hat  \\theta_n$ is dependent on the realization of $\\epsilon_i$'s. Thus, $\\epsilon_i / m(x_i^\\top \\hat  \\theta_n)$ is no longer a sub-Gaussian random variable, and $\\mathbb{E}[  G_n(\\hat \\theta_n)]= 0$ may not hold anymore.  Thus, the approach of \\cite{li2017provably} cannot be directly applied to our problem. \n\n\\item Finally, consider the confidence ball.  In contrast to the case in \\cite{li2017provably},  $\\mathbf V_n = \\sum_{i=1}^n x_i x_i^\\top  Y_i $ is correlated with the realizations of $Y_i$'s and thus the value of derived $\\hat \\theta_n$ in our problem, as both $\\mathbf V_n$ and $ \\hat  \\theta_n$  depend on the history of observations.  The randomness of $\\mathbf V_n $ leads to a more  complex analysis as well.\n\\end{itemize}\n\\textbf{Confidence $\\mathbf V_n$-ellipsoid.}\nTo solve our problem, we propose a novel method that avoids using a link function $G(\\hat \\theta_n)$ with non-sub-Gaussian terms $\\epsilon_i/m(x_i^\\top \\hat \\theta_n)$.  Instead, we investigate more deeply the relationship between $\\mathbf V_n$ and $\\hat \\theta_n$. Our result is as follows.\n\\begin{theorem}\\label{thm:confidence_ball}\nSuppose Assumption \\ref{assumption:feature_norm} holds. Suppose there exist $\\kappa >0$ satisfying \\eqref{def:kappa} and  $\\rho >0$ such that $\\mathbf V_n \\succeq \\rho \\mathbf M_n \\succ 0$, and let $R := \\max_{e \\in \\mathcal{E}} 1/p(e)$. For any $\\delta_1 >0$ such that $\\kappa^2  \\rho^2 \\lambda_{\\min}(\\mathbf M_n) \\geq 16R^2\\Big ( d +\\log(1/\\delta_1) \\Big )$, \nwe have \n \\begin{equation} \\label{eq:confidence_ball_1}\n  \\| \\hat \\theta_n - \\theta^*\\|_{\\mathbf V_n}^2 \\leq \\frac{4R^2}{\\kappa^2 \\rho} \\Big ( \\frac{d}{2} \\log(1 + 2n/d) + \\log (1/\\delta_2)\\Big ),\n \\end{equation}\n with probability at least $1-\\delta_1 - \\delta_2$.\n\\end{theorem}\n\\proof{Proof.} To start with, we recall the  first order derivative \\eqref{eq:gradient} \n\\begin{equation*}\n    \\nabla l_n(\\theta) = \\sum_{i=1}^n x_i (\\frac{Y_i}{m(x_i^\\top \\theta)} - 1).\n\\end{equation*}\nBy setting it to zero, the maximal likelihood estimator $\\hat \\theta_n$ satisfies the following equation\n\\begin{equation}\\label{eq:theta_estimator}\n   \\nabla l_n(\\hat \\theta_n)  =  \\sum_{i=1}^n x_i (\\frac{Y_i}{m(x_i^\\top \\hat \\theta_n)} - 1) = 0.\n\\end{equation}\nIt is also easy to see \n\\begin{equation}\\label{eq:first_order_theta_ast}\n\\nabla l_n(\\theta^*) = \\sum_{i=1}^n x_i (\\frac{Y_i}{m(x_i^\\top  \\theta^*)} - 1).\n\\end{equation}\nConsider the unit ball $\\mathbb{B}_1(\\theta^*) = \\{ \\theta : \\| \\theta - \\theta^*\\| \\leq 1 \\}$ around $\\theta^*$, our analysis relies on the event that $\\{ \\hat \\theta_n \\in \\mathbb{B}_1(\\theta^*) \\}$. In what follows, we first characterize the probability that $\\hat \\theta_n \\in \\mathbb{B}_1(\\theta^*)$, and then investigate the performance of $\\hat \\theta_n$ under such condition. \n\nFor any $\\theta \\in \\mathbb{B}_1(\\theta^*)$, by the Mean Value Theorem,  there exists $\\bar \\theta := q \\theta + (1-q) \\theta^*$ for $0 < q < 1$ such that \n\\begin{equation*}\n    \\nabla l_n(\\theta) - \\nabla l_n(\\theta^*) = \\sum_{i=1}^n x_iY_i (\\frac{1}{m(x_i^\\top \\theta)}-\\frac{1}{m(x_i^\\top \\theta^*)}) = \\nabla^2 l_n(\\bar \\theta)(\\theta - \\theta^*)\n\\end{equation*}\nSince $\\bar \\theta = q \\theta + (1-q) \\theta^*$ is a convex combination of $ \\theta$ and $\\theta^*$, $\\bar \\theta$ falls in the unit ball $ \\mathbb{B}_1(\\theta^*)$ as well. By the definition of $\\kappa$ \\eqref{def:kappa}, we have $\\exp(-x_i^\\top \\bar \\theta)/ m^2(x_i^\\top \\bar \\theta) \\geq \\kappa >0$  for $i = 1,\\cdots,n$, and  $\\mathbf V_n = \\sum_{i=1}^n Y_i  x_i x_i^\\top $, which further implies \n$$\n-\\nabla^2 l_n(\\bar \\theta) = \\sum_{i=1}^n x_i x_i^\\top  Y_i \\frac{\\exp(-x_i^\\top \\bar \\theta)}{ m^2(x_i^\\top \\bar \\theta)} \\succeq \\sum_{i=1}^n x_i x_i^\\top  Y_i \\kappa = \\kappa \\mathbf V_n \\succ 0_{d\\times d} .\n$$\nIt is easy to see $(\\theta - \\theta^*)^\\top (\\nabla l_n(\\theta) - \\nabla l_n(\\theta^*)) > 0$ for any $\\theta \\neq \\theta^*$ so that $\\nabla l_n(\\theta)$ is an injection from $\\mathbb{R}^d$ to $\\mathbb{R}^d$. \nDefine $H_n(\\theta) := \\|\\nabla l_n(\\theta) - \\nabla l_n(\\theta^*) \\|_{\\mathbf V_n^{-1}}^2 $, we have $H_n(\\theta^*) = 0$, and   \n\\begin{align*}\n H_n(\\theta) \n    & =  (\\theta - \\theta^*)^\\top \\nabla^2 l_n(\\bar \\theta) \\mathbf V_n^{-1} \\nabla^2 l_n(\\bar \\theta) (\\theta - \\theta^*)\\\\\n&    \\geq  \\kappa^2( \\theta - \\theta^*)^\\top \\mathbf V_n (  \\theta - \\theta^*)\\\\\n&     \\geq \\kappa^2  \\lambda_{\\min}(\\mathbf V_n) \\| \\theta - \\theta^*\\|^2 \\geq \\kappa^2 \\rho \\lambda_{\\min}(\\mathbf M_n) \\| \\theta - \\theta^*\\|^2,\n\\end{align*}\nwhere we use the fact that $\\nabla^2 l_n(\\bar \\theta) \\succeq \\kappa \\mathbf V_n \\succ 0$ in the first inequality and $\\mathbf V_n \\succeq \\rho \\mathbf M_n$ in the last one.\nClearly, $H_n(\\theta) \\geq \\kappa^2 \\rho \\lambda_{\\min}(\\mathbf M_n)$ for $\\| \\theta - \\theta^* \\| = 1$.\nAs $H_n(\\cdot)$ is a continuous function and $H_n(\\theta^*) = 0$, by basic topology, we have \n$$\n\\left \\{\\theta \\Big | H_n(\\theta) \\leq   \\kappa^2 \\rho \\lambda_{\\min}(\\mathbf M_n) \\right \\} \\subset \\mathbb{B}_1(\\theta^*).\n$$\nThat is, for any $\\theta $ such that $H_n( \\theta) \\leq \\kappa^2 \\rho \\lambda_{\\min}(\\mathbf M_n)$, it is contained in the unit ball $ \\mathbb{B}_1(\\theta^*)$. \nAs we aim to characterize the behavior of $\\hat \\theta_n$, our first target is to show $H_n(\\hat \\theta_n) < \\kappa^2 \\rho \\lambda_{\\min}(\\mathbf M_n)$ with high probability, which leads to $\\| \\hat \\theta_n - \\theta^*\\| \\leq 1$.\n\\\\\n\\textbf{Bound for $\\mathbb{P}(\\| \\hat \\theta_n - \\theta^*\\| \\leq 1)$: }\nConsider $H_n(\\hat \\theta_n)$. Since $\\nabla l_n(\\hat \\theta_n) = 0$, it is easy to see $H_n(\\hat \\theta_n) = \\|\\nabla l_n(\\hat \\theta_n) - \\nabla l_n(\\theta^*) \\|_{\\mathbf V_n^{-1}}^2 = \\| \\nabla l_n(\\theta^*) \\|_{\\mathbf V_n^{-1}}^2$. Using the condition $\\mathbf V_n \\succeq \\rho \\mathbf M_n \\succ 0$, we have \n $$H_n( \\hat \\theta_n) = \\|\\nabla l_n(\\theta^*) \\|_{\\mathbf V_n^{-1}}^2   = \\nabla l_n(\\theta^*)^\\top  \\mathbf V_n^{-1} \\nabla l_n(\\theta^*) \\leq \\nabla l(\\theta^*)^\\top  (\\rho \\mathbf M_n)^{-1} \\nabla l_n(\\theta^*)  =  \\|\\nabla l_n(\\theta^*) \\|^2_{\\mathbf M_n^{-1}} /\\rho. $$\n Suppose there exists an intermediate term $U_n$  such that $ \\|\\nabla l_n(\\theta^*) \\|^2_{\\mathbf M_n^{-1}} \\leq U_n \\leq \\kappa^2 \\rho^2 \\lambda_{\\min}(\\mathbf M_n)$, then we obtain  \n \\begin{equation}\\label{eq:H_n}\n  H_n( \\hat \\theta_n) \\leq \\frac{\\|\\nabla l(\\theta^*) \\|^2_{\\mathbf M_n^{-1}} }{\\rho} \\leq \\frac{U_n }{\\rho} \\leq \\kappa^2 \\rho \\lambda_{\\min}(\\mathbf M_n),    \n \\end{equation}\nwhich further implies $\\| \\hat \\theta_n - \\theta^* \\| \\leq 1$. \nWe then show the existence of such an $U_n$ in the following analysis. \n\nRecall that \n$Y_i = m(x_i^\\top \\theta^*) + \\epsilon_i$, \nwhere $\\epsilon_i$ is a zero-mean bounded sub-Gaussian random variable such that $\\epsilon_i = 1- m(x_i^\\top \\theta^*)$ with probability $m(x_i^\\top \\theta^*)$ and $\\epsilon_i = - m(x_i^\\top \\theta^*)$ with probability $1- m(x_i^\\top \\theta^*)$. Let $v_i := \\frac{Y_i}{m(x_i^\\top \\theta^*)} - 1$.  Then $v_i$ can be equivalently expressed as \n$$v_i = \\frac{m(x_i^\\top \\theta^*) + \\epsilon_i}{m(x_i^\\top \\theta^*) }- 1 = \\frac{\\epsilon_i}{ m(x_i^\\top\\theta^*)}. $$\nClearly, $v_i$ is a zero-mean bounded $1/m(x_i^\\top\\theta^*)$-sub-Gaussian random variable. Let $R = \\max_{e \\in \\mathcal{E}} 1/m(x_e^\\top \\theta^*) = 1/p_{\\min}$.  By  \\eqref{eq:subGaussian}, we have \nfor any $s>0$,\n\\begin{equation*}\n    \\mathbb{E}[\\exp(s v_i)] \\leq \\exp(\\frac{s^2 R^2}{2}), \\,\\,\\,\\text{ for }i=1,\\cdots, n.\n\\end{equation*}\nThen, by recalling \\eqref{eq:first_order_theta_ast}, it can be seen that \n$\\nabla l_n(\\theta^*) = \\sum_{i=1}^n x_i (\\frac{Y_i}{m(x_i^\\top \\theta^*)} - 1)  = \\sum_{i=1}^n x_i v_i $ is also sum of $R$-subgaussian random vectors. To ensure $\\|  \\hat \\theta_n - \\theta^*\\| \\leq 1$,\na bound on $\\| \\nabla l_n(\\theta^*)  \\|_{\\mathbf M_n^{-1}}^2$ is provided by Lemma \\ref{lemma:loose_bound} in Appendix Section \\ref{app:B5}  for $R$-subgaussian random $v_i$'s. In particular, with probability at least $1-\\delta_1$, we have \n\\begin{equation*}\n\\| \\nabla l_n(\\theta^*)  \\|_{\\mathbf M_n^{-1}}^2  = \\| \\sum_{i=1}^n x_i v_i \\|_{\\mathbf M_n^{-1}}^2 \\leq 16R^2 \\Big ( d +\\log(1/\\delta_1) \\Big ).\n\\end{equation*}\n\nClearly, by choosing any $\\delta_1 > 0 $ such that $\\kappa^2  \\rho^2 \\lambda_{\\min}(\\mathbf M_n) \\geq 16R^2\\Big ( d +\\log(1/\\delta_1) \\Big )$, and setting $U_n = 16R^2\\Big ( d +\\log(1/\\delta_1) \\Big )$ in \\eqref{eq:H_n}, \nwe obtain  \n\\begin{equation}\\label{eq:unit_prob_bound}\n\\mathbb{P} \\Big ( \\| \\hat \\theta_n - \\theta^*\\| \\leq 1  \\Big ) \\geq 1-\\delta_1.    \n\\end{equation}\n\\textbf{Confidence $\\mathbf V_n$-ellipsoid:}\nUnder the scenario  $\\| \\hat \\theta_n - \\theta^*\\| \\leq 1$, we have\n\\begin{equation}\\label{eq:confidence_region_VM}\n  \\|  \\hat \\theta_n - \\theta^*\\|_{\\mathbf V_n}^2 \\leq \\frac{H_n(\\hat \\theta_n)}{\\kappa^2}= \\frac{\\| \\nabla l_n(\\theta^*) \\|_{\\mathbf V_n^{-1}}^2}{\\kappa^2} \\leq \\frac{\\|\\nabla l_n(\\theta^*) \\|^2_{\\mathbf M_n^{-1}} }{ \\kappa^2 \\rho}.  \n\\end{equation}\nNext, we focus on the case where $\\|  \\theta_n - \\theta^*\\|^2 \\leq 1$   and provide a bound for  $\\|\\nabla l_n(\\theta^*) \\|^2_{\\mathbf M_n^{-1}}$. When $\\lambda_{\\min}(\\mathbf M_n) \\geq 1$, by using Lemma \\ref{lemma:tight_bound} in Appendix Section \\ref{app:B5} for a general $R$, we have with probability at least $1-\\delta_2$,\n$$\n\\| \\nabla l_n(\\theta^*) \\|_{\\mathbf M_n^{-1}}^2 = \\| \\sum_{i=1}^n x_i v_i \\|_{\\mathbf M_n^{-1}}^2 \\leq 4R^2\\Big ( \\frac{d}{2} \\log(1 + 2n/d) + \\log (1/\\delta_2)\\Big ).\n$$\n Putting the above inequality into \\eqref{eq:confidence_region_VM}, and applying \\eqref{eq:unit_prob_bound}, then for any $\\delta_1 > 0 $ such that $\\kappa^2  \\rho^2 \\lambda_{\\min}(\\mathbf M_n) \\geq 16R^2\\Big ( d +\\log(1/\\delta_1) \\Big )$, we obtain \n \\begin{equation*}\n    \\|\\hat \\theta_n - \\theta^*\\|_{\\mathbf V_n}^2 \\leq \\frac{4R^2}{\\kappa^2 \\rho} \\Big ( \\frac{d}{2} \\log(1 + 2n/d) + \\log (1/\\delta_2)\\Big )\n \\end{equation*}\n with probability at least $1-\\delta_1 - \\delta_2$, which completes the proof. \n \\ \\hfill\\rule[-2pt]{6pt}{12pt} \\medskip\n\n\nIn the above theorem, we establish a confidence ball around the maximum likelihood estimator $\\hat \\theta_n$ given that $\\mathbf V_n \\succeq \\rho \\mathbf M_n$, where $\\rho$ is derived by solving the SDP \\eqref{prob:sdp_rho}. To the best of our knowledge, this is the first confidence region result for node-level IC models, which provides a theoretical guarantee on the performance of maximum likelihood estimators and preserves substantial importance for real-world applications.\n\n\n\n\\section{Online Learning with Node-level Feedback} \\label{sec:node_learning}\nIn Section \\ref{sec:confidence_region}, we  establish a confidence region for the derived maximum likelihood estimator, which could  apply directly to offline parameter estimation. Despite that result, in real-world scenarios the data are not typically available as a prior and can only be collected by making decisions and interacting repeatedly with the environment. \nIn this section, we consider OIM under node-level feedback, and propose an online learning algorithm that actively learns the influence probabilities and selects seed sets simultaneously in multiple rounds. In this section, we propose an online learning algorithm and derive its theoretical worse-case regret guarantee. \n\n\\subsection{Challenges in Algorithm Design and Analysis:} \\label{sec:challenges}\nDespite \nthe confidence region established in Section \\ref{sec:confidence_region}, it remains unclear how to design an efficient online learning algorithm and analyze its theoretical performance because of the three significant challenges below:\n\\begin{itemize}\n    \\item First, the classical UCB approaches do not apply directly  to node-level feedback settings. Briefly speaking, in each learning round, classical UCB-type algorithms construct an \\emph{optimistic estimator} of the influence probability for each edge and select a seed set using the optimistically estimated influence probabilities. With edge-level feedback, each edge's estimators are updated whenever observed and become more accurate with more observations.  Nevertheless, the information structure is different under node-level feedback. In particular, as the agent can only observe the combined outcome for a set of edges, the agent's optimistic estimators for every single edge are not necessarily improved using node-level information. \n    \n    \\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.7]{node.png}\n    \\caption{An example where individual optimistic estimators cannot be improved by UCB-Type approaches.\n    }\n    \\label{fig:1}\n\\end{figure}\n\n    \n    To illustrate this, we consider a simple directed graph consisting of four nodes, $O, A, B$, and $C$, and four directed edges $(O,A), (O,B), (A,C)$, and $(A,B)$ shown in Figure \\ref{fig:1}, with $p_{O,A} =  p_{O,B} = 1, p_{A,C} = 0.6$, and $p_{B,C} = 0.3$. When node $O$ is selected as the seed set, nodes $O$ and $A$ would always be activated simultaneously at time step $t=1$. Subsequently, $A, B$ would attempt to influence their inactive neighbor $C$, with probability $p_{A,C} = 0.6$ and $p_{B,C} = 0.3$, respectively. Notably, under node-level feedback, only the combined effect of both $A$ and $B$ rather than the realizations on each edges $(A,C)$ and $(B,C)$ are observed. Suppose we were to apply classical UCB-type approaches to design online algorithms.  Optimistic estimators are required for each edge. Nevertheless, using node-level information alone, the optimistic estimators could only be improved to their true \\emph{aggregated} probability, $u_{A,C} = u_{B,C} = 0.72$, even with infinite observations.  These bounds are far from the true \\emph{individual} influence probabilities.\nWithout better knowledge of the optimistic estimators, the selected seed set will not necessarily improve, making UCB approaches inapplicable.\n    For this reason, we need to design alternative algorithms to overcome this issue significantly. \n    \n    \\item The second challenge  is the unknown distribution of $\\rho_n^*$ involved in the confidence region established by Theorem \\ref{thm:confidence_ball}. Recall that $\\rho_n^*$ is the optimal solution to Prob.\\eqref{prob:sdp_rho}. In particular, we have\n$ \\rho_n^* = \\max \\{ \\rho | \\mathbf V_n - \\rho \\mathbf M_n \\succeq 0 \\}$ \nand $\\mathbf M_n \\succeq \\mathbf V_n \\succeq \\rho_n^* \\mathbf M_n $.  \nClearly, as both $\\mathbf V_n$ and $\\mathbf M_n$ are positive semi-definite matrices, $\\rho_n^* \\geq 0$ always holds. However, the trivial solution $\\rho_n^* = 0$ to \\eqref{prob:sdp_rho} does not shed light on the relationship between $\\mathbf V_n$ and $\\mathbf M_n$. Moreover, let  $\\mathcal{C}_n$ be the confidence region constructed in  \\eqref{eq:confidence_ball_1}, as the radius of this ellipsoid is proportional to $1/\\rho_n^*$, we must investigate the distribution of $\\rho_n^*$ to ensure the boundedness of the confidence region. \n\n\\item Third, the confidence region $\\mathcal{C}_n$ \\eqref{eq:confidence_ball_1}  is constructed using feature information from \\emph{activated} hyper-edges, in particular, $\\mathbf V_n$, instead of the \\emph{full} information matrix $\\mathbf M_n$. Existing approaches in the bandits literature can only analyze the regret when we build up the confidence region $\\mathcal{C}_n$ using the full information matrix $\\mathbf M_n$ instead of $\\mathbf V_n$. It remains unclear how to investigate the performance of the online algorithm in the latter scenario. \n\\end{itemize}\nThese challenges mentioned above make node-level IC learning highly nontrivial. To address these issues, we develop a novel learning algorithm that consists of exploration and exploitation phases and study its theoretical performance. \n\n\n\\subsection{Assumptions and Preliminaries}\nRecall the Maximum Likelihood Estimation in Section \\ref{sec:confidence_region},  for each edge $e = (u,v) \\in \\mathcal{E}$ with activation probability $ p_{u,v}$, by taking the log-transformation $ x_{u,v}^\\top \\theta^* = -\\log (1-  p_{u,v})$, the activation probability on each edge can be expressed as $p_{u,v} = 1- \\exp(- x_{u,v}^\\top \\theta^*)$. \nAlso, we define $p(\\theta)$ as the collection of influence probabilities under parameter $\\theta$ such that $p_e(\\theta) = 1-\\exp(x_e^\\top  \\theta)$ for all $e\\in \\mathcal{E}$. In particular, we have the true influence probability $ p_e = p_e(\\theta^*)$. \n\nWe first introduce some mild assumptions on the characteristics of the input networks.\n\n\\begin{assumption} \\label{assump:exploration_nodes}\n    There exist $d$ edges $e^{\\circ}_i$, $1\\leq i\\leq d$ such that $X = [x_{e_1^o},\\cdots,x_{e_d^o}] $ is non-singular, with minimum singular value $\\sigma_{\\min}^o := \\sigma_d(X) > 0$  and minimum eigenvalue $\\lambda_{\\min}^o := \\lambda_{\\min} \\Big ( \\sum_{i=1}^d x_{e_i^o} x_{e_i^o}^\\top \\Big ) > 0$.\n\\end{assumption}\nA singular value decomposition is a generalization of an eigenvalue decomposition.  Every $d \\times d$ matrix $X$ can always be decomposed as $X = U \\Sigma V^T$, where both $U, V$ are unitary matrices and $\\Sigma$ is a diagonal matrix with $\\sigma_i := \\Sigma_{ii} \\geq 0$ for all $1 \\leq i \\leq d$.  The minimum singular value being positive, i.e., $\\sigma_{\\min}^o >0$, is equivalent to $X$ being invertible, or the columns of $X$ being linearly independent. \n\n\nThe above assumption can be easily satisfied by projecting the features onto a lower dimensional space.  Under this assumption, if we continue to explore the set of diverse edges consistently, the confidence region for our parameters will contract in all directions, so that our estimator of $\\theta^*$ at time $t$, namely $\\hat \\theta_t$, will converge, i.e., $\n\\hat \\theta_t \\rightarrow \\theta^\\ast$ as $t \\rightarrow \\infty$. \n\nFor any matrix $A \\in \\mathbb{R}^{d\\times d}$, we define the \\emph{spectral norm} induced by Euclidean norm $\\| \\cdot\\|_2$ as \n$$\n\\| A\\|_2 = \\sup \\{ \\|A x\\|_2: x\\in \\mathbb{R}^d, \\| x\\|_2 \\leq 1\\}.\n$$\nIndeed, for any matrix $A \\in \\mathbb{R}^{d \\times d}$, the spectral norm $\\|A \\|_2$ has the same value as its largest singular value $\\sigma_{\\max}(A)$.\nNext, we make the following boundedness assumption on edge features.\n\\begin{assumption}\\label{assump:upper_bound_sum_features}\nFor all $v \\in \\mathcal{V}$ and all $\\mathcal{B}\\subseteq \\cN^{in}(v)$,\nthere exists a constant $D > 0$ such that $\\Big \\| \\big ( (\\sum_{u\\in \\mathcal{B}} x_{u,v})(\\sum_{u\\in \\mathcal{B}} x_{u,v})^\\top \\big ) \\big ( (\\sum_{u\\in \\mathcal{B}} x_{u,v})(\\sum_{u\\in \\mathcal{B}} x_{u,v})^\\top \\big ) \\Big \\|_2 \\leq D$.\n\\end{assumption}\n As the features and parameters we consider are finite, this assumption is without loss of generality and can be satisfied with the feature space's appropriate scaling. \n\n\n\n\n\n\n\n\n\n\n\n\n\nFurthermore, \na classical IM oracle takes the influence probability on every single edge as input and greedily adds the node that maximizes marginal expected reward into the seed set $S$. Fortunately, the objective function $f(S)$ is both monotone and submodular with respect to $S$, i.e.,\n $f(S) \\leq f(T)$ and $f(T \\cup \\{v\\}) - f(T) \\leq f(S \\cup \\{v\\}) - f(S)$ for all $S \\subset T$ and $v \\notin T$, so that the greedy algorithm returns an  $(1-1/e)$-approximation to the optimum \\cite{kempe2003maximizing}.  \n\nAs discussed in Section \\ref{sec:challenges}, UCB-type algorithms construct an optimistic estimator for every single edge, which is not guaranteed to improve in the node-level setting. Consequently, the reward will not necessarily improve over the learning process. In contrast, if the estimators derived by a specific parameter $\\tilde \\theta \\in \\mathcal{C}_t$, in particular, $p_e(\\tilde \\theta)$ for all $e \\in \\mathcal{E}$, are adopted as the input, we might be able to improve our decisions by collecting more observations. This motivates us to use a more sophisticated design of oracle. \n\n\nIn what follows, we introduce a special $(\\alpha,\\beta)$-Oracle that returns an $(\\alpha,\\beta)$-approximation solution to the optimum associated with a set of influence probabilities.  To be specific, consider a graph $\\mathcal{G}$, cardinality constraint $K$, and confidence ellipsoid $\\mathcal{C}$, \nwe define\n\\begin{equation*}\n    ( S_{\\mathcal{C}}^*, \\theta_{\\mathcal{C}}^* ) = \\mathop{\\mathrm{argmax}}_{ |S| \\leq K, \\theta \\in \\mathcal{C}} f \\big (S,p(\\theta) \\big )\n\\end{equation*}\nas the optimal seed-parameter pair that yields the highest expected reward among all possible parameters $\\theta \\in \\mathcal{C}$ with cardinality no greater than $K$. \nWe assume access to the following offline approximation oracle:\n\\begin{assumption}\\label{assump:oracle}\nLet graph $\\mathcal{G}$, seed-size $K$, and confidence region $\\mathcal{C}$ be the inputs, \nthere exists an $(\\alpha,\\beta)$-Oracle \nwhich returns a solution $\\tilde S$ with cardinality $|\\tilde S| \\leq K$ such that \n\\begin{equation*}\n    \\mathbb{P} \\Big (f \\big (\\tilde S,p(\\tilde \\theta) \\big ) \\geq \\alpha \\cdot f \\big (S_{\\mathcal{C}}^*, p( \\theta_{\\mathcal{C}}^* ) \\big ) \\Big ) \\geq \\beta,\n\\end{equation*}\nfor some $\\tilde \\theta \\in \\mathcal{C}$, $\\alpha ,\\beta >0$.\n\\end{assumption}\n\n\nUnlike the UCB-type approaches discussed in Section \\ref{sec:challenges} that take an optimistic estimator for each single edge as input, the Oracle seeks a seed set $\\tilde S$ associated with an influence parameter $\\tilde \\theta$ that makes the \\emph{entire} reward function $ f \\big (\\tilde S,p(\\tilde \\theta) \\big ) $ an \\emph{optimistic} and \\emph{well-performing} estimator of the scaled optimal reward. \nTo be specific, \nfor any returned solution $\\tilde S$, we can see there exists at least one $\\tilde \\theta \\in \\mathcal{C}$ such that\n\\begin{equation*}\n    f \\big (\\tilde S,p(\\tilde \\theta) \\big ) \\geq \\alpha \\cdot f \\big (S_{\\mathcal{C}}^*, p( \\theta_{\\mathcal{C}}^* ) \\big ) \\geq  \\alpha \\cdot f \\big (S^*, p( \\theta^* ) \\big ), \n\\end{equation*}\nso that $\\alpha \\cdot f \\big (S^*, p( \\theta^* ) \\big ) $ is optimistically represented by  $f \\big (\\tilde S,p(\\tilde \\theta) \\big )$. Meanwhile, for any set of edges $b$ (hyper-edge $x_b$) being observed, \nthe aggregated diffusion probability $p(b,\\tilde \\theta)$ is always bounded by its optimistic and pessimistic estimators within confidence region $\\mathcal{C}$, i.e.,\n\\begin{equation*}\n   \\min_{\\theta \\in \\mathcal{C}} p(b,\\theta) \\leq  p(b,\\tilde \\theta) \\leq \\max_{\\theta \\in \\mathcal{C}} p(b,\\theta).\n\\end{equation*}\nUtilizing this property, the incurred loss could be further characterized with the help of the confidence region $\\mathcal{C}$, so that $ f \\big (\\tilde S,p(\\tilde \\theta) \\big ) $ is well-performing. \n\n\n\nIntuitively, to achieve such an oracle, if $\\mathcal{C}$ consists of a finite number of parameter candidate $\\theta$'s, we could implement this oracle by simply running a greedy algorithm under each $\\theta \\in \\mathcal{C}$. If $\\mathcal{C}$ is continuous, we could obtain an approximate solution by discretizing the confidence region $\\mathcal{C}$. Indeed, a similar PairOracle under the Linear Threshold models could be implemented effectively for Bipartite and Directed Acycle Graphs \\citep{li2020online}. As this paper focuses on online learning solutions, we omit  detailed discussion of this offline oracle and assume  access to such an oracle throughout this section. \n\n\n\n\\subsection{Online Learning Algorithm}\nThis part addresses the challenges in node-level learning and provides an online learning algorithm that learns influence probabilities and high-reward seed sets simultaneously.   \nOur algorithm consists of \\emph{exploration} and \\emph{exploitation} phases. We use the exploration phase to sample edges within $\\mathcal{D}^o$ to increase the feature diversity of success edges, regardless of our current estimate $\\hat \\theta_t$.  An \\emph{exploration super-round} consists of $d$ separate exploration rounds, each round selecting a single node $v_i^o := \\text{head}(e_i^o)$ for every $e_i^o \\in \\mathcal{D}^o$. \nOn the other hand, we use the exploitation phase to select good seed sets based on our current estimate $\\hat \\theta_t$, and update our estimator $\\hat \\theta_t$ based on node-level observations simultaneously.\n\nIn what follows, we provide detailed solutions to the challenges discussed in Section \\ref{sec:challenges}. \n\n\\textbf{Lower Bound on $\\rho_k^*$:} Our first task is to investigate the distribution of $\\rho_k^*$. \nRecall that $\\rho_n^*$ is the optimal solution to Prob.\\eqref{prob:sdp_rho}. \nTo further explore the distribution of $\\rho_n^*$, we provide Lemma \\ref{lemma:psd} to show that $\\rho_n^*$ is lower bounded by a positive constant with a certain probability.  This result is essential to our confidence ball analysis. We provide the detailed proof in Appendix Section \\ref{app:B1}.\n\n\n\n \\begin{lemma} \\label{lemma:psd}\n  Let $x_1,\\cdots, x_n\\in \\mathbb{R}^d$ be $n$ vectors with $d \\ll n$.  Let $Y_1,\\cdots,Y_n$ be $n$ independent binary random variables such that $P(Y_i  = 1) = p$ and $P(Y_i = 0) = 1-p$. Suppose $\\| (x_i x_i^\\top ) (x_i x_i^\\top ) \\|_2 \\leq D$ for $i=1,\\cdots, n$, \n  and let $\\lambda^* := \\lambda_{\\min}(\\sum_{i=1}^n x_i x_i^\\top )$. \nFor any $ c \\in [0,1/p)$, we have \n\\begin{equation} \\label{eq:psd_vector_sum}\n\\sum_{i=1}^n Y_i x_i x_i^\\top  \\succeq cp \\sum_{i=1}^n x_i x_i^\\top , \n\\end{equation}\nwith probability at least $1 -  d \\exp \\Big (\\frac{-(1-c)^2p^2{\\lambda^*}^2/2}{nD + (1-c)p^2 \\lambda^*/3} \\Big ) $.\n \\end{lemma}\nRecall that $p_{\\min}: = \\min_{e \\in \\mathcal{E}} p(e)$. By setting $c = \\frac{1}{2}$, the above result suggests that $\\rho_n^*$ is lower bounded by $p_{\\min}/2$ with probability at least $1 - d \\exp \\Big (\\frac{-0.25p_{\\min}^2{\\lambda^*}^2/2}{nD + 0.5p_{\\min}^2 \\lambda^*/3} \\Big )$.\n\n\\textbf{Tradeoff between Exploration and Exploitation:}\nIntuitively, to ensure that the above  probability approaches to 1 as $n \\to \\infty$, it is required that ${\\lambda^*}^2$ should grow faster than $n$.\nRecall \\eqref{def:V} that $\\mathbf M_n = \\sum_{i=1}^n x_ix_i^\\top $, we can see $\\lambda^* = \\lambda_{\\min}(\\sum_{i=1}^n x_i x_i^\\top ) = \\lambda_{\\min}(\\mathbf M_n) $.  In the next result, we rigorously show that when $\\lambda^* \\geq \\Omega (\\sqrt{n} \\log n)$, the condition $\\mathbf V_n \\succeq p_{\\min} \\mathbf M_n/2$ holds with high probability.\n\\begin{lemma} \\label{lemma:mat_concentration}\nSuppose Assumption \\ref{assump:upper_bound_sum_features} holds and $ \\Omega (\\sqrt{n} \\log n) \\leq \\lambda_{\\min}(\\mathbf M_n)$, then we have $\\mathbf V_n \\succeq  0.5 p_{\\min} \\mathbf M_n$ with probability at least $1- \\mathcal{O}(1/n^{2})$. \n\\end{lemma}\nWe provide the detailed proof in  Appendix Section \\ref{app:B2}. We note that $\\lambda^* = \\lambda_{\\min}(\\mathbf M_n) $ depends on the number of observed hyper-edges $n$ and their features.\nMotivated by this result, to ensure that  $\\lambda_{\\min}(\\mathbf M_n) \\geq \\Omega (\\sqrt{n} \\log n) $, we could conduct exploration to improve the diversity of the observed features. \nThe following lemma characterizes the relationship between $\\lambda_{\\min}(\\mathbf M_n)$ and the number of exploration super-rounds. \n\\begin{lemma}\\label{lemma:eig}\nSuppose Assumption \\ref{assump:exploration_nodes} holds, and the algorithm has run $\\tau$ exploration super-rounds $\\tau$.  Then we have \n$$\n\\lambda_{\\min}(\\mathbf M_\\tau) \\geq \\tau \\cdot \\lambda_{\\min}^o.\n$$\n\\end{lemma}\nDue to space limitation, the detailed proof is deferred to Appendix Section~\\ref{app:B3}.\nA corollary for the number of exploration rounds $\\tau$ directly follows Lemma \\ref{lemma:mat_concentration} and \\ref{lemma:eig}. \n\\begin{corollary}\\label{corollary:1}\nSuppose the algorithm has conducted $ \\tau =  \\Omega(\\sqrt{T} \\log T)$ exploration super-rounds and $T- d \\tau $ exploitation rounds, then for all $d\\tau \\leq t \\leq T$, we have $\\mathbf V_t \\succeq 0.5 p_{\\min} \\mathbf M_t $ with probability at least $1- \\mathcal{O}(\\frac{1}{T^{\\log T}})$.\n\\end{corollary}\n\n\n \n\\textbf{The Algorithm:} \nIn what follows,\nwe carefully balance the trade-off between exploration and exploitation,  and propose an online learning algorithm. We partition the time horizon into\ntwo phases, exploration and exploitation. In the exploration phase, we conduct $\\tau$ exploration super-rounds ($\\tau$ single exploration rounds for each $ x_{e_j^o} \\in [x_{e_1^o},\\cdots,x_{e_d^o}] $ ). Using Corollary \\ref{corollary:1}, we can see that the lower bound, $\\mathbf V_t \\succeq 0.5 p_{\\min} \\mathbf M_t $, is achieved  with high probability under the choice $\\tau = \\sqrt{T}\\log T$. We then conduct an exploitation phase in the remaining $T - d\\tau$ rounds.  In this phase, we  iteratively invoke the Oracle, and update our estimators $\\hat \\theta_t$ as well as the confidence region $\\mathcal{C}_t$ by using node-level observations. \n\n\n\nThe complete Two-Phase Node-level Influence Maximization (TPNodeIM) algorithm is summarized in Algorithm \\ref{alg:IM01}. Note that by contrast with the LT-LinUCB algorithm of \\cite{li2020online} for a  node-level Linear Threshold model that performs exploitation alone, our algorithm requires unique treatment to ensure that the value $\\rho_t^*$ returned by the SDP \\eqref{prob:sdp_rho} is lower bounded by a constant.\n\n\\begin{algorithm}[t]\n \\caption{Two-Phase Node-level Influence Maximization (TPNodeIM)} \\label{alg:IM01}\n\\begin{algorithmic}\n\\STATE{\\bfseries Input:}{ graph $\\mathcal{G}$, $(\\alpha,\\beta)$-Oracle, feature vector $x_e$'s, total rounds $T$, tuning parameter $\\tau$.}\n\\STATE{\\bfseries Initialization:}{ round $t \\leftarrow 0$, $\\mathbf V_0 \\leftarrow 0_{d\\times d}$, $\\mathbf M_0 \\leftarrow 0_{d\\times d}$. }\n\\STATE{\\bfseries Exploration Phase:}\n\\FOR{ $k= 1,2,\\cdots, \\tau$} \n\\STATE{Run exploration super-rounds for each edge $ x_{e_j^o} \\in [x_{e_1^o},\\cdots,x_{e_d^o}] $.}\n\\ENDFOR\n\\STATE{\nObserve feedback in past rounds.  Update $\n \\mathbf V_{d\\tau} = \\sum_{i=1}^{d\\tau}  x_i x_i^\\top  Y_i $ and $\\mathbf M_{d\\tau}  = \\sum_{i=1}^{d\\tau}  x_i x_i^\\top $ \nby \\eqref{def:V}. \n}\n\\STATE{ \nDerive MLE $\\hat \\theta_{d\\tau }$ by \\eqref{eq:gradient} \n   $  \\nabla l(\\hat \\theta_{d\\tau}) = 0$, solve SDP \\eqref{prob:sdp_rho} $\\rho_{d\\tau}^* = \\max \\{ \\rho | \\mathbf V_{d\\tau} - \\rho \\mathbf M_{d\\tau} \\succeq 0 \\} $.\n}\n\n\\STATE{\nConstruct confidence ellipsoid \\eqref{eq:confidence_ball_1} using $\\mathbf V_{d\\tau}, \\mathbf M_{d\\tau}, \\rho^*_{d\\tau}$ and $\\hat \\theta_{d\\tau}$. \n}\n\\STATE{\\bfseries Exploitation Phase:}\n\\FOR{$t=d\\tau + 1,\\cdots, T$}\n\\STATE{Set $S_t \\in Oracle(\\mathcal{G},K,\\mathcal{C}_{t-1})$ as the seed set. }\n\\STATE{Observe node-level feedback, update $\\mathbf V_t$, $\\mathbf M_t$, solve $ \\hat \\theta_t, \\rho_t^*$.}\n\\STATE{Construct confidence ellipsoid $\\mathcal{C}_t$ by \n$\\mathbf V_{t}, \\mathbf M_t, \\rho^*_t$ and $\\hat \\theta_t$.\n}\n\\ENDFOR\n \\end{algorithmic}\n\\end{algorithm}\n\\subsection{Terminology}\nWe introduce some terminology to investigate important network characteristics better. \nIn a directed graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, given any seed set $S \\subset \\mathcal{V}$, for any node $v \\in \\mathcal{V}\\setminus S$, we denote by $\\mathcal{G}_{S,v} = (\\mathcal{V}_{S,v}, \\mathcal{E}_{S,v})$ the subgraph consisting of directed paths from any seed node $s\\in {\\cal S}$ to $v$.\nMeanwhile, a set of edges $b \\subset \\mathcal{E}_{S,v}$ is said to be \\emph{relevant} to $v$ if every edge $e\\in b$ shares a same follower node $u \\in \\mathcal{V}_{S,v}$, and we denote by $\\mathcal{B}_{S,v}$ the collection of all relevant edge sets. \n\nNext we consider the topology of graph $\\mathcal{G}$.  For any given edge set $b \\in \\mathcal{B}_{S,v}$, we denote by \n $N_{S,b} :=  \\sum_{v \\in \\mathcal{V} \\setminus S} \\mathbf 1 \\{b \\text{ is relevant to } v \\text{ under } S\\}$ the number of nodes that $b$ is relevant to, and denote by $P_{S,b} := \\mathbb{P}(\\,\\, b \\text{ is observed }|\\,\\, S)$ the probability that the edge set $b$ is observed. Also, we denote by $\\mathcal{B}_{S}: = \\cup_{v \\in \\mathcal{V}} \\mathcal{B}_{S,v}$ the collection of all relevant edge sets under $S$, denote by $G_{S} :=  \\sum_{b \\in \\mathcal{B}_{S}} P_{S,b}\\cdot N_{S,b}^2$, and denote by $G^* : = \\max_{|S| \\leq K } G_S$.\n\n\\subsection{Regret Analysis} \nWe analyze the cumulative regret of Algorithm \\ref{alg:IM01}, which is defined to be the cumulative loss in reward compared to the optimal policy. The regret is incurred by  both inaccurate estimation of diffusion probability and the randomness involved in the $(\\alpha,\\beta)$-oracle. To be specific, the oracle only guarantees an $\\alpha$-approximation with probability $\\beta$, which results in an inevitable loss if the optimal solution is set as the benchmark. Thus, we adopt the $(\\alpha,\\beta)$-scaled regret proposed in \\cite{wen2017online} as our formal metric. Let $f^* = f \\big (S^*, p( \\theta^* )$ be the optimal expected influence nodes, for any seed set $S$, following this benchmark, we define $R^{\\alpha \\beta} (S) := f^* - \\frac{1}{\\alpha \\beta} \\mathbb{E}[ f \\big (S, p(\\theta^*) \\big )] $ as the expected \\emph{$(\\alpha,\\beta)$-scaled regret} with seed set $S$ and  $\\theta^*$, and denote by $R_t^{\\alpha \\beta} := R^{\\alpha \\beta}(S_t)$ the $(\\alpha,\\beta)$-scaled regret incurred in round $t$ by running Algorithm \\ref{alg:IM01}. \nNote that this $(\\alpha,\\beta)$-scaled regret reduces to the standard regret with the existence of a more potent offline oracle for which $\\alpha = \\beta = 1$. We have the following theorem providing a guarantee on the performance of Algorithm \\ref{alg:IM01}.\n\\begin{theorem}\n Suppose Assumptions \\ref{assumption:feature_norm}, \\ref{assump:exploration_nodes}, \\ref{assump:upper_bound_sum_features},\n and \\ref{assump:oracle} hold. Let $\\kappa >0$ be a constant satisfying \\eqref{def:kappa}\n, let $R := \\max_{e \\in \\mathcal{E}} 1/p(e)$, and let $\\rho = p_{\\min}/2 = \\min_{e \\in \\mathcal{E}} p(e)/2 >0$.\n  Let  $\\tau = \\max \\Big ( \\sqrt{T} \\log T, 16R^2(d +2 \\log(T) )/(\\kappa^2 \\rho^2 \\lambda_{\\min}^o)  \\Big ) $, let $c =\\sqrt{ \\frac{4R^2}{\\kappa^2 \\rho } \\Big ( \\frac{d}{2} \\log(1 + 2T |\\mathcal{E}|/d) +2 \\log (T)\\Big )}$, and let Algorithm \\ref{alg:IM01} run $T$ rounds.\n Then we have\n \\begin{equation*}\n     \\sum_{t=1}^T R_t^{\\alpha \\beta}\\leq d\\tau f^*+ \\frac{2c}{\\sqrt{\\rho} \\alpha \\beta} \\cdot  \\sqrt{(T - d \\tau)  G^*} \\sqrt{  2d |\\mathcal{E}|  \\log(1+\\frac{(T-d\\tau) |\\mathcal{E}|}{d})} +  \\mathcal{O} \\big ((f^*-K)\\pi^2/3 \\big ). \n \\end{equation*}\n \\end{theorem}\n\\textit{Proof.}\nRecall that our algorithm first conducts $\\tau =\\max \\Big ( \\sqrt{T} \\log T, 16R^2(d +2\\log(T) )/(\\kappa^2 \\rho^2 \\lambda_{\\min}^o)  \\Big ) $ exploration super-rounds ($d\\tau$ exploration rounds) with $\\rho = 0.5 p_{\\min}$.  For any exploitation round $t \\geq d\\tau +1$, we define the favorable event $\\zeta_t$ as $$\\zeta_t := \\mathbb{I} \\{ \\mathbf V_t \\succeq 0.5p_{\\min} \\cdot \\mathbf M_t\\}.$$\nBy Corollary \\ref{corollary:1}, we have that  $\\mathbb{P}(\\bar \\zeta_t)  \\leq \\mathcal{O}(1/t^2)$ for every exploitation round $t=  d \\tau+1,  d\\tau+ 2,\\cdots, T$. \n\nMeanwhile, we denote by $n_t^{\\text{ob}}$ the total number of observed hyper-edges up to round $t$. Under event $\\zeta_t$, \nby setting $\\delta_1 = \\delta_2 = 1/t^2$, we have $\\rho^2\\kappa^2 \\lambda_{\\min}(\\mathbf M_t) \\geq \\rho^2\\kappa^2 \\tau \\lambda_{\\min}^o \\geq   16R^2\\Big ( d + 2\\log(t) \\Big ) $, so that the condition in Theorem \\ref{thm:confidence_ball} is satisfied. By Theorem \\ref{thm:confidence_ball}, we obtain that $\\theta^* $ falls in the confidence ball\n$$\n  \\mathcal{C}_t : = \\left \\{ \\theta: (\\theta - \\theta_t)^\\top  \\mathbf V_t (\\theta - \\theta_t) \\leq \\frac{4R^2}{\\kappa^2 \\rho} \\Big ( \\frac{d}{2} \\log(1 + 2n_t^{\\text{ob}}/d) + 2\\log (t)\\Big )   \\right \\} \n$$\nwith probability at least $1-2/t^2$. Next, for any exploitation round $t = d\\tau +1,\\cdots,  T$, we define a more restrict favorable event $\\xi_{k}$ as\n\\begin{equation*}\n    \\xi_{t}:= \\mathbb{I} \\left\\{ \\theta^* \\in \\mathcal{C}_t, \\mathbf V_t \\succeq 0.5 p_{\\min} \\cdot \\mathbf M_t \\right\\} = \\mathbb{I} \\left\\{ \\theta^* \\in \\mathcal{C}_t \\right \\} \\cap \\zeta_t , \\label{good_event}\n\\end{equation*}\nand define $\\bar \\xi_{t}$ as its complement. Then we have \n\\begin{equation}\\label{eq:xi}\n\\mathbb{P}(\\bar \\xi_{t}) =  \\mathbb{P}(\\theta^* \\notin \\mathcal{C}_t | \\zeta_t) \\mathbb{P}(\\zeta_t) + \\mathbb{P}(\\bar \\zeta_t) \\leq \\mathbb{P}(\\theta^* \\notin \\mathcal{C}_t | \\zeta_t) + \\mathcal{O}(1/t^2) \\leq 2/t^2 + \\mathcal{O}(1/t^2) = \\mathcal{O}(1/t^2). \n\\end{equation}\nIn an exploitation round $t$, the regret $R_t^{\\alpha \\beta}$ incurred in this round is upper bounded by $\\mathbb{E}[  f(S^{opt},\\bar p) - \\frac{1}{\\alpha \\beta} (S_t, p( \\theta^* )]$ if  $\\xi_{t-1}$ holds. Otherwise, since a set of size $K$ is chosen as the seed, the regret $R_t^{\\alpha \\beta}$ is upper bounded by $(f^*-K)$. In summary, for any exploitation round $t = d\\tau +1,\\cdots, T$, we have \n\\begin{equation}\\label{eq:regret_k}\n\\mathbb{E}[R^{\\alpha \\beta}(S_t)]   \\leq \n\\mathbb{P}(\\xi_{t-1})  \\mathbb{E} \\Big [  f\\big (S^*, p(\\theta^*) \\big )  - \\frac{1}{\\alpha \\beta} f\\big (S_t, p(\\theta^*) \\big )  \\Big | \\xi_{t-1} \\Big ] + \\mathbb{P}(\\bar \\xi_{t-1}) [f^*-K]. \n\\end{equation}\nSuppose $\\xi_{t-1}$ holds, recall that $S_t$ is the solution returned by the $(\\alpha,\\beta)$-Oracle, then there exists at least one $\\tilde \\theta_t \\in \\mathcal{C}_t$ such that with probability at least $\\beta$,\n$$\nf\\big (S_t, p(\\tilde \\theta_t )\\big ) \\geq \\alpha \\cdot \\max_{|S|\\leq K, \\theta \\in \\mathcal{C}_t}  f \\big (S, p(\\theta)\\big ) \\geq  \\alpha \\cdot f\\big (S^*, p(\\theta^*) \\big ),\n$$\nwhere the last inequality holds by the fact that $\\theta^* \\in \\mathcal{C}_t$ under $\\xi_{t-1}$. This further implies \n$$\nf(S^*, p( \\theta^*)) \\leq \\frac{1}{\\alpha}f \\big (S_t, p( \\tilde \\theta_t ) \\big ) \\leq \\frac{1}{\\alpha \\beta} \\mathbb{E} \\Big [ f \\big (S_t, p( \\tilde \\theta_t ) \\big ) \\Big | \\xi_{t-1}\\Big ].\n$$\nPlugging the above inequality into  \\eqref{eq:regret_k}, we have \n\\begin{equation}\\label{eq:reg_decompose}\n\\mathbb{E}[R_t^{\\alpha \\beta}]   \\leq \n\\frac{\\mathbb{P}(\\xi_{t-1})}{\\alpha \\beta} \\cdot  \\mathbb{E} \\Big [  f \\big (S_t, p( \\tilde \\theta_t) \\big ) - f\\big (S_t,p( \\theta^*) \\big ) \\Big | \\xi_{t-1} \\Big ] + \\mathbb{P}(\\bar \\xi_{t-1}) [f^*-K].\n\\end{equation}\nFurthermore, we denote by $O_t(b)$ the event that a set of edges $b \\in \\mathcal{B}_{S_t}$ is observed in round $t$. Then, we can see that every such edge set $b$ corresponds to a hyper-edge with feature $x_b = \\sum_{e \\in b} x_e$ and activation probability $p(b,\\theta^*) = 1-\\exp(-x_b^\\top \\theta^*)$. \nSince we only have access to node-level feedback observations, given a seed set $S_t$, we characterize the differences between the expected number of nodes influenced under $\\tilde \\theta_t$ with that under the true diffusion parameter $\\theta^*$ as follows:\n\\begin{lemma}\\label{lemma:node_level_diff}\nFor any $t$, any filtration $\\mathcal{F}_{t-1}$, and seed set $S_t$ such that $\\xi_{t-1}$ holds, we have \n\\begin{equation*}\n    \\Big | f \\big (S_t, p( \\tilde \\theta_t) \\big ) - f\\big (S_t,p( \\theta^*) \\big ) \\Big | \\leq  \\sum_{v\\in\\mathcal{V}}  \\sum_{b \\in \\mathcal{B}_{S_t, v} } \\mathbb{E} \\left [\\mathbb{I} \\{O_t(b)\\} \\cdot |p (b, \\tilde \\theta_t) - p(b,\\theta^*)| \\Big| \\mathcal{F}_{t-1}, S_t \\right ],\n\\end{equation*}\nwhere $\\mathcal{B}_{S_t,v}$ is the set of relevance subgraphs $\\mathcal{G}_{S_t,v}$. \n\\end{lemma}\nThe details of this proof are provided in Appendix Section \\ref{app:B4}.\nMeanwhile, using the fact that $\\tilde \\theta_t, \\theta^* \\in \\mathcal{C}_{t-1}$ under $\\xi_{t-1}$, for any observed hyper-edge $x_b$, we have \n\\begin{equation*}\n|p(x_b,\\tilde \\theta_t) - p(x_b, \\theta^*)|  =|\\exp(x_b^\\top \\tilde \\theta_t) - \\exp(x_b^\\top  \\theta^*)| \\leq |x_b^\\top \\tilde \\theta_t - x_b^\\top  \\theta^*|  \\leq 2 c \\sqrt{x_b^\\top  \\mathbf V_{t-1}^{-1} x_b}  \\leq \\frac{2c}{\\sqrt{\\rho}} \\sqrt{x_b^\\top  \\mathbf M_{t-1}^{-1} x_b}, \n\\end{equation*}\nwhere the last inequality holds by the fact that $\\mathbf V_t \\succeq \\rho \\cdot \\mathbf M_t $ under $\\xi_{t-1}$. Combining the above inequality with Lemma \\ref{lemma:node_level_diff}, we conclude that \n\\begin{equation*}\n    \\begin{split}\n      &  \\mathbb{E} \\Big [  f\\big (S_t,p( \\tilde \\theta_t) \\big ) - f \\big (S_t,p(\\theta^*) \\big ) \\Big | \\xi_{t-1} \\Big ] \\leq \n        \\mathbb{E} \\Big [ \\sum_{v\\in \\mathcal{V}\\setminus S_t} \n        \\sum_{b\\in \\mathcal{B}_{S_t,v}} \\mathbb{I} \\{O_t(b)\\} \\cdot |p (b, \\tilde \\theta_t) - p(b,\\theta^*)|  \\Big | \\xi_{t-1} \\Big ]   \\\\\n    &   \\quad  \\leq  \\frac{2c}{\\sqrt{\\rho} }\\mathbb{E} \\Big [\n       \\sum_{v \\in \\mathcal{V} \\setminus S_t} \\sum_{b\\in \\mathcal{B}_{S_t}} \\mathbb{I} \\{O_t(b)\\} \\cdot  \n         \\sqrt{x_b^\\top  \\mathbf M_{t-1}^{-1} x_b}\n        \\Big | \\xi_{t-1} \\Big ] \\\\\n    & \\quad     =  \\frac{2c}{\\sqrt{\\rho} }\\mathbb{E} \\Big [\n    \\sum_{b\\in \\mathcal{B}_{S_t}} \\mathbb{I} \\{O_t(b)\\} N_{S_t,b}\\cdot  \n         \\sqrt{x_b^\\top  \\mathbf M_{t-1}^{-1} x_b}\n        \\Big | \\xi_{t-1} \\Big ].\n    \\end{split}\n\\end{equation*}\nSumming the above inequality over all exploitation rounds $t= d\\tau+ 1,\\cdots, T$, we have \n\\begin{equation*}\n    \\begin{split}\n       & \\sum_{t=d\\tau + 1}^T \\frac{1}{\\alpha \\beta}\\mathbb{E} \\Big [  f \\big (S_t, p( \\tilde \\theta_t) \\big ) - f\\big (S_t,p( \\theta^*) \\big ) \\Big | \\xi_{t-1} \\Big ] \\\\\n & \\quad     \\leq  \\sum_{t=d\\tau + 1}^T \\frac{2c}{\\sqrt{\\rho} \\alpha \\beta}\\mathbb{E} \\Big [\n        \\sum_{b\\in \\mathcal{B}_{S_t}} \\mathbb{I} \\{O_t(b)\\} \\cdot   N_{S_t,b}\n         \\sqrt{x_b^\\top  \\mathbf M_{t-1}^{-1} x_b}\n        \\Big  | \\xi_{t-1}\\Big ] \n         \\\\\n     & \\quad  \\leq  \\frac{2c}{\\sqrt{\\rho}  \\alpha \\beta} \\cdot \\mathbb{E} \\left [ \\sqrt{  \\sum_{t=d\\tau +1}^T \\sum_{b\\in \\mathcal{B}_{S_t}} \\mathbb{I} \\{O_t(b)\\}  N_{S_t,b}^2 }\\cdot\n       \\sqrt{ \\sum_{t=d\\tau+1}^T \\sum_{b\\in \\mathcal{B}_{S_t}} \\mathbb{I} \\{O_t(b)\\}  x_b^\\top  \\mathbf M_{t-1}^{-1} x_b\n        }\n        \\right ] \\\\\n       & \\quad  \\leq   \\frac{2c}{\\sqrt{\\rho}  \\alpha \\beta} \\cdot   \\sqrt{  \\sum_{t=d \\tau+1}^T \\mathbb{E} [ \\sum_{b\\in \\mathcal{B}_{S_t}}  \\mathbb{I} \\{O_t(b)\\}  N_{S_t,b}^2 ] }\\cdot\n       \\sqrt{  2d |\\mathcal{E}|  \\log \\big (1+\\frac{(T - d \\tau )|\\mathcal{E}|}{d}\\big )} \\\\\n     & \\quad   \\leq \\frac{2c}{\\sqrt{\\rho}  \\alpha \\beta} \\cdot  \\sqrt{(T - d \\tau)  G^*}  \\sqrt{  2d |\\mathcal{E}|  \\log \\big (1+\\frac{(T - d \\tau )|\\mathcal{E}|}{d} \\big )},\n    \\end{split}\n\\end{equation*}\nwhere the second last inequality applies standard techniques in bandits literatures (see Lemma 2 in \\cite{wen2017online}) and the fact that at most $|\\mathcal{E}|$ hyper-edges could be observed in every learning round. \nSumming \\eqref{eq:reg_decompose} over $t=d\\tau + 1,\\cdots, T$ and \napplying the above inequality, we conclude \n\\begin{equation*}\n\\begin{split}\n\\sum_{t=d\\tau+1}^T \\mathbb{E}[R_t^{\\alpha \\beta}]  & \\leq \n\\sum_{t=d\\tau + 1} ^T \\frac{\\mathbb{P}(\\xi_{t-1})}{\\alpha \\beta} \\cdot  \\mathbb{E}\\Big [  f \\big (S_t, p( \\tilde \\theta_t) \\big ) - f\\big (S_t,p( \\theta^*) \\big ) \\Big | \\xi_{t-1} \\Big ]  + \\sum_{t=d\\tau + 1} ^T \\mathbb{P}(\\bar \\xi_{t-1}) [f^*-K]  \\\\\n& \\leq  \\frac{2c}{\\sqrt{\\rho}  \\alpha \\beta} \\cdot  \\sqrt{(T - d \\tau)  G^*} \\cdot   \\sqrt{  2d |\\mathcal{E}|  \\log \\big (1+\\frac{(T - d\\tau) |\\mathcal{E}|}{d} \\big )} + \\sum_{t=d\\tau + 1} ^T \\mathbb{P}(\\bar \\xi_{t-1})(f^*-K) \n\\\\\n& \\leq   \\frac{2c}{\\sqrt{\\rho} \\alpha \\beta} \\cdot  \\sqrt{(T - d \\tau)  G^* } \\cdot   \\sqrt{  2d |\\mathcal{E}|  \\log \\big (1+\\frac{(T - d\\tau) |\\mathcal{E}|}{d} \\big )} + \\sum_{t=d\\tau + 1} ^T  2(f^*-K)/(t-1)^2 \\\\\n& \\leq   \\frac{2c}{\\sqrt{\\rho} \\alpha \\beta} \\cdot  \\sqrt{(T - d \\tau)  G^*} \\cdot   \\sqrt{  2d |\\mathcal{E}|  \\log \\big (1+\\frac{(T - d\\tau) |\\mathcal{E}|}{d}\\big )} +  \\mathcal{O} \\big ((f^*-K)\\pi^2/3 \\big ),\n\\end{split}\n\\end{equation*}\nwhere the second inequality uses  \\eqref{eq:xi} that $\\mathbb{P}(\\bar \\xi_{t})\\leq \\mathcal{O}(1/t^2)$ and the last inequality holds by the fact that $\\sum_{t=1}^\\infty 1/t^2 = \\pi^2/6$. \n\nFinally, since the regret incurred in exploration rounds $t=1,\\cdots, d\\tau$ is at most $f^*$, we obtain\n\\begin{equation*}\n    \\begin{split}\n       & \\sum_{t=1}^T R_t^{\\alpha \\beta} =  \\sum_{t=1}^{d \\tau} f^* + \\sum_{t=d\\tau+1}^T \\mathbb{E}[R_t^{\\alpha \\beta} ]  \\\\\n     & \\quad     \\leq  d\\tau \\cdot f^* + \\frac{2c}{\\sqrt{\\rho}  \\alpha \\beta} \\cdot  \\sqrt{(T - d \\tau)  G^*} \\cdot  \\sqrt{  2d |\\mathcal{E}|  \\log \\big (1+\\frac{(T - d\\tau) |\\mathcal{E}|}{d} \\big )} +  \\mathcal{O}\\big ((f^*-K)\\pi^2/3 \\big ),\n    \\end{split}\n\\end{equation*}\nwhich completes the proof. \n\\ \\hfill\\rule[-2pt]{6pt}{12pt} \\medskip \n\n\nThe above theorem implies that our algorithm achieves $\\tilde \\mathcal{O}( \\sqrt{T})$ cumulative regret for node-level IC models, which matches the regret achieved under edge-level models despite having less information available. To the best of our knowledge, this is the first problem-independent bound for node-level IC models.\nMeanwhile, we note that a comparable $\\tilde \\mathcal{O}(\\sqrt{T})$ cumulative regret for node-level LT models is achieved by LT-LinUCB of \\cite{li2020online}. Compared with LT-LinUCB, due to the unique inherent likelihood function for node-level IC models, our TPNodeIM algorithm conducts extra $\\Theta( \\sqrt{T} \\log(T))$ exploration rounds to increase the diversity of observed features and further ensure $\\mathbf V_t \\succeq 0.5 p_{\\min} \\mathbf M_t$ throughout the exploitation phase. Furthermore, we point out that both exploration and exploitation phases of TPNodeIM induce regrets of $\\tilde \\mathcal{O}(\\sqrt{T})$, which implies that the \nexploration phase does not adversely affect the total regret along the time horizon $T$.\n\n\\section{Conclusion}\\label{sec:conclusion}\nIn this paper, we study the online influence maximization under node-level feedback Independent Cascade models, where only the status of nodes instead of edges are observed. This model finds widespread applications in investigating complex real-world networks but lacks analysis due to a few challenges in constructing confidence ellipsoid and designing online algorithms. We build up the confidence ellipsoid and develop a two-phase learning algorithm TPNodeIM that effectively learns the influence probabilities and achieves $\\tilde \\mathcal{O}(\\sqrt{T})$ regret. \n\n\n\n\n\\bibliographystyle{informs2014} \n", "meta": {"timestamp": "2021-09-08T02:10:32", "yymm": "2109", "arxiv_id": "2109.02519", "language": "en", "url": "https://arxiv.org/abs/2109.02519"}}
{"text": "\\section{Introduction}\n\n\\subsection{The first models for rumor spreading}\\label{ss:intro}\n\nIn this work we contribute with the literature of mathematical models for information transmission. As far as we know many of the well-known basic models appeared in the earlier 1960. One of them is widely cited to nowadays and was communicated by Daley and Kendall in \\cite{daley_nature}. Such a model appeared as an alternative to the well-known SIR epidemic model for describing the propagation of infectious diseases. While a SIR model assumes that a population is subdivided into susceptible, infected and removed individuals, in Daley and Kendall's model, these classes are called ignorants (those not aware of the rumor), spreaders (who are spreading it) or stiflers (who know the rumor but have ceased communicating it after meeting somebody who has already heard it), respectively. Although the mechanisms for transmission are similar; indeed, ignorants in rumors are similar to susceptible in epidemics, spreaders are similar to infected cases, and stiflers correspond to removals; the difference between these two processes is in the stopping mechanisms. In the propagation of a disease an infected individual becomes removed only after a random time, which is in general independent of what happens with other individuals. On the other hand, in the transmission of a rumor a spreader stops talking about the rumor right after getting involved in a contact with another individual who already knows it. In other words, in a rumor process, a spreader becomes a stifler at a rate which depends on the number of non-ignorant individuals in the population. The communication in \\cite{daley_nature} and the results obtained in \\cite{kendall} gave rise to a theory of mathematical models for rumor transmission. For a review of some works dealing with rumors we refer the reader to \\cite[Section 1]{AECP} and the references therein.\n\nHere we deal with the simplified version of the Daley--Kendall model due to Maki and Thompson in \\cite{MT}. The Maki--Thompson model describes the spreading of a rumor on a closed homogeneously mixed population, subdivided into the three classes of individuals mentioned before: ignorants, spreaders, and stiflers. Formally, the Maki--Thompson model may be written as the continuous-time Markov chain $\\{(X^{N}(t), Y^{N}(t))\\}_{t \\in [0,\\infty)}$ which evolves according to the following transitions and rates\n\\begin{equation}\\label{eq:transMT}\n\\begin{array}{cc}\n\\text{transition} \\quad &\\text{rate} \\\\[0.1cm]\n(-1, 1) \\quad &X Y, \\\\[0.1cm]\n(0, -1) \\quad &Y (N -1 - X).\n\\end{array}\n\\end{equation}\n\nFor $t\\geq 0$, the random variables $X^{N}(t)$, $Y^{N}(t)$ and $Z^{N}(t)$ denote, respectively, the number of ignorants, spreaders and stiflers at time $t$. The basic version of the model assumes that $X^{N}(0) = N-1$, $Y^{N}(0) = 1$, $Z^{N}(0) = 0$, and $X^{N}(t) + Y^{N}(t) + Z^{N}(t) = N $ for all~$t$. Thus defined, \\eqref{eq:transMT} means that if the process is in state $(i,j)$ at time $t$, then the probabilities that it jumps to states $(i-1,j+1)$ or $(i,j-1)$ at time $t+h$ are, respectively, $i\\,j\\,h + o(h)$ and $j (N - 1 - i)\\,h + o(h)$, where $o(h)$ represents a function such that $\\lim_{h\\to 0}o(h)/h =0$. This describes the situation in which individuals interact by contacts initiated by the spreaders: the two possible transitions in \\eqref{eq:transMT} correspond to spreader-ignorant, and spreader-(spreader or  stifler) interactions. In the first case, the spreader tells the rumor to the ignorant, who becomes a spreader. The other transition represents the transformation of a spreader into a stifler after initiating a meeting with a non-ignorant. Note that the last event describes the loss of interest in propagating the rumor derived from learning that it is already known by the other individual in the meeting. This is the main difference between rumor models and mathematical models for infectious diseases. \n\nA question of interest when studying a stochastic rumor model on a finite population is what we can say about the remaining proportion of people who never hear the rumor. Note that, since the process eventually ends, this is one way of having a measure of the ``size'' of the rumor. The first rigorous results in this direction are limit theorems for the remaining proportion of ignorants when the process ends, as the population size grows to $\\infty$. It has been proved that, for both the Daley--Kendall and the Maki--Thompson models, such final  proportion of ignorants equals approximately $20\\%$, see \\cite{kendall,sudbury,watson}. In recent works, the main approach to deal with such a question for generalized rumor models is a suitable application of the theory of convergence of density dependent Markov chains. This allows to find a system of ordinary differential equations whose solution represents an approximation, for sufficiently large $N$, of a scaled version of the entire trajectories of the stochastic model. Moreover, these solutions may be seen as good approximations for the proportion of ignorants and spreaders, respectively, at time $t$, for $t>0$ and $N$ large enough. An extensive treatment of rumor models can be found in \\cite[Chapter~5]{dg}. In addition, for a deeper discussion of the use of density dependent Markov chains for studying rumor models we refer the reader to \\cite{arrudaroyal,lebensztayn/machado/rodriguez/2011a,lebensztayn/machado/rodriguez/2011b,AECP}.\n\n\n\n\\subsection{Changing the structure of the population. Why random trees?}\n\nThe Maki--Thompson model studied by the references mentioned in Subsection \\ref{ss:intro} is formulated assuming a closed homogeneously mixed population. If we represent the population with a graph, that assumption means that the considered graph is the complete graph, see Figure \\ref{fig:graphs}(a). With such an interpretation, vertices represent individuals and edges represent possible contacts between them. By considering a complete graph, the theoretical analysis of the model is simplified, in some sense, because it is enough to deal with the proportions of individuals in each one of the classes. This is why it is less difficult to deal with generalizations of the model in homogeneously mixed populations. However, when one want to consider a different structure of the population the approach should be changed and, in most cases, it depends on the considered graph. \n\n\n\\begin{figure}[h!]\n\\begin{center}\n\\subfigure[][Complete graph $\\mathbb{K}_{10}$.]{\n\n\\includegraphics[width=5.5cm]{complete.png}\n\n}\\qquad\\qquad\\subfigure[][Hypercubic lattice $\\mathbb{Z}^2$.]{\n\\begin{tikzpicture}[scale=0.7]\n\\draw (-3.5,3) -- (3.5,3);\n\\draw (-3.5,2) -- (3.5,2);\n\\draw (-3.5,1) -- (3.5,1);\n\\draw (-3.5,0) -- (3.5,0);\n\\draw (-3.5,-1) -- (3.5,-1);\n\\draw (-3.5,-2) -- (3.5,-2);\n\\draw (-3.5,-3) -- (3.5,-3);\n\\draw[dotted] (-3.8,0) -- (-4.1,0);\n\\draw[dotted] (-3.8,2) -- (-4.1,2);\n\\draw[dotted] (-3.8,-2) -- (-4.1,-2);\n\\draw[dotted] (3.8,0) -- (4.1,0);\n\\draw[dotted] (3.8,2) -- (4.1,2);\n\\draw[dotted] (3.8,-2) -- (4.1,-2);\n\\draw[dotted] (0,-3.8) -- (0,-4.1);\n\\draw[dotted] (2,-3.8) -- (2,-4.1);\n\\draw[dotted] (-2,-3.8) -- (-2,-4.1);\n\\draw[dotted] (0,3.8) -- (0,4.1);\n\\draw[dotted] (2,3.8) -- (2,4.1);\n\\draw[dotted] (-2,3.8) -- (-2,4.1);\n\n\\draw (-3,-3.5) -- (-3,3.5);\n\\draw (-2,-3.5) -- (-2,3.5);\n\\draw (-1,-3.5) -- (-1,3.5);\n\\draw (0,-3.5) -- (0,3.5);\n\\draw (1,-3.5) -- (1,3.5);\n\\draw (2,-3.5) -- (2,3.5);\n\\draw (3,-3.5) -- (3,3.5);\n\n\n\n\\filldraw (-3,3) circle (1.5pt);\n\\filldraw (-3,2) circle (1.5pt);\n\\filldraw (-3,1) circle (1.5pt);\n\\filldraw (-3,0) circle (1.5pt);\n\\filldraw (-3,-1) circle (1.5pt);\n\\filldraw (-3,-2) circle (1.5pt);\n\\filldraw (-3,-3) circle (1.5pt);\n\\filldraw (-2,3) circle (1.5pt);\n\\filldraw (-2,2) circle (1.5pt);\n\\filldraw (-2,1) circle (1.5pt);\n\\filldraw (-2,0) circle (1.5pt);\n\\filldraw (-2,-1) circle (1.5pt);\n\\filldraw (-2,-2) circle (1.5pt);\n\\filldraw (-2,-3) circle (1.5pt);\n\\filldraw (3,3) circle (1.5pt);\n\\filldraw (3,2) circle (1.5pt);\n\\filldraw (3,1) circle (1.5pt);\n\\filldraw (3,0) circle (1.5pt);\n\\filldraw (3,-1) circle (1.5pt);\n\\filldraw (3,-2) circle (1.5pt);\n\\filldraw (3,-3) circle (1.5pt);\n\\filldraw (2,3) circle (1.5pt);\n\\filldraw (2,2) circle (1.5pt);\n\\filldraw (2,1) circle (1.5pt);\n\\filldraw (2,0) circle (1.5pt);\n\\filldraw (2,-1) circle (1.5pt);\n\\filldraw (2,-2) circle (1.5pt);\n\\filldraw (2,-3) circle (1.5pt);\n\\filldraw (-1,3) circle (1.5pt);\n\\filldraw (-1,2) circle (1.5pt);\n\\filldraw (-1,1) circle (1.5pt);\n\\filldraw (-1,0) circle (1.5pt);\n\\filldraw (-1,-1) circle (1.5pt);\n\\filldraw (-1,-2) circle (1.5pt);\n\\filldraw (-1,-3) circle (1.5pt);\n\\filldraw (1,3) circle (1.5pt);\n\\filldraw (1,2) circle (1.5pt);\n\\filldraw (1,1) circle (1.5pt);\n\\filldraw (1,0) circle (1.5pt);\n\\filldraw (1,-1) circle (1.5pt);\n\\filldraw (1,-2) circle (1.5pt);\n\\filldraw (1,-3) circle (1.5pt);\n\\filldraw (0,3) circle (1.5pt);\n\\filldraw (0,2) circle (1.5pt);\n\\filldraw (0,1) circle (1.5pt);\n\\filldraw (0,0) circle (1.5pt);\n\\filldraw (0,-1) circle (1.5pt);\n\\filldraw (0,-2) circle (1.5pt);\n\\filldraw (0,-3) circle (1.5pt);\n\n\\draw [->,line width=0.4mm,white] (3.1,1.5) to [out=10, in=180]  (5.3,2);\n\n\\end{tikzpicture}}\n\n\\subfigure[][A small-world like graph.]{\n\n\\includegraphics[width=5.7cm]{small-world.png}\n\n}\\subfigure[][Cayley tree $\\mathbb{T}_2$.]{\n\n\\tikzstyle{level 1}=[sibling angle=120]\n\\tikzstyle{level 2}=[sibling angle=80]\n\\tikzstyle{level 3}=[sibling angle=60]\n\\tikzstyle{level 4}=[sibling angle=30]\n\n\\tikzstyle{edge from parent}=[segment length=0.6cm,segment angle=10,draw]\n\\begin{tikzpicture}[scale=0.45,grow cyclic,shape=circle,minimum size = 1pt,inner sep=0.8pt,level distance=15mm,\n                    cap=round]\n\\node[fill] {} child [\\A] foreach \\A in {black,black,black}\n    { node[fill] {} child [color=\\A!50!\\B] foreach \\B in {black,black}\n        { node[fill] {} child [color=\\A!50!\\B!50!\\C] foreach \\C in {black,black}\n            { node[fill] {} child [color=\\A!50!\\B!50!\\C!50!\\D] foreach \\D in {black,black}\n            { node[fill] {} }\n        }\n    }\n    };\n    \n\\node at (-0.5,0) {\\tiny ${\\bf 0}$};\n\n\\draw[dotted] (6.5,0) -- (7,0);\n\\draw[dotted] (-6.5,0) -- (-7,0);\n\\draw[dotted] (0,6.5) -- (0,7);\n\\draw[dotted] (0,-6.5) -- (0,-7);\n\\draw[dotted] (4.5,4.5) -- (4.8,4.8);\n\\draw[dotted] (4.5,-4.5) -- (4.8,-4.8);\n\\draw[dotted] (-4.5,-4.5) -- (-4.8,-4.8);\n\\draw[dotted] (-4.5,4.5) -- (-4.8,4.8);\n\\end{tikzpicture}}\n\\caption{Some families of graphs used to represent a population in rigorous mathematical models for rumor spreading.}\\label{fig:graphs}\n\\end{center}\n\\end{figure}\n\nIn a general graph the model may be defined, roughly speaking, by assuming that the set of vertices representing the population is subdivided into three classes; namely, ignorants, spreaders and stiflers. Then a spreader tells the rumor to any of its nearest ignorant neighbors at rate one, and, at the same (but could be other) rate, a spreader becomes a stifler after a contact with other nearest neighbor spreaders, or stiflers. The mathematical model, which is stochastic, is an interacting particle system. We shall formalize this type of model in Section \\ref{s:model}. The first rigorous result for a modified version of the Maki--Thompson in a non-complete graph is due to \\cite{CRS}, where the authors represent the population with an hypercubic lattice $\\mathbb{Z}^d$, for $d\\geq 2$, see Figure \\ref{fig:graphs}(b), and obtain results of phase-transition according to the values of general rates of interactions between the classes of individuals. In such a work the approach relies on coupling techniques and a combination of results coming from oriented percolation and the so-called contact process. See \\cite{grimmett} for a review of these subjects. Note that $\\mathbb{Z}^d$ is an infinite graph. On the other hand, \\cite{EAP} studies the Maki--Thompson model on a small-world graph. The graph is constructed by starting from a $k$-regular ring and by inserting, in the average, $c$ additional links in such a way that $k$ and $c$ are tuneable parameters for the population structure; see Figure \\ref{fig:graphs}(c). The authors prove that this system exhibits a kind of transition between regimes of localization (where the final number of stiflers is at most logarithmic in the population size) and propagation (where the final number of stiflers grows algebraically with the population size) depending of the network parameter $c$ being small our large enough, respectively. The approach for that analysis was, again, coupling techniques, but now combined together with the theory of branching processes. More recently, in order to understand the behavior of the model on trees, \\cite{junior} consider the generalization called the Maki--Thomposn model with $k$-stifling on infinite Cayley trees $\\mathbb{T}_d$, for $d\\geq 2$; see Figure \\ref{fig:graphs}(d). These are deterministic infinite trees such that any vertex has degree $d+1$. The theory of branching processes was used for this study as well and the main results are related to the analysis of the existence of a critical $d$ around which the rumor either becomes extinct almost-surely or survives with positive probability. In addition, the authors obtain information about the range of spreading. \nIn this work we extend the approach of \\cite{junior} to deal with stochastic rumors on random trees, which can be seen as the family trees produced by a branching process. In words, we start with one vertex which is the root of the tree and let it connect according to a given discrete distribution. Each of these neighbors (if there are any) then connect with new vertices independently with the same law, and so on forever or until some generation goes extinct. These trees are called Galton-Watson trees, or random trees for short. \nThis case gains in interest if we realize that random trees naturally appears ``embebed'' in many models of random graphs and complex networks. It is well-known that many of such models, which are more appropriated to represent a population, behave locally as trees. Up to now, stochastic rumors in random graphs or complex networks were studied mainly by mean of computational simulations or mean-field approximations. See for example \\cite{arruda3,MNP-PRE2004,moreno-PhysA2007,zanette02}. As a sideline, we point out that recent research suggest that random trees are suitable structures to represent the spreading of information between population, see for example \\cite{gleeson} where branching processes are used to model propagation of information on Twitter.  On the other hand, for a review of some long-range propagation models on (random) trees we refer the reader to \\cite{cone,cone2}.    \n\nWe analyze the existence of a critical value, depending of the law generating the tree, around which the rumor either becomes extinct almost-surely or survives with positive probability. Following \\cite{junior}, we also obtain information about the range of spreading. Our work is complemented with some examples and a discussion for further research.  \n\nThe paper is organized as follows. In Section 2 we formalize the mathematical model and present the main results. Subsection 2.1 is dedicated to a review of definitions for the model on infinite Cayley trees, and from Subsection 2.2 onward the model on random trees and all the results of our work are presented. Section 3 es devoted to the proof of our main theorems and Section 4 is left to a discussion of further research. \n\n\n\\section{The model and main results}\\label{s:model}\n\n\\subsection{The model on an infinite Cayley tree}\n\nIn this work we shall work with infinite trees $\\mathbb{T}=(\\mathcal{V},\\mathcal{E})$. As usual $ \\mathcal{V} $ stands for the set of vertices and $\\mathcal{E} \\subset \\{\\{u,v\\}: u,v \\in \\mathcal{V}, u \\neq v\\}$ stands for the set of edges. We shall abuse notation by writing $\\mathcal{V}=\\mathbb{T}$. We consider rooted trees in the sense that we shall identify one vertex as the root and denote it by ${\\bf 0}$, see Figure \\ref{fig:graphs}(d). If $\\{u,v\\}\\in \\mathcal{E}$, we say that $u$ and $v$ are neighbors, and we denote it by $u\\sim v$. The degree of a vertex $v$, denoted by $deg(v)$, is the number of its neighbors. A path in $\\mathbb{T}$ is a finite sequence $v_0, v_1, \\dots, v_n $ of distinct vertices such that $ v_i \\sim v_{i+1} $ for each $i$, and a ray in $\\mathbb{T}$ is a path with infinite vertices starting at ${\\bf 0}$. For any tree, there is a unique path connecting any pair of distinct vertices $u$ and $v$. Therefore we define the distance between them, denoted by $d(u,v)$, as the number of edges in such path. We denote by $\\mathbb{T}_d$ the infinite Cayley tree of coordination number $d+1$, with $d\\geq 2$. The notation is because the same graph is known as $(d+1)$-dimensional homogeneous tree, which is a graph with an infinite number of vertices, without cycles and such that every vertex has degree $d+1$. For each $v\\in \\mathcal{V}$ define $|v|:=d({\\bf{0}},v)$. For $ u,v \\in \\mathcal{V} $, we say that $u\\leq v$ if $u$ is one of the vertices of the path connecting ${\\bf{0}}$ and $v$; $u<v$ if $u\\leq v$ and $u\\neq v$. We call $v$ a \\textit{successor} of $u$ if $u\\leq v$ and $u \\sim v$. We denote by $\\partial \\mathbb{T}_{n}$ the set of vertices at distance $n$ from the root. That is, $\\partial \\mathbb{T}_{n}= \\{v \\in \\mathbb{T}: |v|=n\\}$. \n\n\nThe Maki-Thompson model on an infinite tree $\\mathbb{T}$ may be defined as a continuous-time Markov process $(\\eta_t)_{t\\geq 0}$ with states space $\\mathcal{S}=\\{0,1,2\\}^{\\mathbb{T}}$; i.e. at time $t$ the state of the process is some function $\\eta_t: \\mathbb{T} \\longrightarrow \\{0,1,2\\}$. We assume that each vertex $x \\in \\mathbb{T}$ represents an individual, which is said to be an ignorant if $\\eta(x)=0,$ a spreader if $\\eta(x)=1$, and a stifler if $\\eta(x)=2.$ Then, if the system is in configuration $\\eta \\in \\mathcal{S},$ the state of vertex $x$ changes according to the following transition rates\n\n\\begin{equation}\\label{rates}\n\\begin{array}{rclc}\n&\\text{transition} &&\\text{rate} \\\\[0.1cm]\n0 & \\rightarrow & 1, & \\hspace{.5cm}  n_{1}(x,\\eta),\\\\[.2cm]\n1 & \\rightarrow & 2, & \\hspace{.5cm}   n_{1}(x,\\eta) + n_{2}(x,\\eta),\n\\end{array}\n\\end{equation}\n\n\\noindent where $$n_i(x,\\eta)= \\sum_{y\\sim x} 1\\{\\eta(y)=i\\}$$ \nis the number of nearest neighbors of vertex $x$ in state $i$ for the configuration $\\eta$, for $i\\in\\{1,2\\}.$ Formally, \\eqref{rates} means that if the vertex $x$ is in state, say, $0$ at time $t$ then the probability that it will be in state $1$ at time $t+h$, for $h$ small, is $n_{1}(x,\\eta) h + o(h)$, where $o(h)$ represents a function such that $\\lim_{h\\to 0} o(h)/h = 0$. Note that the rates in \\eqref{rates} represent how the changes of states of individuals depend on the states of its neighbors. While the change of state of an ignorant is influenced by its spreader neighbors, the change of state for a spreader is influenced by the number of non-ignorant neighbors. We point out that stiflers do not interact with ignorants. See Figure \\ref{fig:realization} for an illustration of these transitions.\n\n\n\\begin{figure}[h!]\n    \\centering\n\\begin{tikzpicture}\n\n\\filldraw [black] (0,4) circle (3pt);\n\\node at (0.9,4) {\\footnotesize : ignorant};\n\n\\filldraw [red!80!black] (3,4) circle (3pt);\n\\node at (3.9,4) {\\footnotesize : spreader};\n\n\\filldraw [blue!30!gray] (6,4) circle (3pt);\n\\node at (6.7,4) {\\footnotesize : stifler};\n\\draw[gray] (-1,3) -- (9,3);\n\n\\node at (-2,-1.5) {\\footnotesize $\\bf (a)$};\n\n\n\\node[rotate=330] at (0,0) {\n\n\\begin{tikzpicture}\n\n\n\\draw[thick] (0,0) -- (2,0) -- (4,0);\n\\draw[thick] (2,0) -- (4,1.5);\n\\draw[thick] (2,0) -- (4,-1.5);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,0);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,0.5);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,-0.5);\n\\draw[thick,gray,dashed] (4,1.5) -- (4.5,1.95);\n\\draw[thick,gray,dashed] (4,1.5) -- (4,2);\n\\draw[thick,gray,dashed] (4,1.5) -- (4.5,1.5);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4.5,-1.95);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4,-2);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4.5,-1.5);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,0);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,0.5);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,-0.5);\n\n\\filldraw [red!80!black] (0,0) circle (3pt);\n\n\\draw [->,line width=0.4mm,gray] (0.1,0.2) to [out=30, in=150]  (1.9,0.2);\n\n\\filldraw [black] (2,0) circle (3pt);\n\\filldraw [black] (4,0) circle (3pt);\n\\filldraw [black] (4,1.5) circle (3pt);\n\\filldraw [black] (4,-1.5) circle (3pt);\n\\end{tikzpicture}};\n\n\\node at (6,-1.5) {\\footnotesize  $\\bf (b)$};\n\n\n\\node[rotate=330] at (8,0) {\n\n\n\n\\begin{tikzpicture}\n\n\\draw[thick] (0,0) -- (2,0) -- (4,0);\n\\draw[thick] (2,0) -- (4,1.5);\n\\draw[thick] (2,0) -- (4,-1.5);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,0);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,0.5);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,-0.5);\n\\draw[thick,gray,dashed] (4,1.5) -- (4.5,1.95);\n\\draw[thick,gray,dashed] (4,1.5) -- (4,2);\n\\draw[thick,gray,dashed] (4,1.5) -- (4.5,1.5);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4.5,-1.95);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4,-2);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4.5,-1.5);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,0);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,0.5);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,-0.5);\n\n\\filldraw [red!80!black] (0,0) circle (3pt);\n\\filldraw [red!80!black] (2,0) circle (3pt);\n\n\\draw [->,line width=0.4mm,gray] (2,0.2) to [out=90, in=180]  (3.8,1.5);\n\n\\filldraw [black] (4,0) circle (3pt);\n\\filldraw [black] (4,1.5) circle (3pt);\n\\filldraw [black] (4,-1.5) circle (3pt);\n\\end{tikzpicture}};\n\n\n\\node at (-2,-7.5) {\\footnotesize   $\\bf (c)$};\n\n\n\n\\node[rotate=330] at (0,-6) {\n\n\\begin{tikzpicture}\n\n\n\\draw[thick] (0,0) -- (2,0) -- (4,0);\n\\draw[thick] (2,0) -- (4,1.5);\n\\draw[thick] (2,0) -- (4,-1.5);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,0);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,0.5);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,-0.5);\n\\draw[thick,gray,dashed] (4,1.5) -- (4.5,1.95);\n\\draw[thick,gray,dashed] (4,1.5) -- (4,2);\n\\draw[thick,gray,dashed] (4,1.5) -- (4.5,1.5);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4.5,-1.95);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4,-2);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4.5,-1.5);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,0);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,0.5);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,-0.5);\n\n\\filldraw [red!80!black] (0,0) circle (3pt);\n\n\\draw [->,line width=0.4mm,gray] (0.1,0.2) to [out=30, in=150]  (1.9,0.2);\n\n\\filldraw [red!80!black] (2,0) circle (3pt);\n\\filldraw [black] (4,0) circle (3pt);\n\\filldraw [red!80!black] (4,1.5) circle (3pt);\n\\filldraw [black] (4,-1.5) circle (3pt);\n\\end{tikzpicture}};\n\n\n\\node at (6,-7.5) {\\footnotesize $\\bf (d)$};\n\n\n\\node[rotate=330] at (8,-6) {\n\n\\begin{tikzpicture}\n\n\n\\draw[thick] (0,0) -- (2,0) -- (4,0);\n\\draw[thick] (2,0) -- (4,1.5);\n\\draw[thick] (2,0) -- (4,-1.5);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,0);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,0.5);\n\\draw[thick,gray,dashed] (4,0) -- (4.5,-0.5);\n\\draw[thick,gray,dashed] (4,1.5) -- (4.5,1.95);\n\\draw[thick,gray,dashed] (4,1.5) -- (4,2);\n\\draw[thick,gray,dashed] (4,1.5) -- (4.5,1.5);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4.5,-1.95);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4,-2);\n\\draw[thick,gray,dashed] (4,-1.5) -- (4.5,-1.5);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,0);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,0.5);\n\\draw[thick,gray,dashed] (0,0) -- (-0.5,-0.5);\n\n\\filldraw [blue!30!gray] (0,0) circle (3pt);\n\n\\draw [->,line width=0.4mm,white] (0.1,0.2) to [out=30, in=150]  (1.9,0.2);\n\n\\filldraw [red!80!black] (2,0) circle (3pt);\n\\filldraw [black] (4,0) circle (3pt);\n\\filldraw [red!80!black] (4,1.5) circle (3pt);\n\\filldraw [black] (4,-1.5) circle (3pt);\n\\end{tikzpicture}};\n\n\\end{tikzpicture}\n\n    \\caption{Possible realization of the MT-model on a tree $\\mathbb{T}$. The vertices of the tree represent individuals belonging to one of the following tree classes: ignorants, spreaders and stiflers, which we identify by black, red, and blue vertices, respectively. (a) A spreader tells the rumor to any of its nearest ignorant neighbors at rate one. (b) After the previous interaction the contacted ignorant becomes a spreader and starts to transmit the information. (c)-(d) At the same rate, a spreader becomes a stifler after a contact with other nearest neighbor spreaders, or stiflers.}\n    \\label{fig:realization}\n\\end{figure}\n\nWe call the Markov process $(\\eta_t)_{t\\geq 0}$ the Maki-Thompson rumor model on $\\mathbb{T}$, and we abbreviate as MT-model on $\\mathbb{T}$. In addition, we refer to the case when $\\eta_0({\\bf{0}})=1$ and $\\eta_0(x)=0$ for all $x\\neq {\\bf 0}$ as the \\textit{standard initial configuration}.     \n\n\\smallskip\n\\begin{definition}\\cite[Definition 1]{junior}\nConsider the MT-model on $\\mathbb{T}$ with the standard initial configuration. We say that there is survival of the rumor if there exist a sequence $\\{(v_i,t_i)\\}_{i\\geq 0}$, with $(v_i,t_i)\\in \\mathbb{T} \\times \\mathbb{R}^+$, such that $ v_0={\\bf{0}}$, $t_0=0$, $v_{i+1}$ is a successor of $v_i$, $t_i < t_{i+1}$, and $\\eta_{t_i}(v_i)=1$, for all $i\\geq 0.$ If there is not survival, we say that the rumor becomes extinct. We denote by $\\theta(\\mathbb{T})$ the survival probability.\n\\end{definition}\n\nNote that there is survival of the rumor provided we can guarantee the existence of a ray from the root of $\\mathbb{T}$ such that all the vertices in the ray were spreaders at some time. The MT-model on an infinite Cayley tree of coordination number $d+1$ was studied in \\cite{junior}. In what follows, for simplicity, we let $\\theta(d):=\\theta(\\mathbb{T}_d)$.\n\n\\smallskip\n\\begin{theorem}\\label{thm:MTpt}\\cite[Theorem 1]{junior}\nConsider the MT-model on $\\mathbb{T}_d$ with the standard initial configuration. Then $\\theta(d)>0$ if, and only if, $d\\geq 3$. Moreover, \n$$\\theta(d) =  1-  \\psi^2,$$\nwhere $\\psi$ is the smallest non-negative root of the equation\n$$\\sum_{i=0}^{d} i! \\dbinom{d}{i}\\left(\\frac{s}{d+1}\\right)^i\\left(\\frac{i+1}{d+1}\\right)=s.$$\n\\end{theorem}\n\n\\smallskip\n\nIn words, Theorem \\ref{thm:MTpt} states that the MT-model exhibit a phase transition. Indeed, for $d=2$, we obtain that the rumor propagates, almost surely, only to a finite set of individuals. In the other cases, for $d\\geq 3$, the rumor propagates to infinitely many individuals with positive probability. We point out that although is not considered here, the case $d=1$ may be seen as the path graph with infinite vertices. In that case it is not difficult to see that the number of spreaders will be bounded from above by the sum of two random variables with geometric law. At this point, a curious reader could ask what we can say about more general trees? We address this question by considering the model on random trees. This is the purpose of the next section where we include the main results of this work.\n\n\n\\subsection{The model on a random tree and first results}\\label{ss:randomtree}\n\nOne way to construct trees randomly is though a the family trees produced by a branching process. In words, we start with one vertex which is the root of the tree and let it connect according to a given discrete distribution; i.e., it has $n$ neighbors with probability $p_n$ for any $n\\in\\mathbb{N}\\cup\\{0\\}$. Each of these neighbors (if there are any) then connect with new vertices independently with the same law, and so on forever or until some generation goes extinct. These trees are called Galton-Watson trees, or random trees for short. For the sake of simplicity in the exposition, and without loss of generality, our results will be obtained for the MT-model on the tree obtained from a random tree by adding a particular vertex, that we call $s$, connected to its root. We assume that $s$ is the source of the rumor; i.e., the only vertex with the information at time zero. Since $s$ is only connected with the root, then it will spreads the information to it so, from now on, we shall call the standard initial configuration the one for which $\\eta_0(s)=\\eta_0({\\bf 0})=1$, and $\\eta_0(x)=0$ for all $x\\in V\\setminus \\{s,{\\bf 0}\\}$. In addition, as we are interested in to define the model on a tree with infinitely many vertices, we consider super-critical Galton-Watson trees on the event of non-extinction. Notice that this happens provided $\\mathbb{E}(\\xi)>1$.  In what follows we use the notation $\\mathbb{T}_{\\xi}$ for such a random tree for which the number of successors of a given vertex $v\\neq s$ is given by an independent copy of the non-negative integer valued random variable $\\xi$ with $1<\\mathbb{E}(\\xi)<\\infty$. Note that whether $\\xi\\equiv d$, for $d\\geq 2$, we obtain the infinite Cayley tree $\\mathbb{T}_d$ with an additional vertex connected to the root.\n\nWe start by characterizing the law of the number of spreaders one spreader generates.\n\n\\begin{prop}\\label{prop:distxxi}\nLet $\\xi$ be a non-negative integer valued random variable, and let $(\\eta_t)_{t\\ge0}$ be the MT-model on $\\mathbb{T}_{\\xi}$ with the standard initial configuration. If $X(\\xi)$ denotes the number of spreaders one spreader generates, then\n\\begin{equation}\\label{eq:xxi}\n\\mathbb{P}( X(\\xi) = i) = (i+1)! \\sum_{d=i}^{\\infty} \\dbinom{d}{i} \\left (\\frac{1}{d+1} \\right )^{i+1} \\mathbb{P}(\\xi = d).\n\\end{equation}\nMoreover, the generating probability function and the mean of $X(\\xi)$ are given, respectively, by\n\\begin{equation}\\label{eq:gflouca}\n    G_{\\xi}(s) = \\sum_{d=0}^{\\infty}e^{\\frac{d+1}{s}}  \\left [ \\Gamma \\left (d+1,\\frac{d+1}{s} \\right ) - \\frac{d(d+1)}{s}  \\Gamma \\left (d,\\frac{d+1}{s} \\right ) \\right ] \\mathbb{P}( \\xi = d) \\left (\\frac{s}{d+1} \\right )^d\n\\end{equation}\nand\n\\begin{equation}\\label{eq:meanxxi}\n\\mathbb{E}(X(\\xi)) = \\sum_{d=1}^{\\infty} \\left [ \\frac{e^{d+1} \\Gamma (d+1,d+1)}{(d+1)^d}  \\mathbb{P}(\\xi = d) \\right ]- \\mathbb{P}(\\xi \\neq 0),\n\\end{equation}\nwhere $\\Gamma\\left(k,x\\right):=(k-1)!e^{-x}\\displaystyle\\sum_{i=0}^{k-1}\\frac{x^i}{i!},$ is the incomplete gamma function.\n\\end{prop}\n\n\\begin{proof}\n\nThe law of $X(\\xi)$ is obtained by noting that \n$$\\mathbb{P}( X(\\xi) = i)=\\sum_{d=0}^{\\infty}\\mathbb{P}( X(\\xi) = i|\\xi = d)\\mathbb{P}(\\xi = d),$$\nwhere $\\mathbb{P}( X(\\xi) = i|\\xi = d)=0$ for $d<i$, and  \n$$\\mathbb{P}( X(\\xi) = i|\\xi = d)= (i+1)! \\dbinom{d}{i} \\left (\\frac{1}{d+1} \\right )^{i+1},$$\nfor $d\\geq i$, see \\cite[Lemma 2]{junior}. In order to obtain the expression \\eqref{eq:gflouca} for the generating probability function note that:\n\\[ G_{\\xi}(s) = \\sum_{i=0}^{\\infty}s^i\\mathbb{P}( X(\\xi) = i) = \\sum_{i=0}^{\\infty}s^i(i+1)! \\sum_{d=i}^{\\infty} \\dbinom{d}{i} \\left (\\frac{1}{d+1} \\right )^{i+1} \\mathbb{P}(\\xi = d)  \n\\]\n\nThen, by a sequence of suitable arrangement of terms we have, \n\n$$\n\\begin{array}{ccl}\nG_{\\xi}(s) & =&\\displaystyle \\sum_{d=0}^{\\infty} \\sum_{i=0}^{d}\\frac{(i+1)!}{d+1} \\dbinom{d}{i} \\left (\\frac{s}{d+1} \\right )^{i} \\mathbb{P}(\\xi = d)\\\\[.5cm] \n& = &\\displaystyle \\sum_{d=0}^{\\infty} \\frac{d!\\mathbb{P}(\\xi = d)}{d+1}  \\sum_{i=0}^{d} \\frac{(i+1)}{(d-i)!} \\left (\\frac{s}{d+1} \\right )^{i}\\\\[.5cm] \n& =& \\displaystyle\\sum_{d=0}^{\\infty} \\frac{d!\\mathbb{P}(\\xi = d)}{d+1}  \\sum_{i=0}^{d} \\frac{(i+1)}{(d-i)!} \\left (\\frac{s}{d+1} \\right )^{i}\\\\[.5cm]   \n& =& \\displaystyle\\sum_{d=0}^{\\infty} \\frac{d!\\mathbb{P}(\\xi = d)}{d+1}  \\sum_{j=0}^{d} \\frac{(d-j+1)}{j!} \\left (\\frac{s}{d+1} \\right )^{d-j}\\\\[.5cm] \n& =&\\displaystyle \\sum_{d=0}^{\\infty} \\frac{d!\\mathbb{P}(\\xi = d)}{d+1}\\left (\\frac{s}{d+1} \\right )^{d} \\left [ (d+1)\\sum_{j=0}^{d} \\frac{1}{j!}\\left (\\frac{d+1}{s} \\right )^{j} - \\sum_{j=1}^{d} \\frac{1}{(j-1)!}\\left (\\frac{d+1}{s} \\right )^{j}  \\right ]\n\\\\[.5cm]\n& =&\\displaystyle \\sum_{d=0}^{\\infty} \\frac{d!\\mathbb{P}(\\xi = d)}{d+1}\\left (\\frac{s}{d+1} \\right )^{d} \\left [ (d+1)\\sum_{j=0}^{d} \\frac{1}{j!}\\left (\\frac{d+1}{s} \\right )^{j} - \\frac{(d+1)}{s}\\sum_{l=0}^{d-1} \\frac{1}{l!}\\left (\\frac{d+1}{s} \\right )^{l}  \\right ]\n\\end{array}.\n$$\n\nNow, consider the incomplete gamma function \n\n$$\\Gamma\\left(k,x\\right):=(k-1)!e^{-x}\\displaystyle\\sum_{i=0}^{k-1}\\frac{x^i}{i!},$$ \n\nand note that we can rewrite $G_{\\xi}(s)$ as follows.\n\n\n$$\\begin{array}{ccl}\nG_{\\xi}(s) &=&\\displaystyle \\sum_{d=0}^{\\infty} \\frac{d!\\mathbb{P}(\\xi = d)}{d+1}\\left (\\frac{s}{d+1} \\right )^{d} \\left [ \\frac{(d+1)e^{\\frac{d+1}{s}}}{d!}\\Gamma\\left(d+1, \\frac{d+1}{s}\\right) - \\frac{(d+1)e^{\\frac{d+1}{s}}}{s(d-1)!}\\Gamma\\left(d, \\frac{d+1}{s}\\right)\\right]\\\\[.5cm]\n& = &\\displaystyle \\sum_{d=0}^{\\infty} \\frac{e^{\\frac{d+1}{s}} d!\\mathbb{P}(\\xi = d)}{d+1}\\left (\\frac{s}{d+1} \\right )^{d} \\left [ \\frac{(d+1)}{d!}\\Gamma\\left(d+1, \\frac{d+1}{s}\\right) - \\frac{d(d+1)}{s}\\Gamma\\left(d, \\frac{d+1}{s}\\right) \\right ]\\\\[.5cm]\n& =&\\displaystyle \\sum_{d=0}^{\\infty}e^{\\frac{d+1}{s}}  \\left [ \\Gamma \\left (d+1,\\frac{d+1}{s} \\right ) - \\frac{d(d+1)}{s}  \\Gamma \\left (d,\\frac{d+1}{s} \\right ) \\right ] \\mathbb{P}( \\xi = d) \\left (\\frac{s}{d+1} \\right )^d\n \\end{array}.\n $$\n\nThe last equality is \\eqref{eq:gflouca}. The mean of $X(\\xi)$ is obtained by noting that \n$$\\mathbb{E}( X(\\xi))=\\sum_{d=0}^{\\infty}\\mathbb{E}( X(\\xi)|\\xi = d)\\mathbb{P}(\\xi = d).\n$$\n\nTo find an expression for $\\mathbb{E}(X(\\xi)|\\xi = d)$ let us give an enumeration for the ignorant neighbors of a spreader with $d+1$ neighbors. In order to do it,  let us write $X(\\xi)|\\xi = d$ as the sum of indicator random variables $I_1, I_2, \\ldots I_d$, where $I_j$ indicates whether the $jth$-ignorant neighbor is informed. Thus, \n\n\\[ \\mathbb{E}(X(\\xi)|\\xi = d) = d\\, \\mathbb{P}(I_1 = 1) = d\\, \\mathbb{P} \\left( \\bigcup_{i=1}^{d}A_i \\right) = d\\,\\sum_{i=1}^{d}\\mathbb{P}(A_i)\n\\]\nwhere $A_i$ is the event of the first neighbor be the $ith$ to be informed by the spreader. Note that the last equality follows from the fact that the events $A_1, A_2, \\ldots, A_d$ are $2$ to $2$ mutually exclusive. Then,\n\n$$\\mathbb{E}(X(\\xi)|\\xi = d)= d! \\sum_{i=1}^{d} \\frac{1}{(d-i)!} \\left (\\frac{1}{d+1} \\right )^{i} = \\frac{e^{d+1}\\Gamma\\left(d+1,d+1\\right)}{(d+1)^d} - 1,$$\nfor $d\\geq i$. \\end{proof}\n\nWe can now state the main theorem of our work.\n\n\\begin{theorem}\\label{theo:geral}\nLet $\\xi$ be a non-negative integer valued random variable, and let $(\\eta_t)_{t\\ge0}$ be the MT-model on $\\mathbb{T}_{\\xi}$ with the standard initial configuration. Then $\\theta(\\xi) = 1 - \\psi(\\xi)$, where $\\psi:=\\psi(\\xi)$ is the smallest non-negative root of the equation\n\\begin{equation}\\label{eq:laly}\nG_{\\xi}(s)=s.\n\\end{equation}\nMoreover $\\theta(\\xi)>0$ if, and only if, $\\mathbb{E}(X(\\xi))>1$.\n\\end{theorem}\n\nAs an application of the above-theorem, whose proof we left to Section \\ref{s:proofs}, we consider the MT-model on a random tree produced by random pruning the infinite Cayley tree; i.e., we assume that $\\xi$ follows a Binomial distribution of parameters $n$ and $p$.\n\n\\begin{corollary}\\label{coro:binom}\nLet $\\xi\\sim Binomial(n,p)$, with $n\\geq 3$, and let $(\\eta_t)_{t\\ge0}$ be the MT-model on $\\mathbb{T}_{\\xi}$ with the standard initial configuration. Then $\\theta(n,p) = 1 - \\psi$, where $\\psi$ is the smallest non-negative root of the equation\n\\[ \\sum_{d=0}^{n} \\displaystyle \\left[\\frac{1}{d+1} \\dbinom{n}{d} p^d(1-p)^{n-d} \\sum_{i=0}^{d} (i+1)! \\dbinom{d}{i} \\left(\\frac{s}{d+1} \\right)^i \\right]= s.\\]\nMoreover $\\theta(n,p)>0$ if, and only if,\n\n\\begin{equation}\\label{eq:corobinom}\n    \\sum_{d=1}^n \\frac{1}{(n-d)!}\\left(\\frac{p}{1-p}\\right)^d\\sum_{j=1}^d\\frac{1}{(d+1)^j(d-j)!}>\\dfrac{1}{(1-p)^n n!}.\n\\end{equation}\n\\end{corollary}\n\n\\begin{proof} Let $ \\xi \\sim Binomial(n,p) $. Proposition \\ref{prop:distxxi} give us an expression for the generating probability function $G_{\\xi}(s)$ using the incomplete Gamma function. Since here we are considering a simple law for $\\xi$ it is enough to note that\n\n$$G_{\\xi}(s)=\\displaystyle\\sum_{d=0}^{\\infty} \\frac{d!\\mathbb{P}(\\xi = d)}{d+1}  \\sum_{i=0}^{d} \\frac{(i+1)}{(d-i)!} \\left (\\frac{s}{d+1} \\right )^{i},$$\n\n\\noindent\nso we can obtain a version of \\eqref{eq:laly} only by replacing the law of $\\xi$ in the previous expression. Now, by Theorem \\ref{theo:geral}, we have $\\theta(n,p)>0$ if, and only if, $\\mathbb{E}(X(\\xi))>1.$ Note that \n\\begin{align}\\label{eq:esp}\n\\mathbb{E}(X(\\xi))&=\\sum_{d=1}^n \\mathbb{E}(X|\\xi=d)\\mathbb{P}(\\xi=d)\\nonumber\\\\\n&= \\sum_{d=1}^n d!\\sum_{j=1}^d\\frac{1}{(d+1)^j(d-j)!}\\binom{n}{d}p^d(1-p)^{n-p}\\nonumber\\\\\n&=\\sum_{d=1}^n d!\\binom{n}{d}p^d(1-p)^{n-d}\\sum_{j=1}^d\\frac{1}{(d+1)^j(d-j)!}\\nonumber\\\\\n&=(1-p)^n \\sum_{d=1}^n \\frac{d!n!}{d!(n-d)!}\\left(\\frac{p}{1-p}\\right)^d\\sum_{j=1}^d\\frac{1}{(d+1)^j(d-j)!}\\nonumber\\\\\n&=(1-p)^n n! \\sum_{d=1}^n \\frac{1}{(n-d)!}\\left(\\frac{p}{1-p}\\right)^d\\sum_{j=1}^d\\frac{1}{(d+1)^j(d-j)!}.\\nonumber\\\\\n\\end{align}\nThen  $\\theta(n,p)>0$ if, and only if,\n$$\n(1-p)^n n! \\sum_{d=1}^n \\frac{1}{(n-d)!}\\left(\\frac{p}{1-p}\\right)^d\\sum_{j=1}^d\\frac{1}{(d+1)^j(d-j)!}>1\n$$\n\n\\noindent\nso we get \\eqref{eq:corobinom} and this complete the proof.\n\\end{proof}\n\nCorollary \\ref{coro:binom} gains in interest if we realize that it give us a way to find the value of $p$ for which the behavior of the model moves from extinction to survival of the rumor provided $n$ is fixed. Moreover, it is not difficult to see that we recover Theorem \\ref{thm:MTpt} by letting $n=d$ and $p=1$. Now, let us consider an example to illustrate what happens for a tree for which $\\mathbb{E}(\\xi)$ is between $2$ and $3$.\n\n\n\\begin{exa}\\label{exa:binom3p}Let $\\xi\\sim Binomial(3,p)$ and consider the MT-model on $\\mathbb{T}_{\\xi}$ with the standard initial configuration. Our interest is for $p\\in(2/3,1)$; that is $\\mathbb{E}(\\xi)\\in (2,3)$. Indeed, if $p<2/3$ we known that the rumor becomes extinct almost surely (by comparison with Theorem \\ref{thm:MTpt} for $d=2$). On the other hand, for $p=1$ the rumor survives with positive probability (see Theorem \\ref{thm:MTpt} for $d=3$). By Corollary \\ref{coro:binom} we have $\\theta(3,p) = 1 - \\psi$ where $\\psi$ is the smallest non-negative root of the equation\n\\[ (1-p)^3 +\\frac{3}{2}(p-1)^2 p(s+1) - \\frac{1}{3}(p-1)p^2(2s^2+4s+3) + \\frac{1}{32} p^3(3s^3 +9s^2 +12s +8) = s.\n\\]\nThus $\\theta(3,p) > 0$ if, and only if, $5p^3 -32p^2 +144p -96 >0$. This in turns implies that\n\\begin{equation}\n\\theta(3,p)=\\left\\{\n\\begin{array}{cl}\n \\displaystyle\\frac{-5p^2 +32 p -2\\sqrt{p(-5p^3 -8p^2 -68p+216)}}{9p^2},&\\text{ if }p > p_c,\\\\[.5cm]\n 0,&\\text{ if }p \\leq p_c,\n\\end{array}\\right.\n\\end{equation}\nwhere\n$$ p_c := \\frac{2}{15} \\left(16 - (142 \\times 2^{\\frac{2}{3}})/(45 \\sqrt{5689} - 2407)^{\\frac{1}{3}} + (2 (45 \\sqrt{5689} - 2407))^{\\frac{1}{3}} \\right)\\approx0.78753.\n$$\nMoreover, note that $\\max_{p \\in [0,1]} \\theta(3,p) = 3- \\frac{2}{3}\\sqrt{15} \\approx 0.418.$ See Figure \\ref{fig:ptbinom}.\n\\end{exa}\t\n\n\n\n\\bigskip\n\t\\begin{figure}[!htb]\n\t\\pgfplotsset{my style/.append style={axis x line=middle, axis y line=\nmiddle, xlabel={$p$}, ylabel={$\\theta(3,p)$}}}\n\t\\begin{tikzpicture}[scale=1.1]\n\\begin{axis}[my style,\nxmin=0, xmax=1, ymin=0, ymax=1, minor tick num=1]\n\n \n  \n   \n    \n     \n \\addplot +[mark=none,gray, dashed] coordinates {(1,0) (1, 0.418)(0,0.418)};\n\\addplot[domain=0:0.78753, line width=1mm] {0};\n\\addplot[domain=0.78753:1, line width=0.8mm] {(-5*x^2+32*x-2*sqrt(x*(-5*x^3-8*x^2-68*x+216)))/(9*x^2)};\n\\end{axis}\n\\end{tikzpicture}\n\n\t\n\n\t\t\\caption{Survival probability for the MT-model on $\\mathbb{T}_{\\xi}$ with $\\xi\\sim Binomial(3,p)$ and the standard initial configuration. For $p\\leq 2/3$ we known that $\\theta(3,p)=0$ as a consequence of the same behavior for the infinite Cayley tree of coordination number $3$, see Theorem \\ref{thm:MTpt} for $d=2$, and a coupling argument. Corollary \\ref{coro:binom} allows to understand the behavior of the model for $p>2/3$ localizing the critical parameter in $\\approx0.78753.$}\\label{fig:ptbinom}\n\t\\end{figure}\n\t\n\t\\subsection{Further examples and properties}\n\nOf course, as a consequence of Theorem \\ref{theo:geral}, different distributions give rise to different versions of Corollary \\ref{coro:binom}. However, the exact localization of the critical parameter is a task that can be difficult depending of the considered law. In those cases one of the usual strategies is the computation of non-trivial bounds for such a value. Let us illustrate it with some examples. \n\n\\begin{exa}\\label{exa:poisson}\nLet $\\xi\\sim Poisson(\\lambda)$, with $\\lambda > 1$, and let $(\\eta_t)_{t\\ge0}$ be the MT-model on $\\mathbb{T}_{\\xi}$ with the standard initial configuration. A direct application of Theorem \\ref{theo:geral} allow us to conclude that there exist a critical value $\\lambda_c \\in (2.49782; 2.49785)$ such that $\\theta(\\lambda)>0$ if, and only if, $\\lambda > \\lambda_c$. Indeed, note that Equation \\eqref{eq:meanxxi} from Proposition \\ref{prop:distxxi} leads to\n$$\ng(\\lambda) :=\\mathbb{E}(X(\\xi))= e^{-\\lambda +1}\\sum_{d=1}^{\\infty} \\left [ \\frac{(\\lambda e)^d \\Gamma (d+1,d+1)}{(d+1)^d d!} \\right] +e^{-\\lambda}-1.\n$$\nOn the one hand we have\n\\begin{equation}\\label{eq:poisson1}\ng(\\lambda) < e^{-\\lambda}-1  + e^{-\\lambda +1}\\sum_{d=1}^{17} \\left [ \\frac{(\\lambda e)^d \\Gamma (d+1,d+1)}{(d+1)^d d!} \\right] + e^{-\\lambda +1}\\sum_{d=18}^{\\infty} \\left [ \\frac{(\\lambda e)^d}{(d+1)^d} \\right],\n\\end{equation}\nwhile, on the other hand we obtain\n\\begin{equation}\\label{eq:poisson2}\ng(\\lambda) >  e^{-\\lambda}-1  + e^{-\\lambda +1}\\sum_{d=1}^{17} \\left [ \\frac{(\\lambda e)^d \\Gamma (d+1,d+1)}{(d+1)^d d!} \\right].\n\\end{equation}\nFinally, by taking $\\lambda=2.49782$ in \\eqref{eq:poisson1} the right side is equal to\n$$e^{-2.49782}-1  + e^{-1.49782}\\sum_{d=1}^{17} \\left [ \\frac{(2.49782 e)^d \\Gamma (d+1,d+1)}{(d+1)^d d!} \\right] + e^{-1.49782}\\sum_{d=18}^{\\infty} \\left [ \\frac{(2.49782e)^d}{(d+1)^d} \\right]\n$$\nso $g(2.49782) < 0.999997 + 2.332.10^{-9} < 1.$ Similarly, by taking $\\lambda = 2.49785$ in \\eqref{eq:poisson2} we get\n$$  g(2.49785) >  e^{-2.49785}-1  + e^{-2.49785}\\sum_{d=1}^{17} \\left [ \\frac{(2.49785 e)^d \\Gamma (d+1,d+1)}{(d+1)^d d!} \\right] \\geq 1.00001 > 1.\n$$\nTherefore we conclude the result from Theorem \\ref{theo:geral}.\n\\end{exa}\n\n\\begin{exa}\\label{exa:geom}\nLet $\\xi\\sim Geom(p)$, and let $(\\eta_t)_{t\\ge0}$ be the MT-model on $\\mathbb{T}_{\\xi}$ with the standard initial configuration. Then, there exist a critical value $p_c\\in (0.25894,  0.25895)$ such that $\\theta(p)>0$ if, and only if, $p < p_c$. In this case, Equation \\eqref{eq:meanxxi} from Proposition \\ref{prop:distxxi} leads to\n\\begin{equation*}\ng(p)=ep\\sum_{d=1}^\\infty \\Big(\\frac{(1-p)e}{d+1}\\Big)^d\\Gamma(d+1,d+1)-(1-p).\n\\end{equation*}\nOn the one hand we have\n\\begin{equation}\\label{eq:geom1}\ng(p) > ep\\sum_{d=1}^{300} \\Big(\\frac{(1-p)e}{d+1}\\Big)^d\\Gamma(d+1,d+1)-(1-p).\n\\end{equation}\nwhile, on the other hand we obtain\n\\begin{equation}\\label{eq:geom2}\ng(p) < ep\\sum_{d=1}^{300} \\Big(\\frac{(1-p)e}{d+1}\\Big)^d\\Gamma(d+1,d+1) + ep\\sum_{d=301}^{\\infty} \\left [ \\left ( \\frac{(1-p)e}{2.2} \\right )^d  \\right ]-(1-p).\n\\end{equation}\nwhich follows from the following sequence of inequalities:\n$$\n\\begin{array}{ccl}\n\\displaystyle\\sum_{d=301}^{\\infty} \\left(\\frac{(1-p)e}{d+1}\\right)^d\\Gamma(d+1,d+1)& \\leq&\\displaystyle \\displaystyle\\sum_{d=301}^{\\infty} \\left(\\frac{(1-p)e}{d+1}\\right)^d d! \\\\[.5cm]\n&\\leq& \\displaystyle\\sum_{d=301}^{\\infty} \\left(\\frac{(1-p)ed}{2.2(d+1)}\\right)^d \\frac{d!}{(\\frac{d}{2.2})^d}\\\\[.5cm]\n&\\leq&\\displaystyle \\sum_{d=301}^{\\infty} \\left(\\frac{(1-p)ed}{2.2(d+1)}\\right)^d\\\\[.5cm]  &\\leq& \\displaystyle\\sum_{d=301}^{\\infty} \\left(\\frac{(1-p)e}{2.2}\\right)^d\n\\end{array}\n$$\nBy taking $p=0.25894$ in \\eqref{eq:geom1} we obtain\n$$ g(0.25894)  > 0.25894 e \\sum_{d=301}^{\\infty} \\left(\\frac{0.74106ed}{2.2(d+1)}\\right)^d \\Gamma(d+1,d+1) - 0.74106 = 1.00003 > 1.\n$$\nBy taking $p=0.25895$ in \\eqref{eq:geom2} we have\n$$g(0.25895) < 0.25895 e \\sum_{d=1}^{300} \\left(\\frac{0.74105e}{d+1}\\right)^d\\Gamma(d+1,d+1) + \\sum_{d=301}^{\\infty} \\left [ \\left ( \\frac{0.74105e}{2.2} \\right )^d  \\right ]-0.74105\n$$\nand this in turns implies\n$$\ng(0.25895) < 0.999993 + 2.50463 \\times 10^{-11} < 1.\n$$\nThus, the result is a consequence of Theorem \\ref{theo:geral}.\n\n\\end{exa}\n\n\n\n\nIn what follows we discuss an alternative strategy when a direct application of Theorem \\ref{theo:geral}, to a given law of $\\xi$, is not possible.\n\n\\begin{prop}\\label{prop:mu}\nLet $(\\xi_n)_{n\\in\\mathbb{N}}$ and $\\xi$ be non-negative integer valued random variables, and assume that $\\lim_{n\\to \\infty} \\xi_n =\\xi$, in law. Let $(\\eta^n_t)_{t\\ge0}$ and $(\\eta_t)_{t\\ge0}$ be MT-models on $\\mathbb{T}_{\\xi_n}$ and $\\mathbb{T}_{\\xi}$, respectively, with the standard initial configuration. If $\\mu_n$ and $\\mu$ denote the expected value of the number of spreaders one spreader generates for each one of these models, then $\\lim_{n \\to \\infty} \\mu_n >1$ if, and only if, $\\mu>1$.\n\\end{prop}\n\n\\begin{proof}\nSince $\\mu_n:=\\mathbb{E}(X(\\xi_n))$ and $\\mu:=\\mathbb{E}(X(\\xi))$, then by \\eqref{eq:meanxxi} we have\n$$\\mu_{n} = \\sum_{d=1}^{\\infty} \\left [ \\frac{e^{d+1} \\Gamma (d+1,d+1)}{(d+1)^d}  \\mathbb{P}(\\xi_n = d) \\right] - \\mathbb{P}(\\xi_n \\neq 0)$$\nand\n$$\\mu = \\sum_{d=1}^{\\infty} \\left [ \\frac{e^{d+1} \\Gamma (d+1,d+1)}{(d+1)^d}  \\mathbb{P}(\\xi = d) \\right ]- \\mathbb{P}(\\xi \\neq 0).$$\n\n\\noindent\nBut\n$$\n\\begin{array}{ccl}\n\\displaystyle \\lim_{n \\to \\infty}\\mu_{n} &=&\\displaystyle \\lim_{n \\to \\infty} \\left \\{ \\sum_{d=1}^{\\infty} \\left [ \\frac{e^{d+1} \\Gamma (d+1,d+1)}{(d+1)^d}  \\mathbb{P}(\\xi_n = d) \\right ] - \\mathbb{P}(\\xi_n \\neq 0) \\right \\}\\\\[.5cm] \n&=&\\displaystyle \\sum_{d=1}^{\\infty} \\left [ \\frac{e^{d+1} \\Gamma (d+1,d+1)}{(d+1)^d}  \\mathbb{P}(\\xi = d) \\right ] - \\mathbb{P}(\\xi \\neq 0)\\\\[.5cm] \n&=& \\mu.\n\\end{array}\n$$\n\nWe point out that the interchange of limit and summation is guarantee by the Dominated Convergence Theorem, see \\cite[Theorem 9,1, p. 26]{thorison}.\n\n\\end{proof}\n\n\\begin{exa}\\label{cor:binompoi}\nLet $\\xi_n\\sim Binomial(n,p)$, for any $n\\geq 3$, and let $(\\eta_t)_{t\\ge0}$ be the MT-model on $\\mathbb{T}_{\\xi}$ with the standard initial configuration. Consider for any $n\\geq 3$ the critical parameter:   \n\\begin{equation}\n    p_c (n) = \\inf\\{p \\in [0,1]: \\theta(n,p) > 0\\}.\n\\end{equation}\nIf we assume $\\lim_{n\\to \\infty}np=\\lambda$ and we let $\\xi\\sim Poisson(\\lambda)$, then Proposition \\ref{prop:mu} applies and therefore $\\lim_{n \\to \\infty} n p_c(n) \\approx 2.4978$, where the limit is obtained from Example \\ref{exa:poisson}. See Table \\ref{tab:my_label}.\n\n\\end{exa}\n\n\\begin{table}[h!]\n    \\centering\n\\begin{tabular}{c|c|c|c|c|c|c|c|c}\n\n  $n$ & $3$ & $4$ & $5$ & $10$ & $25$ & $50$ & $100$ & $150$\\\\\\hline\n  $p_c(n)\\approx$ & $ 0.78753$ & $0.599322 $ & $0.483563$  & $0.24582$ & $0.09928$  & $0.04979$ & $0.02495$ & $0.01663$ \\\\\\hline\n  $np_c(n)\\approx$ & $2.362591$ & $2.397288$ & $2.417815$ & $2.4582$ & $2.48200$ & $2.48950$ & $2.49390$ & $2.49510$\n  \\end{tabular} \n    \\caption{Illustration of the behavior of the critical parameter of Example \\ref{cor:binompoi}.}\n    \\label{tab:my_label}\n\\end{table}\n\n\n\nWe finish the section with a similar analysis for the survival probabilities.\n\n\n\n\n\n\n\\begin{theorem}\\label{teo:lim}\nLet $(\\xi_n)_{n\\in\\mathbb{N}}$ be a non-decreasing sequence of non-negative integer valued random variables of vectors parameters given by $(\\nu_n)_{n\\in\\mathbb{N}}$. Let $\\xi$ be another non-negative integer valued random variable, of vector parameters $\\nu$, and assume that \n$$\n\\lim_{n\\to \\infty} \\xi_n =\\xi,\\text{ in law}.\n$$\nLet $(\\eta^n_t)_{t\\ge0}$ and $(\\eta_t)_{t\\ge0}$ be MT-models on $\\mathbb{T}_{\\xi_n}$, for $n\\in\\mathbb{N}$, and $\\mathbb{T}_{\\xi}$, with the standard initial configuration and survival probabilities given by $\\theta(\\nu_n)$ and $\\theta(\\nu)$, respectively. Then \n\\begin{equation}\n    \\lim_{n \\to \\infty} \\theta(\\nu_n) = \\theta(\\nu).\n\\end{equation}\n\\end{theorem}\n\n\n\\begin{proof}\nLet $x$ a non-negative integer and note that\n\n$$\\mathbb{P}( X(\\xi) \\leq x | \\xi = d) = 1 -\\prod_{i=1}^{x} \\left( 1- \\frac{i}{d}\\right) \\geq 1 -\\prod_{i=1}^{x} \\left( 1- \\frac{i}{d+1}\\right)\n = \\mathbb{P}( X(\\xi) \\leq x | \\xi = d+1). \n$$\nIn general, for any $x$, we can prove by induction that $d_1 < d_2$ implies\n$$\\mathbb{P}( X \\leq x | \\xi = d_1)  \\geq \\mathbb{P}( X \\leq x | \\xi = d_2)$$\nOn the other hand, if $\\xi_n$ converges in law to $\\xi$, then the \nDominated Convergence Theorem, see \\cite[Theorem 9,1, p. 26]{thorison}, allows the interchange of limit and summation, and so:\n\n$$\n\\begin{array}{ccl}\n\\displaystyle\\lim_{ n \\to \\infty} \\mathbb{P}(X(\\xi_n) = i) &=& \\displaystyle\\lim_{ n \\to \\infty} (i+1)! \\sum_{d=i}^{\\infty} \\dbinom{d}{i} \\left (\\frac{1}{d+1} \\right )^i \\mathbb{P}(\\xi_n = d)\\\\[.5cm]\n&=& (i+1)! \\displaystyle\\sum_{d=i}^{\\infty} \\dbinom{d}{i} \\left (\\frac{1}{d+1} \\right )^i \\displaystyle\\lim_{n \\to \\infty} \\mathbb{P}(\\xi_n = d)\\\\[.5cm] \n& =& (i+1)! \\displaystyle\\sum_{d=i}^{\\infty} \\dbinom{d}{i} \\left (\\frac{1}{d+1} \\right )^i \\mathbb{P}(\\xi = d)\\\\[.5cm] \n&=& \\mathbb{P}(X(\\xi)= i).\n\\end{array}\n$$\nThat is, $X(\\xi_n) \\to X(\\xi)$ in law. Finally, it is not difficult to see that $X(\\xi_n)$ is stochastically dominated by $X(\\xi)$ provided $\\xi_n$ be stochastically dominated by $\\xi$. See \\cite[Equation (3.1), page 4]{thorison} for more details. Therefore, by applying \\cite[Proposi\u00e7\u00e3o 4.2]{FAV} we get \n$$ \\lim_{n \\to \\infty} \\theta_n (\\nu) = \\theta (\\nu).\n$$\n\n\n\\end{proof}\n\n\n\n\n\\subsection{The range of the rumor on a random tree}\n\nAs mentioned in the Introduction, one way to measure the ``size'' of the rumor on a finite graph is through the analysis of the remaining proportion of ignorants at the end of the process. In the case of a tree we do it through the notion of range of the rumor, which is the longest distance, from the root, at which there is a non-ignorant individual.    \n\\begin{theorem}\\label{theo:range}\nLet $\\xi$ be a non-negative integer valued random variable, and let $(\\eta_t)_{t\\ge0}$ be the MT-model on $\\mathbb{T}_{\\xi}$ with the standard initial configuration. Consider $X(\\xi)$, and let $\\mu_{\\xi}$ and $G_{\\xi}$ be its mean and its p.g.f, respectively, as defined by \\eqref{eq:xxi}. Let \n\\begin{equation}\\label{eq:range} \nR(\\xi):=\\max\\{n\\geq 1: \\eta_{t}(x)=1 \\text{ for some }x\\in \\partial \\mathbb{T}_{\\xi,n}, \\text{ and } t\\in \\mathbb{R}^+\\},\n\\end{equation} \nbe the range of the rumor. If $\\mu_{\\xi} G_{\\xi}''(s) - G_{\\xi}'(s)G_{\\xi}''(1) \\geq 0$, then \n\n\\begin{equation}\\label{eq:relouca} \\frac{ (\\ell-1) \\mu_{\\xi}^{m+1}}{\\ell - \\mu_{\\xi}^{m+1}} \\leq \\mathbb{P}(R(\\xi) > m) \\leq \\frac{ (u-1) \\mu_{\\xi}^{m+1}}{u - \\mu_{\\xi}^{m+1}},\\end{equation}\n\n\\noindent\nfor any $m\\geq 0$, where \n\n$$u := \\left\\{\\mu_{\\xi} \\,\\mathbb{E} \\left(\\frac{1}{\\xi+1} \\right)\\right\\}\\left\\{\\mu_{\\xi} + \\mathbb{E} \\left(\\frac{1}{\\xi+1} \\right)  - 1\\right\\}^{-1},$$\n\n\\noindent\nand\n\n$$\\ell := 1 - \\left\\{\\frac{2}{G_{\\xi}''(1)}\\right\\}\\,\\mu_{\\xi}\\, (\\mu_{\\xi}-1).$$\n\nMoreover,\n\n\\begin{equation}\\label{eq:relouca2}\n    (u-1) \\sum_{m=0}^{\\infty} \\frac{\\mu_{\\xi}^{m+1}}{u - \\mu_{\\xi}^{m+1}} \\leq \\mathbb{E}(R(\\xi)) \\leq (\\ell-1) \\sum_{m=0}^{\\infty} \\frac{\\mu_{\\xi}^{m+1}}{\\ell - \\mu_{\\xi}^{m+1}}.\n\\end{equation}\n \\end{theorem}\n\nThe proof of the above-theorem is left to Section \\ref{s:proofs}. Let us illustrate the applicability of Theorem \\ref{theo:range} with an example.\n\n\n\\begin{exa}\\label{exe:lala}\nLet $\\xi\\sim Binomial(3,p)$ and let $(\\eta_t)_{t\\ge0}$ be the MT-model on $\\mathbb{T}_{\\xi}$ with the standard initial configuration. In this case,\n\n$$\n\\begin{array}{ccl}\nG_{\\xi}(s) &=&\\displaystyle \\frac{3p^3}{32}s^3 + \\left( \\frac{2p^2}{3} - \\frac{37p^3}{96} \\right)s^2 + \\left( \\frac{13p^3}{24} -\\frac{5p^2}{3} +\\frac{3p}{2} \\right )s + \\left ( \\frac{-p^3}{4} + p^2 -\\frac{3p}{2} +1 \\right),\\\\[.4cm]\nG'(s) &=&\\displaystyle \\frac{9p^3}{32}s^2 + \\left ( \\frac{4p^2}{3} - \\frac{74p^3}{96}  \\right )s + \\left( \\frac{13p^3}{24} -\\frac{5p^2}{3} +\\frac{3p}{2} \\right ),\\\\[.4cm]\nG''(s) &=&\\displaystyle \\frac{9p^3}{32}s + \\left ( \\frac{4p^2}{3} - \\frac{74p^3}{96}  \\right ),\\\\[.2cm]\n\\end{array}\n$$\n\n\\noindent\nwhich lead to\n$$\n\\begin{array}{ccl}\nG'(1) &=& \\displaystyle\\frac{5p^3}{96}  - \\frac{p^2}{3} + \\frac{3p}{2},\\\\[.3cm]  G''(1) &=& \\displaystyle\\frac{4p^2}{3} - \\frac{5p^3}{24},\\\\[.3cm] \nG(0)&=&\\displaystyle \\frac{-p^3}{4} +p^2 - \\frac{3p}{2} +1.\n\\end{array}\n$$\n\nNote that\n\n$$G'(1)G''(s) - G'(s)G''(1) \\geq 0\\,\\,\\, \\text{ if }\\,\\,\\, 335p^2 -3424p+4304 >0,$$\n\n\\noindent\nwhich holds for any $p \\in [0,1]$. Therefore, we can apply Theorem \\ref{theo:range} by noting that\n\n$$\\mu_{\\xi} =\\displaystyle \\frac{p(5p^2 -32p+144)}{96},$$ \n\n$$\\ell =\\displaystyle \\frac{-25p^5 + 320p^4 -2464p^3 +8736p^2 -17664p+13824}{192p(32-5p)},$$\n\n$$u =\\displaystyle \\frac{(2-p)(p^4 -6p^3 +16p^2 -20p+12)(5p^2-32p+144)}{4p(64-19p)(p^2-4p+6)}.$$\n\n\\smallskip\n\\noindent\nSee Figure \\ref{fig:range} for an illustration of the resulting bounds of $\\mathbb{E}(R(\\xi))$.\n\n\\end{exa}\n\n\t\\begin{figure}[!htb]\n\t\\pgfplotsset{my style/.append style={axis x line=middle, axis y line=\nmiddle, xlabel={$p$}, ylabel={}}}\n\t\\begin{tikzpicture}[scale=1.1]\n\\begin{axis}[my style,\nxmin=0, xmax=0.8, ymin=0, ymax=25, minor tick num=1]\n\\addplot[\n    color=black,\ndotted,\nmark=*,\nmark options={solid},\nsmooth\n    ]\n    coordinates {\n    (0.05,0.078)(0.1,0.164)(0.15,0.259)(0.2,0.365)(0.25,0.483)(0.3,0.617)(0.35,0.770)(0.4,0.948)(0.45,1.156)(0.5,1.405)(0.55,1.712)(0.6,2.105)(0.65,2.641)(0.7,3.456)(0.75,5.074)(0.78,8.351)(0.7875,19.832)\n    };\n\n    \n    \\addplot[\n    color=red,\ndotted,\nmark=*,\nmark options={solid},\nsmooth\n    ]\n    coordinates {\n    (0.05,0.078)(0.1,0.164)(0.15,0.26)(0.2,0.367)(0.25,0.488)(0.3,0.626)(0.35,0.785)(0.4,0.973)(0.45,1.197)(0.5,1.471)(0.55,1.816)(0.6,2.269)(0.65,2.904)(0.7,3.898)(0.75,5.928)(0.78,10.113)(0.7875,24.936)\n    };\n\n\\end{axis}\n\\end{tikzpicture}\n\n\t\n\n\t\t\\caption{Comparison between the bounds obtained in Theorem \\ref{theo:range} for the range of spreading in the MT-model on $\\mathbb{T}_{\\xi}$ with $\\xi\\sim Binomial(3,p)$, see Example \\ref{exe:lala}. For some values of $p$ the lower bound of $E(R(\\xi))$ is represented by a black dot while the upper bound is represented by a red dot.}\\label{fig:range}\n\t\\end{figure}\n\t\n\t\n\n\n\\section{Proof of Theorems \\ref{theo:geral} and \\ref{theo:range}}\\label{s:proofs}\n\n\\subsection{The rumor model seen as a branching process}\nThe main idea behind the proofs of Theorems \\ref{theo:geral} and \\ref{theo:range} is the identification of an underlying branching process related to the rumor model. After doing that we can apply well-known results of the theory of Branching Processes. This approach has been used before in \\cite{junior} to study the MT-model on infinite Cayley trees. For a deeper discussion of branching processes we refer the reader to \\cite[Chapter 2]{schinazi}. \n \nConsider a non-negative integer valued random variable $\\xi$ and the MT-model on $\\mathbb{T}_{\\xi}$, $(\\eta_t)_{t\\ge0}$, with the standard initial configuration. For any $n\\geq 0$ we let\n$$\\mathcal{Z}_{n}(\\xi):=\\left\\{v\\in \\partial \\mathbb{T}_{\\xi,n+1}: \\bigcup_{t> 0} \\{\\eta_{t}(v)=1\\right\\},$$\nand we define the random variable $Z_n:= |\\mathcal{Z}_n(\\xi)|$. Thus defined, $\\mathcal{Z}_{0}(\\xi)$ is formed by those vertices at distance one from ${\\bf 0}$ which are spreaders at some time, $\\mathcal{Z}_{1}(\\xi)$ is formed by those vertices at distance two from ${\\bf 0}$ which are spreaders at some time, and so on. Moreover, notice that the number of spreaders one spreader generates behaves according to a independent copy of a nonnegative integer valued  random variable $X(\\xi)$. Then $Z_0$ is equal to $X(\\xi)$ in law, and \n\\begin{equation}\\label{eq:BPZ}\nZ_{n+1}(\\xi)=\\sum_{i=1}^{Z_n(\\xi)}X_i(\\xi),\n\\end{equation}\nwhere $X_1(\\xi),X_2(\\xi),\\ldots$ are independent copies of $X(\\xi)$. As a direct consequence of the construction above we obtain the following result which is the key to prove our theorems.\n\n\\begin{lemma}\\label{lem:bp}\nThe stochastic process $(Z_n(\\xi))_{n\\geq 0}$ defined by \\eqref{eq:BPZ} is a branching process with offspring's distribution given by the law of $X(\\xi)$.\n\\end{lemma}\n\n\n\\subsection{Proof of Theorem \\ref{theo:geral}} Let $\\xi$ be a non-negative integer valued random variable, and let  $(\\eta_t)_{t\\ge0}$ be the MT-model on $\\mathbb{T}_{\\xi}$ with the standard initial configuration. By Lemma \\ref{lem:bp} we have that the rumor survives if, and only, if, the branching process with offspring's distribution given by the law of $X(\\xi)$ survives. Therefore, $\\psi(\\xi)$ coincides with the extinction probability for such a branching process, which is the smallest non-negative root of the equation $G_{\\xi}(s)=s.$ Moreover, also as a consequence of well-known result of branching process we have that $\\psi(\\xi)<1$ if, and only if, $\\mathbb{E}(X(\\xi))>1$. See \\cite[Theorem 1.1, p. 19]{schinazi} for more details.\n\n\n\\subsection{Proof of Theorem \\ref{theo:range}} Let $\\xi$ be a non-negative integer valued random variable, and let  $(\\eta_t)_{t\\ge0}$ be the MT-model on $\\mathbb{T}_{\\xi}$ with the standard initial configuration. Thanks to the connection between our model with a branching process, stated in Lemma \\ref{lem:bp}, it is enough to point out that the range of the rumor defined in \\eqref{eq:range}, plus $1$, it is the same than the extinction time of the associated branching processes. Therefore we appeal to arguments of \\cite{agresti} for deriving bounds for the tail of such a random time.  The main approach of \\cite{agresti} consists of reducing the problem of deriving these bounds to a problem involving the analysis of a family of $p.g.f.$'s whose iterates can be calculated; i.e., the fractional linear generating functions (f.l.g.f.). We refer the reader to \\cite[Definition 2]{wang}, for a suitable definition of a fractional linear generating function. Our first task is to obtain two f.l.g.f. $f_l(s)$ and $f_u(s)$ such that $f_l(s) \\leq G_{\\xi}(s) \\leq f_u(s), 0 \\leq s \\leq 1$. In order to do it we apply \\cite[Theorem, p. 450]{wang}, verifying its corollary of p. 451. Then, $f_l(s)$ and $f_u(s)$ are such that\n\\[ c_l = \\frac{G_{\\xi}''(1)}{2G_{\\xi}'(1) + G_{\\xi}''(1)} = \\frac{G_{\\xi}''(1)}{2\\mu_{\\xi} + G_{\\xi}''(1)} \n\\]\nand\n\\[ c_u = \\frac{G_{\\xi}'(1) + G_{\\xi}(0) -1}{G_{\\xi}'(1)} = \\frac{\\mu_{\\xi} + \\mathbb{E} \\left(\\frac{1}{\\xi+1} \\right) - 1}{\\mu_{\\xi}}.\n\\]\nNote that the inequalities hold also for the $m$-th compositions of these functions. Thus, \n\\[ f_l^m(0) \\leq  G_{\\xi}^m(0)  \\leq f_u^m(0).\n\\]\nand this in turns implies\n\\[ f_l^m(0) \\leq  \\mathbb{P}(R(\\xi) \\leq m+1) \\leq f_u^m(0).\n\\]\n\nNow, the inequalities in \\eqref{eq:relouca} follow by \\cite[Eq. (3.1), p. 330]{agresti}, with\n\n\\[ \\ell =  \\frac{2G_{\\xi}'(1) + G_{\\xi}''(1) - 2 [G_{\\xi}'(1)]^2}{G_{\\xi}''(1)}  = 1 - \\left\\{\\frac{2}{G_{\\xi}''(1)}\\right\\}\\,\\mu_{\\xi}\\, (\\mu_{\\xi}-1)\n\\]\nand\n\\[ u =  \\frac{G_{\\xi}(0)G_{\\xi}'(1)}{G_{\\xi}'(1) + G_{\\xi} (0)-1} = \\left\\{\\mu_{\\xi} \\,\\mathbb{E} \\left(\\frac{1}{\\xi+1} \\right)\\right\\}\\left\\{\\mu_{\\xi} + \\mathbb{E} \\left(\\frac{1}{\\xi+1} \\right)  - 1\\right\\}^{-1}.\n\\]\nFinally, \\eqref{eq:relouca2} is a direct consequence of \\eqref{eq:relouca}.\n\n\\bigskip\n\\section{Discussion}\n\nThe theory of mathematical models for rumor spreading has increased in the last years. In this work we contributed with this field by advancing our understanding of the behavior of the Maki-Thompson rumor model on a random tree. Our approach rely on the comparison of the rumor model with a suitable defined branching process. This allows to obtain results regarding a phase-transition for the model, in the sense of the existence of a critical value around which the rumor either becomes extinct almost-surely or survives with positive probability. Our arguments also allow to obtain an approximation for the mean range of the rumor. The analysis proposed here can be adapted to study the model in other tree-like graphs. In addition, we emphasize that an exploration on the connection between our approach and the one developed recently by \\cite{gleeson} for the description of information cascades on Twitter may be an issue of interesting research. Below, we suggest other two directions for further research. \n\n\\subsection{The role of the underlying distribution for the localization of the critical point}\n\nIn this work we consider locally finite rooted trees with infinitely many vertices. It is more intuitive to imagine the tree as growing away from its root, in the sense that each vertex has branches leading to its successors, which are its neighbors that are further from the root. With the aim of assign an average branching number to an arbitrary infinite locally finite tree it was introduced in \\cite{lyons} the so called branching number of a tree. Given a tree $\\mathbb{T}$, its branching number is denoted by $br\\left(\\mathbb{T}\\right)$ and may be seen as the exponential of the Hausdorff dimension of its boundary. The formal definition of the branching number uses the concept of flows on trees but we do not develop this point here since it exceeds the scope of the paper. A more complete theory may be obtained from \\cite[Chapter 3]{lyons}. The important point to note here is that if $\\mathbb{T}_{\\xi}$ is a Galton-Watson tree with mean $\\mathbb{E}(\\xi)>1$, then $br\\left(\\mathbb{T}_{\\xi}\\right)= \\mathbb{E}(\\xi)$ a.s. given nonextinction. See \\cite[Corollary 5.10]{lyons} for more details and note that, as a consequence, $br\\left(\\mathbb{T}_d\\right)=d$. \n\nOne could suspect that the branching number of a given tree has an important role for the localization of the critical parameter. More precisely, is there a critical branching number around which the rumor either becomes extinct almost-surely or survives with positive probability? With this question in mind, the known results for $\\mathbb{T}_d$, for $d\\geq 2$, would localize such a value between $2$ and $3$. In Example \\ref{exa:binom3p} we exhibit a family of random trees $\\mathbb{T}_{\\xi}$ for which the rumor survives with positive probability, if and only if, $br\\left(\\mathbb{T}_{\\xi}\\right)> 2.36259$. However, in Examples \\ref{exa:poisson} and \\ref{exa:geom} we have a different picture. While on one hand, Example \\ref{exa:poisson} provides a family of random trees for which the critical mean is approximately $2.4978$, on the other hand Example \\ref{exa:geom} exhibit a family of random trees for which the critical mean is localized approximately in $2.8625$. So the question that remains is how the underlying distribution generating the tree affect the localization of the critical point. \n\n\\subsection{The number of spreaders one spreader generates and the coupon collector\u00b4s problem}\n\nAs pointed out in \\cite[Remark 1]{junior}, there exist an interesting connection between the random variables arising in our model and some variables coming from the Coupon Collector\u2019s Problem. The problem is well-known from the Probability literature and it is usually stated as the problem of a collector who obtains, at each stage, a coupon which is\nequally likely to be any one of $n$ types. Assuming that the results of successive stages are independent, among other results, one could have interest into the first stage at which all $n$ coupons have been\npicked at least once. This problem gave rise to a series of generalizations which have been addressed in the literature, see for example \\cite{boneh}. Also, the underlying process has been used to study a related information transmission model, see \\cite{comets}. In our model, note that the number of of spreaders one spreader generates can be seen as the number of coupons that could be collected up to seeing a duplicate; that is, a coupon that already is part of the collection. Moreover, the law presented in Proposition \\ref{prop:distxxi} is the one of this quantity provided we start with a random number of coupon types. Note that for the case of a fixed number $n$ of coupons, Proposition \\ref{prop:distxxi} claims that the mean number of coupons that could be collected up to seeing a duplicate, provided we already have one coupon, is given as $e^{n}\\, n^{-(n-1)}\\, \\Gamma (n,n),$ where $\\Gamma\\left(k,x\\right)$ is the incomplete gamma function. Therefore, some advances in the understanding of the (asymptotic) behavior of this distribution could bring some new results for rumor models like ours.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section*{Acknowledgements}\nThis work has been developed with support of the Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES), Financial Code 001. This work has been supported also by FAPESP (2017/10555-0). The authors thanks E. Lebensztayn for the fruitful discussions.\n\n\n\n", "meta": {"timestamp": "2021-09-07T02:37:53", "yymm": "2109", "arxiv_id": "2109.02564", "language": "en", "url": "https://arxiv.org/abs/2109.02564"}}
{"text": "\\section{Introduction}\n\\label{sec:Intro}\n\nDark matter halos form the basic building blocks in the bottom-up structure formation of $\\Lambda$CDM cosmology. They constitute the dominant matter component of the astrophysical objects they are associated with: the largest halos with galaxy clusters; smaller halos with individual galaxies and dwarf galaxies. Their properties can be studied by ``observations'' in the virtual universes arising from large-scale-structure formation simulations  \\citep[e.g.,][]{zandanel_etal18}. In our universe, main observational constraints on galaxy-cluster dark matter halos come from the study of the kinematics of cluster galaxies \\citep[starting from][]{zwicky33}, from measurements of X-ray emission from intracluster baryonic gas \\citep[e.g.,][]{ettori_etal13}, and from analyses of weak and strong gravitational lensing of background galaxies \\citep[e.g.,][]{limousin_etal07,okabe_etal13}. Gravitational lensing analyses are particularly useful as tools for studying the finer-scale substructure of cluster halos, such as subhalos of individual cluster galaxies, local clumps or other inhomogeneities. A detailed analysis of 11 galaxy clusters by \\cite{meneghetti_etal20} revealed a surprisingly high efficiency of substructure lensing, more than an order of magnitude higher than expected from CDM simulations. This result indicates the need for a better understanding of the lensing effects of individual bodies within the cluster.\n\nThe goal of our work is to study the gravitational lensing influence of a compact massive body in a dark matter halo. For this purpose, we use a simple model consisting of a point mass embedded in a spherical Navarro--Frenk--White (NFW) density profile \\citep{navarro_etal96}. In the first part of this work \\citep[][hereafter \\citetalias{karamazov_etal21}]{karamazov_etal21}, we studied the critical curves and caustics of the lens model as a function of the mass and position of the point mass. We discovered that the model exhibited a rich diversity of critical-curve topologies and caustic geometries. We mapped the boundaries separating the corresponding lensing regimes in the point-mass parameter space and identified the accompanying caustic metamorphoses. Among other findings, we demonstrated the existence of a critical value of the mass parameter. For centrally positioned lighter (sub-critical) point masses, the lens has two radial critical curves. Heavier (super-critical) point masses are strong enough to fully eliminate the radial critical curves. For critical point masses the lens has a single radial critical curve with peculiar properties, which are described in Appendix~B of \\citetalias{karamazov_etal21}. We discussed the relevance of the model to the lensing by galaxies in galaxy-cluster halos as well as other astrophysical scenarios, such as the lensing influence of a satellite galaxy or a (super-)massive black hole in a galactic dark matter halo.\n\nIn this sequel to \\citetalias{karamazov_etal21}, we explore other lensing properties of the model. Here we concentrate on the shear and phase and their relation to the geometric distortions of images formed by the lens. In the weak-lensing regime the relation is tight, with the shear specifying the semi-axis ratio and the phase specifying the orientation of the major axis of the image. However, this will not be the case in the regions with high convergence (near the halo center) or high shear (near the point mass and near the halo center). In Section~\\ref{sec:NFW} we describe the shear, phase, and image geometry for a NFW-halo-only lens. We study the images in Section~\\ref{sec:NFW-images}, starting from the eigenvalue decomposition of the inverse of the Jacobian matrix and utilizing the convergence--shear diagram, a new tool described in detail in Appendix~\\ref{sec:Appendix-images}. In Section~\\ref{sec:NFW-weak} we introduce the weak shear and weak phase and compare these weak-lensing estimates with the shear and phase.\n\nIn Section~\\ref{sec:NFWP} we proceed with the analysis of the NFW halo + point-mass lens in a similar manner. More specifically, in Section~\\ref{sec:NFWP-csp} we derive formulae for the shear and phase of the combined mass distribution. We describe the emergence and occurrence of points with zero shear, which may constitute umbilic points under conditions discussed in Section~\\ref{sec:NFWP-Jacobian}. In Section~\\ref{sec:NFWP-images} we study the properties of images using the convergence--shear diagram. We present the main results in Section~\\ref{sec:NFWP-plots} in the form of grids of image-plane maps of different lens characteristics and convergence--shear diagrams, utilizing the same point-mass parameter grid as in \\citetalias{karamazov_etal21}. We discuss the results and their broader relevance in Section~\\ref{sec:discussion} and summarize our findings in Section~\\ref{sec:summary}.\n\n\n\\section{Lensing by a NFW Halo}\n\\label{sec:NFW}\n\n\\subsection{Convergence, Shear, and Phase}\n\\label{sec:NFW-csp}\n\nThe surface density of a halo with a spherical NFW profile expressed in units of the critical surface density $\\Sigma_{\\text{cr}}$ yields the dimensionless convergence profile (\\citealt{bartelmann96}; \\citetalias{karamazov_etal21}),\n\\beq\n\\kappa_{\\text{\\tiny NFW}}(x)=2\\,\\kappa_{\\text{s}}\\;\\frac{1-\\mathcal{F}(x)}{x^2-1}\\,,\n\\label{eq:NFW_kappa}\n\\eeq\nwhere $x$ is the plane-of-the-sky distance from the halo center expressed in units of the halo scale radius $r_{\\text{s}}$, and $\\kappa_{\\text{s}}$ is the halo convergence parameter. The function\n\\beq\n\\mathcal{F}(x)=\\begin{cases}\n\\frac{\\displaystyle\\arctanh{\\sqrt{1-x^2}}}{\\displaystyle\\sqrt{1-x^2}} & \\text{for $x<1$}\\,,\\\\\n\\hfil 1 & \\text{for $x=1$}\\,,\\\\\n\\frac{\\displaystyle\\arctan{\\sqrt{x^2-1}}}{\\displaystyle\\sqrt{x^2-1}} & \\text{for $x>1$}\\,,\n\\end{cases}\n\\label{eq:f(x)}\n\\eeq\nhas a similar radial behavior to the convergence $\\kappa_{\\text{\\tiny NFW}}(x)$: both decrease monotonically from $\\infty$ at the halo center to $0$ for $x\\gg 1$ (see \\citetalias{karamazov_etal21} for details). The radius $x_0$ at which the convergence is equal to $1$ can be computed numerically from\n\\beq\n\\frac{1-x_0^2}{\\mathcal{F}(x_0)-1}=2\\,\\kappa_{\\text{s}}\\,.\n\\label{eq:unit-convergence_radius}\n\\eeq\nThis unit-convergence radius and the circle that it defines play a key role when studying the geometry of images formed by a lens. For the NFW halo $x_0$ increases monotonically with the convergence parameter $\\kappa_{\\text{s}}$, as illustrated in \\citetalias{karamazov_etal21}. We note here that for $\\kappa_{\\text{s}}=3/2$ the unit-convergence radius is equal to the scale radius, $x_0=1$. For lower values of $\\kappa_{\\text{s}}$ the unit-convergence circle lies inside the scale-radius circle; for higher values outside.\n\nA light ray passing through the halo at a position $\\boldsymbol x$ in the plane of the sky is deflected by an angle\n\\beq\n\\boldsymbol \\alpha(\\boldsymbol x)=\\frac{4\\,\\kappa_{\\text{s}}\\,r_{\\text{s}}\\,D_{\\text{s}}}{D_{\\text{l}}\\,D_{\\text{ls}}}\\, \\left[\\ln{\\frac{x}{2}}+\\mathcal{F}(x)\\right]\\,\\frac{\\boldsymbol x}{x^2}\\,,\n\\label{eq:NFW_deflection}\n\\eeq\nwhere $D_{\\text{l}}$, $D_{\\text{s}}$, and $D_{\\text{ls}}$ are the angular-diameter distances from the observer to the lens, from the observer to the source, and from the lens to the source, respectively. Expressed in units of the angular scale radius, the position of a source $\\boldsymbol y$ and the position of its image $\\boldsymbol x$ formed by the gravitational field of the NFW halo are connected by the lens equation\n\\beq\n\\boldsymbol y=\\boldsymbol x - 4\\,\\kappa_{\\text{s}}\\,\n\\left[\\ln{\\frac{x}{2}}+\\mathcal{F}(x)\\right]\\,\\frac{\\boldsymbol x}{x^2}\\,.\n\\label{eq:NFW_lens_equation}\n\\eeq\n\nFor illustration, in the top row of Figure~\\ref{fig:images} we show the lensing of a circular source by a NFW halo with convergence parameter $\\kappa_{\\text{s}}\\approx 0.239035$, the fiducial value used in \\citetalias{karamazov_etal21}. As seen in the top left panel, in this example the black circular source centered at \\mbox{$\\bm{y_{\\text{c}}}=(0.015,\\,-0.005)$} with radius $y_{\\text{r}}=0.005$ lies inside the radial caustic without overlapping the central point-like tangential caustic. Solving the lens Equation~(\\ref{eq:NFW_lens_equation}) numerically for a point $\\boldsymbol y$ on the circumference of the source yields three points $\\boldsymbol x$ on the boundaries of the three black images shown in the top right panel. One image lies outside the tangential critical curve, a second image lies between the tangential and radial critical curves, and the smallest third image lies inside the radial critical curve. The dashed circle with radius $x_0\\approx0.0936$ is the unit-convergence circle of this halo. In this case, the first two images are elongated in the tangential (azimuthal) direction, while the third image is elongated in the radial direction.\n\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{0cm}\n\\includegraphics[width=8.5 cm]{f01.pdf}\n\\caption{Gravitational lensing of a circular source. Top row: lensing by a NFW halo with convergence parameter $\\kappa_{\\text{s}}\\approx 0.239035$. Bottom row: lensing by the same halo with an additional point mass with mass parameter $\\kappa_{\\text{P}}\\approx 2.714\\cdot 10^{-4}$ positioned at $\\bm{x_{\\text{P}}}=(0.2,\\,0)$. Left column: source-plane plots indicating the position of the source (black circle) with respect to the lens caustic (red lines). Right column: image-plane plots indicating the positions of images (black patches) with respect to the critical curve (solid black lines). The cyan circle in the bottom right panel marks the Einstein circle of the point mass; the dashed black lines in the right panels mark the unit-convergence circle.\\label{fig:images}}}\n\\efi\n\nFor a source lying inside the radial caustic and overlapping the central tangential caustic, the first two images would merge along the tangential critical curve, forming an Einstein ring. For a smaller source positioned close to the inner side of the radial caustic, the second image would lie inside the unit-convergence circle and be elongated in the radial direction. For a source positioned on the radial caustic, the second and third images would merge at the radial critical curve. For a source lying outside the radial caustic, these two images would vanish, leaving only the first image.\n\nThe deformations and orientations of the images are best studied by computing the lens shear and its phase, quantities that may be introduced by means of the lens potential. The deflection angle can be written in terms of the gradient of the lens potential $\\psi(x)$, which in this case is circularly symmetric,\n\\beq\n\\boldsymbol \\alpha(\\boldsymbol x)=\\frac{D_{\\text{s}}}{D_{\\text{ls}}}\\,\\nabla_{\\boldsymbol \\theta}\\,\\psi(x)=\\frac{D_{\\text{s}}\\,D_{\\text{l}}}{D_{\\text{ls}}\\,r_{\\text{s}}}\\,\\frac{\\boldsymbol x}{x}\\, \\frac{{\\rm d}\\psi}{{\\rm d} x}\\,,\n\\label{eq:NFW_gradient}\n\\eeq\nwhere we converted the angular position in radians $\\boldsymbol \\theta$ to the angular position in scale-radius units, $\\boldsymbol x = \\boldsymbol \\theta \\, D_{\\text{l}}/r_{\\text{s}}$. By substituting for the deflection angle from Equation~(\\ref{eq:NFW_deflection}) we can express the lens-potential derivative\n\\beq\n\\frac{{\\rm d}\\psi_{\\text{\\tiny NFW}}}{{\\rm d} x}= \\frac{4\\,\\kappa_{\\text{s}}\\,r_{\\text{s}}^2}{D_{\\text{l}}^2}\\,\\frac{1}{x}\\, \\left[\\ln{\\frac{x}{2}}+\\mathcal{F}(x)\\right]\\,.\n\\label{eq:NFW_potential,x}\n\\eeq\nIntegration yields the following expressions for the NFW halo lens potential \\citep{meneghetti_etal03,golse_kneib02}:\n\\beq\n\\psi_{\\text{\\tiny NFW}}(x)=\\begin{cases}\n\\frac{2\\,\\kappa_{\\text{s}}\\,r_{\\text{s}}^2}{D_{\\text{l}}^2}\\left[\\ln{x}\\ln{\\frac{x}{4}}- (\\displaystyle\\arctanh{\\sqrt{1-x^2}}\\,)^2\\right]\\\\\n\\hfil 0\\\\\n\\frac{2\\,\\kappa_{\\text{s}}\\,r_{\\text{s}}^2}{D_{\\text{l}}^2}\\left[\\ln{x}\\ln{\\frac{x}{4}}+ (\\displaystyle\\arctan{\\sqrt{x^2-1}}\\,)^2\\right]\\,,\n\\end{cases}\n\\label{eq:NFW_psi}\n\\eeq\nwhere the expression in the first row holds for $x<1$, in the second row for $x=1$, and in the third row for $x>1$. For $x\\ll 1$, the potential close to the halo center\n\\beq\n\\psi_{\\text{\\tiny NFW}}(x)= -\\,\\frac{2\\,\\kappa_{\\text{s}}\\,r_{\\text{s}}^2}{D_{\\text{l}}^2} \\left(\\ln^2{2}+\\frac{x^2}{2}\\,\\ln{\\frac{x}{2}}\\right) +\\mathcal{O}(x^4\\,\\ln{x})\\,,\n\\label{eq:NFW_psi_origin}\n\\eeq\nstarting from a finite negative value and increasing monotonically outward, crossing zero at the scale radius. Note that the expressions for the potential in \\cite{meneghetti_etal03} and \\cite{golse_kneib02} are higher by the constant $-\\psi_{\\text{\\tiny NFW}}(0)$ and thus they start at zero.\n\n\\begin{figure*}\n{\\centering\n\\vspace{0cm}\n\\hspace{0cm}\n\\includegraphics[width=15.5 cm]{f02.pdf}\n\\caption{Image-plane color maps of lensing characteristics of the NFW halo from the top row of Figure~\\ref{fig:images}. For orientation, solid black circles mark the critical curves in all panels. First panel: shear $\\gamma_{\\text{\\tiny NFW}}(\\boldsymbol x)$ with dotted contours from inside to outside marking $95\\%, 90\\%,$ and $85\\%$ of the central shear $\\gamma_{\\text{\\tiny NFW}}(0)=\\kappa_{\\text{s}}\\approx 0.239035$. Second panel: weak shear $\\gamma_{\\text{w,\\tiny NFW}}(\\boldsymbol x)$ computed from image flattening, using the same color bar as in the first panel. Third panel: relative weak-shear deviation from the shear, $\\gamma_{\\text{w,\\tiny NFW}}(\\boldsymbol x)/\\gamma_{\\text{\\tiny NFW}}(\\boldsymbol x)-1$. Fourth panel: weak phase $\\varphi_{\\text{w,\\tiny{NFW}}}(\\boldsymbol x)$ defined by image orientation, with orange corresponding to images oriented counterclockwise, and blue to images oriented clockwise from the horizontal. Dot-dashed lines mark the unit-convergence circle, and locations of horizontal (white background) and vertical images (high-saturation blue/orange boundary).\\label{fig:NFW-line}}}\n\\end{figure*}\n\nFor a circularly symmetric lens potential the lens shear $\\gamma$ can be computed as\n\\beq\n\\gamma = \\frac{D_{\\text{l}}^2}{2\\,r_{\\text{s}}^2} \\left|\\frac{{\\rm d}^2\\psi}{{\\rm d} x^2}-\\frac{1}{x}\\frac{{\\rm d}\\psi}{{\\rm d} x}\\right|\\,.\n\\label{eq:shear_sym}\n\\eeq\nFor the NFW profile the second derivative of the lens potential,\n\\begin{multline}\n\\frac{{\\rm d}^2\\psi_{\\text{\\tiny NFW}}}{{\\rm d} x^2}= \\frac{4\\,\\kappa_{\\text{s}}\\,r_{\\text{s}}^2}{D_{\\text{l}}^2}\\\\ \\times \\left[\\frac{1}{x^2-1}+\\frac{2x^2-1}{x^2(1-x^2)}\\,\\mathcal{F}(x)- \\frac{1}{x^2}\\ln{\\frac{x}{2}}\\right]\\,,\n\\label{eq:NFW_potential,xx}\n\\end{multline}\ncan be used together with the first derivative from Equation~(\\ref{eq:NFW_potential,x}) in Equation~(\\ref{eq:shear_sym}) to yield the shear of the NFW halo,\n\\begin{multline}\n\\gamma_{\\text{\\tiny NFW}}(x)=2\\,\\kappa_{\\text{s}}\\left[\\frac{2}{x^2}\\ln{\\frac{x}{2}} +\\frac{1}{1-x^2}+ \\frac{2-3x^2}{x^2(1-x^2)}\\,\\mathcal{F}(x)\\right]\\,,\\\\\n\\label{eq:NFW_gamma}\n\\end{multline}\nas shown by \\cite{wright_brainerd00}. In order to understand its behavior close to the origin, we expand Equation~(\\ref{eq:NFW_gamma}) for $x\\ll 1$ and obtain\n\\beq\n\\gamma_{\\text{\\tiny NFW}}(x)= \\kappa_{\\text{s}}\\,\\left[\\,1+\\frac{3}{2}\\,x^2\\,\\ln{\\frac{x}{2}}+\\frac{13}{8}\\,x^2\\,\\right] +\\mathcal{O}(x^4\\,\\ln{x})\\,.\n\\label{eq:NFW_gamma_origin}\n\\eeq\nBy setting $x=0$ we see that the NFW shear at the center is equal to the convergence parameter of the halo, $\\gamma_{\\text{\\tiny NFW}}(0)=\\kappa_{\\text{s}}$. From this value the shear decreases outward monotonically. An expansion close to the scale radius shows that for $x\\to 1$\n\\begin{multline}\n\\gamma_{\\text{\\tiny NFW}}(x)= 2\\,\\kappa_{\\text{s}}\\,\\left[\\,\\frac{5}{3}-2\\,\\ln{2}-4\\,\\left(\\frac{11}{15}-\\ln{2}\\right)\\, (x-1)\\,\\right]\\\\[1ex] +\\mathcal{O}((x-1)^2)\\,,\n\\label{eq:NFW_gamma_radius}\n\\end{multline}\nwhich yields $\\gamma_{\\text{\\tiny NFW}}(1)\\approx 0.561\\,\\kappa_{\\text{s}}$. The NFW shear decreases for $x\\gg 1$ to zero,\n\\begin{multline}\n\\gamma_{\\text{\\tiny NFW}}(x)=2\\,\\kappa_{\\text{s}}\\,\\left[\\,\\frac{2}{x^2}\\,\\ln{\\frac{x}{2}}-x^{-2}+\\frac{3\\pi}{2}\\,x^{-3} -4\\,x^{-4}\\,\\right]\\\\ +\\mathcal{O}(x^{-5})\\,.\n\\label{eq:NFW_gamma_infty}\n\\end{multline}\nThe first panel in Figure~\\ref{fig:NFW-line} shows a contour plot of the shear $\\gamma_{\\text{\\tiny NFW}}(\\boldsymbol x)$ in the central part of a NFW halo. Going outward from the center, the dotted contours correspond to 95\\%, 90\\%, and 85\\% of the central shear $\\gamma_{\\text{\\tiny NFW}}(0)=\\kappa_{\\text{s}}$. Clearly, the shear changes very slowly on this scale, as indicated also by the practically homogeneous color, with the color bar set for comparison with further figures. The solid black circles mark the radial (smaller) and tangential (larger) critical curve for the fiducial halo convergence parameter $\\kappa_{\\text{s}}\\approx 0.239035$.\n\nThe NFW shear can be written in terms of its two components, defined as\n\\beq\n(\\gamma_{\\text{\\tiny NFW}1},\\gamma_{\\text{\\tiny NFW}2})=\\gamma_{\\text{\\tiny NFW}}\\, (\\cos{2\\varphi_{\\text{\\tiny NFW}}},\\sin{2\\varphi_{\\text{\\tiny NFW}}})\\,,\n\\label{eq:NFW_gamma12}\n\\eeq\nwhere the trigonometric functions of the phase $\\varphi_{\\text{\\tiny NFW}}$ can be computed for a point $\\boldsymbol x = (x_1,x_2)= x(\\cos{\\phi},\\sin{\\phi})$ in the image plane as\n\\begin{multline}\n(\\cos{2\\varphi_{\\text{\\tiny NFW}}},\\sin{2\\varphi_{\\text{\\tiny NFW}}})=x^{-2}(x_2^2-x_1^2,-2x_1 x_2)\\\\=-(\\cos{2\\phi},\\sin{2\\phi})\\,.\n\\label{eq:NFW_phi}\n\\end{multline}\nThe negative sign in front of the last parentheses indicates that the phase $\\varphi_{\\text{\\tiny NFW}}=\\phi+\\pi/2+k\\pi$, i.e., its orientation is always perpendicular to the position vector of the point. Note that this also means that the phase and the shear components are undefined at $x=0$, since the phase depends on the direction of approach to the center.\n\n\n\\subsection{Jacobian}\n\\label{sec:NFW-Jacobian}\n\n\\begin{figure*}\n{\\centering\n\\vspace{0cm}\n\\hspace{0cm}\n\\includegraphics[width=18 cm]{f03.pdf}\n\\caption{Geometry of an image of a small circular source. Left panel: source with radius $y_\\text{r}$; the black and white points lie in the directions of the eigenvectors of the inverse Jacobian matrix $\\mathbb{A}$ parallel and perpendicular to the phase $\\varphi$. Right panels: elliptical image for $(\\kappa,\\gamma)=(0.08,0.16)$ and the three other combinations producing an ellipse of the same shape and size, as marked at the bottom left of each panel. The sizes of the semi-axes are marked in red; the positions of the images of the two points marked on the circumference are determined by the signs of the eigenvalues marked at the bottom right of each panel.\\label{fig:small-source-images}}}\n\\end{figure*}\n\nThe Jacobian matrix of a general lens equation expressed in terms of the convergence $\\kappa(\\boldsymbol x)$, shear $\\gamma(\\boldsymbol x)$, and phase $\\varphi(\\boldsymbol x)$ has the form \\citep[e.g.,][]{schneider_etal92}\n\\beq\nJ(\\boldsymbol x)=\\frac{\\partial\\,\\boldsymbol y}{\\partial\\,\\boldsymbol x}=\n\\begin{pmatrix}\n  1-\\kappa-\\gamma\\cos{2\\varphi} & -\\gamma\\sin{2\\varphi} \\\\\n  -\\gamma\\sin{2\\varphi} & 1-\\kappa+\\gamma\\cos{2\\varphi}\n\\end{pmatrix}\\,.\n\\label{eq:Jacobi_matrix}\n\\eeq\nIts determinant, the Jacobian, can be computed from the convergence and the shear:\n\\beq\n\\mathrm{det}\\,J(\\boldsymbol x)=\\left[\\,1-\\kappa(\\boldsymbol x)-\\gamma(\\boldsymbol x)\\,\\right]\\,\\left[\\,1-\\kappa(\\boldsymbol x)+\\gamma(\\boldsymbol x)\\,\\right]\\,.\n\\label{eq:Jacobian}\n\\eeq\nThe critical curves, explored in detail in \\citetalias{karamazov_etal21}, can be obtained by setting $\\mathrm{det}\\,J(\\boldsymbol x)=0$. We note here merely that for an axially symmetric lens such as the studied NFW halo, the first term in Equation~(\\ref{eq:Jacobian}) yields the tangential critical curve and the second term yields the radial critical curve. Hence, the shear is related to the convergence by $\\gamma(x_{\\text{T}})=1-\\kappa(x_{\\text{T}})$ at the tangential critical curve, and by $\\gamma(x_{\\text{R}})=\\kappa(x_{\\text{R}})-1$ at the radial critical curve, with the two critical curves separated by the unit-convergence circle.\n\n\n\\subsection{Geometry of Images}\n\\label{sec:NFW-images}\n\nTo study the geometry of the images, we invert the Jacobian matrix to obtain the mapping from the source plane to the image plane. We can write the inverse matrix in terms of its two eigenvalues,\n\\beq\n\\lambda_\\parallel(\\boldsymbol x)=(1-\\kappa-\\gamma)^{-1},\\qquad \\lambda_\\perp(\\boldsymbol x)=(1-\\kappa+\\gamma)^{-1}\\,,\n\\label{eq:A_eigenvalues}\n\\eeq\nas\n\\begin{multline}\n\\mathbb{A}(\\boldsymbol x)=\\lambda_\\parallel\n\\begin{pmatrix}\n  \\cos^2{\\varphi} & \\cos{\\varphi}\\sin{\\varphi} \\\\\n  \\cos{\\varphi}\\sin{\\varphi} & \\sin^2{\\varphi}\n\\end{pmatrix}\\\\\n+\\lambda_\\perp\n\\begin{pmatrix}\n  \\sin^2{\\varphi} & -\\cos{\\varphi}\\sin{\\varphi} \\\\\n  -\\cos{\\varphi}\\sin{\\varphi} & \\cos^2{\\varphi}\n\\end{pmatrix}\\,,\n\\label{eq:A_matrix}\n\\end{multline}\nwhere $\\kappa$, $\\gamma$, and $\\varphi$ are functions of the image-plane position $\\boldsymbol x$. The matrix accompanying $\\lambda_\\parallel$ is a projection matrix onto the eigenvector $(\\cos{\\varphi},\\sin{\\varphi})$; the matrix accompanying $\\lambda_\\perp$ is a projection matrix onto the eigenvector $(-\\sin{\\varphi},\\cos{\\varphi})$. Equation~(\\ref{eq:A_matrix}) shows that an image at position $\\boldsymbol x$ is scaled by a factor $\\lambda_\\parallel$ in the direction parallel to the phase $\\varphi$, and by a factor $\\lambda_\\perp$ in the direction  perpendicular to the phase, $\\varphi+\\pi/2$.\n\nA small circular source with radius $y_{\\text{r}}$ centered at $\\bm y_\\text{c}$ is thus portrayed by the lens as a set of $n$ small elliptical images with semiaxes $y_{\\text{r}}/|1-\\kappa(\\bm x^{[i]}_\\text{c})-\\gamma(\\bm x^{[i]}_\\text{c})|$ and $y_{\\text{r}}/|1-\\kappa(\\bm x^{[i]}_\\text{c})+\\gamma(\\bm x^{[i]}_\\text{c})|$. Their positions $\\bm x^{[i]}_\\text{c}(\\bm y_\\text{c})$, $i=1\\ldots n$, can be found by solving the lens equation, i.e., Equation~(\\ref{eq:NFW_lens_equation}) for the NFW halo. We illustrate the geometry of one such image in the second panel of Figure~\\ref{fig:small-source-images} for convergence $\\kappa(\\bm x^{[i]}_\\text{c})=\\kappa=0.08$ and shear $\\gamma(\\bm x^{[i]}_\\text{c})=\\gamma=0.16$. In the three right panels we include all other $(\\kappa, \\gamma)$ combinations that lead to the same combination of semiaxes $|\\lambda_\\parallel|\\,y_{\\text{r}}$ and $|\\lambda_\\perp|\\,y_{\\text{r}}$, i.e., they generate an elliptical image of the same shape and size. The images differ in their orientation and parity. For the combinations in the two right panels with $\\kappa>1$ the major axis is oriented perpendicular to the phase $\\varphi$ rather than parallel to it. The images in the third and fourth panels have negative parity, as indicated by the positions of the images of the black and white points on the circumference of the source. The signs of the eigenvalues are marked in each panel, with negative values indicating mirroring along the corresponding eigenvector.\n\nFor a general source, the distortion of its image can be quantified by the dimensionless flattening,\n\\beq\nf(\\kappa,\\gamma)=1-\\min\\left({\\left|\\frac{1-\\kappa-\\gamma}{1-\\kappa+\\gamma}\\right|, \\left|\\frac{1-\\kappa+\\gamma}{1-\\kappa-\\gamma}\\right|}\\right)\\,,\n\\label{eq:flattening}\n\\eeq\ndefined here using the ratio of the smaller to larger eigenvalues, with their definitions taken from Equation~(\\ref{eq:A_eigenvalues}). For an elliptical image of a circular source, $f$ is equal to its ellipticity. For the sample images in Figure~\\ref{fig:small-source-images}, $f\\approx0.30$. While the absolute value of the ratio of the eigenvalues determines the distortion of the image, their product determines its magnification and parity. Hence, it is sufficient to know the convergence $\\kappa$ and shear $\\gamma$ at the position of an image of a small source in order to fully determine its distortion, magnification, parity, and orientation with respect to the phase.\n\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{0cm}\n\\includegraphics[width=8.5 cm]{f04.pdf}\n\\caption{CS diagram illustrating the geometry of images formed by the NFW halo from the top row of Figure~\\ref{fig:images} with $\\kappa_{\\text{s}}\\approx 0.239035$. The green line marks all $\\left(\\kappa_{\\text{\\tiny NFW}}(x),\\gamma_{\\text{\\tiny NFW}}(x)\\right)$ combinations of the lens, with radial positions $x$ marked by tick marks, starting from $x=0.002$ near the right edge and ending with $x=2$ close to the origin of the diagram. The orange dots mark the $(\\kappa,\\gamma)$ combinations at the positions of the three images in the top right panel of Figure~\\ref{fig:images}. For more details on the diagram and its interpretation see Figure~\\ref{fig:CS-diagram}.\\label{fig:CS-NFW}}}\n\\efi\n\nFor a given gravitational lens, this information can be condensed into the convergence--shear (hereafter CS) diagram, introduced in Appendix~\\ref{sec:Appendix-images}. The geometry of images formed by a NFW halo lens as a function of their radial position $x$ can be easily identified from its CS diagram, shown in Figure~\\ref{fig:CS-NFW} for the fiducial convergence parameter $\\kappa_{\\text{s}}\\approx 0.239035$. The green curve connecting the $\\left(\\kappa_{\\text{\\tiny NFW}}(x),\\gamma_{\\text{\\tiny NFW}}(x)\\right)$ points is obtained by using the NFW halo convergence from Equation~(\\ref{eq:NFW_kappa}) and the NFW halo shear from Equation~(\\ref{eq:NFW_gamma}). The center of the halo corresponds to position $(\\kappa,\\gamma)=(\\infty,\\kappa_{\\text{s}})$ in the diagram, while for $x\\to\\infty$ the green curve reaches the origin, $(\\kappa,\\gamma)\\to(0,0)$. The tick marks along the curve correspond to radii (from the right side of the plot) $x\\in\\{0.002,0.003,\\ldots,0.01,0.02,\\ldots,0.1,0.2,\\ldots,1,2\\}$.\n\nIn order to interpret the image geometries in a NFW halo from Figure~\\ref{fig:CS-NFW}, we follow the green curve, starting from the origin for very distant images and progressing toward the halo center. We first recall the discussion following Equation~(\\ref{eq:NFW_phi}), which implies that for the NFW profile the direction of the phase corresponds in polar coordinates to the tangential, and the direction perpendicular to the phase to the radial direction, respectively. Initially, the image has positive parity with magnification increasing from $1$ and the flattening increasing from $0$. The image is expanded tangentially ($|\\lambda_\\parallel|>1$) but contracted radially ($|\\lambda_\\perp|<1$), as indicated by the position above the dashed line.\n\nBetween $x=2$ and $x=1$ the green line crosses the diagonal and the image becomes expanded also radially ($|\\lambda_\\perp|>1$). For lower $x$, the green curve approaches the solid black tangential-critical-curve line, along which the magnification and $|\\lambda_\\parallel|$ become infinite, the flattening increases to $1$, and $|\\lambda_\\perp|$ remains finite. After crossing the tangential critical curve at $x=x_{\\text{T}}\\approx0.155$, the image parity changes to negative (due to the sign of $\\lambda_\\parallel$ changing to negative) and the magnification and flattening decrease. The maximum distortion stays oriented tangentially until reaching the unit-convergence circle, $x=x_0\\approx0.0936$, corresponding to the vertical solid black line at $\\kappa=1$. To the right of this line the maximum distortion is oriented radially. At the unit-convergence radius the negative-parity image is close to its lowest magnification (though it stays higher than $16$ in this case, as indicated by the hyperbolic contours), and with zero flattening its shape is undistorted.\n\nProceeding further toward the halo center, the magnification and flattening increase again, with the radial expansion $|\\lambda_\\perp|$ growing rapidly while the tangential expansion $|\\lambda_\\parallel|$ decreases. At the solid black radial-critical-curve line, which corresponds to $x=x_{\\text{R}}\\approx0.056$, the magnification and $|\\lambda_\\perp|$ become infinite, the flattening increases to $1$, and $|\\lambda_\\parallel|$ remains finite. For lower radii, image parity changes back to positive (due to the sign of $\\lambda_\\perp$ changing to negative), and the magnification and flattening decrease. The expansion in both perpendicular directions decreases, until the intersection with the dashed black line at $x\\approx 0.019$. Images lying closer to the halo center are tangentially contracted rather than expanded ($|\\lambda_\\parallel|<1$). After crossing the solid black unit-magnification hyperbola at $x\\approx 0.0105$, all images are demagnified. The last important intersection occurs at $x\\approx 0.0068$; images to the right of the last dashed black line are contracted even radially ($|\\lambda_\\perp|<1$), and their magnification and flattening decrease to $0$ at the halo center.\n\nThe three orange points marked along the green curve in Figure~\\ref{fig:CS-NFW} correspond to the radial positions of the centers of the three images in the top right panel of Figure~\\ref{fig:images}. The first point at $x\\approx 0.188$ corresponds to the image outside the tangential critical curve at the right side of the panel. The second point at $x\\approx 0.115$ corresponds to the image between the tangential critical curve and the unit-convergence circle at the left side of the panel. The third point at $x\\approx 0.014$ corresponds to the smallest image inside the radial critical curve. The magnification, parity, flattening, orientation, and the two scaling factors of the corresponding images can be determined from the positions of these points in the diagram in Figure~\\ref{fig:CS-NFW}. Note that the values obtained from the diagram are technically valid at the positions of the source-center images and thus correspond to the local ``point-source'' values. Taking into account the radial extent of each image, their position (and the relevant range of their properties) should be marked by line segments along the green line in Figure~\\ref{fig:CS-NFW} rather than by points.\n\n\n\\subsection{Weak Shear and Phase}\n\\label{sec:NFW-weak}\n\nWeak-lensing cluster-mass reconstructions are based on statistical analyses of the images of background galaxies \\citep{kaiser_squires93}. The convergence map is computed from maps of the shear components. These are constructed from the shear and the phase, which are in turn determined from the shapes and orientations of the images. In the weak-lensing limit, the geometry of the elliptical image of a small circular source corresponds to the second panel of Figure~\\ref{fig:small-source-images}. Its axis ratio yields the shear and the orientation angle of its major axis is equal to the phase. The semi-minor to semi-major axis ratio $b/a$ is obtained by expanding the ratio $\\lambda_\\perp/\\lambda_\\parallel$ of the eigenvalues from Equation~(\\ref{eq:A_eigenvalues}) to first order in $\\kappa$ and $\\gamma$. The ellipticity, which is equal to the flattening of the image defined in Equation~(\\ref{eq:flattening}), reduces in this limit to\n\\beq\nf = 1-b/a\\simeq2\\,\\gamma\\,,\n\\label{eq:ellipticity}\n\\eeq\ni.e., double the value of the local shear. For illustration, for the image in the second panel of Figure~\\ref{fig:small-source-images} Equation~(\\ref{eq:ellipticity}) yields  an approximate flattening $2\\,\\gamma=0.32$, which is an $8\\%$ overestimate of the actual value $f\\approx0.30$ from Equation~(\\ref{eq:flattening}).\n\nBased on the weak-lensing regime, we introduce the weak shear and weak phase, which are computed from the images using the weak-lensing relations from the previous paragraph. We define the weak shear as\n\\beq\n\\gamma_{\\text{w}}(\\boldsymbol x)=\\frac{1}{2}\\,f\\left(\\kappa(\\boldsymbol x),\\gamma(\\boldsymbol x)\\right)\\,,\n\\label{eq:weak_shear}\n\\eeq\nwhere the flattening $f$ is computed from Equation~(\\ref{eq:flattening}) using lens-specific convergence and shear functions. Note that $\\gamma_{\\text{w}}$ by its definition attains only values from the interval $\\left[0,\\,0.5\\right]$. In the weak-lensing regime $\\gamma_{\\text{w}}\\approx \\gamma$, but as $\\gamma$ and $\\kappa$ increase, the weak shear computed from the image flattening deviates from the shear.\n\nFor the NFW halo we compute the weak shear $\\gamma_{\\text{w,\\tiny NFW}}(\\boldsymbol x)$ from Equation~(\\ref{eq:weak_shear}) using its convergence from Equation~(\\ref{eq:NFW_kappa}) and shear from Equation~(\\ref{eq:NFW_gamma}). The second panel in Figure~\\ref{fig:NFW-line} shows a plot of $\\gamma_{\\text{w,\\tiny NFW}}(\\boldsymbol x)$ in the central part of a NFW halo, using the same color scale as for the shear in the first panel of the figure. Due to its relation to the flattening in Equation~(\\ref{eq:weak_shear}), the plot can be interpreted following the $f$ values along the green curve in the diagram in Figure~\\ref{fig:CS-NFW}. At the halo center the flattening and thus also the weak shear are equal to zero. Going outward from the center, the weak shear increases to its maximum value of $0.5$ at the radial critical curve, marked by the inner black circle. From there it drops to $0$ at the unit-convergence radius and increases back to $0.5$ at the tangential critical curve, marked by the outer black circle. Beyond the tangential critical curve the weak shear drops asymptotically to $0$. Comparison with the left panel shows the substantial difference between the shears in the central region of the NFW halo, in terms of amplitude as well as radial pattern. Here the variations in image distortion are primarily driven by the convergence rather than by the shear.\n\nIn the third panel of Figure~\\ref{fig:NFW-line} we illustrate the difference between the first two panels by plotting the relative deviation of the weak shear from the shear, $\\gamma_{\\text{w,\\tiny NFW}}/\\gamma_{\\text{\\tiny NFW}}-1$. The blue regions in which the weak shear underestimates the shear are limited to the vicinity of the origin and the vicinity of the unit-convergence circle. In both cases the weak shear drops to zero and the relative deviation thus reaches $-1$, its minimum possible value. Everywhere else the weak shear overestimates the shear, with the positive deviation peaking at the critical curves and dropping to $0$ asymptotically. Note that the maxima at the critical curves may be negative for NFW halos with a sufficiently high convergence parameter $\\kappa_{\\text{s}}$, for which $\\gamma_{\\text{\\tiny NFW}}(x_{\\text{R}})$ or even $\\gamma_{\\text{\\tiny NFW}}(x_{\\text{T}})$ exceeds the weak-shear value at critical curves (i.e., $0.5$).\n\nIn addition to the weak shear, we define the image-based weak phase $\\varphi_{\\text{w}}$ as the angle between the major axis of the image of a small circular source and the horizontal ($x_1$) axis of the image plane. Taking into account Figure~\\ref{fig:small-source-images} and the discussion preceding Equation~(\\ref{eq:flattening}), in the case of the NFW halo it is related to the phase $\\varphi_{\\text{\\tiny NFW}}$ as follows:\n\\beq\n\\varphi_{\\text{w,\\tiny{NFW}}}(\\boldsymbol x)=\\Bigg\\{\n\\begin{array}{ll}\n\\,\\varphi_{\\text{\\tiny NFW}}(\\boldsymbol x) & \\,\\text{for $\\kappa_{\\text{\\tiny NFW}}(\\boldsymbol x)<1$}\\,,\\\\\n\\,\\varphi_{\\text{\\tiny NFW}}(\\boldsymbol x)+\\pi/2 & \\,\\text{for $\\kappa_{\\text{\\tiny NFW}}(\\boldsymbol x)>1$}\\,.\n\\end{array}\n\\label{eq:weak_phase}\n\\eeq\nWe use values from the interval $\\left[-\\pi/2,\\,\\pi/2\\right]$ for both $\\varphi_{\\text{\\tiny NFW}}$ and $\\varphi_{\\text{w,\\tiny{NFW}}}$.\n\nThe fourth panel of Figure~\\ref{fig:NFW-line} shows a color map of the weak phase $\\varphi_{\\text{w,\\tiny{NFW}}}$ of the NFW halo. The white regions with $\\varphi_{\\text{w,\\tiny{NFW}}}=0$ correspond to horizontally elongated images, in the orange regions with $\\varphi_{\\text{w,\\tiny{NFW}}}>0$ the images are oriented counterclockwise and in the blue regions with $\\varphi_{\\text{w,\\tiny{NFW}}}<0$ the images are oriented clockwise from the horizontal. The bright orange/blue boundaries correspond to images elongated exactly vertically, with $|\\varphi_{\\text{w,\\tiny{NFW}}}|=\\pi/2$. The weak phase flips by $\\pi/2$ along the unit-convergence circle, separating the inner radially oriented from the outer tangentially oriented images. The weak phase is undefined along this circle as well as at the origin, which corresponds to zero flattening. The dot-dashed lines added for orientation mark the positions of all images with exactly horizontal, exactly vertical, or undefined orientation.\n\nSince the weak phase differs from the phase only by the $\\pi/2$ flip inside the unit-convergence circle, a similar color map of the phase $\\varphi_{\\text{\\tiny NFW}}(\\boldsymbol x)$ would differ merely by having inverted color and saturation inside the circle. In other words, the color and saturation outside the circle in the fourth panel of Figure~\\ref{fig:NFW-line} would be radially extended to the halo center. Hence, the first and third quadrants would be entirely blue and the second and fourth quadrants would be entirely orange.\n\nThe plots of the different quantities in Figure~\\ref{fig:NFW-line} are presented as reference plots to aid the interpretation of the results for the NFW halo + point-mass lens model presented in Section~\\ref{sec:NFWP-plots}.\n\n\n\\section{Lensing by a NFW Halo + Point Mass}\n\\label{sec:NFWP}\n\n\\subsection{Convergence, Shear, and Phase}\n\\label{sec:NFWP-csp}\n\nAdding a compact massive object modeled by a point mass positioned at $\\bm{x_{\\text{P}}}$ changes the convergence to\n\\beq\n\\kappa(\\boldsymbol x)=2\\,\\kappa_{\\text{s}}\\;\\frac{1-\\mathcal{F}(x)}{x^2-1} +\\pi\\,\\kappa_{\\text{P}}\\,\\delta(\\boldsymbol x-\\bm{x_{\\text{P}}})\\,,\n\\label{eq:NFWP_kappa}\n\\eeq\nwhere the mass parameter $\\kappa_{\\text{P}}$ corresponds to the ratio of the solid angles subtended by the point-mass Einstein circle and by the halo scale-radius circle \\citepalias[for more details, see][]{karamazov_etal21}. Hence, $\\sqrt{\\kappa_{\\text{P}}}$ is the point-mass Einstein radius in units of the halo scale radius. The convergence in Equation~(\\ref{eq:NFWP_kappa}) is identical to the NFW halo convergence from Equation~(\\ref{eq:NFW_kappa}), except exactly at the position of the added point mass. The lens equation can be written as\n\\beq\n\\boldsymbol y=\\boldsymbol x - 4\\,\\kappa_{\\text{s}}\\,\n\\left[\\ln{\\frac{x}{2}}+\\mathcal{F}(x)\\right]\\,\\frac{\\boldsymbol x}{x^2}-\\kappa_{\\text{P}}\\,\\frac{\\boldsymbol x-\\bm{x_{\\text{P}}}}{|\\boldsymbol x - \\bm{x_{\\text{P}}}|^2}\\,,\n\\label{eq:NFWP_lens_equation}\n\\eeq\nin the form used in \\citetalias{karamazov_etal21}.\n\nFor illustration, in the bottom row of Figure~\\ref{fig:images} we show the lensing of the same circular source as in the top row, by the same NFW halo with an additional point mass with mass parameter $\\kappa_{\\text{P}}\\approx 2.714\\cdot 10^{-4}$ positioned at $\\bm{x_{\\text{P}}}=(0.2,\\,0)$. This parameter combination is selected from the parameter-space grid used in \\citetalias{karamazov_etal21}. The position of the point mass is indicated by its Einstein circle (cyan) in the bottom right panel. As seen in the bottom left panel, the black circular source lies inside the weakly perturbed radial caustic, with its upper part lying also inside the strongly perturbed tangential caustic. For a source not lying on the caustic, lens Equation~(\\ref{eq:NFWP_lens_equation}) yields $2, 4,$ or $6$ images. For a source lying on the caustic, several images appear combined into a lower number of macro-images. In the example shown in the bottom right panel there are five macro-images. Four of them are images of the full source; the fifth macro-image to the top left of the point mass consists of two additional images of the upper part of the source joined along the critical curve. Comparing the images with those in the top right panel, we see that the left and central images are affected only weakly by the point mass. The right image is affected more strongly, plus there are two new images closer to the point mass. For these images in particular, their distortion cannot be simply classified as tangential or radial.\n\nIn order to compute the shear we start from the lens potential, which has an additional term due to the point mass,\n\\beq\n\\psi(\\boldsymbol x)=\\psi_{\\text{\\tiny NFW}}(x)+\\frac{r_{\\text{s}}^2}{D_{\\text{l}}^2}\\,\\kappa_{\\text{P}}\\, \\ln{|\\boldsymbol x-\\bm{x_{\\text{P}}}|}\\,,\n\\label{eq:NFWP_potential}\n\\eeq\nwhere $\\psi_{\\text{\\tiny NFW}}(x)$ is given by Equation~(\\ref{eq:NFW_psi}).\n\nThe point-mass shear has the simple form\n\\beq\n\\gamma_{\\text{P}}(\\boldsymbol x)=\\frac{\\kappa_{\\text{P}}}{|\\boldsymbol x - \\bm{x_{\\text{P}}}|^2}\\,,\n\\label{eq:P_gamma}\n\\eeq\ndivergent at the point-mass position and dropping rapidly outward. Since the NFW halo shear peaks at its central value $\\kappa_{\\text{s}}$, the added point mass will dominate the lens shear in its vicinity, wherever it may be positioned. The shear can be generally computed from the second derivatives of the lens potential, namely\n\\beq\n\\gamma=\\frac{D_{\\text{l}}^2}{r_{\\text{s}}^2}\\,\\sqrt{ \\frac{1}{4}\\left(\\,\\psi,_{\\scriptscriptstyle 11}-\\psi,_{\\scriptscriptstyle 22}\\right)^2+\\left(\\psi,_{\\scriptscriptstyle 12}\\right)^2}\\,,\n\\label{eq:shear}\n\\eeq\nwhere the commas denote partial derivatives with respect to image-plane coordinates $(x_1,x_2)$. For our combined lens we compute the derivatives of the lens potential from Equation~(\\ref{eq:NFWP_potential}) and get\n\\beq\n\\gamma(\\boldsymbol x)=\\sqrt{\\left[\\gamma_{\\text{\\tiny{NFW}}}(x)-\\gamma_{\\text{P}}(\\boldsymbol x)\\right]^2+ 4\\,\\gamma_{\\text{\\tiny{NFW}}}(x)\\,\\gamma_{\\text{P}}(\\boldsymbol x)\\,\\cos^2{\\omega(\\boldsymbol x)}}\\,,\n\\label{eq:NFWP_gamma}\n\\eeq\nwhere $\\gamma_{\\text{\\tiny{NFW}}}(x)$ and $\\gamma_{\\text{P}}(\\boldsymbol x)$ are given by Equation~(\\ref{eq:NFW_gamma}) and Equation~(\\ref{eq:P_gamma}), respectively, and\n\\beq\n\\cos{\\omega(\\boldsymbol x)}=\\frac{{\\bm x}\\cdot(\\boldsymbol x - \\bm{x_{\\text{P}}})}{x\\,|\\boldsymbol x - \\bm{x_{\\text{P}}}|}\n\\label{eq:cos}\n\\eeq\n\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{0cm}\n\\includegraphics[width=6 cm]{f05.pdf}\n\\caption{Viewing angle $\\omega(\\boldsymbol x)$ of the line segment from the halo center to the point mass, appearing in Equation~(\\ref{eq:NFWP_gamma}) for computing the shear $\\gamma(\\boldsymbol x)$ of the combined lens. Points on the horizontal axis mark the halo center and the point-mass position $\\bm{x_{\\text{P}}}$; the dot at the vertex of the angle marks the position $\\boldsymbol x$. Contours of constant $\\omega$ are symmetric pairs of circular arcs connecting the centers of the two lens components, with the dashed line marking the arc passing through position $\\boldsymbol x$. The values of $\\omega$ range from $\\pi$ along the line segment to $0$ along the rest of the horizontal axis.\\label{fig:omega}}}\n\\efi\n\nis the dot product of the unit vectors pointing to $\\bm x$ from the halo center and from the point-mass position. In terms of image-plane geometry, $\\omega$ is the viewing angle from point $\\boldsymbol x$ of the line segment connecting the halo center and the point-mass position. As shown in Figure~\\ref{fig:omega}, curves of constant $\\omega$ are circular arcs connecting symmetrically the halo center and the point-mass position. Note that $\\omega$ is also equal to the angle\nbetween the tangent to the arc at either of its end points and the outward horizontal direction, as follows from the tangent--chord theorem (alternate segment theorem).\n\nAlong the line segment connecting the center and the point mass the viewing angle reaches its maximum, $\\omega=\\pi$, while along the rest of the horizontal axis it reaches its minimum, $\\omega=0$. In both cases, $\\cos^2{\\omega}=1$ and the total shear from Equation~(\\ref{eq:NFWP_gamma}) is $\\gamma(\\boldsymbol x)=\\gamma_{\\text{\\tiny{NFW}}}(x)+\\gamma_{\\text{P}}(\\boldsymbol x)$. In this case both shears act in the same orientation, so that their combination is maximal. Along the circle bisected by the line segment, we find $\\omega=\\pi/2$ according to Thales's theorem and the total shear is $\\gamma(\\boldsymbol x)=|\\gamma_{\\text{\\tiny{NFW}}}(x)-\\gamma_{\\text{P}}(\\boldsymbol x)|$. In this case the two shears act in perpendicular directions, so that their combination is minimal. For the pair of small arcs in Figure~\\ref{fig:omega} with $\\omega=3\\pi/4$ and for the pair of large arcs with $\\omega=\\pi/4$ we get $\\cos^2{\\omega}=1/2$ and the total shear is $\\gamma(\\boldsymbol x)=\\sqrt{\\gamma_{\\text{\\tiny{NFW}}}^2(x)+\\gamma_{\\text{P}}^2(\\boldsymbol x)}$.\n\nAs discussed in Section~\\ref{sec:NFW-csp}, for the NFW halo the central shear is defined, $\\gamma_{\\text{\\tiny{NFW}}}(0)=\\kappa_\\text{s}$, while the phase and shear components are undefined. The same holds for the central properties of the point-mass lens. However, for the combined lens even the shear at the halo center is undefined. For $x\\to 0$ Equation~(\\ref{eq:NFWP_gamma}) yields\n\\beq\n\\gamma(\\boldsymbol x)\\to\\sqrt{\\left[\\kappa_{\\text{s}}-\\kappa_{\\text{P}}\\,x_{\\text{P}}^{-2}\\right]^2+ 4\\,\\kappa_{\\text{s}}\\kappa_{\\text{P}}\\,x_{\\text{P}}^{-2}\\,\\cos^2{\\omega}}\\,,\n\\label{eq:NFWP_gamma_center}\n\\eeq\na value that depends on the direction of approach to the center, due to the directional dependence of $\\omega$. As seen from Figure~\\ref{fig:omega} and as explained in the discussion above, approaching the center along the horizontal axis leads to the highest value (the sum of the two shears) while an approach along the vertical axis leads to the lowest value (the absolute value of the difference of the two shears). The situation at the position of the point mass is similar, though here the angular differences are suppressed by the divergence of $\\gamma_{\\text{P}}$.\n\nThe range of values of the shear occurring in the studied central region of the image plane is larger than for the halo or the point mass separately. Its upper limit is $\\infty$, due to the divergence of $\\gamma_{\\text{P}}$ at the point-mass position $\\bm{x_{\\text{P}}}$. Its lower limit may reach $0$. As seen from the form of Equation~(\\ref{eq:NFWP_gamma}), this may occur only along the $\\omega=\\pi/2$ circle at points where $\\gamma_{\\text{\\tiny{NFW}}}(x)=\\gamma_{\\text{P}}(\\boldsymbol x)$. Following the circle from the halo center to the point mass, $\\gamma_{\\text{\\tiny{NFW}}}$ decreases while $\\gamma_{\\text{P}}$ increases. Zero-shear points thus exist only if $\\gamma_{\\text{\\tiny{NFW}}}$ is equal to or larger than $\\gamma_{\\text{P}}$ at the halo center. Hence, for $x_{\\text{P}}<\\sqrt{\\kappa_{\\text{P}}/\\kappa_{\\text{s}}}$ there are no zero-shear points. In this range, for point masses closest to the halo center, the minimum shear $\\gamma=\\kappa_{\\text{P}}\\,x_{\\text{P}}^{-2}-\\kappa_{\\text{s}}$ occurs at the halo center when approached along the vertical axis.\n\nZero-shear points exist for all larger point-mass distances from the halo center. For $x_{\\text{P}}=\\sqrt{\\kappa_{\\text{P}}/\\kappa_{\\text{s}}}$ there is one zero-shear point located directly at the halo center. For any $x_{\\text{P}}>\\sqrt{\\kappa_{\\text{P}}/\\kappa_{\\text{s}}}$ there are two zero-shear points lying symmetrically above and below the horizontal axis on the $\\omega=\\pi/2$ circle. With increasing distance of the point mass from the halo center the zero-shear points shift along the circle toward the position of the point mass, so that for larger distances they lie nearly vertically above and below the point mass. Their separation from the point mass, which is approximately $\\sqrt{\\kappa_{\\text{P}}/\\gamma_{\\text{\\tiny{NFW}}}(x_\\text{P})}$ in this regime, increases with distance as the halo shear decreases.\n\nImage-plane maps of the shear $\\gamma(\\boldsymbol x)$ for different masses and positions of the point mass are presented and discussed in Section~\\ref{sec:plots-shear}.\n\nThe shear components are defined by\n\\beq\n(\\gamma_1,\\gamma_2)=\\gamma\\, (\\cos{2\\varphi},\\sin{2\\varphi})\\,,\n\\label{eq:NFWP_gamma12}\n\\eeq\nwhere the trigonometric functions of the phase $\\varphi$ can be computed for a point $\\boldsymbol x = (x_1,x_2)$ in the image plane as\n\\begin{eqnarray}\n\\label{eq:NFWP_phi}\n \\nonumber \\cos{2\\varphi} &=& \\frac{1}{\\gamma(\\boldsymbol x)} \\left[\\frac{x_2^2-x_1^2}{x^2}\\, \\gamma_{\\text{\\tiny{NFW}}}(x)\\right.\\\\\n \\nonumber & & \\left.\\hspace{1cm}+\\frac{(x_2-x_{\\text{P}2})^2-(x_1-x_{\\text{P}1})^2}{|\\boldsymbol x - \\bm{x_{\\text{P}}}|^2}\\, \\gamma_{\\text{P}}(\\boldsymbol x)\\right]\\\\[1ex]\n & & \\\\\n \\nonumber \\sin{2\\varphi} &=& \\frac{-2}{\\gamma(\\boldsymbol x)}\\left[\\frac{x_1 x_2}{x^2}\\, \\gamma_{\\text{\\tiny{NFW}}}(x)\\right.\\\\\n \\nonumber & & \\left.\\hspace{1cm}+\\frac{(x_1-x_{\\text{P}1})(x_2-x_{\\text{P}2})}{|\\boldsymbol x - \\bm{x_{\\text{P}}}|^2}\\, \\gamma_{\\text{P}}(\\boldsymbol x)\\right]\\,,\n\\end{eqnarray}\nwhere the shears $\\gamma(\\boldsymbol x),\\gamma_{\\text{\\tiny{NFW}}}(x),$ and $\\gamma_{\\text{P}}(\\boldsymbol x)$ are given by Equations~(\\ref{eq:NFWP_gamma}), (\\ref{eq:NFW_gamma}), and (\\ref{eq:P_gamma}), respectively. Note that in this case the phase and the shear components are undefined at the halo center and at the point-mass position, since the phase as well as the shear $\\gamma(\\boldsymbol x)$ depend on the direction of approach to these points.\n\nWe would like to point out that Equation~(\\ref{eq:NFWP_gamma}) is a special case of the more general formula\n\\beq\n\\gamma=\\sqrt{(\\gamma_\\text{A}-\\gamma_\\text{B})^2+ 4\\,\\gamma_\\text{A}\\gamma_\\text{B}\\cos^2{(\\varphi_\\text{A}-\\varphi_\\text{B})}}\n\\label{eq:shear_combination}\n\\eeq\nfor the shear of a combination of two mass distributions with shears $\\gamma_\\text{A}, \\gamma_\\text{B}$ and phases $\\varphi_\\text{A}, \\varphi_\\text{B}$. For two circularly symmetric mass distributions with the same sign of the expression $\\psi''-x^{-1}\\psi'$ that appears in Equation~(\\ref{eq:shear_sym}), the absolute value of the phase difference in Equation~(\\ref{eq:shear_combination}) is equal to the viewing angle $\\omega$.\n\nThe expression $\\psi''-x^{-1}\\psi'$ is globally negative for a range of mass distributions, such as for the NFW profile, for a point mass, for a singular or a non-singular (cored) isothermal sphere. For a combination of two such distributions, the formula for the shear in Equation~(\\ref{eq:NFWP_gamma}), the following discussion, and the formulae for the phase in Equation~(\\ref{eq:NFWP_phi}) are valid. For example, the case of two point masses was studied by \\cite{schneider_weiss86}, and the case of two isothermal spheres was studied by \\cite{shin_evans08}.\n\n\n\\subsection{Jacobian and Umbilic Points}\n\\label{sec:NFWP-Jacobian}\n\nThe Jacobian of the lens equation can be computed from Equation~(\\ref{eq:Jacobian}) using the convergence from Equation~(\\ref{eq:NFWP_kappa}) and the shear from Equation~(\\ref{eq:NFWP_gamma}). Its explicit form is presented in \\citetalias{karamazov_etal21}, together with a detailed analysis of the critical curves which are obtained by setting the Jacobian equal to zero. The parts of the critical curve lying outside the unit-convergence circle ($x>x_0$) satisfy the equation\n\\beq\n\\gamma(\\boldsymbol x)=1-\\kappa(\\boldsymbol x)\\,,\n\\label{eq:NFWP_cc-tangential}\n\\eeq\nwhich yields the tangential critical curve in absence of the point mass. The parts lying inside the unit-convergence circle ($x<x_0$) satisfy the equation\n\\beq\n\\gamma(\\boldsymbol x)=\\kappa(\\boldsymbol x)-1\\,,\n\\label{eq:NFWP_cc-radial}\n\\eeq\nwhich yields the radial critical curve in absence of the point mass.\n\nEquation~(\\ref{eq:Jacobian}) also indicates that for the Jacobian to be equal to zero at a point lying directly on the unit-convergence circle, the shear must be zero at such a point. From the properties of zero-shear points discussed in Section~\\ref{sec:NFWP-csp} it follows that such critical-curve points must lie at the intersections of the unit-convergence circle and the $\\omega=\\pi/2$ circle extending from the halo center to the point-mass position.\n\nFor $x_{\\text{P}}<x_0$ these circles have no intersection and, thus, there are no critical-curve points along the unit-convergence circle. For $x_{\\text{P}}=x_0$ these circles have an intersection exactly at the position of the point mass. However, at this point the shear is not zero, so that even in this case there is no critical-curve point along the unit-convergence circle. For any $x_{\\text{P}}>x_0$ these circles have two intersections. In this case the requirement of zero shear leads to the condition\n\\beq\nx_{\\text{P}}=x_0\\,\\sqrt{1+\\kappa_{\\text{P}}\\,/ \\left[4\\kappa_{\\text{s}}\\left(1+\\ln{\\frac{x_0}{2}}\\right)+2-3 x_0^2\\right]}\\,.\n\\label{eq:umbilic_condition}\n\\eeq\nWe conclude that for any value of the mass parameter $\\kappa_{\\text{P}}$ Equation~(\\ref{eq:umbilic_condition}) yields a single corresponding distance of the point mass from the halo center, for which the critical curve has points lying on the unit-convergence circle.\n\nIf we place the point mass along the horizontal axis in the image plane at $\\bm{x_{\\text{P}}}=(x_{\\text{P}},0)$, the positions of these critical-curve points are\n\\beq\n\\boldsymbol x=\\left(\\frac{x_0}{x_{\\text{P}}}, \\pm\\sqrt{1-\\frac{x_0^2}{x_{\\text{P}}^2}}\\right)\\,x_0\\,,\n\\label{eq:umbilic_points}\n\\eeq\nwith the value of $x_{\\text{P}}$ given by Equation~(\\ref{eq:umbilic_condition}). These points with $\\kappa=1$ and $\\gamma=0$ have special significance. As discussed in Appendix~\\ref{sec:Appendix-images}, such critical-curve points correspond to umbilics. Equation~(\\ref{eq:umbilic_condition}) thus presents a condition for the existence of umbilics in the studied lens system. In the $(\\kappa_{\\text{P}},x_{\\text{P}})$ parameter-space plots in Figure~6 of \\citetalias{karamazov_etal21}, Equation~(\\ref{eq:umbilic_condition}) describes the green and violet umbilic boundary, starting at $x_{\\text{P}}=x_0$ at the $\\kappa_{\\text{P}}=0$ vertical axis and increasing monotonically for higher $\\kappa_{\\text{P}}$. In the image plane, the umbilic points lie along the unit-convergence circle. For $\\kappa_{\\text{P}}\\ll 1$ they are located close to the horizontal axis in the direction of the point mass. Their displacement from the axis increases with increasing $\\kappa_{\\text{P}}$.\n\n\n\\subsection{Geometry of Images}\n\\label{sec:NFWP-images}\n\nThe geometry of the images can be studied using the eigenvalue decomposition of the inverse of the Jacobian matrix given by Equation~(\\ref{eq:A_matrix}) and the convergence--shear (CS) diagram introduced in Appendix~\\ref{sec:Appendix-images}. For a point mass placed at the center of the halo, the convergence and shear are purely radial functions. Hence, the range of their possible combinations is limited to the $\\left(\\kappa(x),\\gamma(x)\\right)$ curve in the CS diagram. In this case the analysis of possible image geometries can directly follow the example presented in Section~\\ref{sec:NFW-images} for images formed by the NFW halo.\n\nEven when the point mass is positioned away from the halo center, the convergence given by Equation~(\\ref{eq:NFWP_kappa}) preserves its circular symmetry (with the exception of the single point at the position of the point mass). This means that any convergence value $\\kappa$ can be one-to-one translated to the corresponding radial distance from the halo center $x$. However, the shear given by Equation~(\\ref{eq:NFWP_gamma}) loses circular symmetry. In the CS diagram this results in the range of possible $\\left(\\kappa(\\boldsymbol x),\\gamma(\\boldsymbol x)\\right)$ combinations covering a two-dimensional region. In terms of image distortions and orientations, the lack of symmetry means that instead of the terms ``tangential'' and ``radial'' we revert to the more general ``in the direction of the phase'' and ``perpendicular to the phase'', respectively.\n\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{0cm}\n\\includegraphics[width=8.5 cm]{f06.pdf}\n\\caption{CS diagram illustrating the geometry of images formed by the NFW halo with a $\\kappa_{\\text{P}}\\approx 2.714\\cdot 10^{-4}$ point mass at $x_{\\text{P}}=0.2$, from the bottom row of Figure~\\ref{fig:images}. The purple-shaded area marks the range of $\\left(\\kappa(\\boldsymbol x),\\gamma(\\boldsymbol x)\\right)$ combinations of the lens; the green line marks the $\\left(\\kappa_{\\text{\\tiny NFW}}(x),\\gamma_{\\text{\\tiny NFW}}(x)\\right)$ combinations of the halo-only lens from Figure~\\ref{fig:CS-NFW}. The green tick marks and labels indicate the radial distance $x$ along vertical lines in this diagram. The top purple line marks the maximum shear along a circle with radius $x$ centered on the halo, which always occurs in the direction of the point mass. The bottom purple line marks the minimum shear along the circle; for larger radii $x$ (to the left of the purple tick mark) this occurs in the direction opposite the point mass; for smaller radii $x$ (to the right of the purple tick mark) this occurs at two symmetric off-axis points along the circle. The orange dots mark the $(\\kappa,\\gamma)$ combinations at the positions of the four full images in the bottom right panel of Figure~\\ref{fig:images}.\\label{fig:CS-NFWP}}}\n\\efi\n\nFor illustration, in Figure~\\ref{fig:CS-NFWP} we present the CS diagram for a NFW halo with a $\\kappa_{\\text{P}}\\approx 2.714\\cdot 10^{-4}$ point mass located at $x_{\\text{P}}=0.2$. The purple-shaded region bounded by the bold purple lines shows the range of $(\\kappa,\\gamma)$ combinations occurring in the image plane. Added for orientation is the green curve from Figure~\\ref{fig:CS-NFW} showing the $(\\kappa,\\gamma)$ combinations of the NFW halo without the point mass. As in Figure~\\ref{fig:CS-NFW}, the tick marks along the curve mark radial distances from the halo center. At any value of $x$ along this axis, the vertical extent between the bold purple lines indicates the range of shear values $\\gamma(\\boldsymbol x)$ occurring along the circle $|\\boldsymbol x|=x$. The maximum shear always occurs in the direction of the point mass, i.e., for $\\boldsymbol x=(x,0)$. At large distances $x$, the minimum shear occurs in the direction opposite to the point mass, i.e., for $\\boldsymbol x=(-x,0)$. At lower distances, to the right of the purple vertical tick mark (in the case of Figure~\\ref{fig:CS-NFWP} near $x=0.4$), minimum shear occurs at two points offset symmetrically from the axis connecting the halo center and the point mass. Close to the halo center, at the right edge of the diagram, minimum shear occurs at points offset nearly perpendicularly from the halo center, i.e., for $\\boldsymbol x\\approx(0,\\pm\\,x)$. Equation~(\\ref{eq:NFWP_gamma_center}) shows that at the center of the halo the maximum shear is $\\kappa_{\\text{s}}+\\kappa_{\\text{P}}\\,x_{\\text{P}}^{-2}$ and the minimum shear is $|\\kappa_{\\text{s}}-\\kappa_{\\text{P}}\\,x_{\\text{P}}^{-2}|$.\n\nIn the example shown in Figure~\\ref{fig:CS-NFWP}, the shear range at large radii does not visibly deviate from the green NFW halo shear. At radii lower than the purple tick mark near $x=0.4$, the maximum shear starts to deviate substantially from the green curve. The minimum shear starts to deviate visibly between $x=0.3$ and $x=0.2$. Along the circle with the radius of the point-mass distance, $x=x_{\\text{P}}=0.2$, the maximum shear diverges at the position of the point mass. At a slightly lower radius, the minimum shear reaches $0$ at the positions of the off-axis zero-shear points. For lower radii, the shear interval shrinks back toward the NFW shear at the green curve. However, instead of reaching the central NFW shear $\\gamma_{\\text{\\tiny{NFW}}}(0)=\\kappa_s\\approx 0.2390$, the limiting shear at $x=0$ varies within the interval $[0.2322,0.2458]$, as discussed in the previous paragraph.\n\nThe four orange points marked in the purple region in Figure~\\ref{fig:CS-NFWP} correspond to the positions of the four images of the source center in the bottom right panel of Figure~\\ref{fig:images}. Note that the fifth macro-image lying on the critical curve in Figure~\\ref{fig:images} does not include an image of the source center. The point appearing at $(\\kappa,\\gamma)\\approx (0.637,1.255)$ corresponds to the image just to the right of the point mass in Figure~\\ref{fig:images}. The three remaining points in Figure~\\ref{fig:CS-NFWP} correspond to perturbed versions of the three images appearing in the absence of the point mass in the top right panel of Figure~\\ref{fig:images}. The point at $(\\kappa,\\gamma)\\approx (0.694,0.194)$ corresponds to the lower right image outside the critical curve in the bottom right panel of Figure~\\ref{fig:images}. The point at $(\\kappa,\\gamma)\\approx (0.897,0.233)$ corresponds to the image just outside the unit-convergence circle at the left side of the panel in Figure~\\ref{fig:images}. The fourth point at $(\\kappa,\\gamma)\\approx (1.962,0.244)$ corresponds to the small image close to the halo center in Figure~\\ref{fig:images}.\n\nThe properties of the images can be determined from the positions of the points in the diagram; the changes in the properties of the latter three due to the presence of the point mass can be studied by comparing the diagrams in Figures~\\ref{fig:CS-NFWP} and \\ref{fig:CS-NFW}. Note that, in this case, taking into account the full extent of each image would require marking them in the CS diagram by exact patches covering the corresponding range of $(\\kappa,\\gamma)$ combinations instead of by the points used in Figure~\\ref{fig:CS-NFWP}. This would permit including even partial images that do not contain an image of the source center, such as the fifth macro-image in the bottom right panel of Figure~\\ref{fig:images}.\n\nCS diagrams for different masses and positions of the point mass are presented and discussed in Section~\\ref{sec:plots-diagrams}.\n\n\n\\subsection{Weak Shear and Phase}\n\\label{sec:NFWP-weak}\n\nFollowing the example in Section~\\ref{sec:NFW-weak}, we use the geometry of image distortions to introduce the weak-lensing shear and phase estimates for the NFW halo + point-mass lens. We compute the weak shear $\\gamma_{\\text{w}}$ from Equation~(\\ref{eq:weak_shear}), substituting the convergence from Equation~(\\ref{eq:NFWP_kappa}) for $\\kappa(\\boldsymbol x)$, and the shear from Equation~(\\ref{eq:NFWP_gamma}) for $\\gamma(\\boldsymbol x)$.\n\nThe angle between the major axis of the image of a small circular source and the $x_1$ axis of the image plane is equal to the phase outside the unit-convergence circle; it is perpendicular to the phase inside the unit-convergence circle. Hence, the weak phase $\\varphi_{\\text{w}}$ is related to the phase $\\varphi$ as follows:\n\\beq\n\\varphi_{\\text{w}}(\\boldsymbol x)=\\Bigg\\{\n\\begin{array}{ll}\n\\,\\varphi(\\boldsymbol x) & \\,\\text{for $\\kappa(\\boldsymbol x)<1$}\\,,\\\\\n\\,\\varphi(\\boldsymbol x)+\\pi/2 & \\,\\text{for $\\kappa(\\boldsymbol x)>1$}\\,,\n\\end{array}\n\\label{eq:NFWP_weak_phase}\n\\eeq\nwhere $\\varphi(\\boldsymbol x)$ is given by Equation~(\\ref{eq:NFWP_phi}) and $\\kappa(\\boldsymbol x)$ by Equation~(\\ref{eq:NFWP_kappa}).\nWe use values from the interval $\\left[-\\pi/2,\\,\\pi/2\\right]$ for both $\\varphi$ and $\\varphi_{\\text{w}}$.\n\nImage-plane maps of the weak shear $\\gamma_{\\text{w}}(\\boldsymbol x)$ and weak phase $\\varphi_{\\text{w}}(\\boldsymbol x)$ for different masses and positions of the point mass are presented and discussed in Section~\\ref{sec:plots-weak_shear} and Section~\\ref{sec:plots-weak_phase}, respectively.\n\n\n\\subsection{Lens Characteristics as a Function of Point-mass Parameters}\n\\label{sec:NFWP-plots}\n\n\\begin{deluxetable}{Lp{7.5cm}}[t]\n\\tabletypesize{\\footnotesize}\n\\tablecaption{List of Symbols\\label{tab:symbols_small}}\n\\tablehead{\\colhead{\\hspace{-0.3cm}Symbol} & \\colhead{\\hspace{-3.9cm}Description; First Appearance}}\n\\startdata\n \\mkern-9mu\\gamma & $\\mkern-8mu$Shear (combined model or general); Equation~(\\ref{eq:Jacobi_matrix}) \\\\\n \\mkern-9mu\\gamma_{\\text{\\tiny{NFW}}} & $\\mkern-8mu$Shear of NFW-halo lens; Equation~(\\ref{eq:NFW_gamma}) \\\\\n \\mkern-9mu\\gamma_{\\text{P}} & $\\mkern-8mu$Shear of a point-mass lens; Equation~(\\ref{eq:P_gamma}) \\\\\n \\mkern-9mu\\gamma_{\\text{w}} & $\\mkern-8mu$Weak shear (combined model or general); Equation~(\\ref{eq:weak_shear}) \\\\\n \\mkern-9mu\\gamma_{\\text{w,\\tiny{NFW}}} & $\\mkern-8mu$Weak shear of NFW-halo lens; Section~\\ref{sec:NFW-weak} \\\\\n \\mkern-9mu\\delta\\varphi_\\text{w} & $\\mkern-8mu$Weak-phase deviation due to the point mass; Section~\\ref{sec:plots-weak_phase_change} \\\\\n \\mkern-9mu\\kappa & $\\mkern-8mu$Convergence (combined model or general); Equation~(\\ref{eq:Jacobi_matrix}) \\\\\n \\mkern-9mu\\kappa_{\\text{\\tiny NFW}} & $\\mkern-8mu$Convergence of NFW-halo lens; Equation~(\\ref{eq:NFW_kappa}) \\\\\n \\mkern-9mu\\kappa_{\\text{P}},\\, \\kappa_{\\text{PC}} & $\\mkern-8mu$Mass parameter of a point mass and its critical value; \\\\\n  & $\\mkern347mu$Equation~(\\ref{eq:NFWP_kappa}) \\\\\n \\mkern-9mu\\kappa_{\\text{s}} & $\\mkern-8mu$NFW halo convergence parameter; Equation~(\\ref{eq:NFW_kappa}) \\\\\n \\mkern-9mu\\varphi & $\\mkern-8mu$Phase (combined model or general); Equation~(\\ref{eq:Jacobi_matrix}) \\\\\n \\mkern-9mu\\varphi_{\\text{\\tiny NFW}} & $\\mkern-8mu$Phase of NFW-halo lens; Equation~(\\ref{eq:NFW_gamma12}) \\\\\n \\mkern-9mu\\varphi_{\\text{w}} & $\\mkern-8mu$Weak phase (combined model or general); Equation~(\\ref{eq:NFWP_weak_phase}) \\\\\n \\mkern-9mu\\varphi_{\\text{w,\\tiny{NFW}}} & $\\mkern-8mu$Weak phase of NFW-halo lens; Equation~(\\ref{eq:weak_phase}) \\\\\n \\mkern-9mu\\psi & $\\mkern-8mu$Lens potential (combined model or general); Equation~(\\ref{eq:NFW_gradient}) \\\\\n \\mkern-9mu\\psi_{\\text{\\tiny NFW}} & $\\mkern-8mu$Lens potential of NFW-halo lens; Equation~(\\ref{eq:NFW_psi}) \\\\\n \\mkern-9mu\\omega & $\\mkern-8mu$Viewing angle of the line segment connecting the halo center and the point-mass position; Equation~(\\ref{eq:cos}) \\\\\n\\enddata\n\\end{deluxetable}\n\n\\renewcommand{\\thefigure}{7.A}\n\\begin{figure*}\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[width=15.5 cm]{f07_01.pdf}\n\\caption{Image-plane maps of the shear $\\gamma(\\boldsymbol x)$ of a NFW halo + point-mass lens, described in Section~\\ref{sec:plots-shear}. Columns correspond to sub-critical, critical, and super-critical mass parameters $\\kappa_{\\text{P}}$ marked at the top; rows correspond to point-mass positions $x_{\\text{P}}$ marked along the left side. Critical curves are plotted in black, and the point-mass location is marked by its Einstein ring (cyan). Magenta marks all positions with $\\gamma\\geq 1.5$.\\label{fig:shear}}}\n\\end{figure*}\n\nIn Sections~\\ref{sec:NFWP-csp}--\\ref{sec:NFWP-weak} we defined the lensing quantities of interest and described their general properties. In this section we present plots illustrating these lens characteristics for different point masses embedded in a NFW halo with a fiducial convergence parameter $\\kappa_{\\text{s}}\\approx 0.239035$. With the exception of Figure~\\ref{fig:CS-diagrams} and Figure~\\ref{fig:CS-diagrams-online}, all of the plots are presented as color maps in the image plane. For better orientation in these maps, we plot the critical curves (solid black) and mark the point-mass position by its Einstein ring (cyan).\n\nIn each of the following figures, the three columns of the plot grid correspond to the same three values of the mass parameter $\\kappa_{\\text{P}}$ of the point mass used in \\citetalias{karamazov_etal21}. These differ in the number of radial critical curves they generate for $x_{\\text{P}}=0$: sub-critical $\\kappa_{\\text{P}}=10^{-4}$ with two radial critical curves; critical $\\kappa_{\\text{P}}=\\kappa_{\\text{PC}}\\approx 2.714 \\cdot 10^{-4}$ with one radial critical curve; super-critical $\\kappa_{\\text{P}}=10^{-3}$ with no radial critical curve.\n\nFor each of the characteristics discussed in Sections~\\ref{sec:plots-shear}--\\ref{sec:plots-weak_phase_change} we present two plot grids. The rows in the first grid correspond to seven values of the point-mass position $x_{\\text{P}}$ increasing in steps of $0.05$ from $0$ to $0.3$. These parameter combinations correspond to the critical-curve and caustic gallery in Figure~5 of \\citetalias{karamazov_etal21}; they are marked by red crosses in the parameter-space plot in Figure~6 of \\citetalias{karamazov_etal21}. The rows in the second grid correspond to nineteen values of $x_{\\text{P}}$ increasing in steps of $0.01$ from $0$ to $0.15$, then in steps of $0.05$ to $0.3$ in the top row. These parameter combinations are marked by red and black crosses in Figure~6 of \\citetalias{karamazov_etal21}.\n\nFor better orientation in the notation of the different shears, convergences, phases, and other lensing quantities, we list selected symbols together with their first appearance in the text in Table~\\ref{tab:symbols_small}.\n\n\n\\subsubsection{Shear}\n\\label{sec:plots-shear}\n\n\\renewcommand{\\thefigure}{7.B}\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[height=22 cm]{f07_B.pdf}\n\\caption{Image-plane maps of the shear $\\gamma(\\boldsymbol x)$ of a NFW halo + point-mass lens, for a finer grid of point-mass positions than in Figure~\\ref{fig:shear}. Notation same as in Figure~\\ref{fig:shear}.\\label{fig:shear-online}}}\n\\efi\n\nImage-plane maps of the shear $\\gamma(\\boldsymbol x)$ are presented in Figure~\\ref{fig:shear}. The shear color scale is the same as in the first two panels of Figure~\\ref{fig:NFW-line}, ranging from white for $\\gamma=0$ to magenta for all positions with $\\gamma\\geq 1.5$. In the absence of the point mass the shear varies very slowly in this region, as indicated by the featureless plot in the first panel of Figure~\\ref{fig:NFW-line}.\n\nIn the bottom row ($x_{\\text{P}}=0$) of Figure~\\ref{fig:shear} the point mass is located at the center of the halo and the whole system thus exhibits axial symmetry. From Equations~(\\ref{eq:NFWP_gamma})~and~(\\ref{eq:cos}) it follows that in this case the total shear at any position is a simple sum of the NFW and point-mass shears. Near the halo center the NFW shear $\\gamma_{\\text{\\tiny{NFW}}}(x)$, shown in the first panel of Figure~\\ref{fig:NFW-line}, is surpassed by the point-mass shear $\\gamma_{\\text{P}}(\\boldsymbol x)$ which diverges at the origin. A comparison of the bottom row in the three columns shows that the magenta high-shear region with the strongest point-mass influence naturally increases with its mass parameter $\\kappa_{\\text{P}}$.\n\nNext, we focus on the left column illustrating the sub-critical case with $\\kappa_{\\text{P}}=10^{-4}$. Already in the second plot from the bottom ($x_{\\text{P}}=0.05$), many phenomena described in detail in Section~\\ref{sec:NFWP-csp} can be clearly seen. A pale circle corresponding to viewing angle $\\omega=\\pi/2$ connects the point mass and the halo center, marking a region with decreased shear. The shear drops to zero at two points of this circle located above and below the horizontal axis of symmetry. These zero-shear points lie inside the perturbed NFW radial critical curve close to the point mass. Their presence restricts the bright red high-shear region around the point-mass divergence to a smaller extent than in the $x_{\\text{P}}=0$ case. The value of the shear in the vicinity of the halo center depends on the direction of approach. Maximum shear can be seen in the horizontal and minimum shear can be seen in the vertical direction, tangent to the $\\omega=\\pi/2$ circle.\n\nGoing further to $x_{\\text{P}}=0.1$, we see that the paler lower-shear circle and the directional dependence near the origin become less pronounced, and the zero-shear points move even closer to the point mass, lying almost vertically above and below it. They are positioned inside the pair of tiny critical curves seen in the white low-shear areas. This hints at the fact that umbilics can only occur at zero-shear points that lie at the intersection of the $\\omega=\\pi/2$ circle and the $\\kappa=1$ circle, as explained in Section~\\ref{sec:NFWP-Jacobian}.\n\nFor even higher values of $x_{\\text{P}}$ in the sub-critical case, the directional dependence at the halo center becomes indiscernible and the pattern close to the point mass becomes more regular. The region of high shear around the point mass has a horizontally elongated oval shape and the zero-shear points lie above and below it in the white spots outside the critical curve. With increasing $x_{\\text{P}}$, this shear pattern around the point mass resembles the total shear of the Chang--Refsdal model, which consists of a point mass and a constant external shear \\citep{chang_refsdal84}.\n\nThe preceding discussion made for the sub-critical case holds also for the critical case \\mbox{($\\kappa_{\\text{P}}=\\kappa_{\\text{PC}}\\approx 2.714 \\cdot 10^{-4}$)} in the central column. In terms of shear, the corresponding plots portray qualitatively the same sequence of situations as in the left column; the difference is merely quantitative. More specifically, the pattern around the point mass is considerably larger and the directional dependence around the halo center is more pronounced, observable even for higher values of $x_{\\text{P}}$.\n\nThese patterns are even larger and more distinct in the right column illustrating the super-critical case with $\\kappa_{\\text{P}}=10^{-3}$. However, there are some important differences. In the second plot from the bottom, \\mbox{$x_{\\text{P}}=0.05$} does not exceed the threshold value of \\mbox{$\\sqrt{\\kappa_{\\text{P}}/\\kappa_{\\text{s}}}\\approx 0.0647$}. Hence, there are no zero-shear points. Minimum shear, which is now non-zero in the central region, can be found at the halo center when approached vertically. For point-mass positions \\mbox{$x_{\\text{P}}\\gtrsim 0.0647$} this minimum shear drops to zero and its position detaches from the origin, moving along the $\\omega=\\pi/2$ circle. In addition, the third plot from the bottom now depicts the situation before the detachment of the two small critical curves and, thus, the zero-shear points still lie inside the perturbed NFW radial critical curve. Note that in this case the directional dependence of the shear at the halo center can be seen up to the top row.\n\nThe dependence of the shear $\\gamma(\\boldsymbol x)$ on the point-mass position $x_{\\text{P}}$ can be examined in more detail in Figure~\\ref{fig:shear-online}, which includes plots for a finer grid in terms of $x_{\\text{P}}$. Zero-shear points appear at the halo center at point-mass positions $\\{0.0205, 0.0337, 0.0647\\}$ in the sub-critical, critical, and super-critical cases. Other key values of $x_{\\text{P}}$ correspond to changes in the critical-curve topology, as indicated by the color boundaries in Figure~6 of \\citetalias{karamazov_etal21}.\n\nNotice that the zero-shear points always occur in a positive-Jacobian region, as indicated by Equation~(\\ref{eq:Jacobian}): inside the perturbed NFW radial critical curve, inside the symmetric pair of small critical-curve loops, or outside all critical-curve loops. Apart from this, there is hardly any correlation between the shear pattern and the critical-curve geometry.\n\n\n\\subsubsection{Shear Deviation Due to the Point Mass}\n\\label{sec:plots-shear_deviation}\n\nIn Figure~\\ref{fig:shear_deviation_perturbation}, we present image-plane maps of the shear deviation $\\gamma/\\gamma_{\\text{\\tiny{NFW}}}-1$ caused by the presence of the point mass. As indicated by the formula, this quantity represents the relative difference between the shear $\\gamma(\\boldsymbol x)$ of the NFW halo + point-mass lens (shown in Figure~\\ref{fig:shear}) and the shear $\\gamma_{\\text{\\tiny{NFW}}}(x)$ of the NFW halo alone (shown in the first panel of Figure~\\ref{fig:NFW-line}). As the deviation falls rather quickly with increasing distance from the point mass, we introduce a semi-logarithmic color scale to visualize even minor changes in the deviation. In the positive yellow- and orange-hued regions the shear is increased, while in the negative blue regions it is decreased by the point mass. Darkest blue is used for $-1$, the lowest possible deviation. From $-1$ to $-10^{-3}$, blue saturation decreases logarithmically, and then to $0$ linearly, where it reaches white. From $0$ to $10^{-3}$, yellow saturation increases linearly, and then to $10^{-1}$ logarithmically. The logarithmic scale then continues to color red at deviation $10$, beyond which the color is kept constant even though the shear deviation can reach arbitrarily large values near the point mass.\n\nFor better orientation, we also include contours for a few specific values of the shear deviation. The dot-dashed lines represent the zero-deviation contour, along which the shears are equal. Paler and darker shades of orange are used for positive-deviation contours with values $10^{-2}$ and $10^{-1}$, respectively. Similarly, paler and darker shades of blue indicate negative deviations $-10^{-2}$ and $-10^{-1}$, respectively.\n\nWe first inspect the deviation map for a centrally positioned sub-critical point mass (bottom left plot). In this case the deviation is equal to $\\gamma_{\\text{P}}/\\gamma_{\\text{\\tiny{NFW}}}$, which is positive in the entire image plane, i.e., the shear is globally increased by the point mass. The deviation diverges at the halo center, since the point-mass shear increases to $\\infty$ while the halo shear tends to the constant $\\kappa_\\text{s}$. Further from the halo center the deviation approaches zero, as the point-mass shear given by Equation~(\\ref{eq:P_gamma}) falls quickly with distance. Contours representing deviations $10^{-1}$ and $10^{-2}$ are slightly larger than the outer radial and tangential critical curves, respectively.\n\nIn the second row ($x_{\\text{P}}=0.05$) the point mass is displaced from the center and a pair of blue regions with negative deviation appears. These regions reach the halo center from the vertical direction, while the deviation is positive along the full horizontal axis, as indicated by the orange and yellow color and by the dot-dashed zero contour pinched at the halo center. In fact, the deviation along the horizontal axis is always positive for any $x_\\text{P}$ and $\\kappa_\\text{P}$, since here $\\gamma/\\gamma_{\\text{\\tiny{NFW}}}-1=\\gamma_\\text{P}/\\gamma_{\\text{\\tiny{NFW}}}$ according to the discussion in the paragraphs following Equation~(\\ref{eq:NFWP_gamma}). The pattern near the halo center arises from the directional dependence of the shear shown in Equation~(\\ref{eq:NFWP_gamma_center}). Places with the darkest blue color lie above and below the point mass, with the lowest shear deviation $-1$ occurring at the zero-shear points. The region with a deviation larger than $10\\%$ in absolute value is now roughly centered on the point mass, while the region with a deviation lower than $1\\%$ in absolute value lies outside the near-circular pale orange contour and in narrow bands along the dot-dashed zero-deviation contour.\n\nAt $x_{\\text{P}}=0.1$ on the third row we see that the affected area becomes more asymmetric, with the pale orange contour with $10^{-2}$ deviation now broken into two lobes extending to the left of the center and to the right of the point mass. In this case there is a single region with deviation lower than $1\\%$ in absolute value, reaching inside the critical curves, including the zero-deviation contour, and reaching the halo center along it.\n\nFor higher values of $x_{\\text{P}}$, the blue regions of negative deviation expand as the point mass shifts to the right. Their borders indicated by the zero-deviation contour become more and more circular except for the vicinity of the point mass, where they enclose the zero-shear points but avoid the vicinity of the point mass. These dot-dashed contours intersect at the halo center at a right angle and the deviation remains positive in the spindle-shaped region along the horizontal axis from the halo center to the point mass. The orange and blue contours gradually detach from the halo center and for $x_{\\text{P}}\\geq 0.25$ they form a four-lobed structure around the point mass, with positive lobes extending horizontally from the point mass and negative lobes separated vertically from the point mass. The single region with deviation lower than $1\\%$ in absolute value includes the halo center as well as a progressively larger area around it, including the entire image plane except the four lobes around the point mass.\n\nIn the critical and super-critical cases in the two right columns, the plots look similar to those in the sub-critical case, with the colors getting progressively more saturated indicating higher shears $\\gamma_\\text{P}$ from heavier point masses. Naturally, the orange and blue contours also expand with increasing mass. On the other hand, the blue regions of negative deviation inside the dot-dashed contours do not expand with increasing $\\kappa_{\\text{P}}$. On the contrary, they shrink as they recede from a heavier point mass. Away from the point mass, the dot-dashed zero contours are almost circular. They intersect at the halo center at a right angle and reach nearly to the point mass before avoiding it.\n\nComparing the columns in the different rows, we see that the geometry of the zero-deviation contour is generic, with increasing $\\kappa_{\\text{P}}$ affecting only the vicinity of the point mass, and increasing $x_{\\text{P}}$ only enlarging the scale. The pattern arises naturally from Equation~(\\ref{eq:NFWP_gamma}) in the regime $\\gamma_{\\text{P}}\\ll\\gamma_{\\text{\\tiny{NFW}}}$ valid anywhere except in the immediate vicinity of the point mass. In this case we expand the shear and get the simple result $\\gamma/\\gamma_{\\text{\\tiny{NFW}}}-1\\simeq (\\gamma_\\text{P}/\\gamma_{\\text{\\tiny{NFW}}})\\,\\cos{2\\,\\omega}$. The ratio in the parentheses is always positive, hence, the zero-deviation contour is purely given by the condition on the viewing angle requiring $\\cos{2\\,\\omega}=0$. Figure~\\ref{fig:omega} shows that the corresponding $\\omega=\\pi/4$ and $\\omega=3\\pi/4$ circles describe the dot-dashed contours seen in Figure~\\ref{fig:shear_deviation_perturbation} practically exactly, except in the point-mass vicinity where $\\gamma_{\\text{P}}\\gtrsim\\gamma_{\\text{\\tiny{NFW}}}$.\n\nIn the bottom row of the critical and super-critical columns we see the same pattern of globally positive shear deviation as in the sub-critical column. What differs is the larger extent of the orange contours. In fact, in the super-critical case the entire paler-orange contour of deviation $10^{-2}$ lies outside the plotted area. Within the plotted area in all other panels in the right column, the regions with a deviation lower than $1\\%$ in absolute value are limited to a band along the zero-deviation contour. This band expands as $x_{\\text{P}}$ increases, and eventually connects with the outer low-deviation region. For $x_{\\text{P}}=0.05$ in the super-critical case we can see very small blue regions of negative deviation without zero-shear points inside, as these appear at a higher separation, for $x_{\\text{P}}\\geq 0.0645$. Moreover, here the angle of intersection of the dot-dashed contours is very different from a right angle. In this case the influence of the point mass at the halo center is too strong ($\\gamma_\\text{P}/\\gamma_{\\text{\\tiny{NFW}}}\\approx 1.7$), so that the expansion illustrating the generic shape of the zero-deviation contour is not valid here.\n\n\\renewcommand{\\thefigure}{8.A}\n\\begin{figure*}\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[width=15.5 cm]{f08_01.pdf}\n\\caption{Image-plane maps of the relative shear deviation, $\\gamma/\\gamma_{\\text{\\tiny{NFW}}}-1$, caused by the presence of the point mass, described in Section~\\ref{sec:plots-shear_deviation}. The color scale changes from logarithmic to linear in the interval $[-10^{-3},10^{-3}]$. All positions with deviation greater than $10$ are marked in red. Contours are plotted for five deviation values indicated in the color bar. Remaining notation as in Figure~\\ref{fig:shear}. \\label{fig:shear_deviation_perturbation}}}\n\\end{figure*}\n\nA more detailed view of the changing deviation patterns with point-mass position can be seen in Figure~\\ref{fig:shear_deviation_perturbation-online}. Its closer inspection reveals that the deviation is globally positive not only for centrally-positioned point masses, but also for plots up to $x_{\\text{P}}=0.01$ in the sub-critical case, up to $0.02$ in the critical case, and up to $0.04$ in the super-critical case. Imposing the condition $\\gamma\\leq\\gamma_{\\text{\\tiny{NFW}}}$ on Equation~(\\ref{eq:NFWP_gamma}) reveals that negative deviation first appears at the halo center in the $\\omega=\\pi/2$ vertical direction once the point-mass shear at the center decreases to $\\gamma_\\text{P}=2\\,\\gamma_{\\text{\\tiny{NFW}}}$. Using Equation~(\\ref{eq:NFW_gamma_origin}) and Equation~(\\ref{eq:P_gamma}) with $\\boldsymbol{x}=(0,0)$ then yields the condition for the existence of negative-deviation regions: $x_{\\text{P}}\\geq\\sqrt{\\kappa_{\\text{P}}/(2\\,\\kappa_{\\text{s}})}$. In the sub-critical case we find $x_{\\text{P}}\\geq 0.0145$, in the critical $x_{\\text{P}}\\geq 0.0238$, and in the super-critical $x_{\\text{P}}\\geq 0.0457$, in agreement with the deviation maps.\n\nThe sizes of the contours can be used to estimate the areas with a strong effect on the shear due to the presence of the point mass. As an example, for the three different masses we find that at the moment of separation of the critical curve surrounding the point mass from the perturbed NFW tangential critical curve, the darker contours of deviation $\\pm\\,10^{-1}$ extend roughly seven Einstein radii from the point mass.\n\n\n\\subsubsection{Convergence--Shear Diagrams}\n\\label{sec:plots-diagrams}\n\n\\renewcommand{\\thefigure}{8.B}\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[height=22 cm]{f08_B.pdf}\n\\caption{Image-plane maps of the relative shear deviation, $\\gamma/\\gamma_{\\text{\\tiny{NFW}}}-1$, caused by the presence of the point mass, for a finer grid of point-mass positions than in Figure~\\ref{fig:shear_deviation_perturbation}. Notation same as in Figure~\\ref{fig:shear_deviation_perturbation}.\\label{fig:shear_deviation_perturbation-online}}}\n\\efi\n\nIn Figure~\\ref{fig:CS-diagrams} we present a grid of CS diagrams, which provide a description complementary to the image-plane plots of the shear, its deviation due to the point mass, and the quantities discussed in the following sections. For a general understanding of CS diagrams see Appendix~\\ref{sec:Appendix-images} with Figure~\\ref{fig:CS-diagram}, Section~\\ref{sec:NFW-images} with Figure~\\ref{fig:CS-NFW}, and, in particular, Section~\\ref{sec:NFWP-images} with Figure~\\ref{fig:CS-NFWP}.\n\nThe purple-shaded area marks the full range of $(\\kappa,\\gamma)$ combinations of the NFW halo + point-mass lens. Its intersection with the green curve corresponds to the dot-dashed zero-deviation curve in Figure~\\ref{fig:shear_deviation_perturbation}. The part of the area above the green curve then corresponds to the yellow- and orange-hued positive-deviation regions, and the part below the green curve corresponds to the blue negative-deviation regions in Figure~\\ref{fig:shear_deviation_perturbation}.\n\nWe start by describing the sub-critical case shown in the left column. For $x_{\\text{P}}=0$ the point mass lies at the center of the halo and the system therefore has axial symmetry. In addition, the radial dependence of the convergence is monotonic. This implies that only one value of the shear $\\gamma(x)$ can occur for any value of the convergence $\\kappa(x)$. These combinations $\\left(\\kappa(x),\\gamma(x)\\right)$ are plotted here as the bold purple curve. For positions far from the halo center (at the left side of the plot), this curve closely follows the unperturbed-halo green curve, which starts at the origin of the plot. Proceeding to the right (closer to the halo center), the purple curve reaches the bold black line with slope $-1$ representing the tangential critical curve. Here the magnification is infinite and the flattening reaches $1$.\n\n\\renewcommand{\\thefigure}{9.A}\n\\begin{figure*}\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[width=14 cm]{f09_01.pdf}\n\\caption{Convergence--shear (CS) diagrams of a NFW halo + point-mass lens, described in Section~\\ref{sec:plots-diagrams}. Combinations $\\left(\\kappa(\\boldsymbol x),\\gamma(\\boldsymbol x)\\right)$ occurring in each of the lens configurations are marked by the purple regions or curves. The green curve corresponds to the NFW-halo lens from Figure~\\ref{fig:CS-NFW}. For further details on the notation see Figure~\\ref{fig:CS-NFWP}; for the interpretation of CS diagrams see Figure~\\ref{fig:CS-diagram}.\\label{fig:CS-diagrams}}}\n\\end{figure*}\n\nFurther to the right, the curve enters an area of negative parity, where the magnification decreases and the flattening drops to $0$ at the bold vertical $\\kappa=1$ line corresponding to the unit-convergence radius $x_0\\approx0.0936$. To the right of this line, images are elongated perpendicularly to the phase. Roughly here, the purple curve of $\\left(\\kappa(x),\\gamma(x)\\right)$ combinations starts to deviate significantly from the green curve. It rises rapidly and eventually leaves the plot, as the shear diverges at the location of the point mass. Close to the point mass the parity is always negative and both magnification and flattening approach zero. Before this happens, the purple curve intersects the bold black line with slope $1$ twice. The first intersection represents the outer radial critical curve and the second intersection represents the inner radial critical curve. Between them, images have positive parity.\n\nIncreasing the point-mass position from $x_{\\text{P}}=0$ to $x_{\\text{P}}=0.05$ brings about several important changes. As the system loses its axial symmetry, for each value of convergence $\\kappa(x)$ there is a continuous interval of shear values $\\gamma(\\boldsymbol x)$ in the image plane and the set of convergence--shear combinations is represented by a two-dimensional region. At the left side of the plot, far from the halo center, these combinations remain limited to the close vicinity of the green curve of the unperturbed halo. At the right side, close to the halo center, the set of combinations forms a horizontal band, with shear values $\\gamma\\in[\\gamma_{\\text{\\tiny{NFW}}}-\\gamma_{\\text{P}}, \\gamma_{\\text{\\tiny{NFW}}}+\\gamma_{\\text{P}}]$ corresponding to its directional dependence at the halo center, demonstrated by Equations~(\\ref{eq:NFWP_gamma}) and (\\ref{eq:NFWP_gamma_center}). In the vicinity of $x=x_\\text{P}$ we see the broadest range of shear values. On the one hand, the shear diverges at the position of the point mass while on the other hand, the shear drops to $0$ at the zero-shear points occurring here at a radius slightly lower than $x_\\text{P}$. Overall, the set of $\\left(\\kappa(\\boldsymbol x),\\gamma(\\boldsymbol x)\\right)$ combinations looks similar to the case illustrated in Figure~\\ref{fig:CS-NFWP} and described in detail in Section~\\ref{sec:NFWP-images}, with one important difference. The shear divergence and the zero-shear points both lie in the area of $\\kappa>1$, meaning that nearby images would now be elongated perpendicularly to the phase.\n\n\\renewcommand{\\thefigure}{9.B}\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[height=22 cm]{f09_B.pdf}\n\\caption{Convergence--shear (CS) diagrams of a NFW halo + point-mass lens, for a finer grid of point-mass positions than in Figure~\\ref{fig:CS-diagrams}. Notation same as in Figure~\\ref{fig:CS-diagrams}.\\label{fig:CS-diagrams-online}}}\n\\efi\n\n\\renewcommand{\\thefigure}{10.A}\n\\begin{figure*}\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[width=15.5 cm]{f10_01.pdf}\n\\caption{Image-plane maps of the weak shear $\\gamma_{\\text{w}}(\\boldsymbol x)$ of a NFW halo + point-mass lens, described in Section~\\ref{sec:plots-weak_shear}. These maps also illustrate image flattening with values $f(\\boldsymbol x)=2\\,\\gamma_{\\text{w}}(\\boldsymbol x)$. Maximum weak shear $\\gamma_{\\text{w}}=0.5$ occurs exclusively along the critical curves. Remaining notation as in Figure~\\ref{fig:shear}. \\label{fig:weak_shear}}}\n\\end{figure*}\n\nAs $x_{\\text{P}}$ increases in the following rows, both the shear divergence and the zero-shear point shift to the left, indicating that the point mass moves to locations with progressively lower halo convergence. By the third row from the bottom ($x_{\\text{P}}=0.1$), the purple region touches the horizontal axis to the left of the vertical $\\kappa=1$ line, indicating that the lens underwent an umbilic transition at a slightly lower $x_{\\text{P}}$ value. The surroundings of the shear divergence and the zero-shear point now lie in the region $\\kappa<1$, where images are elongated parallel to the phase. The purple regions in the diagram also become narrower with increasing $x_{\\text{P}}$. In the case of the divergence this is due to the changing scale of CS diagrams in terms of image-plane positions, as indicated by the green ticks. In the case of the horizontal band this is due to the decreasing value of $\\gamma_{\\text{P}}(0)$, i.e., the shear due to the point mass at the halo center.\n\nThere are a few differences to notice in the critical case, which is shown in the central column of Figure~\\ref{fig:CS-diagrams}. For a centrally positioned point mass ($x_{\\text{P}}=0$), the purple curve of the $\\left(\\kappa(x),\\gamma(x)\\right)$ combinations deviates from the green curve of halo combinations similarly as it does in the sub-critical case. However, instead of intersecting the bold black line with slope $1$, the curve merely touches it at a single point. This indicates the disappearance of the inner positive-parity region and the presence of a degenerate radial critical curve, described in detail in Appendix~B of \\citetalias{karamazov_etal21}. The plot in the third row from the bottom ($x_{\\text{P}}=0.1$) depicts a configuration extremely close to the elliptic umbilic, as the zero-shear points now occur almost precisely at $(\\kappa,\\gamma)=(1,0)$. Generally speaking, the purple regions of convergence--shear combinations are broader than in the sub-critical case, meaning that at a given distance from the halo center a greater range of shear values occurs.\n\nFor the super-critical mass in the right column, in the axially symmetric $x_{\\text{P}}=0$ case the purple curve of the $\\left(\\kappa(x),\\gamma(x)\\right)$ combinations does not touch the black line with slope $1$ at all. This means that there are no radial critical curves. The second plot from the bottom ($x_{\\text{P}}=0.05$) is now profoundly different than in previous cases. Here, the purple area does not touch the horizontal axis of the plot. As discussed in Section~\\ref{sec:plots-shear}, in this case there are no zero-shear points and minimum shear can be found at the halo center when approached vertically. Moreover, unlike in the lower-mass cases, at the halo center the horizontal band does not spread symmetrically around the green line. In this case, at the center the shear due to the point mass is higher than the shear due to the halo, so that $\\gamma\\in[\\gamma_{\\text{P}}-\\gamma_{\\text{\\tiny{NFW}}}, \\gamma_{\\text{P}}+\\gamma_{\\text{\\tiny{NFW}}}]$ as shown by Equation~(\\ref{eq:NFWP_gamma_center}) and discussed in the following paragraphs and in Section~\\ref{sec:NFWP-images}.\n\n\\renewcommand{\\thefigure}{10.B}\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[height=22 cm]{f10_B.pdf}\n\\caption{Image-plane maps of the weak shear $\\gamma_{\\text{w}}(\\boldsymbol x)$ of a NFW halo + point-mass lens, for a finer grid of point-mass positions than in Figure~\\ref{fig:weak_shear}. Notation same as in Figure~\\ref{fig:weak_shear}.\\label{fig:weak_shear-online}}}\n\\efi\n\nThe variation of the CS diagrams with point-mass position can be inspected in more detail in Figure~\\ref{fig:CS-diagrams-online}. Note that the shear interval at the halo center is centered on $\\gamma_{\\text{P}}$ rather than on $\\gamma_{\\text{\\tiny{NFW}}}$ in all cases with $x_{\\text{P}}<\\sqrt{\\kappa_{\\text{P}}/\\kappa_{\\text{s}}}$, namely: from $x_\\text{P}=0$ to $x_\\text{P}=0.02$ in the sub-critical case; to $x_\\text{P}=0.03$ in the critical case; to $x_\\text{P}=0.06$ in the super-critical case. The $x_\\text{P}=0.01$ diagrams illustrate the nature of the transition from the axially symmetric lens configurations at $x_\\text{P}=0$, best seen in the right panel. The bold purple curve from $x_\\text{P}=0$ gradually expands to a broader band at lower radii for $x_\\text{P}=0.01$. The remaining structure of the purple region lies outside the plotted area in the right panel, but it has a similar nature to the plot in the left panel (also similar to the right panels for higher $x_\\text{P}$ values): shear divergence at the lens position $(x_{\\text{P}}=0.01)$, and the shear interval shrinking to a horizontal band centered on $\\gamma_{\\text{P}}$.\n\nAnother feature to notice for the lower $x_\\text{P}$ values is that the entire purple region lies above the green curve, which means that for such configurations the shear is higher than in the absence of the point mass everywhere in the image plane. As shown in Section~\\ref{sec:plots-shear_deviation}, this is the case for $x_{\\text{P}}<\\sqrt{\\kappa_{\\text{P}}/(2\\,\\kappa_{\\text{s}})}$, namely: plots up to $x_\\text{P}=0.01$ in the sub-critical case; up to $x_\\text{P}=0.02$ in the critical case; up to $x_\\text{P}=0.04$ in the super-critical case.\nFor higher values of $x_{\\text{P}}$ the shear may be lower than in the absence of the point mass, but only in limited parts of the image plane. For example, along the horizontal axis of the lens the shear always stays higher than in the absence of the point mass, as shown in Section~\\ref{sec:plots-shear_deviation}. For lower masses at higher separations $x_\\text{P}$, the generic geometry of the zero-deviation contour discussed in Section~\\ref{sec:plots-shear_deviation} shows that the regions of lower shear are limited to radial distances $x<x_\\text{P}\\,\\sqrt{2}$.\n\n\n\\subsubsection{Weak Shear}\n\\label{sec:plots-weak_shear}\n\nThe plots in Figure~\\ref{fig:weak_shear} show image-plane color maps of the weak shear $\\gamma_{\\text{w}}(\\boldsymbol x)$, which we defined in Section~\\ref{sec:NFW-weak} as the shear that would be measured from image deformations using weak-lensing analysis. At the same time, these maps illustrate image flattening, since $f(\\boldsymbol x)=2\\,\\gamma_{\\text{w}}(\\boldsymbol x)$, as shown in Equation~(\\ref{eq:weak_shear}). This equality also implies that weak shear values range from $0$ (corresponding to white color in the maps) to $0.5$ (bright red color), with the maximum value occurring exclusively along the full length of the critical curve. In spite of this limited range, for purposes of comparison we retain the same color scale as in the first two panels of Figure~\\ref{fig:NFW-line} and in Figure~\\ref{fig:shear}.\n\nWe begin our description in the left column of Figure~\\ref{fig:weak_shear} with a sub-critical point mass, starting from the bottom row corresponding to its central position in the NFW halo. Directly at the center the weak shear is equal to zero, since the image cannot be flattened in any direction due to the axial symmetry of the lens configuration. Going further from the halo center, the weak shear reaches $0.5$ at the small inner radial critical curve, then it decreases slightly before returning to $0.5$ at the outer radial critical curve. The weak shear then drops to zero at the unit-convergence circle, along which images are undistorted, increasing again to $0.5$ at the tangential critical curve, beyond which it drops asymptotically to zero.\n\nIn the second row ($x_{\\text{P}}=0.05$), the outer red ring of high weak shear along the perturbed NFW tangential critical curve is preserved. The same holds for the white $\\gamma_{\\text{w}}=0$ ring along the unit-convergence circle, which is in fact preserved exactly in all the configurations for all point masses. However, several changes can be seen closer to the halo center, where there are now four points with zero weak shear. These points coincide with the halo center, the point-mass position, and the pair of zero-shear points. This can be understood by inspecting Equation~(\\ref{eq:flattening}) and taking its limit at these points. At the halo center, the difference of signs before $\\gamma$ is suppressed by diverging $\\kappa$, which also suppresses the directional dependence of the shear close to the center. Both fractions in Equation~(\\ref{eq:flattening}) tend to $1$, which results in zero flattening. At the position of the point mass, both numerators and denominators are dominated by the diverging shear and the fractions thus again approach unity. At the zero-shear points, the fractions for $\\gamma=0$ are directly equal to $1$. It is worth noting here that zero shear $\\gamma$ implies zero weak shear $\\gamma_{\\text{w}}$, but not vice versa. All four points are connected by a paler low-weak-shear region corresponding to the $\\omega=\\pi/2$ circle, which is interrupted between the zero-shear points and the point-mass position by the perturbed NFW radial critical curve, along which the weak shear reaches $0.5$.\n\nAt $x_{\\text{P}}=0.1$ the region of low weak shear surrounding the point mass is superimposed over the white ring of the unit-convergence circle, while the pair of $\\gamma_{\\text{w}}=0$ zero-shear points is now trapped inside tiny critical curves, here very close to the elliptic umbilic transition. For higher values of $x_{\\text{P}}$, a lobe forms on the perturbed NFW tangential critical curve with the point mass inside and the zero-shear points outside. By $x_{\\text{P}}=0.25$ the curve splits and the point mass is surrounded by a small oval critical-curve loop elongated toward the halo center. Three points with zero weak shear remain associated with this loop: the point-mass position inside, and the two zero-shear points directly above and below the loop. Similar but larger weak-shear patterns near the tangential-critical-curve lobes and the detached ovals can be seen in the two right columns showing the critical and super-critical cases, in the rows with $x_{\\text{P}}\\geq 0.15$.\n\nIn the critical case with a centrally located point mass (bottom plot in central column), there is only one $\\gamma_{\\text{w}}=0.5$ circle along the single radial critical curve between the white center and the white $\\kappa=1$ circle. The third row ($x_{\\text{P}}=0.1$) illustrates the peculiar situation in the presence of elliptic umbilic points (technically, these occur at $x_{\\text{P}}\\approx0.0996$, but the plot is visually identical). These are zero-shear points lying directly on the unit-convergence circle. At these points the weak shear (and flattening) is undefined, as can be seen by substituting $\\gamma=0$ and $\\kappa=1$ in Equation~(\\ref{eq:flattening}). In the plot we can see them as point-like interruptions of the white unit-convergence circle.\n\nIn the super-critical case with $x_{\\text{P}}=0$ (bottom right plot) there is no radial critical curve. Hence, the weak shear increases only slightly between the center and the $\\kappa=1$ circle without reaching $0.5$. In the $x_{\\text{P}}=0.05$ plot the pair of vertically offset zero-weak-shear points is missing, since the condition $x_{\\text{P}}>\\sqrt{\\kappa_{\\text{P}}/\\kappa_{\\text{s}}}$ for the existence of zero-shear points is not fulfilled here. Note in the same plot that the weak-shear pattern near the halo center resembles the central directionally dependent shear pattern. However, while the shear is undefined at the center, the weak shear reaches $0$ from any direction, with only the rate of convergence depending on the direction. The rate is slowest in the horizontal direction with $\\gamma_{\\text{w}}\\sim(\\gamma_{\\text{P}}+\\gamma_{\\text{\\tiny{NFW}}})/(\\kappa-1)$, and fastest in the vertical direction with $\\gamma_{\\text{w}}\\sim|\\gamma_{\\text{P}}-\\gamma_{\\text{\\tiny{NFW}}}|/(\\kappa-1)$ to first order in $1/(\\kappa-1)$. Clearly, the directionality will be most pronounced when $\\gamma_{\\text{P}}=\\gamma_{\\text{\\tiny{NFW}}}$ at the halo center, i.e., at the appearance of the zero-shear points. The directionality will be least pronounced when either of the component shears $\\gamma_{\\text{\\tiny{NFW}}}, \\gamma_{\\text{P}}$ dominates over the other at the halo center.\n\nThe emergence and evolution of the features and structures discussed above can be studied in more detail in Figure~\\ref{fig:weak_shear-online}. For example, the formation of the tiny critical-curve loops around the zero-(weak)-shear points is well visible in the sub-critical case at $x_{\\text{P}}=0.08$ or in the critical case at $x_{\\text{P}}=0.09$. The location of the white zero-(weak)-shear points inside these loops is best visible for $x_{\\text{P}}=0.12$ for all three masses, as well as for $x_{\\text{P}}=0.13$ in the sub-critical case.\n\n\n\\subsubsection{Weak-shear Deviation Due to the Point Mass}\n\\label{sec:plots-weak_shear_deviation}\n\nThe plots in Figure~\\ref{fig:weak-shear_deviation_perturbation} depict the weak-shear deviation caused by the presence of the point mass, given by \\mbox{$\\gamma_{\\text{w}}/\\gamma_{\\text{w,\\tiny{NFW}}}-1$}. As indicated by the formula, it is defined as the relative difference between the weak shear $\\gamma_{\\text{w}}(\\boldsymbol x)$ of a NFW halo with a point mass (see Figure~\\ref{fig:weak_shear}) and the weak shear $\\gamma_{\\text{w,\\tiny{NFW}}}(\\boldsymbol x)$ of a NFW halo alone (see the second panel of Figure~\\ref{fig:NFW-line}). This means that the plots are the weak-shear equivalents of the plots from Figure~\\ref{fig:shear_deviation_perturbation} described in Section~\\ref{sec:plots-shear_deviation}. Therefore, we use the same color scale and the same set of contours as in Figure~\\ref{fig:shear_deviation_perturbation}. Note that due to Equation~(\\ref{eq:weak_shear}), the plots in Figure~\\ref{fig:weak-shear_deviation_perturbation} also exactly portray the relative image-flattening deviation, including the color bar and the values of the contours. The yellow- and orange-hued regions thus correspond to higher flattening and weak shear, while the blue regions correspond to lower flattening and weak shear than in the absence of the point mass.\n\nThe striking patterns of the colored areas in Figure~\\ref{fig:weak-shear_deviation_perturbation} look remarkably complex at first. However, especially in the top rows, away from the halo center and from the vicinity of the point mass, the plots are very similar to those in Figure~\\ref{fig:shear_deviation_perturbation}. This should be expected, since in these regions with low values of $\\gamma$ and $\\kappa$ the weak shear is a good approximation of the shear. Closest to the point mass, the plots differ from those in Figure~\\ref{fig:shear_deviation_perturbation} fundamentally. In Figure~\\ref{fig:shear_deviation_perturbation}, the relative shear deviation diverges to $\\infty$ at the position of the point mass. In Figure~\\ref{fig:weak-shear_deviation_perturbation}, even inside the positive weak-shear deviation contours in the top rows, there is a blue negative area in which the deviation falls to the minimum possible value of $-1$ at the point-mass position. This is a consequence of the weak shear $\\gamma_{\\text{w}}$ converging to $0$, while the shear $\\gamma$ diverges there. Interestingly, this result holds even for $x_{\\text{P}}=0$ at the halo center, where even $\\gamma_{\\text{w,\\tiny{NFW}}}$ converges to $0$. Nevertheless, in the immediate vicinity of the center the weak-shear ratio $\\gamma_{\\text{w}}/\\gamma_{\\text{w,\\tiny{NFW}}}\\simeq 4\\,(x\\,\\ln{x})^2\\,\\kappa_\\text{s}/\\kappa_\\text{P}\\to 0$, so that even in this case the weak-shear deviation at the point-mass position is $-1$.\n\nIn order to decipher the alternating positive and negative regions in Figure~\\ref{fig:weak-shear_deviation_perturbation}, we concentrate on their boundaries which are indicated by the dot-dashed zero-weak-shear-deviation contour. Especially in the top rows, for higher $x_{\\text{P}}$, we can see that parts of the contours look identical to the zero-shear-deviation contours from Figure~\\ref{fig:shear_deviation_perturbation}. However, here there is an additional strong effect closely associated with the critical curve, unlike in the case of the shear-deviation patterns which show very little influence of the critical curve. More specifically, this additional effect reflects the relative deformations of the critical curve caused by the point mass.\n\n\\renewcommand{\\thefigure}{11.A}\n\\begin{figure*}\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[width=15.5 cm]{f11_01.pdf}\n\\caption{Image-plane maps of the relative weak-shear deviation, $\\gamma_{\\text{w}}/\\gamma_{\\text{w,\\tiny{NFW}}}-1$, caused by the presence of the point mass, described in Section~\\ref{sec:plots-weak_shear_deviation}. The maps also exactly portray the relative deviation in image flattening due to the point mass. Color scale, contours, and remaining notation as in Figure~\\ref{fig:shear_deviation_perturbation}. \\label{fig:weak-shear_deviation_perturbation}}}\n\\end{figure*}\n\nThe zero-weak-shear-deviation contour can be defined using the flattening from Equation~(\\ref{eq:flattening}) by setting $f(\\kappa,\\gamma)=f(\\kappa,\\gamma_{\\text{\\tiny{NFW}}})$. Since the convergence is the same with or without the point mass (except at the point-mass position), we immediately see that the zero-shear-deviation contour\n\\beq\n\\gamma(\\boldsymbol x)=\\gamma_{\\text{\\tiny{NFW}}}(x)\n\\label{eq:contour_pt1}\n\\eeq\nautomatically also forms a component of the zero-weak-shear-deviation contour. All dot-dashed contours from Figure~\\ref{fig:shear_deviation_perturbation} thus appear also in Figure~\\ref{fig:weak-shear_deviation_perturbation}. The remaining components can be obtained by solving the flattening equality, which yields the additional non-trivial solution\n\\beq\n\\gamma(\\boldsymbol x)\\,\\gamma_{\\text{\\tiny{NFW}}}(x)=\\left[\\,1-\\kappa(x)\\,\\right]^2\\,.\n\\label{eq:contour_pt2}\n\\eeq\nThis equation describes all components of the zero-weak-shear-deviation contour that do not appear in Figure~\\ref{fig:shear_deviation_perturbation}.\n\nIn particular, these include components closely associated with the critical curves, which is best illustrated when the NFW critical curves are only weakly perturbed by the point mass. Along the NFW tangential critical curve we recall that $\\gamma_{\\text{\\tiny{NFW}}}(x)=1-\\kappa(x)$, as discussed in Section~\\ref{sec:NFW-Jacobian}. Similarly, along the perturbed NFW tangential critical curve which lies outside the unit-convergence circle, the shear satisfies $\\gamma(\\boldsymbol x)=1-\\kappa(x)$, as discussed in Section~\\ref{sec:NFWP-Jacobian}. It is straightforward to show that the product of the two shears is lower than $\\left[\\,1-\\kappa(x)\\,\\right]^2$ along a section of one of these critical curves and at the same time it is higher than $\\left[\\,1-\\kappa(x)\\,\\right]^2$ along the corresponding section of the other critical curve. Due to continuity, there is a contour between the critical curves of the two models along which Equation~(\\ref{eq:contour_pt2}) is satisfied. When the point-mass perturbation is weak, such as in the top left plot in Figure~\\ref{fig:weak-shear_deviation_perturbation}, the corresponding component of the zero-weak-shear-deviation contour is indistinguishable from the perturbed NFW tangential critical curve. The contour can be distinguished for example in the top right plot near the horizontal axis in the direction of the point mass.\n\nA similar argument can be made for the NFW radial critical curve, along which $\\gamma_{\\text{\\tiny{NFW}}}(x)=\\kappa(x)-1$, as discussed in Section~\\ref{sec:NFW-Jacobian}, and for the perturbed NFW radial critical curve inside the unit-convergence circle with $\\gamma(\\boldsymbol x)=\\kappa(x)-1$, as discussed in Section~\\ref{sec:NFWP-Jacobian}. Even in this case there is a contour between the two critical curves along which Equation~(\\ref{eq:contour_pt2}) is satisfied.  The contour can be distinguished from the perturbed NFW radial critical curve in Figure~\\ref{fig:weak-shear_deviation_perturbation} for example in the super-critical $x_\\text{P}=0.15$ plot near the horizontal axis in the direction of the point mass. Note that Equation~(\\ref{eq:contour_pt2}) also accounts for other components of the zero-weak-shear-deviation contour, such as the loop around the point mass in the top row of Figure~\\ref{fig:weak-shear_deviation_perturbation}.\n\nAt the mutual intersections of the components of the zero-weak-shear-deviation contour, Equation~(\\ref{eq:contour_pt1}) and Equation~(\\ref{eq:contour_pt2}) have to be satisfied simultaneously. This implies that these points also represent the intersections of the critical curves of the NFW halo with and without the point mass. The combined geometry of the components with their mutual intersections explains the partitioning of the image plane into the color patterns seen in Figure~\\ref{fig:weak-shear_deviation_perturbation}.\n\n\\renewcommand{\\thefigure}{11.B}\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[height=22 cm]{f11_B.pdf}\n\\caption{Image-plane maps of the relative weak-shear deviation, $\\gamma_{\\text{w}}/\\gamma_{\\text{w,\\tiny{NFW}}}-1$, caused by the presence of the point mass, for a finer grid of point-mass positions than in Figure~\\ref{fig:weak-shear_deviation_perturbation}. Notation same as in Figure~\\ref{fig:weak-shear_deviation_perturbation}. \\label{fig:weak-shear_deviation_perturbation-online}}}\n\\efi\n\n\\renewcommand{\\thefigure}{12.A}\n\\begin{figure*}\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[width=15.5 cm]{f12_01.pdf}\n\\caption{Image-plane maps of the relative weak-shear deviation from the shear, $\\gamma_{\\text{w}}/\\gamma-1$, of a NFW halo + point-mass lens, described in Section~\\ref{sec:plots-weak_shear_deviation_from_shear}. The maps illustrate the relative error of the weak-lensing shear estimate. All positions with deviation greater than $1.5$ are marked in red. Remaining notation as in Figure~\\ref{fig:shear}.\\label{fig:weak-shear_deviation}}}\n\\end{figure*}\n\nStarting with a centrally positioned sub-critical point mass (bottom left plot), the deviation shows a very small negative region around the point mass inside the inner radial critical curve, followed by a positive annulus reaching just beyond the outer radial critical curve, a negative annulus almost to the tangential critical curve, and a positive outer region. All three boundaries separating these regions are described by Equation~(\\ref{eq:contour_pt2}). At $x_\\text{P}=0.05$, the negative region around the point mass is connected with the larger negative annulus. In addition, the zero-shear-deviation boundary given by Equation~(\\ref{eq:contour_pt1}) can be seen, introducing negative areas above and below the halo center, with positive crescents where it reaches beyond the zero-weak-shear-deviation contour associated with the perturbed NFW radial critical curve. At $x_\\text{P}=0.1$, strong positive deviation can be seen close to the two tiny critical-curve loops, along which $\\gamma_{\\text{w}}=0.5$ while $\\gamma_{\\text{w,\\tiny{NFW}}}\\approx 0$. However, note that at the zero-shear points inside the loops the weak-shear deviation equals $-1$, with the indiscernible negative regions around them bordered by contour loops obeying Equation~(\\ref{eq:contour_pt2}). At $x_\\text{P}=0.15$, the region inside the zero-shear-deviation contour flips color again as it crosses the zero-weak-shear-deviation contour associated with the perturbed NFW tangential critical curve. At $x_\\text{P}=0.25$, the point-mass critical-curve loop is detached from the NFW-halo critical curve, showing the characteristic four-lobed contour pattern seen in Figure~\\ref{fig:shear_deviation_perturbation}, here with the central negative region described above.\n\nA similar sequence can be seen in the two right columns, in which the structures are larger and the deviations more prominent. In particular, the zero-weak-shear-deviation contours near the critical curves are better visible in some of the plots here. Note also the change with increasing mass for a central position of the point mass in the bottom row. In the critical case there is only a single degenerate radial critical curve, with the accompanying zero-weak-shear-deviation contour well separated from it. The positive-deviation annulus is narrower as the central negative region is larger, and for a higher mass it disappears entirely. This can be seen in the super-critical case, where there is only a single large negative-deviation region reaching almost to the tangential critical curve.\n\n\\renewcommand{\\thefigure}{12.B}\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[height=22 cm]{f12_B.pdf}\n\\caption{Image-plane maps of the relative weak-shear deviation from the shear, $\\gamma_{\\text{w}}/\\gamma-1$, of a NFW halo + point-mass lens, for a finer grid of point-mass positions than in Figure~\\ref{fig:weak-shear_deviation}. Notation same as in Figure~\\ref{fig:weak-shear_deviation}.\\label{fig:weak-shear_deviation-online}}}\n\\efi\n\nThe transitions of the patterns with increasing point-mass position $x_\\text{P}$ can be studied in more detail in Figure~\\ref{fig:weak-shear_deviation_perturbation-online}. For example, note the appearance of a positive-deviation region in the super-critical case at $x_\\text{P}=0.01$, as soon as the point mass is displaced from the halo center. For larger separations the boundary of this region forms the zero-weak-shear-deviation contour associated with the perturbed NFW radial critical curve. The small negative regions inside the tiny critical-curve loops can be seen here at $x_\\text{P}=0.13$ in the sub-critical case, or at $x_\\text{P}=0.12$ in all cases. In the critical and super-critical cases at $x_\\text{P}=0.13$, we see that these regions persist even after the loops merge with the outer critical curve.\n\nStudying the colored contours, we see that the regions with the strongest negative weak-shear deviation occur close to the point mass and near the zero-shear points (except when they lie between the perturbed NFW radial and tangential critical curves). The strongest positive deviation occurs close to the point mass along the horizontal axis outside its Einstein radius, and just outside the tiny critical-curve loops enclosing the zero-shear points. A closer inspection of the contours reveals that some of them display sharp bends at specific positions. These may appear at three types of locations. First, at the unit-convergence circle, as can be seen for example in the critical case at $x_\\text{P}=0.15$ on the pale blue, pale orange, and dark orange contours. These kinks are caused by the switching of the minimum fractions in Equation~(\\ref{eq:flattening}) at $\\kappa=1$, which causes a discontinuity in the derivatives.\n\nSecond, at the critical curves, as can be seen for example in the super-critical case at $x_\\text{P}=0.25$ on the pale orange and pale blue contours at the left side of the plot, or in the sub-critical case at $x_\\text{P}=0.15$ on the pale orange and pale blue contours above and below the halo center. These kinks are caused by crossing the zero points of the absolute values in Equation~(\\ref{eq:flattening}), which also causes a discontinuity in the derivatives. Third, at the NFW-halo critical curves, as can be seen for example in the super-critical case at $x_\\text{P}=0.15$ on the dark blue contours around the zero-shear points above and below the point mass, or in the critical case at $x_\\text{P}=0.20$ on the dark blue contour extending from the point mass toward the halo center. These kinks are similar to the previous ones, being caused by crossing the zero points of the absolute values in Equation~(\\ref{eq:flattening}) when evaluating the NFW halo weak shear.\n\nRegarding the extent of the contours, they are generally slightly smaller than those in Figure~\\ref{fig:shear_deviation_perturbation}, indicating that the weak-shear deviation falls more quickly with the distance from the point mass than the shear deviation. As a rough estimate, at the moment of separation of the critical curve surrounding the point mass from the perturbed NFW tangential critical curve, the $\\gamma_{\\text{w}}/\\gamma_{\\text{w,\\tiny{NFW}}}-1 = \\pm\\,10^{-1}$ contours extend roughly six Einstein radii from the point mass.\n\n\n\\subsubsection{Weak-shear Deviation from the Shear}\n\\label{sec:plots-weak_shear_deviation_from_shear}\n\nThe relative deviation $\\gamma_{\\text{w}}/\\gamma-1$ of the weak shear (shown in Figure~\\ref{fig:weak_shear}) from the shear (shown in Figure~\\ref{fig:shear}) is plotted in the image-plane color maps in Figure~\\ref{fig:weak-shear_deviation}. We use the same color scale as in the third panel of Figure~\\ref{fig:NFW-line}, which shows the same quantity plotted for the NFW halo alone. By its definition, this deviation shows the relative error of shear estimation using the weak-lensing approximation.\n\nIn the case of the sub-critical mass (left column), the plots are very similar to the plot in the third panel of Figure~\\ref{fig:NFW-line}, with the point mass affecting only its nearby surroundings. Note that the weak shear is always zero at the position of the point mass, so that the deviation reaches its minimum value of $-1$ there, leading to a dark blue spot similar to the one associated with the point mass in Figure~\\ref{fig:weak-shear_deviation_perturbation}. In the bottom row, the central negative spot is thus more prominent than in the absence of the point mass and it includes even the inner radial critical curve. At $x_\\text{P}=0.05$, there is a pair of red spots with a strong positive deviation close to the zero-shear points above and below the point mass. At these points, both $\\gamma_{\\text{w}}$ and $\\gamma$ drop to $0$, but their ratio $\\gamma_{\\text{w}}/\\gamma$ converges to $1/|\\kappa-1|$. The deviation is thus positive as long as $\\kappa<2$, corresponding to zero-shear points located more than circa $0.011$ from the center of our fiducial halo.\n\nAs the point mass crosses the unit-convergence circle near $x_\\text{P}=0.1$, the red spots shrink but their peak deviation increases. When the zero-shear points approach the perturbed NFW tangential critical curve at $x_\\text{P}=0.15$, the spots expand to their largest as they merge with the positive-deviation band along the critical curve. When the point-mass critical-curve loop is detached from the perturbed tangential critical curve of the NFW halo as seen in the two top rows, the positive-deviation spots remain associated with the zero-shear points above and below the point mass, even though their peak deviation declines with increasing point-mass position $x_\\text{P}$. The pattern around the halo center at $x_\\text{P}=0.3$ is virtually identical to the unperturbed-halo pattern in the third panel of Figure~\\ref{fig:NFW-line}.\n\nThe plots for a critical mass in the central column follow a similar sequence, with larger affected regions around the point mass. When it is positioned at the halo center (bottom row), we see that the single radial critical curve shows practically zero deviation, and between it and the blue unit-convergence circle the positive deviation reaches lower values. At $x_{\\text{P}}=0.1$, the red areas of high deviation extend from the perturbed NFW radial critical curve past the zero-shear points and beyond the unit-convergence circle. The pinched pattern at its intersection with the circle is indicative of the elliptic umbilic at which the weak shear is undefined, as discussed in Section~\\ref{sec:plots-weak_shear}.\n\nThe regions strongly influenced by the super-critical mass in the right column are much larger, with nearly half of the pattern around the NFW halo critical curve affected for $x_{\\text{P}}=0.1, 0.15,$ and $0.2$. For $x_{\\text{P}}=0$ there is no region of positive deviation between the halo center and the unit-convergence circle. For $x_{\\text{P}}=0.05$ a positive region is present but there are no red spots, as there are no zero-shear points at this configuration. At $x_{\\text{P}}=0.1$ the red spots are very large and prominent. At \\mbox{$x_{\\text{P}}=0.15$}, in addition to the zero-shear-point red spots there are two adjacent smaller red spots along the perturbed NFW radial critical curve.\n\nThe changing patterns can be studied with a finer step in point-mass positions in Figure~\\ref{fig:weak-shear_deviation-online}. For example, the pattern around the tiny critical-curve loops can be seen at $x_{\\text{P}}=0.12$ and in a few neighboring panels. We also point out the $x_{\\text{P}}=0.03$ panel in the sub-critical case and the $x_{\\text{P}}=0.04$ panel in the critical case. In these panels there are no red spots close to the zero-shear points, which lie still too close to the halo center. However, in the super-critical case at $x_{\\text{P}}=0.06$ we see red (or rather orange) spots at the tips of the perturbed NFW radial critical curve even though the zero-shear points are not present yet. One panel higher, at $x_{\\text{P}}=0.07$, these red spots are more prominent while the zero-shear points are located near their edge closer to the halo center. Clearly, the red spots develop at the perturbed NFW radial critical curve even for lower point-mass positions at which there are no zero-shear points. With a slight increase in $x_\\text{P}$, the points reach the red spots, which then remain associated with them at more distant point-mass positions.\n\nOverall, the pattern of the blue areas shows that the weak shear underestimates the shear near the halo center, along the unit-convergence circle, and roughly within an Einstein radius of the point mass (extending further when it overlaps with the unit-convergence circle). Practically everywhere else the weak shear overestimates the shear, most prominently in the red and orange areas: close to the zero-shear points, along the perturbed NFW tangential critical curve, and along the perturbed NFW radial critical curve (except when a super-critical mass is positioned close to the halo center). Further from the halo center, e.g., close to the right edge of the plots in Figure~\\ref{fig:weak-shear_deviation} for our fiducial halo, the positive deviation is very low, so that the weak shear may serve as a good approximation of the shear. However, the agreement fails in the vicinity of the point mass. This occurs primarily inside its Einstein radius, but also further to the tangentially offset zero-shear points. Note that for $\\kappa_{\\text{\\tiny{NFW}}}(x_{\\text{P}})\\ll 1$ the deviation at the zero-shear points reaches a value $\\gamma_{\\text{w}}/\\gamma-1\\approx\\kappa_{\\text{\\tiny{NFW}}}(x_{\\text{P}})$, so that their influence decreases as the halo convergence declines for higher $x_{\\text{P}}$.\n\n\n\\subsubsection{Weak Phase}\n\\label{sec:plots-weak_phase}\n\nIn Figure~\\ref{fig:weak_phase} we present image-plane plots of the weak phase $\\varphi_{\\text{w}}$ of the NFW halo + point-mass lens, given by Equation~(\\ref{eq:NFWP_weak_phase}). We defined the weak phase in Section~\\ref{sec:NFW-weak} as the phase that would be measured from the orientation of image deformations using weak-lensing analysis. For an elliptical image of a small circular source the weak phase is simply the angle between its major axis and the horizontal axis in the plots. Thus, $\\varphi_{\\text{w}}=0$ corresponds to horizontally and $|\\varphi_{\\text{w}}|=\\pi/2$ to vertically elongated images. The weak phase of the NFW-halo lens is described in Section~\\ref{sec:NFW-weak} and shown in the fourth panel of Figure~\\ref{fig:NFW-line}. Without the point mass, outside the unit-convergence circle the weak phase is equal to the phase and images are oriented tangentially. Inside the unit-convergence circle the weak phase is perpendicular to the phase and images are oriented radially.\n\nIn Figure~\\ref{fig:weak_phase} we use the same weak-shear color scale as in the fourth panel of Figure~\\ref{fig:NFW-line}, with white corresponding to horizontal images, orange to images oriented counterclockwise from the horizontal, and blue to images oriented clockwise from the horizontal. The saturation of both colors increases with the angle from the horizontal, from zero saturation for angle $0$ to maximum saturation for angle $\\pi/2$. The dot-dashed contour marks several special orientations: $\\varphi_{\\text{w}}=0$, $|\\varphi_{\\text{w}}|=\\pi/2$, and undefined $\\varphi_{\\text{w}}$. The weak phase is undefined at all points with zero weak shear, as described in Section~\\ref{sec:plots-weak_shear}. These include the halo center, the point-mass position, the zero-shear points, and the unit-convergence circle. All of these points thus play an important role in the patterns seen in the color maps in Figure~\\ref{fig:weak_phase}.\n\nThanks to the axial symmetry of the lens configurations with $x_{\\text{P}}=0$ in the bottom row of Figure~\\ref{fig:weak_phase}, the color maps are identical to that of the unperturbed halo, as seen in the fourth panel of Figure~\\ref{fig:NFW-line}. This means that outside the $\\kappa=1$ circle images are oriented tangentially, while inside the circle the weak phase is flipped by $\\pi/2$ and images are oriented radially. Exactly at the halo center the weak phase is undefined. Note the colors alternating around the center with orange in the first quadrant and the saturation varying from zero along the horizontal to maximum along the vertical axis. Such a pattern corresponds to radial orientation of images around the center.\n\n\\renewcommand{\\thefigure}{13.A}\n\\begin{figure*}\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[width=15.5 cm]{f13_01.pdf}\n\\caption{Image-plane maps of the weak phase $\\varphi_{\\text{w}}(\\boldsymbol x)$ of a NFW halo + point-mass lens, described in Section~\\ref{sec:plots-weak_phase}. Orange and blue correspond to images oriented counterclockwise and clockwise, respectively, from the horizontal. Dot-dashed curves indicate exactly horizontal, exactly vertical, or undefined image orientation. Remaining notation as in Figure~\\ref{fig:shear}.\\label{fig:weak_phase}}}\n\\end{figure*}\n\nThe variation of the weak-phase maps with point-mass position $x_\\text{P}$ is best seen in the super-critical case (right column). In the top row with $x_\\text{P}=0.3$ the point mass is well separated from the halo center. The left part of the plot closer to the halo center is similar to the unperturbed pattern seen also in the bottom row. Note here the slight shift toward the point mass of the dot-dashed contour extending vertically outward from the unit-convergence circle, which corresponds to horizontal images. The main new feature is the oval region with inverted colors bordered by a loop of the dot-dashed contour, which passes through the position of the point mass at its right side and extends in the direction of the halo center on its left side. The pattern seen around the point mass also has colors alternating around the center, however, here with blue in the first quadrant. In addition, the saturation varies from zero along the vertical to maximum along the horizontal axis. This pattern corresponds to tangential orientation of images around the point mass.\n\nThe left part of the dot-dashed border of the oval separates high-saturation regions and, thus, corresponds to vertically oriented images. The right part of the border lies in the white band separating low-saturation regions and, thus, corresponds to horizontally oriented images. The two parts of the boundary meet at the zero-shear points that lie above and below the point mass. In this panel, horizontal images occur along the horizontal axis inside the unit-convergence circle, along the vertical dot-dashed contour outside the unit-convergence circle, and along the right boundary of the oval region connecting the zero-shear points vertically through the point-mass position. Vertical images occur along the horizontal axis outside the unit-convergence circle, along the vertical dot-dashed contour inside the unit-convergence circle, and along the left boundary of the oval region connecting the zero-shear points and passing vertically through a point between the halo center and the point mass.\n\nGoing down in the right column, the inverted-color oval shrinks slightly as it moves with the point mass closer to the halo center. At $x_{\\text{P}}=0.15$, the oval partly overlaps the unit-convergence circle. The very pale color in the region of their overlap indicates near-horizontal orientation of images there. At the boundary of the oval in this plot, images are oriented horizontally along its left part inside the unit-convergence circle and along its right part from the zero-shear points to the point-mass position. Along the segments extending from the zero-shear points to the left till the unit-convergence circle, images are oriented vertically. Image orientations along the other parts of the dot-dashed contour remain unchanged.\n\n\\renewcommand{\\thefigure}{13.B}\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[height=22 cm]{f13_B.pdf}\n\\caption{Image-plane maps of the weak phase $\\varphi_{\\text{w}}(\\boldsymbol x)$ of a NFW halo + point-mass lens, for a finer grid of point-mass positions than in Figure~\\ref{fig:weak_phase}. Notation same as in Figure~\\ref{fig:weak_phase}.\\label{fig:weak_phase-online}}}\n\\efi\n\n\\renewcommand{\\thefigure}{14.A}\n\\begin{figure*}\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[width=15.5 cm]{f14_01.pdf}\n\\caption{Image-plane maps of the weak-phase deviation $\\delta\\varphi_\\text{w}(\\boldsymbol x)$ caused by the presence of the point mass, described in Section~\\ref{sec:plots-weak_phase_change}. The maps also exactly portray the change in image orientation due to the point mass. Red and blue indicate counterclockwise and clockwise deviation, respectively. The color scale changes from logarithmic to linear in the interval $[-\\pi/2000,\\pi/2000]$. Four colored contours correspond to $\\delta\\varphi_\\text{w}$ values indicated in the color bar; the dot-dashed contour corresponds to $\\delta\\varphi_\\text{w}=0$ and $\\delta\\varphi_\\text{w}=\\pm \\pi/2$. Remaining notation as in Figure~\\ref{fig:shear}.\\label{fig:weak-phase_change}}}\n\\end{figure*}\n\nAt $x_{\\text{P}}=0.1$, the zero-shear points now lie inside the unit-convergence circle, so that horizontally oriented images now occur along the left boundary of the oval up to the zero-shear points and along the part of the right boundary outside the unit-convergence circle passing through the point-mass position. Vertically oriented images occur along the segments extending from the zero-shear points to the right till the unit-convergence circle. The pattern inside the overlap of the oval and the unit-convergence circle reflects two general properties of its boundaries. Crossing the boundary of the oval inverts only the color at $\\varphi_{\\text{w}}=0$ or $|\\varphi_{\\text{w}}|=\\pi/2$, which corresponds to a continuous change in orientation. Crossing the unit-convergence circle inverts the color as well as the saturation, which corresponds to a $\\pi/2$ flip in orientation. Note here also the more prominent bulging of the vertical dot-dashed contour passing through the halo center as the point mass lies closer.\n\nBetween $x_{\\text{P}}=0.1$ and $x_{\\text{P}}=0.05$, the boundary of the inverted-color oval undergoes reconnection with the bulging vertical dot-dashed contour. At $x_{\\text{P}}=0.05$ this contour passes through the point-mass position, while a small inverted-color oval lies to the left of it, with its left boundary passing through the halo center. Note that at $x_{\\text{P}}=0.05$ there are no zero-shear points, so that here the entire boundary of the small oval corresponds to horizontal images. What is more striking in this plot is the pattern around the point mass, which now lies inside the unit-convergence circle. Orange in the first quadrant and zero saturation along the horizontal axis indicate that images are now oriented radially around the point mass. As the point mass moves closer to the halo center, at a certain value $0\\leq x_{\\text{P}} < 0.05$ the small oval disappears and the dot-dashed vertical curve passes through the origin for $x_{\\text{P}}=0$.\n\nIn the critical and sub-critical cases (the central and left columns, respectively) the same sequence of changes occurs as in the super-critical case, although for slightly different values of $x_{\\text{P}}$. Note that the inverted-color oval increases in size with increasing mass parameter $\\kappa_{\\text{P}}$. In the top rows, the point at which the left side of its boundary crosses the horizontal axis marks a boundary of influence of the halo and the point mass (at least as far as image orientation is concerned). Both boundaries passing through this point correspond to vertically oriented images. However, images located off the horizontal axis to the left of this point are tilted tangentially to the halo, while those to the right of this point are tilted tangentially to the point mass.\n\nDetails of the changing patterns and contours can be studied in more detail in Figure~\\ref{fig:weak_phase-online}. For example, it can be seen that the reconnection of the vertical and oval dot-dashed contours occurs at the zero-shear points. At higher values of $x_{\\text{P}}$ the points lie on the larger oval passing through the point-mass position, while at lower values they lie on the smaller oval passing through the halo center. For progressively lower point-mass distances, the zero-shear points move along the smaller oval to the center, where they disappear at $x_{\\text{P}}=\\sqrt{\\kappa_{\\text{P}}/\\kappa_{\\text{s}}}$.\n\n\\renewcommand{\\thefigure}{14.B}\n\\bfi\n{\\centering\n\\vspace{0cm}\n\\hspace{1cm}\n\\includegraphics[height=22 cm]{f14_B.pdf}\n\\caption{Image-plane maps of the weak-phase deviation $\\delta\\varphi_\\text{w}(\\boldsymbol x)$ caused by the presence of the point mass, for a finer grid of point-mass positions than in Figure~\\ref{fig:weak-phase_change}. Notation same as in Figure~\\ref{fig:weak-phase_change}.\\label{fig:weak-phase_change-online}}}\n\\efi\n\nThe plot for the critical case at $x_{\\text{P}}=0.1$ illustrates the situation at the elliptic umbilics, which occur for a slightly lower point-mass distance. In this case, the full length of the oval contour corresponds to horizontal image orientations (except the point-mass position and the zero-shear points with undefined orientation). Similarly, the $x_{\\text{P}}=0.09$ plots for the critical and super-critical cases indicate the interesting pattern near the point mass when it lies on the unit-convergence circle (in both cases for a slightly higher point-mass distance). To the right of the point mass images are oriented vertically (tangentially), to the left horizontally (radially), and above and below the orientation is undefined (i.e., the images are circular).\n\nThe weak-phase plots presented in Figure~\\ref{fig:weak_phase} can also be used to visualize the phase, which we do not present in a separate plot. The reason is indicated by Equation~(\\ref{eq:NFWP_weak_phase}), which shows that outside the unit-convergence circle the phase is equal to the weak phase, while inside it differs by $\\pi/2$. The phase plots would thus differ from the weak-phase plots in Figure~\\ref{fig:weak_phase} by having the colors and saturations flipped inside the unit-convergence circle around the halo center. The circle would disappear in such plots, and the color and saturation outside would extend continuously inside all the way to the halo center. The dot-dashed contour would then consist only of the horizontal axis, the perturbed vertical line through the halo center, and the large oval associated with the point mass reconnecting to the small oval associated with the halo center.\n\nOverall, note that unlike the weak shear in Figure~\\ref{fig:weak_shear}, the weak phase in Figure~\\ref{fig:weak_phase} shows very little correlation with the geometry of the critical curve. Finally, we point out that placing the point mass off the horizontal axis would not lead to a simple rotation of the patterns as in the other presented plot grids. Instead, the overall halo pattern and the pattern close to the point mass would remain unchanged. A point mass in a blue region of the halo would thus locally generate a pair of orange lobes, while in an orange region it would generate a pair of blue lobes.\n\n\n\\subsubsection{Weak-phase Deviation Due to the Point Mass}\n\\label{sec:plots-weak_phase_change}\n\nThe image-plane color maps presented in Figure~\\ref{fig:weak-phase_change} depict the weak-phase deviation due to the point mass, defined as the difference between the weak shear of the NFW halo + point-mass lens (shown in Figure~\\ref{fig:weak_phase}) and the weak shear of the NFW halo alone (shown in the fourth panel of Figure~\\ref{fig:NFW-line}), $\\delta\\varphi_\\text{w}(\\boldsymbol x)= \\varphi_\\text{w}-\\varphi_{\\text{w,\\tiny{NFW}}}$. We correct the difference if necessary by adding or subtracting $\\pi$ to keep $\\delta\\varphi_\\text{w}$ in the interval $\\left[-\\pi/2,\\,\\pi/2\\right]$. The deviation is also equal to the angle by which the orientation of an image changes due to the presence of the point mass. Red positive values of $\\delta\\varphi_\\text{w}$ correspond to a counterclockwise change, blue negative values to a clockwise change in orientation. The color saturation is scaled linearly for $|\\delta\\varphi_\\text{w}|\\leq\\pi/2000$ and logarithmically for $|\\delta\\varphi_\\text{w}|\\geq\\pi/2000$. Contours are plotted for $|\\delta\\varphi_\\text{w}|=\\pi/20=9^{\\circ}$ (dark red and blue), and for $|\\delta\\varphi_\\text{w}|=\\pi/200=0.9^{\\circ}$ (light red and blue).\n\nWhen the point mass is located exactly at the center of the halo (bottom row of Figure~\\ref{fig:weak-phase_change}), the lens has axial symmetry and the weak-phase deviation is zero everywhere, which explains the completely white plots. Note that this result also reflects the fact that images are oriented radially around a point mass lying inside the unit-convergence circle, as shown in Section~\\ref{sec:plots-weak_phase}.\n\nAll the other plots with an off-center point mass share the same characteristic color pattern; they differ only in its scale and in color saturation. The blue and red regions are separated by the dot-dashed contour, which marks all positions with zero deviation $\\delta\\varphi_\\text{w}=0$ or maximum deviation $|\\delta\\varphi_\\text{w}|=\\pi/2$. The geometry of the contour is simple and independent of the mass parameter $\\kappa_\\text{P}$: it includes the horizontal axis and the $\\omega=\\pi/2$ circle reaching from the halo center to the position of the point mass (see Figure~\\ref{fig:omega}). Along the horizontal axis, images are always horizontal inside and vertical outside the unit-convergence circle, unaffected by the presence of a point mass, as shown in Section~\\ref{sec:plots-weak_phase}. Hence, this part of the dot-dashed contour corresponds to $\\delta\\varphi_\\text{w}=0$.\n\nAlong the $\\omega=\\pi/2$ circle, there is a right angle between the direction to the halo center and the direction to the point mass. The shear from the halo and the shear from the point mass thus act in perpendicular directions. Going from the halo center along the $\\omega=\\pi/2$ circle, the halo shear decreases and the point-mass shear increases, as described in Section~\\ref{sec:NFWP-csp}. Hence, from the halo center to the zero-shear points the halo shear dominates and the image orientation remains unchanged, so that $\\delta\\varphi_\\text{w}=0$ along this part of the circle. From the zero-shear points to the point mass, the point-mass shear dominates and the images are oriented perpendicular to the orientation they would have in absence of the point mass, so that $|\\delta\\varphi_\\text{w}|=\\pi/2$ along the remaining part of the circle. For point-mass positions $x_{\\text{P}}<\\sqrt{\\kappa_{\\text{P}}/\\kappa_{\\text{s}}}$, there are no zero-shear points and the entire $\\omega=\\pi/2$ circle corresponds to $|\\delta\\varphi_\\text{w}|=\\pi/2$.\n\nAbove the horizontal axis and outside the $\\omega=\\pi/2$ circle, the weak-shear deviation is positive meaning that in this region the image orientation changes counterclockwise. Conversely, under the axis and outside the circle, the deviation is negative and the image orientation changes clockwise. Inside the $\\omega=\\pi/2$ circle, the sign of the deviation in either half-plane is switched and image orientations change in the opposite sense. At the center of the halo and at the location of the point mass, four regions of alternating positive and negative deviation meet.\n\nDeviations $|\\delta\\varphi_\\text{w}|$ peak close to the point mass (particularly along the  $\\omega=\\pi/2$ circle), and fall rapidly with increasing distance from it. The color pattern near the point mass indicates that images are oriented tangentially around it (when it lies outside the unit-convergence circle) or radially around it (when it lies inside the unit-convergence circle). The color pattern around the halo center indicates that the point mass orients the images more horizontally there. When zero-shear points are present, zero deviations occur in the horizontal and vertical directions from the center and strongest deviations occur along the diagonals. In the absence of zero-shear points, for $x_{\\text{P}}<\\sqrt{\\kappa_{\\text{P}}/\\kappa_{\\text{s}}}$, zero deviations occur in the horizontal directions and strongest $|\\delta\\varphi_\\text{w}|=\\pi/2$ deviations occur in the vertical directions.\n\nThe areas with high deviation $|\\delta\\varphi_\\text{w}|$ extend further from the point mass for greater values of $\\kappa_{\\text{P}}$, as seen in the central and right columns of Figure~\\ref{fig:weak-phase_change}. For point masses far from the halo center (in the top rows), all contours form four-lobed butterfly-like shapes. For point masses closer to the halo center, the left lobes of the contours become pointy (e.g., $x_{\\text{P}}=0.2$ in the central column) and eventually they extend to the halo center (e.g., $x_{\\text{P}}=0.15$ in the central column). To illustrate the area affected by the point mass, we estimate the extent of the contours at the moment of separation of the critical curve surrounding the point mass from the perturbed NFW tangential critical curve, i.e., between $x_{\\text{P}}=0.20$ and $x_{\\text{P}}=0.25$ for the sub-critical and critical cases, and between $x_{\\text{P}}=0.25$ and $x_{\\text{P}}=0.30$ in the super-critical case. In all three cases, the inner \\mbox{$|\\delta\\varphi_\\text{w}|=\\pi/20$} contours extend about four Einstein radii, while the outer $|\\delta\\varphi_\\text{w}|=\\pi/200$ contours extend as far as twelve Einstein radii from the point mass.\n\nThe changes in the contours for different point-mass positions can be studied in more detail in Figure~\\ref{fig:weak-phase_change-online}. The abrupt change from zero deviation to $|\\delta\\varphi_\\text{w}|=\\pi/2$ along the $\\omega=\\pi/2$ circle at the zero-shear points can be best seen in the presence of the small critical-curve loops surrounding them, e.g., for $x_{\\text{P}}=0.12$. Note also the very small extent of the colored regions for the lowest separations $x_\\text{P}$.\n\nSimilarly to Figure~\\ref{fig:weak_phase}, the weak-phase deviation in Figure~\\ref{fig:weak-phase_change} shows very little correlation with the geometry of the critical curve. However, unlike Figure~\\ref{fig:weak_phase}, it also shows no influence of the unit-convergence circle.\n\n\n\\section{Discussion}\n\\label{sec:discussion}\n\nThe examples of images formed by the NFW halo + point-mass lens in the bottom right panel of Figure~\\ref{fig:images} can be compared with the results presented in Section~\\ref{sec:NFWP-plots}, specifically with the corresponding $x_\\text{P}=0.2$, $\\kappa_\\text{P}\\approx 2.714\\cdot 10^{-4}$ panels of the plot grids. In particular, the weak-shear map in Figure~\\ref{fig:weak_shear} shows the image flattening, the weak-phase map in Figure~\\ref{fig:weak_phase} shows the image orientation, and the weak-shear-deviation-from-shear map in Figure~\\ref{fig:weak-shear_deviation} shows the relative shear error that would be incurred by assuming the weak-lensing relation between image distortion and shear.\n\nA few points should be noted regarding the interpretation of such comparisons. First, Figures~\\ref{fig:shear}--\\ref{fig:weak-phase_change} present maps of their respective quantities for point-like sources. For images of extended sources such as those shown in Figure~\\ref{fig:images}, one has to consider the full variation of the quantities within the area of the image. For example, an image lying on the unit-convergence circle will have its inner part extended radially and its outer part tangentially, as seen from Figure~\\ref{fig:weak_phase}.\n\nSecond, when studying the changes in image shape and orientation due to the point mass using Figure~\\ref{fig:images} with Figure~\\ref{fig:weak-shear_deviation_perturbation} or Figure~\\ref{fig:weak-phase_change}, a direct comparison can be made when there is at least a partial overlap of the images formed by halos with and without the point mass. In the bottom right panel of Figure~\\ref{fig:images}, the left and central images have a large overlap with the images in the top right panel, the lower right image has a smaller overlap, and the two images close to the point mass have no overlap. For these two images, Figure~\\ref{fig:weak-shear_deviation_perturbation} and Figure~\\ref{fig:weak-phase_change} show the deviations from images formed at the same positions by the halo-only lens, but for different source positions. In this particular case, the sources would lie above the horizontal axis at different radial distances in the top left panel of Figure~\\ref{fig:images}. In fact, even in the case of overlapping images, the overlapping parts may be images of different parts of the source. Thus, Figure~\\ref{fig:weak-shear_deviation_perturbation} and Figure~\\ref{fig:weak-phase_change} compare the properties of images formed by the two lens models at a same position in the image plane of sources at different positions in the source plane. They do not compare the properties of images of a fixed source formed by a halo with and without a point mass.\n\nThird, in this work we do not explore the changes in image positions due to the point mass. This would be difficult to present in general, if only due to the change in the number of images. However, it could be done in a perturbative regime, by studying the displacement and distortion of particular images of a fixed source due to the presence of a point mass. This approach was taken by \\cite{wagner18}, who studied general perturbations of large arc-like images along the tangential critical curve of axisymmetric lenses. One provided example shows the influence of a point-mass perturber on the radii and lengths of arcs formed by a singular isothermal sphere lens. Even though NFW halos have different lensing properties, the generic pattern of angular distortions seen for example in the $x_\\text{P}=0.25$, $\\kappa_\\text{P} = 10^{-4}$ panel of Figure~\\ref{fig:weak-phase_change} indicates that an arc-like image between the tangential critical curve and the point mass inside the $\\omega=\\pi/2$ circle would be straightened by its influence. Hence, its radius of curvature would be increased, in agreement with the example in \\cite{wagner18}. Overall, image-plane maps such as those in Figures~\\ref{fig:weak-shear_deviation_perturbation} and \\ref{fig:weak-phase_change} are suitable for assessing the influence of compact masses in observed lensing clusters or galaxies where the image positions are fixed.\n\nThe results presented in this work correspond to one fiducial value of the halo convergence parameter $\\kappa_\\text{s}$. The variation of the critical curves and the unit-convergence circle with $\\kappa_\\text{s}$, as well as the variation of $\\kappa_\\text{s}$ and $\\kappa_\\text{P}$ with source redshift are discussed in \\citetalias{karamazov_etal21}. The plots presented above in Sections~\\ref{sec:plots-shear}--\\ref{sec:plots-weak_phase_change} can be expected to follow the combined geometry of the critical curve, the unit-convergence circle, and the $\\omega=\\pi/2$ circle. Note that the last mentioned circle is given purely by the value of $x_\\text{P}$, which is independent of the source redshift and the halo convergence parameter.\n\nSome of the obtained results have more general relevance than just for a point mass embedded in a spherically symmetric NFW halo. These include some of the analytic results, such as the shear of a combination of two mass distributions described in Section~\\ref{sec:NFWP-csp}. Among the numerical results shown in Figures~\\ref{fig:shear}--\\ref{fig:weak-phase_change}, the patterns seen around the point mass at sufficient separation from the halo critical curves will be very similar for other halo mass distributions with low spatial variation on the scale of the point-mass Einstein radius \\citep[e.g.,][]{chang_refsdal84}, as discussed further below. Best seen in the top left plots for $x_\\text{P}=0.30$, $\\kappa_\\text{P} = 10^{-4}$, the patterns are relevant not only for point masses separated from other mass concentrations, but also for extended bodies with compact mass distributions that do not extend significantly beyond their Einstein radius.\n\nThe applicability of the studied lens model to the astrophysical scenarios of a galaxy within a galaxy cluster, of a satellite galaxy within a galactic halo, and of a massive black hole in a galactic halo is discussed in \\citetalias{karamazov_etal21}. In addition, possible extensions toward more advanced models are pointed out there, such as replacing the point mass by an extended mass distribution. For distributions that do not extend significantly beyond their total-mass Einstein radius, the lensing quantities in the surroundings will not differ significantly from the patterns seen in Figures~\\ref{fig:shear}--\\ref{fig:weak-phase_change}. However, for more extended distributions the patterns will be affected more substantially. Nevertheless, at large separations the lensing impact of any compact object may be approximated by that of a point mass.\n\nThe explored model can be extended also by altering the properties of the NFW halo. On the one hand, one may change its central properties by adding a core radius, or by changing its density divergence \\citep{evans_wilkinson98}. Such changes would alter the radii of the halo critical curves and caustics, the reference plots in Figure~\\ref{fig:NFW-line}, and the critical value $\\kappa_{\\text{PC}}$ of the mass parameter of the point mass. This would impact the presented results primarily for point-mass positions close to the halo center. Regardless of the nature of the alterations, for sufficiently large distances the point-mass critical-curve loop disconnects from the halo critical curves. In this distant regime the patterns in the vicinity of the point mass will have the same character as seen in the figures.\n\nOn the other hand, one may abandon spherical symmetry and study the effect of a massive object embedded in a more realistic elliptical NFW halo. The lensing properties of an NFW halo with an elliptically symmetric mass distribution are poorly studied due to the lack of simple analytic expressions to describe them \\citep{oguri21}. Nevertheless, for low values of the halo ellipticity the properties can be approximated by those of the pseudo-elliptical model, which has an elliptically symmetric lens potential \\citep{golse_kneib02,meneghetti_etal03,dumet-montoya_etal12}. In this model the critical curve typically consists of two nested oval loops instead of the two circles of the spherical model. The point-like tangential caustic of the spherical model is replaced by a four-cusped loop; the circular radial caustic by an oval loop. In this model one may expect an even more complex dependence of the critical curves and caustics of the combined lens on the mass and two-dimensional position of the point mass than in the spherical case described in \\citetalias{karamazov_etal21}.\n\nIn the elliptical model, the unit-convergence circle which plays a key role in the spherical model when studying the geometry of images is replaced by a unit-convergence ellipse. The grids of the plots corresponding to those in Figures~\\ref{fig:shear}--\\ref{fig:weak-phase_change} would be complicated by an additional parameter, the angular position of the point mass with respect to the axes of the elliptical halo. Nevertheless, the main factors driving the patterns described in the text, such as the existence and location of zero-shear points or the geometry of the perturbations of the critical curve, would remain the same. The asymptotic patterns would have the same nature as seen in the spherical model, as mentioned above. However, they would be less symmetric and the extent of the contours would additionally depend on the angular position within the halo.\n\nThe presented single-point-mass results will be useful for interpreting the properties of cluster lens models with multiple (point) masses embedded in an NFW halo. Masses that are sufficiently separated from the perturbed NFW halo critical curves as well as from other masses should display similar patterns in their vicinity, as discussed above. Each of these masses will also produce deviation patterns near the halo center similar to those seen in the top left plots in Figures~\\ref{fig:shear_deviation_perturbation}, \\ref{fig:weak-shear_deviation_perturbation}, or \\ref{fig:weak-phase_change}. Due to the directional dependence of these patterns, their superposition for a sufficient number of isotropically distributed masses would drive the amplitude of the central deviations to zero. Strong differences can be expected when one or more of the masses are positioned close to the halo center, or when two or more neighboring masses are mutually separated by less than a few Einstein radii. These situations cannot be simply extrapolated from the results presented in this work and their study requires direct simulations.\n\n\n\\section{Summary}\n\\label{sec:summary}\n\nIn this paper we proceeded in our study of gravitational lensing by a compact massive object in a dark matter halo. In \\citetalias{karamazov_etal21} we analyzed the critical curves and caustics of a lens consisting of a point mass embedded in a spherical NFW halo. Here we concentrated on the shear and phase of the same lens model, focusing on their relation to the geometry of images formed by the lens.\n\nIn Section~\\ref{sec:NFW-csp}, we described the properties of the shear and phase of a lens consisting only of a NFW halo. In order to study the images, we used the eigenvalue decomposition of the inverse matrix of a general lens-equation Jacobian matrix presented in Equation~(\\ref{eq:A_matrix}). Based on it, we introduced the convergence--shear diagram (CS diagram) in Appendix~\\ref{sec:Appendix-images}, which illustrates concisely the connection between arbitrary convergence and shear values and the geometry of an image corresponding to them. Specific lens models occupy characteristic regions in the diagram, which then define the properties of all images that could be formed by the lens. We described the properties of images formed by the NFW halo by reading them off the CS diagram in Section~\\ref{sec:NFW-images}. In Section~\\ref{sec:NFW-weak} we defined the weak shear and weak phase as the shear and phase values obtained by assuming weak-lensing relations involving the semiaxes of images and their orientation.\n\nWe followed the same outline for the NFW halo + point-mass lens in Section~\\ref{sec:NFWP}. In particular, we derived the formula for the shear in Equation~(\\ref{eq:NFWP_gamma}) which provides a geometric interpretation in terms of the halo and point-mass shears and the viewing angle $\\omega$ of the line segment separating them. The formula which is valid for combinations of other axisymmetric lenses is a special case of the more general formula in Equation~(\\ref{eq:shear_combination}) for the shear of a combination of two arbitrary lenses. For the NFW halo + point-mass lens, we discuss the appearance and location of zero-shear points in Section~\\ref{sec:NFWP-csp} and describe the conditions under which they form umbilic points in Section~\\ref{sec:NFWP-Jacobian}.\n\nFigures~\\ref{fig:shear}--\\ref{fig:weak-phase_change} illustrate the main results in terms of image-plane maps of different lens characteristics and CS diagrams, all presented for the same grid of point-mass parameter combinations as the grid used in \\citetalias{karamazov_etal21}. Important features and trends seen in the figures are described in the corresponding Sections~\\ref{sec:plots-shear}--\\ref{sec:plots-weak_phase_change}. As discussed in Section~\\ref{sec:discussion}, the obtained results have broader implications beyond the specific properties of the studied lens model.\n\n\n\n\\vskip 8mm\n\nWe thank the anonymous referee for comments and suggestions that helped improve the manuscript. Work on this project was supported by Charles University Grant Agency project GA UK 1000218.\n\n\n\\clearpage\n\n", "meta": {"timestamp": "2021-12-21T02:18:37", "yymm": "2109", "arxiv_id": "2109.02495", "language": "en", "url": "https://arxiv.org/abs/2109.02495"}}
{"text": "\\section{Introduction}\n\nWith over 4.6 billion active internet user worldwide \\citep{digitalpop}, online platforms have the potential to reach consumers at unprecedented rates.  Major online platforms typically provide personalized recommendations for their users---these system tailor search results, suggest products to purchase, customize front pages for news and opinions sites, curate social media posts, recommended movies, and generate music playlists.  Basic recommendation systems are relatively straightforward to build; Google's free recommendation systems course\\footnote{https://developers.google.com/machine-learning/recommendation} provides an estimate that the course will take a mere four hours to complete (given prerequisite skills).  Because of the ease of implementing a basic system, algorithmic recommendations are prolific online, deployed by business large and small to impact millions of people each day.\\looseness=-1\n\nRecently, there has been increasing interest in understanding the impact of personalized recommendation systems from perspectives that firms have historically not considered, such as: how much do these systems contribute to polarization?  Do they homogenize users?  Are they fair to all users or do they disadvantage portions of users?  These questions are difficult to answer from observational data and it may be risky, or even unethical, for firms to explore these ideas with A/B tests.  \nSimulations provide an alternative approach to understand the impacts of recommendation systems; this is an increasingly popular approach to evaluating these systems and broad simulation tools are being developed \\citep{lucherini2021t}.  Additionally, some recommendation methods, such as reinforcement learning techniques, require more sophisticated approaches to offline evaluation prior to moving to A/B testing; simulations are similarly appropriate in this and other contexts.\\looseness=-1\n\nWhen constructing a recommendation system simulation, there are two key challenges. \nThe first is defining a model of user choice.  There are a variety of well-established consumer choice models from economics and marketing, which will be described in more detail in \\Cref{sec:consumer_choice_models}.  In accordance with the notion that ``all models are wrong,''  the true nature of consumer decision-making is likely more complicated and heterogeneous than defined by these choice models. \nThat said, these models of choice offer us hope: if machine learning models can capture behavior patterns under these simplified assumptions, we have a chance of recovering and understanding users' true preferences in a more nuanced world. This would allow us to to begin describing the limitations of machine learning recommendation systems (e.g., by answering the previously posed questions about their impact) and therefore use them more judiciously in practice. \n\nThe other key challenge is defining how users encounter items that are not recommended, including in the counterfactual world in which they do not receive algorithmic recommendations at all.  In the real world, consumers encounter content in a wide variety of ways (and sometimes even using a variety of similar platforms): search, promotions, lists of new content or content by theme, and recommendations from friends or experts are a few alternative mechanisms.\nA simulation that does not include the notion of users accessing items in alternate ways will overestimate the effects of algorithmic recommendations on user behavior. This second challenge is not wholly disentangled from the first one of defining a model of user choice, but we will address it distinctly in \\Cref{sec:alternarives_to_rec}.\n\nWhile there are undoubtedly a variety of other challenges associated with building a recommendation system simulator, this position paper will delve into both of these key challenges, reviewing simulation assumptions for existing research in this context, as well as proposing alternative assumptions worth further exploration.  We will conclude with a broader discussion limitations of simulations and an outline of open questions in this area (\\Cref{sec:discussion}).\n\n\n\n\\section{Models of Consumer Choice}\n\\label{sec:consumer_choice_models}\n\nIn order for users to select items from among the recommendations, a simulator must include some model of user choice; this is our first key challenge.  Existing work in recommendation system simulations typically represents both user preferences and items attributes in some relatively low-dimensional space, not unlike the very recommendation systems they seek to evaluate.  For example, the general purpose T-RECS tool \\citep{lucherini2021t} uses vector representations for each item and user, with the ``score'' of an item for a user being either the inner product between the user preferences and item attributes, or the cosine similarity between the two representations.  Even within this construction, however, there are a wide variety of assumptions about how users make their choices.\n\nSome simulations assume that users are uncertain of their preferences.  \\citet{aridor2020deconstructing} model users as considering items sequentially, updating their beliefs as they go; under this paradigm, recommendations amount to adjusting the values which users rely on for decisions.\nOther work frames the utility of an item for a user as being comprised of two parts: known and unknown to the user; a user then acts based on a function of the recommended rank of an item and their known utility for that item \\citep{chaney2018algorithmic}.  Still other simulations forgo the individual element, assuming a universal quality value for each item with items being selected based on a function of eitehr rank (popularity) or quality based on a popularity bias parameter \\citep{ciampaglia2018algorithmic}.\nSometimes users are represented as points in a low-dimensional attitude space with items being selected based on distance metrics \\citep{geschke2019triple}.\n\\citet{yao2017beyond} use stochastic block models to generate data: users probabilistically belong to groups and users within a group probabilistically like items.\nUnfortunately, some work does not make the exact choice model clear, specifying only that users select a subset of recommended items and provide feedback on this set\n\\citep{jiang2019degenerate}; the mechanism for selection is unspecified.  In other cases, simulations assume that users act on every single recommendation, rating each according to their preferences \\citep{sun2019debiasing}.\\looseness=-1\n\nRegardless of the choice model used, researchers need to be explicit about these assumptions as it is important for replicability; simple alterations in assumptions may drastically alter results.  In addition, it would be beneficial to work towards a set of standard choice models.  In doing so, I encourage us to look to consumer choice models from economics and marketing.  Relying on these existing models confers multiple benefits to researchers and analysts: these models have been critiqued extensively, theoretically vetted, used to analyze real-world behaviors in multiple contexts, and bring with them established terminology; we need not reinvent the wheel.  While no choice model will perfectly capture all the nuances in user behavior, it stands to reason that we should draw on and integrate with existing consumer choice models in defining how simulated users behave when interacting with recommendation systems.  To aid in this effort, we will discuss a few consumer choice models, adapting them to the context of recommendation systems.\\looseness=-1\n\n\nA fundamental premise shared among consumer choice models is that consumers, or users, make choices based on the utility of different actions.  The utility $U_{nit}$ of user $n$ consuming an item $i$ (e.g., reading, watching, or purchasing it) at choice instance $t$ is typically comprised of two parts:  a deterministic function $f$ of the attributes of the item ($\\mathbf{a}_i = \\{a_{i1}, \\dots a_{iJ}\\}$ for $J$ attributes) and a random component $\\epsilon$ specific to the choice instance $t$.\nIn computing a given overall utility $U_{nit}$, the utility of a given attribute can be unique for each user, giving us \\looseness=-1\n\\begin{equation}\n    \\label{eq:general_utility}\n    U_{nit} = f(\\mathbf{a}_{i}, \\mathbf{u}_{n}) + \\epsilon_{nit},\n\\end{equation}\nwhere $u_{nj}$ is the utility weight of attribute $j$ for user $n$.  The deterministic function $f$ is usually (but not always) linear: the deterministic utility of consuming an item is a linear combination of the utility weights of each of the item's attributes, or\n\\begin{equation}\n    \\label{eq:linear_utility}\n    f(\\mathbf{a}_{i}, \\mathbf{u}_{n}) = \\sum_{j=1}^J a_{ij} u_{nj}.\n\\end{equation}\nThis linear construction of utility has parallels with many of the simulation assumptions already used in recommendation system simulations, which draw from the matrix factorization model for recommendation \\citep{koren2008factorization}.  It is also used in the seminal work of \\citet{guadagni1983logit}, which introduced the multinomial logit model of choice; under this model, the random component $\\epsilon$ is assumed to be drawn from a standard Gumbel distribution.\nAs an alternative to this multinomial logit model, the multinomial probit model assumes a different distribution of the random component of the utility $\\epsilon$; specifically, it assumes $\\epsilon_{nit}$ is drawn from a multivariate normal $\\boldsymbol{\\epsilon}_{nt} \\sim \\mathcal{N}(0, \\boldsymbol{\\Sigma})$, allowing for arbitrary correlation.  In both cases, consumers maximize their utility in expectation with the probability of choosing an item being a function of all the item alternatives available $\\mathcal{I}_t$ at a given choice instance~$t$.  In the simulation setting, we use behavior models to \\emph{generate} data rather than to describe observed behaviors, so to use this model in simulating choices we need only draw the collection of random components $\\boldsymbol{\\epsilon}$ from the appropriate distributions; each user $u$  then selects the item $i$ that maximizes their utility $U_{nit}$ for a given choice instance $t$.  However, care must be taken in choosing the magnitude of user utility weights $\\mathbf{u}_n$ relative to the random components $\\boldsymbol{\\epsilon}$;  if the deterministic component of utility is too large, then there will be no randomness in consumer behavior, but if it is too small, user behavior will be exclusively random.\n\nPersonalized recommendation systems are only used in domains where users select multiple items, meaning the models of choice we consider must accommodate multiple selection.  The linear multinomial logit and probit model just described easily integrate this constraint but allow for the same item to be selected multiple times.  In some contexts, however, this may not be a desirable property.  For these cases, we may modify the models to exclude items previously selected by the user $n$ from consideration, giving us a user-specific set item alternatives $\\mathcal{I}_{nt}$.  These two sets of assumptions represent extreme ends of a spectrum: repeated consumption of the same item could yield identical utility in expectation or zero utility after the first instance of consumption.  In many domains, the reality for consumers would be a hybrid of the two: \\emph{diminishing} utility for repeat consumption.  For simplicity, one could consider only the two extremes and argue that a diminishing utility model would perform somewhere in between the two.\\looseness=-1\n\nWhen consumers can easily consider all items, the model assumptions discussed thus far seem appropriate.  Unfortunately, recommendation systems are deployed in exactly the context in which users \\emph{cannot} consider all items.\nBecause of this context, the order in which users consider items matters, which brings us to the notion of ``satisficing'' choice~\\citep{simon1955behavioral}.  This gives us a choice model wherein a consumer picks first item to meet a minimum threshold.\nUnder a the model proposed by \\citet{stuttgen2012satisficing}, users have an ``aspiration level'' for each item attribute: this is the point at which the item would be considered satisfactory; for an item $i$ to be satisfactory overall, each attribute $j$ needs to be satisfactory.  With a large number of attributes, however, it is unrealistic for consumers to consider each attribute individually. Instead, we have user $n$ accept the first item $i$ with utility $U_{nit}$ (\\Cref{eq:general_utility}) greater than or equal to a user-specific threshold, or overall aspiration level $\\alpha_n$.  These aspiration levels may be fixed or learned by the user as a function of their experiences on the platform.  Anecdotally, this may be the best model for simulating consumer choice in the recommendation system context due to its simplicity.  However, like with determining the magnitude of utility weights $\\mathbf{u}_n$, care must be taken in choosing the aspiration thresholds $\\alpha_n$; too large thresholds yield minimal user interactions and overemphasize deterministic preferences whereas too small thresholds generate a large number of highly random user interactions.\\looseness=-1\n\nIn addition to the utilities of the items themselves, we can incorporate the cost of thinking \\citep{shugan1980cost} or search costs \\citep{stigler1961economics} into our choice models.  This leads to another family of models based on consideration sets \\citep{hauser1990evaluation, roberts1991development}. In the words of \\citet{hauser1990evaluation}, these models capture the ``trade-offs between decision costs and the incremental benefits of choosing from a larger set.''  Given a consideration set of items $\\mathcal{C}$, a consumer $n$ must choose between evaluating the items in the consideration set to select one, or searching for a new item to consider, with search cost $s_n$.  If they choose to search and discover a new item, they must determine if it is worth the decision cost $d_n$ to add the item to the consideration set.  Items may also be dropped from a consideration set if their contributions to the expected utility of the final choice do not outweigh the decision cost $d_n$ of including them in the set.  One could assume that each user $n$ has their own decision costs $d_n$ and search costs $s_n$ that are constant.  Under this model, each user $n$ starts with an empty consideration set $\\mathcal{C}_n$ and selects items using the following procedure, based on \\citet{hauser1990evaluation} for a given choice instance $t$.\\looseness=-1\n\\vspace{-2.5px}\n\\begin{itemize}\n    \\item Drop any item $i$ from the consideration set $\\mathcal{C}_n$ if its expected contribution to utility is not worth the decision cost, or $ \\mathbb{E}_t \\left[\\max_{i' \\in \\mathcal{C}_n} U_{ni't}\\right] - \\mathbb{E}_t \\left[\\max_{i' \\in \\mathcal{C}_n \\setminus \\left\\{i\\right\\}} U_{ni't}\\right] < d_n$.\n    \\item \n    If the expected marginal utility of considering a new item with the associated decision costs is less than the search cost, or $\\mathbb{E}_{t,i}\\left[\\max_{i' \\in \\mathcal{C}_n \\cup \\left\\{i\\right\\}}  U_{ni't}\\right] - \\mathbb{E}_t\\left[\\max_{i' \\in \\mathcal{C}_n}  U_{ni't}\\right] + d_n < s_n$, search for a new item $i \\in \\mathcal{I}_{nt}$ and continue.\n    Otherwise, return to the start of the procedure after choosing the highest utility item from $\\mathcal{C}_n$, $y_{nt} = \\arg \\max_{i \\in \\mathcal{C}_n} U_{nit}$, and then removing $y_{nt}$ from $\\mathcal{C}_n$.\n    \\item Add item $i$ to the consideration set $\\mathcal{C}_n$ if the expected marginal utility of adding the item is greater than the decision cost,\n    $\\mathbb{E}_{t}\\left[\\max_{i' \\in \\mathcal{C}_n \\cup \\left\\{i\\right\\}}  U_{ni't}\\right] - \\mathbb{E}_t\\left[\\max_{i' \\in \\mathcal{C}_n}  U_{ni't}\\right] > d_n$.\n\\end{itemize}\n\\vspace{-2.5px}\nIn practice, this model requires extensive computation to estimate all the user-specific expected contributions and marginal utilities.  Once these challenges are overcome, this could be a potentially powerful model for simulating user choices.\n\nIn all of the consumer choice models discussed here, we have assumed that deterministic consumer preferences are static except for small random variations $\\epsilon$; these deterministic preferences do not evolve systematically in any way.  The satisfying model includes some notion of adapting preference if we allow the aspirational thresholds $\\boldsymbol{\\alpha}$ to be learnt; the search model similarly includes evolving preferences based on expected marginal utilities, which are dynamic based on the users' experiences.  However, there is more room to explore the dynamics of evolving consumer preferences in the recommendation systems context; for example, it is established that consumers can learn their own preference weights $\\mathbf{u}_n$ as they search, complicating the process of recommendation \\citep{dzyabura2019recommending}.  These and similar complex dynamics have been explored in economics and marketing, providing recommendation system researchers a plethora of models from which to draw inspiration for their simulation assumptions regarding user choices.\n\n\n\\section{Alternatives to Recommendations}\n\\label{sec:alternarives_to_rec}\n\nWe now turn to an even broader challenge: in the real world, users interact with items through a combination of online and offline mechanisms, including, but not limited to, algorithm recommendation.  For example, a consumer may receive a book recommendation from a friend in conversation and then use a search engine to find it later.  Users will always have alternatives to using a recommendation system, including leaving the platform entirely if it does not meet their needs.\nExcluding this general concept from the simulation setting will vastly overemphasize the impact of any given recommendation system on user behavior.\nTo date, recommendation system simulations address this in simple ways, if at all.   \\looseness=-1 \n\nSome work offers no alternatives to recommendations, focusing only on generating data with bias and the corresponding implications \\citep{yao2017beyond,sun2019debiasing}.\nOthers compare different recommendation systems such as random recommendations, recommendations based on overall popularity, matrix factorization, and recommendations under ideal or oracle conditions \\citep{chaney2018algorithmic, jiang2019degenerate, aridor2020deconstructing}. Along this vein, \\citet{geschke2019triple} compares recommending close content to distant content, but also includes social dynamics in their simulations.  Even more simply, some simulations interleave random items among recommended ones \\citep{chaney2018algorithmic}.\\looseness=-1\n\nMany researchers dismiss the need for alternatives to recommended content or re-frame their questions to avoid this difficult issue.  While this is an understandable starting point, we need to move past it.   An observational study of Bing search and Amazon data estimated that at least 75\\% of Amazon product browsing activity would likely occur in the absence of recommendations \\citep{sharma2015estimating}.  While the exact amount of activity absent recommendations depends on the content and platform features, this example underscores the importance of alternate ways of accessing content.\\looseness=-1\n\nThis second challenge is connected to the first one of defining a model of user choice: users consider not only which items to select, but the sources from which they select them.  On some level, then, there are easy solutions.  For example, a satisficing model presents users with an ordered list of items; in this context, we can extend the concept of interleaving random items to probabilistically selecting items for consideration from different sources.  Items may be algorithmically recommended, but they may also be new items, items returned by search, etc.  Ideally, these probabilities would change as a function of the learned relevancy for each source, not unlike a multi-armed bandit model.  The consideration set model previously discussed can similarly draw on multiple sources with distinct search costs for each source.\nThe true difficulty is that each of these alternate mechanisms, or sources, needs a corresponding model with its own assumptions.\\looseness=-1\n\nAgain, we need standards for alternate mechanisms for accessing content in simulations, including the associated assumptions with each mechanism.  This may involve observational research to understand the breadth of how users access content and to characterize the nature of their interactions with these alternate sources.\n\n\\section{Discussion}\n\\label{sec:discussion}\n\nAs simulation methods become more popular for understanding the impact of recommendation systems, we must be critical of the assumptions we use and work toward establishing standards based on real-world consumer behaviors.  This includes clear definitions of our user choice models (\\Cref{sec:alternarives_to_rec}) and realistic mechanisms for accessing content as alternatives to algorithmic recommendations (\\Cref{sec:alternarives_to_rec}).  In considering the impacts of recommendation systems, we need to define what the comparison is, or what can realistically be changed in the world.  \nFirms cannot deploy an oracle recommendation system, though they undoubtedly wish they could.  Instead, the data that they have is biased by their existing recommendation system or platform design choices; they need to make the best choices they can within those constraints.  Our work should focus on realistic comparisons that lead to actionable results for firms.  This includes how to handle new items and users (the ``cold-start'' problem), understating the benefits and consequences of bias reduction strategies, how to best combine historic and recent data, and comparing pipeline choices such as offline evaluation and A/B testing metrics, as well as the frequency of retraining or updating the system.  \nWe also need standards on the scale of simulations; real-world platforms are large with thousands if not millions of users.  What size of simulations are needed to generate compelling results?  How does the required scale vary based on the simulation assumptions?\\looseness=-1\n\nWhile these challenges may seem daunting, they are part of a larger, enduring task.  As our society continues to integrate with technical systems like recommendation engines, we must do our best to understand the impacts of these systems through simulations and other methods so we may use these technologies wisely.\\looseness=-1\n\n\n\n\\begin{acks}\nThank you to Brandon Stewart, Ryan Dew, Sam Levy, the marketing faculty at both Chicago Booth and Duke Fuqua, and my fellow organizers of the SimuRec workshop for their discussions broadly related to this work.\n\\end{acks}\n\n\\bibliographystyle{ACM-Reference-Format}\n", "meta": {"timestamp": "2021-09-07T02:34:21", "yymm": "2109", "arxiv_id": "2109.02475", "language": "en", "url": "https://arxiv.org/abs/2109.02475"}}
{"text": "\\section{Introduction}\n\n\n\nPhytoplankton are microscopic photosynthetic aquatic organisms that are the main primary producers of many aquatic ecosystems, and play a pivotal role at the base of the food chain. However, the overabundance of phytoplankton species, or algal blooms as it is often referred to, regularly leads to adverse effects both environmentally and economically~\\cite{Huisman2018,Reynolds2006,Watson2015}. For these reasons the study of phytoplankton dynamics is important to enhance the positive effects of phytoplankton while limiting any adverse outcomes. Phytoplankton dynamics depend on inorganic materials, dissolved nutrients and light, creating energy for the entire aquatic ecosystem via photosynthesis~\\cite{Reynolds2006}. As the world becomes more industrialized anthropogenic sources of nutrients drastically increase, more often than not, eutrophication ensues. Eutrophication is defined as the excess amount of nutrients in a system required for life. Thus, in eutrophic conditions light becomes the limiting resource for phytoplankton productivity~\\cite{Paerl2013,Watson2015}. \n\nResource limitation, be it light or nutrient limitation, leads to competition amongst species. By the competitive exclusion principle, any two species competing for a single resource can not stably coexist. However, several cases exist in nature that seemingly contradict the competitive exclusion principle such as Darwin's finches, North American Warblers, Anoles, as well as phytoplankton. However, these contradictions are easily explained through niche differentiation.  The paradox of the plankton (a.k.a Hutchinson paradox) stems from ostensible contradiction between the diversity of phytoplankton typically observed in a water body and the competitive exclusion principle, since phytoplankton superficially compete for the same resources~\\cite{Hutchinson1961}.\nSeveral modelling attempts have been made to shed light on this paradox by considering spatial heterogeneity throughout the water column~\\cite{Jiang2019,Jiang2021,HsuLou2010}. For example, competitive advantage is gained by a species who has better overall access to light, whether it be realized through buoyancy regulation or increased turbulent diffusion. \n\nClassically, light has been treated as a single resource and competitive exclusion is regularly predicted by mathematical models~\\cite{Heggerud2020,Huisman1994,Wang2007,Jiang2019,Jiang2021,HsuLou2010}. However, further investigation shows that phytoplankton species can absorb and utilize wavelengths with varying efficiencies, implying non-uniform absorption spectra~\\cite{Burson2018,Luimstra2020,Holtrop2021,Stomp2007a}. A species' absorption spectrum measures the amount of light absorbed, of a specific wavelength, by the species. Figure \\ref{fig:absspectra} gives examples of absorption spectra for four different species of phytoplankton. These differences between the absorption spectra imply niche differentiation among species and can, in part, help to explain Hutchinson's paradox. \n\n\nLight limitation in aquatic systems occurs through several different mechanisms. For instance, incident light can be variable due to atmospheric attenuation, Rayleigh scattering and the solar incidence angle. All of these factors contribute to the amount of light that enters the water column. Moreover, light is attenuated by molecules and organisms as it penetrates through the vertical water column. Typically this attenuation is modelled using Lambert-Beer's law which assumes an exponential form of light absorbance by water molecules and seston (suspended organisms, minerals, compounds, gilvin, tripton and etc.). However, the amount of light attenuated is not strictly uniform with respect to wavelengths. For example, pure water absorbs green and red wavelengths more than blue, giving water its typical bluish tone whereas waters rich in gilvin, that absorb blue light, typically appear yellow. Additionally, as mentioned, phytoplankton species' absorption spectra are non-uniform across the light spectrum thus contributing to the variable light attenuation. Because absorption depends on wavelength, the available light profile can change drastically throughout the depth of the water column, giving rise to water colour and another mechanism for species persistence.\nFor these reasons, modelling of phytoplankton dynamics should explicitly consider light and its availability throughout the water column. \n\n\nSeveral attempts have been made to study phytoplankton competition and dynamics. Single species models have been well established and give good understanding of the governing dynamics of phytoplankton in general~\\cite{Heggerud2020,HsuLou2010,Du2011,Shigesada1981}. These studies include various modelling approaches including stoichiometric modelling~\\cite{Heggerud2020}, non-local reaction-diffusion equations~\\cite{HsuLou2010,Du2011} and complex limnological interactions~\\cite{Zhang2021}. Non-local reaction diffusion equations are beneficial to the study of phytoplankton population because they are capable of capturing light availability after attenuation throughout the water column,  modelling diffusion and buoyancy/sinking of phytoplankton, and there exists a myriad of mathematical tools and theories to aid in their analysis. One such mathematical theory that we utilize in this paper is the monotone dynamical systems theory popularized by Smith~\\cite{Smith}. The theory of monotone dynamical systems is a powerful tool to study the global dynamics of a complex competition system as utilized in~\\cite{Jiang2019,Jiang2021,HsuLou2010}.\n\n\nIn this paper we extend spatially explicit mathematical models for phytoplankton dynamics to consider competition amongst phytoplankton species with niche differentiation in the absorption spectrum~\\cite{Jiang2019,Jiang2021,Stomp2007}. Furthermore, the underwater light spectrum, and its attenuation, modelled by the Lambert-Beer law,  explicitly depends on the wavelengths of light. In Section \\ref{sec:model}, we propose a reaction-diffusion-advection phytoplankton competition model that non-locally depends on phytoplankton abundance and light attenuation. In Section \\ref{sec:mathresults}, we provide several preliminary results regarding the persistence of a single species via the associated linearized eigenvalue problem. In Section \\ref{sec:Mathniche}, we introduce an index to serve as a proxy for the level of niche differentiation amongst two species and provide coexistence results based on this index. In the absence of niche differentiation we establish the competitive exclusion results based on advantages gained through buoyancy or diffusion. In Section \\ref{sec:NumCoexistmechs}, we numerically explore how niche differentiation via i) specialist versus specialist competition, and ii) specialist versus generalist competition, can overcome competitive advantages that would otherwise result in competitive exclusion. We then consider the case when more than two species compete and show that upon sufficient niche differentiation any number of phytoplankton species may coexist in Section \\ref{sec:Nspecies}. Finally, we offer a realistic competition scenario where the absorption spectra of two competing species are given in Figure \\ref{fig:absspectra} and background attenuation is modelled based on water conditions ranging from clear to highly turbid. Our work offers a possible explanation of Hutchinson's paradox. That is, through sufficient niche differentiation in the light spectrum, many phytoplankton species can coexist. \n\n\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.65\\paperwidth]{absspectra.eps}\n    \\caption{Normalized absorption spectra for four phytoplankton species: green cyanobacteria (\\textit{Synechocystis} strain), red cyanobacteria (\\textit{Synechococcus} strain), green algae (\\textit{Chlorella} strain) and a diatom (\\textit{Nitzschia} strain)~\\cite{Luimstra2020,Burson2018,Stomp2007}. The differences of absorption spectra among species imply niche differentiation throughout the spectrum.}\n    \\label{fig:absspectra}\n\\end{figure}\n\n\n\n\n\\section{The model}\\label{sec:model}\nIn this section we extend a two species non-local reaction-diffusion-advection model proposed in several papers~\\cite{Jiang2019,Jiang2021,HsuLou2010,Du2010} to consider niche differentiation via absorption spectra separation. The PDE system assumes sufficient nutrient conditions so that light is the only factor limiting phytoplankton growth. However, the species are capable of utilizing incident wavelengths at varying efficiency as highlighted in Figure \\ref{fig:absspectra}~\\cite{Stomp2007,Burson2018,Luimstra2020,Holtrop2021}. Because of the attenuation of light through the vertical water column, the diffusivity of the phytoplankton and the potential for buoyancy regulation (advection) the system is spatially explicit. That is, let $x$ denote the vertical depth within the water column then $u_1(x,t)$ and $u_2(x,t)$ are the population density of competing phytoplankton species 1 and 2 at depth $x$ and time $t$. \nThe following model generalizes the one of Stomp et al. \\cite{Stomp2007} \n to the spatial context:\n\\begin{equation}\\label{eq:full}\n\\begin{cases}\n\\partial_t u_1 = D_1 \\partial^2_xu_1 - \\alpha_1 \\partial_x u_1+ [g_1(\\gamma_1(x,t)) - d_1(x)]u_1& \\text{ for } 0 < x < L,\\, t>0,\\\\\n\\partial_t u_2= D_2 \\partial^2_xu_2- \\alpha_2 \\partial_x u_2+ [g_2(\\gamma_2(x,t)) - d_2(x)]u_2& \\text{ for } 0 < x < L,\\, t>0,\\\\\nD_1\\partial_x u_1(x,t) - \\alpha_1 u_1(x,t) = D_2\\partial_x u_2(x,t) - \\alpha_2 u_2(x,t)=0 & \\text{ for }x = 0, L,\\, t>0,\\\\\nu_1(x,0) = u_{1,0}(x),\\, u_2(x,0) =u_{2,0}(x)&\\text{ for }0 < x < L.\n\\end{cases}\n\\end{equation}\nIn this paper,\nthe turbulent diffusion coefficients $D_1,D_2>0$ and sinking/buoyancy coefficients $\\alpha_1,\\alpha_2 \\in \\mathbb{R}$ are assumed to be constants; the functions $d_1(x),\\,d_2(x)\\in C([0,L] )$ are the death rate of the species at depth $x$; the function \n$\\gamma_1(x,t)$ is the number of absorbed photons available for photosynthesis by species $1$ and is given by\n\\begin{equation}\\label{eq:gamma1}\n\\gamma_1(x,t) = \\int_{400}^{700}a_1(\\lambda) k_1(\\lambda) I(\\lambda,x)\\,d\\lambda,\n\\end{equation}\nwhere $k_1(\\lambda)$ and $k_2(\\lambda)$ are the absorption spectra of species $1$ and $2$, respectively. The absorption spectrum is the proportion of incident photons of a given wavelength absorbed by the cell. The respective quantity $\\gamma_2(x,t)$ for species $2$ is similarly defined. \nFor each given wavelength $\\lambda$, the quantities\n$a_1(\\lambda)$ (resp. $a_2(\\lambda)$) converts the absorption spectrum of species $1$, (and $u_2$) into the action spectrum, or the proportion of absorbed photons used for photosynthesis, of phytoplankton species $1$ (resp. $2$). In many cases photons are absorbed and utilized with similar efficiency, thus we take $a_1(\\lambda) = a_2(\\lambda)=1$.  Sunlight enters the water column with an incident light spectrum $I_{\\rm in}(\\lambda)$ and $I(\\lambda,x,t)$ is the light intensity of wavelength $\\lambda$ at depth $x$ which, according to the Lambert-Beer's law, given by\n\\begin{equation}\n  I(\\lambda,x,t) = I_{\\rm in}(\\lambda) \\exp\\left[- K_{BG}(\\lambda)x - k_1(\\lambda)\\int_0^x u_1(y,t)-k_2(\\lambda)\\int_0^x u_2(y,t) \\right], \n\\end{equation}\nwhere $K_{BG}(\\lambda)$ is the background attenuation of the incident light spectrum. \nWe also assume that the specific growth rates $g_1(s)$ and $g_2(s)$ of both phytoplankton species are increasing and saturating functions of the number of absorbed photons available for photosynthesis, i.e. \n\\begin{equation}\\label{eq:ggg}\ng_i(0) = 0,\\quad g'_i(s) >0 \\quad \\text{ for }s \\geq 0, \\quad g_i(+\\infty)<+\\infty \\quad \\text{ for }i=1,2. \n\\end{equation}\nA common choice of growth function is the Monod equation given by \n\\begin{equation}\n    g_i(s) = \\frac{\\bar{g}_i s}{\\overline\\gamma_i + s}, \\quad \\text{ i=1,2},\n\\end{equation}where $\\bar{g}_{i}$ is the maximal growth rate of species $i$ and $\\overline\\gamma_i$ is the half-saturation coefficient.  \nLastly, we assume there is no net movement across the upper and lower boundaries of the water column, resulting in the zero-flux boundary conditions for $x=0, L$. \n\n\\section{Preliminary results}\\label{sec:mathresults}\nIn this section we establish several preliminary theorems for coexistence and competitive exclusion that are used throughout the paper. From the eigenvalue we establish conditions for a single species to persist in absence of a competitor. From this we are able to use the associated linearized eigenvalue problem to establish a sufficient condition for coexistence and competitive exclusion. \n\nThroughout the paper we refer the readers to the following definition and condition.\n    Define the functions $f_i:[0,L] \\times[0,\\infty)\\times [0,\\infty)  \\to \\mathbb{R}$ by:\n\\begin{equation}\\label{eq:f}\nf_i(x,p_1,p_2) = g_i\\left(\\int_{400}^{700} a_i(\\lambda) k_i(\\lambda) I_{\\rm in}(\\lambda) \\exp\\bigg[ -K_{BG}(\\lambda)x - \\sum_{j=1}^2 k_j(\\lambda) p_j\\bigg]\\right)  -d_i(x).\n\\end{equation} \nThen it is not hard to verify that, for $i=1,2$, the function $f_i$ satisfies\n\\begin{description}\n\\item[(H)] \\quad $\\displaystyle \\frac{\\partial f_i}{\\partial p_j}<0$ \\quad and\\quad $\\displaystyle \\frac{\\partial f_i}{\\partial x}<0$ \\quad for $(x,p_1,p_2) \\in [0,L]\\times \\mathbb{R}_+^2$, \\,$j=1,2$.\n\\end{description}\n Although we only consider the autonomous system here, we remark that most of the theoretical results can be generalized to the case of a temporally periodic environment.\n\n\\subsection{Persistence of a single species}\\label{sec:SingleSpecies}\n\n\nIn this subsection we characterize the long-term dynamics of system \\eqref{eq:full} in the absence of competition, i.e when  $u_{1,0} \\equiv 0$ or $u_{2,0} \\equiv 0$. We begin by defining the following eigenvalue problem.\n\n\n\\begin{definition}\\label{def:eval}\nFor given constants $D>0$ and $\\alpha \\in \\mathbb{R}$, and given function $h(x) \\in C([0,L])$, define $\\mu(D,\\alpha,h) \\in \\mathbb{R}$ to be the smallest eigenvalue of the following boundary value problem:\n\\begin{equation}\\label{eq:simpleeigenvalueproblem}\n\\begin{cases}\nD \\partial_{xx} \\phi - \\alpha \\partial_x \\phi + h(x)\\phi  +\\mu \\phi =0 &\\text{ for }(x,t) \\in [0,L]\\times\\mathbb{R^+},\\\\\nD\\partial_x \\phi - \\alpha \\phi = 0&\\text{ for }(x,t) \\in \\{0,L\\}\\times \\mathbb{R^+}.\n\\end{cases}    \n\\end{equation}\n\\end{definition}\n\n \n\n The eigenvalue problem given in Definition \\ref{def:eval} is well associated to the system \\eqref{eq:full} linearized around $E_1$ or $E_2$. The main result of this section is given below and provides a condition for the existence and attractiveness of the semi-trivial solutions $E_1$ and $E_2$.\n\\begin{proposition}\\label{prop:semitrivial}\nSuppose \n\\begin{description}\n\\item[(P)] $\\mu(D_i,\\alpha_i,f_i(x,0,0)) <0$ for $i=1,2$.\n\\end{description}\nThen the system \\eqref{eq:full} has exactly two non-negative exclusion equilibria $E_1=(\\tilde{u}_1,0)$ and $E_2=(0,\\tilde{u}_2)$. \nMoreover, \n$E_1$ (resp. $E_2$) attracts all solutions of \\eqref{eq:full} with initial condition $(u_{1,0},u_{2,0})$ such that \n$$\nu_{1,0} \\geq,\\not\\equiv 0\\,\\,\\text{ and }\\,\\,u_{2,0}\\equiv 0 \\quad  {\\rm(resp. }\\quad \nu_{1,0} \\equiv 0\\,\\,\\text{ and }\\,\\,u_{2,0}\\geq,\\not\\equiv 0).$$\n\\end{proposition}\n\\begin{proof}\nSee~\\cite[Proposition 3.11]{Jiang2019}.\n\\end{proof}\n\n The following corollary gives an explicit condition for  {\\bf(P)}.\n\\begin{corollary}\\label{corr:Pholds}\nLet $f_i$ be defined in \\eqref{eq:f}. If \n\\begin{equation}\\label{eq:persist1}\n \\int_0^L e^{\\alpha_i x/D_i} f_i(x,0,0)\\,dx >  0 \\quad \\text{ for }i=1,2,\n\\end{equation}\nthen {\\bf(P)} holds and the conclusions of Proposition \\ref{prop:semitrivial} concerning the existence and attractivity of semi-trivial solutions $E_1$ and $E_2$ hold.  \n\\end{corollary}\n\\begin{proof}\nThanks to Lemma \\ref{lem:eigen} in the Appendix, \\eqref{eq:persist1} implies {\\rm{\\bf(P)}}. The conclusion thus follows from Proposition \\ref{prop:semitrivial}.\n\\end{proof}\n\n\nIn terms of the physical parameters, \\eqref{eq:persist1} reads\n\\begin{equation}\\label{eq:persist1'}   \n\\int_0^L e^{\\alpha_i x/D_i}  g_i\\left(\\int_{400}^{700} a_i(\\lambda) k_i(\\lambda) I_{\\rm in}(\\lambda) e^{ -K_{BG}(\\lambda)x}\\right)\\,dx >\\int_0^L e^{\\alpha_i x/D_i}   d_i(x) \\,dx,\n\\end{equation}\ngiving an explicit condition for the existence and attractivity of the exclusion equilibrium $E_1$ and $E_2$.  \n\n\n\n\n \n\n\\subsection{Coexistence in two species competition}\\label{sec:twospecies}\n\nWe now consider the outcomes of a two species competition and establish sufficient conditions for coexistence. We begin by connecting the system \\eqref{eq:full} to the general theory of monotone dynamical systems~\\cite{Smith}. For this purpose, consider the cone $\\mathcal{K} = \\mathcal{K}_1 \\times (- \\mathcal{K}_1)$, where\n\\begin{equation}\n    \\mathcal{K}_1 = \\left\\{ \\phi \\in C([0,L])\\,:\\, \\int_0^x \\phi(y)\\,dy \\geq 0 \\quad \\text{ for all }x \\in [0,L] \\right\\}.\n\\end{equation}\nThe cone $\\mathcal{K}$ has non-empty interior, i.e. ${\\rm Int}\\,\\mathcal{K}= ({\\rm Int}\\,\\mathcal{K}_1)\\times (-{\\rm Int}\\,\\mathcal{K}_1)$, where\n\\begin{equation}\\label{eq:interior}\n   \\rm Int\\, \\mathcal{K}_1 = \\left\\{ \\phi \\in C([0,L])\\,:\\,\\phi(0)>0\\, \\int_0^x \\phi(y)\\,dy > 0 \\quad \\text{ for all }x \\in [0,L] \\right\\}.\n\\end{equation}\nFor $i=1,2$,  let $(u_i(x,t),v_i(x,t))$ be two sets of solutions of \\eqref{eq:full} with initial conditions $(u_{i,0}(x),v_{i,0}(x))$. Since $f_1$ and $f_2$ satisfy {\\bf(H)}, it follows by~\\cite[Corollary 3.4]{Jiang2019} that \n$$\n(u_{2,0} - u_{1,0}, v_{2,0}-v_{1,0}) \\in \\mathcal{K} \\setminus\\{(0,0)\\} \\quad \\Rightarrow \\quad  (u_{2,0} - u_{1,0}, v_{2,0}-v_{1,0})(\\cdot,t) \\in {\\rm Int}\\,K \\quad \\forall t>0.\n$$\nIn other words, the system \\eqref{eq:full} generates a semiflow that is strongly monotone with respect to the cone $\\mathcal{K}$. It follows from the property of monotone dynamical systems that the long-time dynamics of the system \\eqref{eq:full} can largely be determined by the local stability of the equilibria.\n\n\n\n\n\nWe now characterize the local stability of $E_1$. \n\\begin{proposition}[{~\\cite[Proposition 4.5]{Jiang2019}}] \\label{prop:4.5}\nSuppose the parameters are chosen such that {\\bf(P)} holds, i.e. the two species system has two exclusion equilibria $E_1=(\\tilde{u}_1,0)$ and $E_2 = (0,\\tilde{u}_2)$. \n\\begin{itemize}\n    \\item[{\\rm(a)}]\n    The equilibria $E_1$ is linearly stable (resp. linearly unstable) if $\\mu_u >0$ (resp. $\\mu_u <0$), where\n\\begin{equation}\\label{eq:muu}\n\\mu_u:=\\mu(D_2,\\alpha_2,f_2(x,\\int_0^x\\tilde{u}_1(y)\\,dy,0)).\n\\end{equation}\n    \\item[{\\rm(b)}]     The equilibria $E_2$ is linearly stable (resp. linearly unstable) if $\\mu_v >0$ (resp. $\\mu_v <0$), where\n\\begin{equation}\\label{eq:muv}\n\\mu_v:=\\mu(D_1,\\alpha_1,f_1(x,\\int_0^x\\tilde{u}_2(y)\\,dy,0)).\n\\end{equation}\n\\end{itemize}\n\n\\end{proposition}\n\\begin{proof}\nWe only prove assertion (a), since assertion (b) follows by a similar argument.\nTo determine the local stability of the exclusion equilibrium $E_1$, we consider\nthe associated linearized eigenvalue problem at $E_1=(\\tilde{u}_1,0)$, which is given by\n\\begin{equation} \\label{eq:lineigproblem\n    \\begin{cases}\n     D_1 \\phi_{xx} - \\alpha_1 \\phi_x + f_1(x,\\int_0^x\\tilde{u}_1(y)\\,dy,0) \\phi &\\\\ \n     \\quad - \\tilde{u}_1 g_1'(\\gamma_1) \n     [A_{11}(x) \\int_0^x \\phi(y)\\,dy + A_{12}(x) \\int_0^x \\psi(y)\\,dy] + \\mu \\phi = 0 &\\text{ in }[0,L],\\\\\n    D_2 \\psi_{xx} - \\alpha_2 \\psi_x + f_2(x,\\int_0^x\\tilde{u}_1(y)\\,dy,0)\\psi + \\mu \\psi = 0 &\\text{ in }[0,L],\\\\\n    D_1\\phi_x - \\alpha_1 \\phi = D_2 \\psi_x - \\alpha_2 \\psi = 0 & \\text{ for }x = 0,L.\n    \\end{cases}\n\\end{equation}\nwhere (recall that we have taken $a_i\\equiv 1$) \n\\begin{equation}\nA_{ij}(x) = \\int_{400}^{700} k_i(\\lambda)I(\\lambda,x)k_j(\\lambda)\\,d\\lambda\n\\end{equation}\nand \n\\begin{equation}\n\\gamma_i(x)=\\int_{400}^{700} k_i(\\lambda)I_{in}(\\lambda) \\exp \\left[ - K_{BG}(\\lambda)x - k_1(\\lambda) \\int_0^x \\tilde{u}_1(y)\\,dy \\right]\\,d\\lambda.\n\\end{equation}\nThanks to the monotonicity of the associated semiflow,\nthe linearized problem \\eqref{eq:lineigproblem} has a principal eigenvalue in the sense that $\\mu_1 \\leq \\textup{Re}\\,\\mu$ for all eigenvalues $\\mu$ of \\eqref{eq:lineigproblem}, and that the corresponding eigenfunction can be chosen in $\\mathcal{K} \\setminus\\{(0,0)\\}$. In particular, $E_1$ is linearly stable (resp. linearly unstable) if $\\mu_1 >0$ (resp. $\\mu_1 <0$). \n\nNext, we apply~\\cite[Proposition 4.5]{Jiang2019}, which says that \n$$\n{\\rm sgn}\\, \\mu_1 = {\\rm sgn}\\, \\mu_u, \n$$\nwhere $\\mu_u$, given in \\eqref{eq:muu}, is the principal eigenvalue of the second equation in \\eqref{eq:lineigproblem}. \nHence, $E_1$ is linearly stable (resp. linearly unstable) if $\\mu_1 >0$ (resp. $\\mu_1 <0$). \n\\end{proof}\n\n\n\n\n\nIf both $E_1$ and $E_2$ exist we can conclude the existence of a positive equilibrium solution by the following proposition. \n\\begin{proposition}\nAssume {\\bf(P)}, so that both semi-trivial equilibria $E_1$ and $E_2$ exist. Suppose further that\n$$\n\\mu_u \\cdot \\mu_v >0,\n$$\nthen \\eqref{eq:full} has at least one positive equilibrium $(\\hat{u}_1,\\hat{u}_2)$. \n\\end{proposition}\n\\begin{proof}\nIf $\\mu_u \\cdot \\mu_v >0$, then the exclusion equilibria $E_1$ and $E_2$ are either both linearly stable or both linearly unstable. The existence of positive equilibrium thus follows from~\\cite[Remark 33.2 and Theorem 35.1]{Hess}.\n\\end{proof}\nIn case both $E_1$ and $E_2$ are linearly unstable, both species persist in a robust manner. \n\\begin{proposition}\\label{prop:suffcoex}\nAssume {\\bf(P)} so that the semi-trivial equilibria $E_1,E_2$ exist. Suppose \n\\begin{equation}\\label{eq:suffcoexist}\n\\mu_u <0 \\quad \\text{ and }\\quad \\mu_v <0,\n\\end{equation}\n(i.e. both $E_1$ and $E_2$ are unstable) then the following holds.\n\\begin{itemize}\n    \\item[{\\rm(i)}] There exists $\\delta_0>0$ that is independent of the initial data such that\n    $$\n    \\liminf_{t \\to \\infty} \\min_{i=1,2} \\int_{0<x<L} u_i(x,t) \\geq \\delta_0;\n    $$\n    \\item[{\\rm(ii)}] System \\eqref{eq:full} has at least one coexistence, equilibrium $(\\hat{u}_1,\\hat{u}_2)$ that is locally asymptotically stable.\n\\end{itemize}\n\\end{proposition}\n\\begin{proof}\nBy \\eqref{eq:suffcoexist}, both exclusion equilibria $E_1, E_2$ are linearly unstable. The result follows from~\\cite[Theorems 33.3]{Hess}.\n\\end{proof}\n\nThe signs of the principal eigenvalue $\\mu_u$ and $\\mu_v$ are often difficult to determine. We now establish an explicit condition for coexistence. To this end, observe from Corollary \\ref{corr:Pholds}  and \\eqref{eq:persist1'} that a sufficient condition for\n\\begin{equation}\n    \\mu_v= \\mu(D_1,\\alpha_1,f_1(x,0,\\int_0^x \\tilde{u}_{2}(y,t)\\,dy))<0.\n\\end{equation}\nis given by\n\\begin{multline}\\label{eq:suffcoexist'}   \n \\int_0^L e^{\\alpha_1 x/D_1}  g_1\\left(\\int_{400}^{700} a_1(\\lambda) k_1(\\lambda) I_{\\rm in}(\\lambda) e^{ -K_{BG}(\\lambda)x- k_2(\\lambda) \\int_0^x \\tilde{u}_2(y,t)\\,dy}\\right)\\,dx \n \\\\>\\int_0^L e^{\\alpha_1 x/D_1}   d_1(x,t) \\,dx.\n\\end{multline}\n\n\nFor $i=1,2$, we will obtain an explicit upper bound for $\\int_0^x \\tilde{u}_i(y)\\,dy$. To this end, define \n$$\nM_i:= \\inf\\left\\{M >0:\\,\\, \\int_0^xf_i(y,0, M\\int_0^ye^{-\\alpha_i z/D_i}\\,dz)e^{-\\alpha_i y/D_i}\\,dy \\leq 0 \\text{ in }[0,L]\\times[0,T]\\right\\}.  \n$$\n\\begin{lemma}\\label{lem:upperboundsM2}\nFor $i=1,2$, \n$$\n\\int_0^x\\tilde{u}_i(y,t)\\,dy \\leq \\frac{M_i D_i}{\\alpha_i} (1-e^{-\\alpha_i x/D_i}) \n\\quad \\text{ for all }(x,t) \\in [0,L]\\times[0,T]. \n$$\n\\end{lemma}\n\\begin{proof}\nIndeed, with such a choice of $M_i$, the function $M_ie^{-\\alpha_i x/ D_i}$ will then qualify as an super-solution for the single species equation for species $i$, in the sense of~\\cite[Subsection 3.2]{Jiang2019}. Hence, by comparison, we have\n$$\nM_i e^{-\\alpha_i x/D_1} - \\tilde{u}_i \\in \\mathcal{K}_1, \n$$\nthat is, \n$$\n\\int_0^x \\tilde{u}_i(y,t)\\,dy \\leq \\int_0^x M_ie^{-\\alpha_i y/ D_i}\\,dy = \\frac{M_i D_i}{\\alpha_i} (1-e^{-\\alpha_i x/D_i}) \\quad \\text{ for }x \\in [0,L].\n$$\nThis completes the proof.\n\\end{proof}\nBy the above discussion, a sufficient condition for \\eqref{eq:suffcoexist'} is\n\\begin{multline}\n    \\int_0^L e^{\\alpha_1 x/D_1}  g_1\\left(\\int_{400}^{700} a_1(\\lambda) k_1(\\lambda) I_{\\rm in}(\\lambda,t) e^{ -K_{BG}(\\lambda)x- k_2(\\lambda)  \\frac{M_2 D_2}{\\alpha_2} (1-e^{-\\alpha_2 x/D_2})}\\right)\\,dx \\\\\n \\qquad > \\int_0^L e^{\\alpha_1 x/D_1}   d_1(x) \\,dx. \\label{eq:suffcoexist''}  \n\\end{multline} \n\nFurthermore, an upper bound, $M_1$, for $\\int_0^x\\tilde{u}_1(y,t)\\,dy$ is easily established following the arguments in Lemma \\ref{lem:upperboundsM2}. Thus, a sufficient condition for \\eqref{eq:suffcoexist} is given by \\eqref{eq:suffcoexist''} and \n\\begin{multline}\n\\int_0^L e^{\\alpha_2 x/D_2}  g_2\\left(\\int_{400}^{700} a_2(\\lambda) k_2(\\lambda) I_{\\rm in}(\\lambda) e^{ -K_{BG}(\\lambda)x- k_1(\\lambda)  \\frac{M_1 D_1}{\\alpha_1} (1-e^{-\\alpha_1 x/D_1})}\\right)\\,dx \\\\\n\\qquad > \\int_0^L e^{\\alpha_2 x/D_2}   d_2(x) \\,dx. \\label{eq:suffcoexistu}  \n \\end{multline} \nThis yields an explicit sufficient condition for coexistence. \n\n\n\n\n\\section{Extreme cases of niche differentiation: competitive outcomes}\\label{sec:Mathniche}\n\nIn this section, we explicitly consider niche differentiation via the absorption spectra, $k_1(\\lambda)$ and $k_2(\\lambda)$. We consider the extreme cases of differentiation, where the niches either completely overlap, or do not overlap at all. Sufficient conditions for exclusion or coexistence are given.\n\nWe establish the following definition to serve as a proxy for niche differentiation.\n\n\\begin{definition}\n\n\\begin{equation}\\label{def:I_S}\n    \\mathcal{I}_S(k_1,k_2)= \\frac{\\|k_1-k_2\\|_{L^1}}{\\|k_1\\|_{L^1} + \\|k_2\\|_{L^1}}. \n\\end{equation}\n\n\\end{definition}\nWe refer to $\\mathcal{I}_S(k_1,k_2)$ as the index of spectrum differentiation among two species. If the two species have the same absorption spectra then $\\mathcal{I}_S(k_1,k_2)=0$ whereas if their absorption spectra are completely non-overlapping then $\\mathcal{I}_S(k_1,k_2)=1$. \n\n\n\n\n\\subsection{Coexistence for disjoint niches}\\label{subsect:exclude}\n Consider the case where the absorption spectra are completely non-overlapping, so that\ncompetition for light is at the extreme minimum. Namely,  $$\\mathcal{I}_S(k_1(\\lambda),k_2(\\lambda))=1.$$ \nWe give a coexistence result.\n\\begin{corollary}\\label{lem:disjoint}\nSuppose {\\bf(P)} holds, so that the exclusion equilibria $E_1$ and $E_2$ exist. If, in addition, $\\mathcal{I}_S(k_1(\\lambda),k_2(\\lambda))=1$, then the coexistence results of Proposition \\ref{prop:suffcoex} hold.\n\\end{corollary}\n\\begin{proof}\nFirst note that $\\mathcal{I}_S(k_1(\\lambda),k_2(\\lambda))=1$ is equivalent to $k_1(\\lambda)k_2(\\lambda)=0$ for each $\\lambda$. \nIt suffices to observe that \n$$\nf_2(x,\\int_0^x \\tilde{u}_1(y)\\,dy,0) = f_2(x,0,0),\\quad \\text{ and }\\quad \nf_1(x,0,\\int_0^x \\tilde{u}_2(y)\\,dy) = f_1(x,0,0)\n$$\nso that {\\bf(P)} implies $\\mu_u <0$ and $\\mu_v<0$. The rest follows from Proposition \\ref{prop:suffcoex}.\n\\end{proof}\n\n\n\\subsection{Competitive exclusion for identical niches}\\label{sec:sameniche}\nNext, we consider the case where the absorption spectra overlap completely ($\\mathcal{I}_S(k_1,k_2)=0$) to consider maximum competition for light. Recall our assumption that $a_1(\\lambda)=a_2(\\lambda)=1$. Thus, Under these assumptions we establish the competitive exclusion scenarios in the following theorems. \n\n\n\\begin{theorem}{~\\cite[Theorem 2.2]{Jiang2019}}\nAssume $\\mathcal{I}_S(k_1,k_2)=0$. Let $D_1 = D_2$, $\\alpha_1 < \\alpha_2$, $f_1 = f_2$, $d_1=d_2$. If ${\\bf (P)}$ holds $($i.e. both $E_1,E_2$ exist$)$, then species $1$ drives the second species to extinction, regardless of initial condition.\n\\label{thm:4.3}\n\\end{theorem}\n\\begin{proof}\nBy the theory of monotone dynamical systems (see, e.g.~\\cite[Theorem B]{Hsu1996competitive} and~\\cite[Theorem 1.3]{Lam2016remark}), it suffices to establish the linear instability of the exclusion equilibria $E_2$, and the non-existence of positive equilibria.\n\n\\noindent {\\bf Step 1.} We claim that $\\mu_v <0$, i.e. $E_2=(0,\\tilde{u}_2)$ is linearly unstable.\n\nRecall that $\\tilde{u}_2$ is the unique positive solution to\n$$\n\\begin{cases}\nD_2 \\tilde u_{xx} - \\alpha_2 \\tilde u_x + f_2(x,0, \\int_0^x \\tilde u(y)\\,dy) \\tilde u = 0 &\\text{ in }[0,L],\\\\\nD_2 \\tilde{u}_x - \\alpha_2 \\tilde u = 0 &\\text{ for }x=0,L,\n\\end{cases}\n$$\nwhere $f_2$ is given in \\eqref{eq:f} and satisfies {\\bf(H)}. Since $\\tilde{u}_2$ can be regarded as a positive eigenfunction, we deduce that $\\mu(D_2,\\alpha_2,f_2(x,0,\\int_0^x \\tilde{u}_2(y)\\,dy)) =0$. \n\nSince $D_1=D_2$, $\\alpha_1<\\alpha_2$ and $f_1=f_2$, we may apply Lemma \\ref{lem:4.3}(a), found in the Appendix, to get\n$$\n\\mu_v=\\mu(D_1,\\alpha_1,f_1(x,0,\\int_0^x \\tilde{u}_2(y)\\,dy)) <\\mu(D_2,\\alpha_2,f_2(x,0,\\int_0^x \\tilde{u}_2(y)\\,dy)) = 0. \n$$\nThus $E_2$ is linearly unstable.\n\n\\noindent {\\bf Step 2.} The system \\eqref{eq:full} has no positive equilibrium.\n\nSuppose to the contrary that $(u_1^*, v^2_*)$ is a positive equilibrium, then deduce that\n$$\n\\mu(D_i,\\alpha_i,f_i(x,\\int_0^x u^*_1(y)\\,dy, \\int_0^x u^*_2(y)\\,dy))=0 \\quad \\text{ for }i=1,2,\n$$\nwhere the respective eigenfunctions are given by $u^*_i>0$. However, this is in contradiction with Lemma \\ref{lem:4.3}(a).\n\\end{proof}\n\n\\begin{theorem}{~\\cite[Theorem 2.3]{Jiang2019}}\nAssume $\\mathcal{I}_S(k_1,k_2)=0$. Let $D_1 < D_2$, $\\alpha_1 = \\alpha_2 \\geq [f_1(L,0,0) - d_1]L$, $f_1 = f_2$, $d_1=d_2$. If ${\\bf (P)}$ holds $($i.e. both $E_1,E_2$ exist$)$, then the faster species , species $2$ drives the slower species, species $1$, to extinction, regardless of initial condition.\n\\label{thm:4.4}\n\\end{theorem}\n\\begin{proof}\nDenote $\\alpha = \\alpha_1=\\alpha_2$ and $f=f_1=f_2$.\nBy the theory of monotone dynamical systems (see, e.g.~\\cite[Theorem B]{Hsu1996competitive} and~\\cite[Theorem 1.3]{Lam2016remark}), it suffices to establish the linear instability of the exclusion equilibria $E_2$, and the non-existence of positive equilibria.\n\n\\noindent {\\bf Step 1.} We claim that $\\mu_u <0$, i.e. $E_1=(\\tilde{u}_1)$ is linearly unstable.\n\nRecall that $\\tilde{u}_1$ is the unique positive solution to\n$$\n\\begin{cases}\nD_1 \\tilde u_{xx} - \\alpha \\tilde u_x + f(x,0, \\int_0^x \\tilde u(y)\\,dy) \\tilde u = 0 &\\text{ in }[0,L],\\\\\nD_1 \\tilde{u}_x - \\alpha \\tilde u = 0 &\\text{ for }x=0,L,\n\\end{cases}\n$$\nwhere $f=f_1=f_2$ is given in \\eqref{eq:f} and satisfies {\\bf(H)}. Since $\\tilde{u}_1$ can be regarded as a positive eigenfunction, we deduce that $\\mu(D_1,\\alpha,f(x,0,\\int_0^x \\tilde{u}_1(y)\\,dy)) =0$. \n\nNext, we claim that \n\\begin{equation}\n\\mu_u=\\mu(D_2,\\alpha,f(x,0,\\int_0^x \\tilde{u}_1(y)\\,dy)) < 0. \n\\end{equation}\nSuppose to the contrary that $H(D_2) \\geq 0$, where\n$$\nH(D) := \\mu(D,\\alpha_1,f_1(x,0,\\int_0^x \\tilde{u}_2(y)\\,dy)).\n$$\nSince $D_1<D_2$, $\\alpha \\geq [f(0,0,0)]L$, we have $H(D_1)=0$ and $H(D_2)\\geq 0$. By Lemma \\ref{lem:4.3}(c) (found in the Appendix), $H'(D_1)<0$, so that there exists $D_3 \\in (D_1,D_2]$ such that $H(D_3)=0$ and $H'(D_3) \\geq 0$. But this is impossible in view of Lemma \\ref{lem:4.3}(c).\nThus $E_1$ is linearly unstable.\n\n\\noindent {\\bf Step 2.} The system \\eqref{eq:full} has no positive equilibrium.\n\nSuppose to the contrary that $(u_1^*, u^*_2)$ is a positive equilibrium, then deduce that\n$$\n\\mu(D_i,\\alpha,f(x,\\int_0^x u^*_1(y)\\,dy, \\int_0^x u^*_2(y)\\,dy))=0 \\quad \\text{ for }i=1,2,\n$$\nwhere the respective eigenfunctions are given by $u^*_i>0$. However, we can argue as in Step 1 that this is in contradiction with Lemma \\ref{lem:4.3}(c).\n\\end{proof}\n\\begin{theorem}{~\\cite[Theorem 2.4]{Jiang2019}}\nAssume $\\mathcal{I}_S(k_1,k_2)=0$. Let $D_1 < D_2$, $\\alpha_1 = \\alpha_2 \\geq 0$, $f_1 = f_2$, $d_1=d_2$. If ${\\bf (P)}$ holds $($i.e. both $E_1,E_2$ exist$)$, then the slower species, species $1$ drives the faster species, species $2$, to extinction, regardless of initial condition.\n\\label{thm:4.5}\n\\end{theorem}\n\\begin{proof}\nThe proof is the same as Theorem \\ref{thm:4.3}, found in the Appendix, where we use Lemma \\ref{lem:4.3}(b) instead of Lemma \\ref{lem:4.3}(a).\n\\end{proof}\nNote that $\\mathcal{I}_S(k_1,k_2)=0$ is equivalent to $k_1(\\lambda)=k_2(\\lambda)$ for all $\\lambda$. \nThe above theorems can be summarized into a single sentence: Suppose both species consume light in the same efficiency, the species that remains at, or moves towards the water's surface at a higher rate will exclude the other species. That is, if both species are sinking either the one sinking slower, or with higher diffusion will exclude. If both species are buoyant then the less buoyant species or the more diffusive species will be excluded.  \n\n\n\n\\section{Numerical investigation of niche differentiation}\\label{sec:NumCoexistmechs} \nTo complement the theorems established in Sections \\ref{sec:mathresults} and \\ref{sec:Mathniche} we present several numerical simulations that show the relatively large regions in parameter space that allow for coexistence. We numerically explore two main competition scenarios: 1) Niche differentiation through specialization of different wavelengths and 2) niche differentiation through specialist and generalist (with respect to light) competition. In each scenario we consider the intermediate levels of niche differentiation evaluated by $\\mathcal{I}_S(k_1,k_2)$.\n\nThe main results of this section can be summarized by the following key points: \n\\begin{itemize}\n    \\item[P1:] Competitive advantage is given to the species whose absorption spectrum overlaps the most with the available incident light. However, significant niche differentiation can promote coexistence for scenarios where incident light does not strongly favour a single species. \n    \\item[P2:] Competitive exclusion through an advection advantage can be overcome by niche differentiation. \n    \\item[P3] Intermediate values of specialization will promote coexistence. Otherwise, the specialist is excluded if its niche is too narrow, or excludes if its niche overlaps with the incident light significantly.  \n\\end{itemize}\n\n\\subsection{Competition outcomes for specialization on separate parts of the light spectrum}\\label{sec:specvsspec}\nHere we assume that the two species with relatively narrow niches are competing for light. We numerically show that through niche differentiation a species can resist competitive exclusion. These results imply that without the assumption of $\\mathcal{I}_S(k_1,k_2)=0$ the theorems in Section \\ref{subsect:exclude} do not hold and that when species' absorption spectra do not significantly overlap, coexistence is readily observed. \n\nTo investigate the extent of which niche differentiation promotes coexistence we consider two scenarios. First, we let $k_1(\\lambda)$ and $k_2(\\lambda)$ be unimodal functions that are horizontal translations of each other. That is, let $g^*(\\lambda)$ be a truncated Gaussian distribution on (-75,75) with mean zero and variance $\\sigma$. Then $k_i(\\lambda)=g^*(\\lambda-\\lambda_{i,0})$ where $\\lambda_{i,0}\\in[475,625]$ is the location of peak absorbance in the visible light spectrum. This ensures $k_1(\\lambda)$ and $k_2(\\lambda)$ have the same $L^1$ norm and are identical in their degree of specialization, giving no advantage through the absorption spectra alone.  We then allow the location of peaks of $k_2(\\lambda)$ to vary along the light spectrum ($\\lambda_{2,0}\\in[475,625]$) while keeping $k_1(\\lambda)$ fixed $\\lambda_{1,0}=475$. By varying the location of the peak of $k_2(\\lambda)$ we in-turn vary  $\\mathcal{I}_S(k_1,k_2)$. Examples of this are shown graphically with the blue curves in Figure \\ref{fig:explainspecvspec}. We also assume that the incident light $I_{in}(\\lambda)$ is a unimodal function with the location of peak incidence at $\\lambda=\\lambda_{I}$. To understand the implications incident light has on coexistence we vary $\\lambda_I$ in the range $[450,650]$). Two example curves for $I_{in}(\\lambda)$ are shown in orange in Figure \\ref{fig:explainspecvspec}. \n\nSecond, we alter $\\mathcal{I}_S(k_1,k_2)$ as above but with a uniform incident light function and allow a competitive advantage through advection by altering the advection rate $\\alpha_2$ of species $2$. Recall that $u_1$ has competitive advantage when $\\alpha_1<\\alpha_2$, and species $2$ has competitive advantage when $\\alpha_1>\\alpha_2$ (see Theorem \\ref{thm:4.3}).\n\nBy varying $\\mathcal{I}_S(k_1,k_2)$ we can then explore the competitive outcomes for various scenarios where exclusion is known to occur when niche differentiation is not considered. Furthermore, we show that the incident light function $I_{in}(\\lambda)$, together with the absorption spectra $k_1(\\lambda),k_2(\\lambda)$, play important roles in the competition outcome by allowing competitive advantages to be overcome, or diminished. Our results of this section are shown in Figure \\ref{fig:specvsSpec} and \\ref{fig:specvsspecAdvdist}.\n\\begin{figure}\n\\centering\n\\begin{subfigure}[b]{0.49\\textwidth}\n     \\centering\n    \\includegraphics[width=\\textwidth]{specvsspecIin.eps}\n    \\caption{}\n    \\label{fig:specvsspecIin}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.49\\textwidth}\n     \\centering\n    \\includegraphics[width=\\textwidth]{specvsspecexplain1.eps}\n    \\caption{}\n    \\label{fig:explainspecvspec}\n\\end{subfigure}\n\n \\caption{In (a) we show the competition outcome as the distance between the peak locations, $\\lambda_{1,0} $ and $\\lambda_I$, is changed versus the degree of niche differentiation between the two species (by varying $\\lambda_{1,0}-\\lambda_{2,0}$) The heat map is given by $\\frac{|u_1|}{|u_1|+|u_2|}$. In (b) we show the shape of $k_i(\\lambda)$ for four reference values of $\\lambda_{i,0}$ in blue, and $I_{in}(\\lambda)$ for two reference values of $\\lambda_I$ in orange.}\n \n\n\\label{fig:specvsSpec}\n\\end{figure}\n\\begin{figure}\n     \\centering\n    \\includegraphics[width=.49\\textwidth]{specvsspecadvection}\n    \\caption{The competition outcome as the advection rate, $\\alpha_2$, is changed versus the degree of niche differentiation between the two species under uniform incident light is shown. Example niches are given in blue in Figure \\ref{fig:explainspecvspec}. The heat map is given by $\\frac{|u_1|}{|u_1|+|u_2|}$. We fix $\\alpha_1=-0.01$ mh$^{-1}$ and $I_{in}(\\lambda)\\equiv 1.67 \\mu$mol$/(\\text{m}^2\n\\cdot \\text{s}\\cdot \\text{nm})$.}\n    \\label{fig:specvsspecAdvdist}\n\\end{figure}\n\nFigure \\ref{fig:specvsspecIin} shows the coexistence regions when varying the location of the peak of incident light and the distance between the two absorption spectra $k_1(\\lambda)$ and $k_2(\\lambda)$ (as measured by $\\mathcal{I}_S(k_1,k_2)$). The point P1 is justified by the following observations in Figure \\ref{fig:specvsspecIin}.\nWe see that exclusion is exhibited for extreme values of $\\lambda_{1,0}-\\lambda_I$ and non-zero $\\mathcal{I}_S(k_1,k_2)$. When the values of $\\lambda_{1,0}-\\lambda_I$ are extreme, one of the species' absorption spectrum overlaps with the incident light significantly more giving it a competitive advantage. However, when the values of $\\lambda_{1,0}-\\lambda_I$ are intermediate and $\\mathcal{I}_S(k_1,k_2)$ is large then each species has sufficient overlap with the incident light spectrum and any competitive advantage is diminished, promoting coexistence. \n\n\n\n Figure \\ref{fig:specvsspecAdvdist} shows the coexistence region when varying the advection rate of species 2 ($\\alpha_2$) and the distance between the two absorption spectra $k_1(\\lambda)$ and $k_2(\\lambda)$ (given by $\\mathcal{I}_S(k_1,k_2)$). \n First, we observe that when $\\mathcal{I}_S(k_1,k_2)=0$, whichever species that is more buoyant excludes the other species, as was established in Section \\ref{sec:sameniche}.\n %\n\n However, when $\\mathcal{I}_S(k_1,k_2)$ is large, the competitive exclusion caused by advection advantage is mitigated and coexistence occurs, thus justifying P2. When considering two species with unimodal absorption spectra, it is possible to overcome competitive exclusion by allowing for niche differentiation in the light spectrum.\n\n\\subsection{Outcomes for generalist versus specialist competition }\nIn this section we numerically explore niche differentiation in the light spectrum through competition between a specialist and a generalist. We say that a generalist species is a species whose absorption spectrum is uniform (or nearly uniform) across all visible wavelengths. Whereas we say a specialist species is one whose absorption spectrum is unimodal or narrow. In other words, a specialist absorbs a specific wavelength, or a small subset of wavelengths with a higher rate than other wavelengths. \n\nWe explore the mechanism of specialist vs. generalist competition in overcoming competitive exclusion by explicitly comparing absorption spectra. We take $k_2(\\lambda)$ to be constant (generalist) and choose $k_1(\\lambda)$ such that $|k_1(\\lambda)|=|k_2(\\lambda)|$ in the $L^1$ norm. We further assume that $k_1(\\lambda)$ is given by a truncated normal distribution, between 400 and 700 nm. By using the truncated normal distribution for $k_1(\\lambda)$ we are able to change the degree of specialization of species 1 by changing the variance,$\\sigma$, of the distribution as shown in Figure \\ref{fig:explainspecvsgen}. Furthermore we allow the location of peak absorption to vary along the incident light spectra, that is $\\lambda_{1,0}\\in[400,700]$, where $\\lambda_{1,0}$ is the mean of the truncated normal distribution and is the location of the local maximum of $k_1(\\lambda)$. \n\n We consider two scenarios to analyze the promotion of coexistence via the niche differentiation mechanism of specialist versus generalist competition. First, we assume an unimodal incident light $I_{in}(\\lambda)$ as in Figure \\ref{fig:specvsspecIin} and vary the location of the peak species absorption spectra, $\\lambda_{1,0}$.\n Additionally, we vary the degree of specialization of species $1$ by changing the variance of the truncated normal distribution that defines its absorption spectrum. That is, by changing the variance we change the narrowness of its niche and thus change the values of $\\mathcal{I}_S(k_1,k_2)$. \n \n Second, we change $\\mathcal{I}_S(k_1,k_2)$ as described above but with a uniform incident light function. We allow a competitive advantage through advection by altering the advection rate of species $2$, $\\alpha_2$. Recall that $u_1$ has competitive advantage when $\\alpha_1<\\alpha_2$, and $u_2$ has competitive advantage when $\\alpha_1>\\alpha_2$ (see Theorem \\ref{thm:4.3}).\n \n \n By varying $\\mathcal{I}_S(k_1,k_2)$ we are able to show the competitive outcomes when niche differentiation via a specialist versus generalist competition is permitted. The results pertaining to competition outcomes of the scenarios discussed in this section are shown in Figures \\ref{fig:2paramSpecivsGEn} and \\ref{fig:specvsgenadv}. \n\n\n\\begin{figure}\n     \\centering\n     \\begin{subfigure}[b]{0.49\\textwidth}\n         \\centering\n         \\includegraphics[width=1\\textwidth]{specvsgenpeakki.eps}\n         \\caption{}\n         \\label{fig:specvsgenIin}\n     \\end{subfigure}\n     \\hfill\n      \\begin{subfigure}[b]{0.49\\textwidth}\n         \\centering\n         \\includegraphics[width=1\\textwidth]{Specvsgensigmuexplain.eps}\n         \\caption{}\n         \\label{fig:explainspecvsgen}\n     \\end{subfigure}\n        \\caption{(a) shows coexistence regions for a specialist (species 1), and a generalist (species 2). The heat maps are given by $\\frac{|u_1|}{|u_1|+|u_2|}$. In (b) we show samples of the absorption spectra in blue and the incident light in orange. We fix the absorption spectrum $k_2(\\lambda$) of the generalist and change the specialization of species 1 by adjusting the variance of its absorption spectrum $k_1(\\lambda)$ as shown by the blue lines in (b). We fix $I_{in}(\\lambda)$ as given in (b) and show the competition outcome with relation to the distance between the specialists location of peak absorption and the incident lights location of peak intensity and the distance between the two absorption spectra given by $\\mathcal{I}_S(k_1,k_2)$ in (a).}\n        \\label{fig:2paramSpecivsGEn}\n\\end{figure}\n\n\n\\begin{figure}\n         \\centering\n         \\includegraphics[width=.49\\textwidth]{specvsgenadvection.eps}\n         \\caption{We show coexistence regions for competing specialist (species 1), and generalist (species 2). The heat map is given by $\\frac{|u_1|}{|u_1|+|u_2|}$. We fix the absorption spectrum $k_2(\\lambda$) of the generalist and adjust $\\mathcal{I}_S(k_1,k_2)$ by changing the variance of $k_1(\\lambda)$ while fixing the mean at $625$nm. We also vary the generalists' advection rate $\\alpha_2$ from $-0.1$ to $0.7$mh$^{-1}$, while fixing the specialists' advection rate $\\alpha_1=-0.01$  mh$^{-1}$.}\n         \\label{fig:specvsgenadv}\n     \\end{figure}\n\nIn Figure \\ref{fig:specvsgenIin} we show the relative abundance of species 1 for various degrees of specialization and overlap with the incident light function. The point P3 is discussed in the following. Species 1 is a strong competitor for a narrow set of wavelengths, whereas species 2 is a weak competitor for a broad set of wavelengths. When species 1 is highly specialized ($\\mathcal{I}_S(k_1,k_2)$ close to one) it strongly out-competes species 2 for a small portion of the light spectrum, however species 2 has little competition for the rest of the light spectrum and is able to exclude species 1. Furthermore, if species 1's niche does not overlap significantly with the incident light spectra or is too specialized then species 1 is faced with limited resource and is thus excluded. On the other hand, for intermediate specialization and relatively small distance between the location of peaks of $k_1(\\lambda)$ and $I_{in}(\\lambda)$ species 1 will out-compete species 2 for nearly all of the resources and thus excluding species 2. Coexistence is then observed when the specialist is relaxed to a generalist niche ($\\mathcal{I}_S(k_1,k_2)$ is close to zero) because weak competition occurs along the entire light spectrum and no significant advantage is given. Additionally, for intermediate values of $\\mathcal{I}_S(k_1,k_2)$ and sufficient overlap between incident light and species 1 niche, coexistence is permitted by the balance between the specialist strongly competing for a sufficient but narrow amount of resource and the generalist weakly competing for wide amount of resource that is not utilized by the specialist. \n\nIn Figure \\ref{fig:specvsgenadv} we show the relative abundance of species 1 for various degrees of specialization and advection rates of species 2 under uniform incident light. The point P2 is reiterated by the following results. Recall that in Theorem \\ref{thm:4.3} we show that competitive exclusion occurs if one species has an advection advantage and there is no niche differentiation. Here we see that niche differentiation in the light spectrum ($\\mathcal{I}_S(k_1,k_2)>0)$ allows for coexistence even though one species has a competitive advantage through advection. We note that if the generalist has an advection advantage then it will always exclude the specialist. On the other hand, if the specialist has the advection advantage it will exclude the generalist unless it becomes too specialized, in which case sufficient light is available for the generalist and either coexistence occurs, or in the case of extreme specialization, the specialist is excluded. Furthermore, there is a region where the competitive advantage of advection is so strong for the specialist that it will always exclude the generalist.         \n\n\n \\section{Coexistence of N species}\\label{sec:Nspecies}\n\nIn this section, we will show the possibility of coexistence of $N$ species, for any number $N\\geq 1$. We numerically verify this result by considering competition among 5 species with varying advection rates.\nWe introduce the $N$-species model analogous to  \\eqref{eq:full}:\n\n\\begin{equation}\\label{eq:fullN}\n\\begin{cases}\n\\partial_t u_i = D_i \\partial^2_x u_i - \\alpha_i \\partial_x u_i + [g_i(\\gamma_i(x,t)) - d_i(x)]u_i & \\text{ for } 0 < x < L,~1\\leq i\\leq N,\\\\\nD_i\\partial_x u_i(x,t) - \\alpha_i u_i(x,t) = 0 & \\text{ for }x = 0,~L,\\,t>0,~1\\leq i\\leq N,\\\\\nu_i(x,0) = u_{i,0}(x)&\\text{ for }0 < x < L,~1\\leq i\\leq N,\n\\end{cases}\n\\end{equation}\nwhere $D_i>0$, $\\alpha_i \\in \\mathbb{R}$ and $d_i$ are the diffusion rate,  buoyancy coefficient and death rate of the $i$-th species, respectively. The functions $g_i$ satisfies \\eqref{eq:ggg}. \nThe functions \n$\\gamma_{i}(x,t)$ is the number of absorbed photons available for photosynthesis by the $i$-th species and is given by\n\\begin{equation}\\label{eq:gammai}\n\\gamma_i(x,t) = \\int_{400}^{700}k_i(\\lambda) I(\\lambda,x)\\,d\\lambda,\n\\end{equation}\nwhere we have chosen $a_i \\equiv 1$ as before, and\n\\begin{equation}\\label{eq:II}\n  I(\\lambda,x) = I_{\\rm in}(\\lambda) \\exp\\left[- K_{BG}(\\lambda)x - \\sum_{i=1}^N k_i(\\lambda)\\int_0^x u_i(y,t)\\,dy \\right].\n\\end{equation}\n\n\\begin{theorem}\n    Let the incident light spectrum $I_{in}(\\lambda)$ be positive on an open set in $[400,700]$. Then for each $N \\geq 1$, there exists a choice of $d_i$ and $\\{k_i(\\lambda)\\}_{i=1}^N$ such that all $N$ species can persist in \\eqref{eq:fullN}, i.e. for any positive initial condition, the solution $(u_i)_{i=1}^N$ of \\eqref{eq:fullN} satisfies\n    $$\n    \\liminf_{t\\to\\infty} \\left[\\inf_{0\\leq x \\leq L} u_i(x,t) \\right]>0 \\quad \\text{ for each }1\\leq i \\leq N.\n    $$\n\\end{theorem}\n\\begin{proof}\nBy the hypotheses of the theorem, there exists $\\lambda_1,\\lambda_2$ such that  $400 \\leq \\lambda_1 < \\lambda_2 \\leq 700$ and that $I_*:= \\inf_{[\\lambda_1,\\lambda_2]}I_{in}(\\lambda) >0$. Let $\\{J_i\\}_{i=1}^N$ be a partition of $[\\lambda_1,\\lambda_2]$, and choose the functions $k_i(\\lambda)$ such that $\\textup{Supp}\\,k_i \\subset {\\rm Int}\\, J_i$. In particular, the support of $k_i$ do not overlap. Hence, \n$$\n  I(\\lambda,x) = I_{\\rm in}(\\lambda) \\exp\\left[- K_{BG}(\\lambda)x - k_i(\\lambda)\\int_0^x u_i(y,t)\\,dy\\right] \\quad \\text{ in }\\textup{Supp}\\,k_i, \n$$\nand the $i$-th species satisfies effectively a single species equation\n$$\n\\begin{cases}\n\\partial_t u_i = D_i \\partial^2_xu_i - \\alpha_i \\partial_x u_i+ [g_i(\\gamma_i(x,t)) - d_i(x)]u_i &  \\text{ for } 0 < x < L,\\, t>0,\\\\\nD_i \\partial_x u_i - \\alpha_i \\partial_x u_i = 0 &\\text{ for }x = 0,L,\\, t>0,\n\\end{cases}\n$$\nwith $\\gamma_i$ being independent of $u_j$ for $j\\neq i$. Precisely,\n\\begin{equation}\\label{eq:gammaii}\n\\gamma_i(x,t) = \\int_{400}^{700} k_i(\\lambda) I_{\\rm in}(\\lambda) \\exp\\left[- K_{BG}(\\lambda)x - k_i(\\lambda)\\int_0^x u_i(y,t)\\,dy\\right]\\,d\\lambda.\n\\end{equation}\n\nNext, we choose $d_i$ to be a positive constant such that\n$$\n\\mu(D_i,\\alpha_i,g_i(\\int k_i(\\lambda)I_{in}(\\lambda) \\exp(-K_{BG}(\\lambda)x)) - d_i) <0.\n$$\nThis is possible since\n\\begin{align*}\n    \\lim_{d_i \\to 0}\\mu(D_i,\\alpha_i,&g_i(\\int k_i(\\lambda)I_{in}(\\lambda) \\exp(-K_{BG}(\\lambda)x)) - d_i)\n    \\\\&=\\mu(D_i,\\alpha_i,g_i(\\int k_i(\\lambda)I_{in}(\\lambda) \\exp(-K_{BG}(\\lambda)x)))<0,\n\\end{align*}\n\nwhere the last inequality follow from Lemma \\ref{lem:eigen}. \nIt then follows from~\\cite[Proposition 3.11]{Jiang2019} that \nthe problem \n$$\n\\begin{cases}\nD_i \\partial^2_x u_i - \\alpha_i \\partial_x u_i + [g_i(\\tilde\\gamma_i(x)) - d_i]u_i & \\text{ for } 0 < x < L,\\\\\nD_i\\partial_x u_i(x) - \\alpha_i u_i(x) = 0 & \\text{ for }x = 0,~L,\n\\end{cases}\n$$\nwith $\\tilde\\gamma_i(x)$ given by \n$$\n\\hat\\gamma_i(x)=\\int_{400}^{700}a_i(\\lambda) k_i(\\lambda) I_{\\rm in}(\\lambda) \\exp\\left[- K_{BG}(\\lambda)x - k_i(\\lambda)\\int_0^x \\tilde{u}_i(y,t)\\,dy\\right]\\,d\\lambda,\n$$\nhas a unique positive solution $\\tilde{u}_i$. Moreover, \n$$\nu_i(\\cdot,t) \\to \\tilde{u}_i \\quad \\text{ in }C([0,L]),\\text{ as }t \\to \\infty,\n$$\nprovided $u_i(\\cdot,0) \\not\\equiv 0.$  \nThis completes the proof.\n\\end{proof}\n\nNext, we numerically demonstrate the possibility of coexistence of five phytoplankton species under niche differentiation. We assume that all five species $D_i=D_j$, $d_i=d_j$, and $g_i=g_j$ for all $i,j$ and that $\\alpha_1=0.01$ with $\\alpha_i=i\\cdot\\alpha_1$ for all $i$. We assume that all absorption spectra are unimodal and are given by the truncated normal distribution. Furthermore, each absorption spectrum $k_i(\\lambda)$ is a horizontal translation of one another. We alter the location of peak absorption (or the mean) to allow for niche differentiation similarly to Figures \\ref{fig:explainspecvspec} and \\ref{fig:explainspecvsgen}. We also assume that the incident light ($I_{in}(\\lambda)$) is unimodal with peak absorption located at 575 nm allowing for a competitive advantage.  \nWe compare the relative abundances of the five species at time $t$ defined by\n\\begin{equation}\n    \\bar u_i(t)=\\frac{|u_i(x,t)|_{L^1}}{\\sum_{j=1}^N |u_j(x,t)|_{L^1}},\n\\end{equation}\nwhere the $L^1$ norm here is taken with respect to the spatial variable $x$. We further denote the relative abundance at equilibrium as $\\bar u_i^*$. \nIn addition, we define the $N$ species niche differentiation index as\n\\begin{equation}\\label{eq:overlapmeasure}\n    \\mathcal{I}_i=\\frac{1}{N-1}\\sum_{j=1,j\\neq i}^N \\mathcal{I}_S(k_i,k_j).\n\\end{equation}\n Consequently the average niche differentiation index is given as \n\\begin{equation}\\label{eq:Ibar}\n \\bar{\\mathcal{I}}=\\frac{1}{N}\\sum_i^N \\mathcal{I}_i.\n\\end{equation}\n\n\n\\begin{figure}\n     \\centering\n         \\includegraphics[width=0.8\\textwidth]{5spadvSSIbar.eps}\n        \\caption{Gives the steady state relative abundance ($\\bar u_i^*$) of 5 competing species and their respective overlap measure defined in \\eqref{eq:overlapmeasure}. The x-axis is labelled as the average overlap measure $\\bar{\\mathcal{I}}$ given in \\eqref{eq:Ibar}. All other model parameters are the same among species expect the competitive advantage obtained through buoyancy: $\\alpha_1=0.01$ mh$^{-1}$, $\\alpha_2=0.02$ mh$^{-1}$, $\\alpha_3=0.03$ mh$^{-1}$, $\\alpha_4=0.04$ mh$^{-1}$, $\\alpha_5=0.05$ mh$^{-1}$.}\n        \\label{fig:5SP} \n\\end{figure}\n\nFigure \\ref{fig:5SP} gives the numerical results of the five species competition. Competitive exclusion occurs when niche differentiation is not sufficient and the species with the lowest advection rate (species 1) excludes all other species. However, as the niche differentiation is increased, more species are able to coexist and all five species can persist when niche differentiation is significant enough. \n\n\\section{Red versus Green cyanobacteria competition}\n\nIn this section we numerically explore a more realistic competition scenario between two phytoplankton species. To incorporate realistic biological assumptions into our model we consider two main things. First, the background attenuation of water is not uniform across the visible light spectrum and depends on the amount of dissolved and particulate organic matter (gilvin and tripton) in the water.  Second, the absorption spectra considered in Section \\ref{sec:NumCoexistmechs} are idealistic for investigation and are not typical for a phytoplankton species. Thus, in this section we consider absorption spectra given empirically as in Figure \\ref{fig:absspectra} and explore competition outcomes. \n\n\\subsection{Background attenuation in water}\nHere we introduce a reasonable function to more accurately model background attenuation of water, gilvin and tripton and phytoplankton.\n\nWe divide the background attenuation into two parts to account for the attenuation of pure water and gilvin and tripton \n\\begin{equation}\n    K_{BG}(\\lambda)=K_W(\\lambda) +K_{GT}(\\lambda),\n\\end{equation}\nwhere $K_W(\\lambda)$ is readily found in the literature and shown in Figure \\ref{fig:waterabsorb}~\\cite{Stomp2007,Pope1997}. $K_{GT}(\\lambda)$ is also found in literature and is given by the following form \\cite{kirk2010}:\n\\begin{equation}\n    K_{GT}(\\lambda)=K_{GT}(\\lambda_r)\\textup{exp}(-S(\\lambda-\\lambda_r)),\n\\end{equation}\nwhere $\\lambda_r$ is a reference wavelength with a known turbidity and $S$ is the slope of the exponential decline. Following literature we take reasonable values for each of these variables with $S=0.017 $nm$^{-1}$ as in~\\cite{Stomp2007} and referenced in~\\cite{kirk2010}. We fix our reference wavelength, $\\lambda_r$, to be $480$nm.  \nThe background attenuation is larger in turbid lakes due to the high concentrations of gilvin and tripton. For this reason, we use $K_{GT}(480)$ as a proxy for the turbidity of a lake, and vary $K_{GT}(480)$ between $0.1-3\\textup{m}^{-1}$. That is, low $K_{GT}(480)$ values correspond to clear lakes whereas high $K_{GT}(480)$ values correspond to  highly turbid lakes. \n\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.6\\paperwidth]{waterabsorb.eps}\n    \\caption{The absorption spectrum of pure water~\\cite{Pope1997,Stomp2007}, and the absorption spectra for lakes with gilvin and tripton concentrations representative of clear oligotrophic or mesotrophic waters ($K_{BG}(480)=0.1$), and turbid eutrophic waters ($K_{BG}(480)=1$).}\n    \\label{fig:waterabsorb}\n\\end{figure}\nLastly, we consider the absorption spectra of red and green cyanobacteria species. In Figure \\ref{fig:absspectra} we see that there are significant differences in the absorption spectra between the phytoplankton allowing for niche differentiation. \n\n\\subsection{Competition outcomes of red and green cyanobacteria}\nWe now show the steady state outcome when red and green cyanobacteria compete for light in lakes of varying turbidity. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.7\\paperwidth]{reallife.eps}\n    \\caption{(a)-(c) show steady state outcomes of competition between green cyanobacteria, $u_1(x,t)$ (shown in blue), and red cyanobacteria, $u_2(x,t)$ (shown in red), for various amounts of gilvin and tripton that correspond to low, intermediate and high turbidity, respectively. (d)-(f) shows the background absorption for those states with $K_{BG}(480)=0.1$, $K_{BG}(480)=1.1$, $K_{BG}(480)=2$, respectively.} \n    \\label{fig:reallife}\n\\end{figure}\n\nIn Figure \\ref{fig:reallife} the competition outcome between green cyanobacteria (\\textit{Synechocystis} strain) and red cyanobacteria (\\textit{Synechococcus} strain) is shown. In Figure \\ref{fig:absspectra}, the green cyanobacteria absorption spectra is shown in blue and the red cyanobacteria is shown in red. Their absorption spectra are sufficiently different so that niche differentiation occurs. That is, the green cyanobacteria mainly absorbs light in the orange-red ranges, whereas the red cyanobacteria absorbs more green light. Both species absorb blue light similarly. Thus, the light availability throughout the water column plays an important role in competition outcome. In Figures \\ref{fig:reallife}(d)-(f) we see that as the gilvin and tripton concentrations increase (shifting from low turbidity to high) the background absorption's shift to absorb proportionally more blue and green light, leaving proportionally more orange and red light available. This shift in available light then modifies the competitive outcome, where red cyanobacteria clearly dominate in less turbid case, whereas green cyanobacteria dominate in the highly turbid case, even though the two species coexist in both situations.\n\n\n\\section{Conclusion}\n\n\n\n\nIn this manuscript we explore niche differentiation along the light spectrum by extending the models of Stomp et al. \\cite{Stomp2007} to the spatial context, using well established reaction-diffusion approach. Differing with previous works~\\cite{Jiang2019,HsuLou2010,Du2011}, in which light was regarded as a single resource with varying intensity, here we treat light as a continuum of resources that have varying availability and are consumed in different efficiency by the phytoplankton species.\nOur main theoretical results, found in Section \\ref{sec:mathresults}, stem from the theory of monotone dynamical systems and include the existence and attractiveness of the equilibrium. These results give a condition for when the semi-trivial equilibria exist and characterize their stability. As an extension, a condition for coexistence is obtained. The condition for coexistence is then made explicit to offer direct biological interpretations based on model parameters.  Niche differentiation is introduced in Section \\ref{sec:Mathniche} by allowing the absorption spectra ($k_i(\\lambda)$) of competing species to change. We consider the case where the competing species niches are completely disjoint and provide a condition for coexistence. Furthermore we consider the case when competing species occupy the same niche and provide competitive exclusion outcomes based on transport related parameters and show that species who are able to stay closer to the surface through either advection or turbulent diffusion will competitively exclude. These results lay the groundwork to study the impacts niche differentiation will have on coexistence outcomes in Section \\ref{sec:NumCoexistmechs}. \n\nWe show numerically, in Section \\ref{sec:NumCoexistmechs}, a myriad of mechanisms in which coexistence can occur. When two specialists compete, the competitive advantages given by advection or incident light can be overcome when niche differentiation is significant. This is shown in Figures \\ref{fig:specvsSpec}. Furthermore, we see that competitive exclusion occurs when the overlap between the incident light and a species' absorption spectrum is large, see Figure \\ref{fig:specvsspecIin}. In addition, the more buoyant species no longer dominates if niche differentiation is significant, as shown in Figure \\ref{fig:specvsspecAdvdist}.   Similarly, in the competition between a specialist and a generalist,  coexistence readily occurs for intermediate degrees of niche differentiation. However,if the niche of the specialist occupies only a narrow part of the incident light spectrum, then \ntheir growth rate can be negatively impacted as shown in Figure \\ref{fig:2paramSpecivsGEn}. In either case niche differentiation in the light spectrum is enough to overcome competitive exclusion caused by diffusion and advection, thus offering an important perspective in resolving the paradox of the plankton in the affirmative direction. \n\nFurthermore, to fully explore the ecological diversity and the paradox of the plankton, we consider a system with $N$ competing species. First, we show analytically that coexistence of $N$ species is possible under sufficient niche differentiation and proper natural death rate ($d_i(\\lambda)$) functions. This result  suggests a possible evolutionary strategies that phytoplankton may take in partitioning  in their usage of the light spectrum for growth~\\cite{Holtrop2021}. To illustrate our result, we provide numerical simulations for a five species competition scenario with an advection and incident light advantage present. Here we choose phytoplankton species that have differential buoyancy properties. In the absence of niche differentiation, competitive exclusion were predicted by previous work \\cite{Jiang2019}. When niche differentiation is significant, we observe that the species are able to coexist in a robust manner.\n\nLastly, we numerically study the competition dynamics for absorption spectra and background attenuation functions that are representative of phytoplankton species found in nature. Precisely, we consider the absorption spectra of green and red cyanobacteria species and explore the competitive outcome as it depends on the nutrient status, or turbidity of the ecosystem as shown in Figure \\ref{fig:reallife}. Our numerical results suggest that clear lakes host higher abundances of red cyanobacteria whereas green cyanobacteria out-compete in highly turbid, eutrophic lakes. Our result in particular confirms with empirical results~\\cite{Stomp2007} and is potentially useful in understanding phytoplankton competition. \n\n\nIn this paper, we explored a potential explanation to the paradox of the plankton by allowing for niche differentiation in the visible light spectrum. To achieve this, we made several simplifying assumptions about the biological system, such as our sufficient nutrient assumption. It is well known that phytoplankton dynamics heavily depend on nutrient dynamics~\\cite{Whitton2012,Klausmeier2004b,Reynolds2006}. Thus, in order to fully understand phytoplankton population dynamics, future attempts at modelling niche differentiation should also allow for the explicit consideration of nutrient and nutrient uptake dynamics. We have also assumed that our model parameters are constant in time. This in general is not true for ecological systems, and in particular those that explicitly consider light. Light availability is periodic on the time scales of days and, in addition, periodic seasonally. In addition to light, parameters related to mortality and motility can depend on water temperature and thus change seasonally. This type of oscillatory forcing can significantly change dynamics and especially when considering transient dynamics~\\cite{Hastings2018}. \n\nEven though our model can be improved in various ways, our results are biologically intuitive and are consistent with the current state of the biological literature. Our work furthers the understanding of niche differentiation and phytoplankton competition and can be used as a basis for future studies of phytoplankton dynamics and predictive modelling. In conclusion, our study shows that niche differentiation can promote coexistence of phytoplankton species in a robust way, thus supporting one explanation of the Hutchinson's paradox. \n\n\n\n\\bibliographystyle{ieeetr}\n", "meta": {"timestamp": "2021-09-07T02:40:04", "yymm": "2109", "arxiv_id": "2109.02634", "language": "en", "url": "https://arxiv.org/abs/2109.02634"}}
{"text": "\\section{Introduction}\n\\label{sec:intro}\nDuring the powered landing of a spacecraft on lunar and planetary bodies, impinging exhaust gases generate a strong recirculation region that fluidize the surface and eject loose granular matter (see Fig.~\\ref{fig:PSI}). Plume-surface interactions (PSI) are capable of destabilizing the lander and dislodging dust and debris at high speeds that can damage exposed hardware, reduce visibility, and spoof landing sensors. Mitigating the risks of PSI requires a detailed understanding of the multiphase dynamics under such extreme conditions. While the last several decades have seen significant advancements in simulation and modeling techniques for incompressible particle-laden flows~\\citep{crowe1996numerical,balachandar2010turbulent,fox2012large,tenneti2014particle}, much less attention has been paid to high-speed (compressible) two-phase flows. This article presents a review and perspectives on this topic.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{Figs/PSI}\n\\caption{Plume-surface interactions during spacecraft propulsive landings represent a key challenge for space exploration. (a-b) Final descent of NASA's Perseverance rover on February 18, 2021 showing (a) ejected dust and rocks and (b) erosion of martian regolith during the Sky Crane maneuver. (c) Craters formed by rocket plumes beneath NASA's InSight lander. Images adapted from NASA/JPL-Caltech.}\n\\label{fig:PSI}\n\\end{figure}\n\nIn this section, examples of PSI from past landing events and the associated flow physics are presented. The remainder of the article focuses on the fundamental dynamics and processes of compressible gas--particle flows. Origins of existing models and new insights gleaned from particle-resolved simulations are reported. Particular attention is paid to interphase coupling under dilute and moderately dense concentrations at finite Mach number. We summarize classical drag laws in addition to models for unsteady contributions due to added mass and `pseudo' turbulence during shock-particle interactions.\n\n\\subsection{Impact of PSI from previous landings}\nThe detrimental effects of PSI from previous lunar and martian missions are well documented~\\citep{christensen1967surveyor,foreman1967interaction,o1970degradation,clark1970effect,jaffe1971blowing,taylor1972apollo,hutton1980surface,o2009direct,gomez2014curiosity}. In fact, four of the six Apollo landings suffered from hindered visibility caused by ejected granular material during landing. Suspension of lunar dust was found to deposit on equipment located 17 m from Apollo 11 and 160 m from Apollo 12~\\citep{o2009direct}. Hardware on the Surveyor III craft experienced pitting and cracking as a result of sandblasting from high-speed ejecta during the Apollo 12 landing~\\citep{jaffe1971blowing,immer2011apolloa,immer2011apollob}. Particles were estimated to have traveled at speeds in excess of 100 m/s. During the Apollo 15 landing, the lunar module experienced a 12-degree tilt at touchdown, almost terminating the mission~\\citep{mcdivitt1971apollo}. Due to the lack of an atmosphere on the Moon, smaller particles suspended during the Apollo landings are estimated to have reached escape velocity, posing hazards to orbital hardware~\\citep{lane2008lagrangian}.\n\nIn an effort to mitigate the negative effects of PSI, NASA's 2012 Mars Science Laboratory (MSL) mission introduced the Sky Crane maneuver for the final descent of the Curiosity lander. The rover was lowered down on a bridle from an altitude of 7.5 m to prevent close contact between the rocket engines and the planet's surface. Compared to the Moon, the thin (but finite) martian atmosphere inhibits the spreading of exhaust gas, resulting in collimated rocket plumes that generate highly localized impingement pressures~\\citep{mehta2013thruster} and the formation of craters (see Fig.~\\ref{fig:PSI}(c)). A significant amount of soil and debris was lifted during MSL, which is believed to have damaged one of the Curiosity rover\u2019s two wind sensors~\\citep{gomez2014curiosity}. The same Sky Crane maneuver was also used for the Mars 2020 mission. As shown in Fig.~\\ref{fig:PSI}, erosion of martian soil was observed during descent of the Perseverance rover, resulting in the liberation and ejection of large rocks. Meanwhile, planned sample return missions and anticipated crewed missions with higher payloads precludes the use of Sky Crane.\n\n\\subsection{Multiphase flow dynamics during PSI}\nIncorporating the effects of PSI into the design stage of next-generation landers is a necessary step to minimize risk and ensure the success of future missions. What makes this so challenging are the wide range of flow regimes that coexist (see Fig.~\\ref{fig:PSI2}). The exhaust gas leaving the rocket nozzle is supersonic and chemically reacting, giving rise to a Mach disk followed by expansion waves and a plate (standoff) shock just above the surface. The Reynolds number at the exit of the nozzle is typically $\\Rey=\\mathcal{O}(10^5)$ and the Knudsen number of the exhaust gas is sufficiently low~\\citep{mehta2013thruster}, thus the flow is continuum and highly turbulent when the lander is within a few meters of the surface.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{Figs/PSI2}\n\\caption{Schematic of the fluid-particle dynamics present during a landing event highlighting regions of varying Mach number, $\\Mac$, and volume fraction, $\\phi_p$.}\n\\label{fig:PSI2}\n\\end{figure}\n\nThe particle volume fraction, $\\phi_p$, generally decreases with elevation, from near close packing ($\\phi_p>0.6$) at the surface to highly dilute above the plate shock. Within the recirculation region (sometimes referred to as the stagnation bubble~\\citep{mehta2013thruster}), high-pressure subsonic flow fluidizes the soil. The impinging gas forms shock waves parallel to the surface and deflects the plume radially outward. The deflected plume accelerates to supersonic speeds, inducing high shear stress on the surface that lifts the soil into the boundary layer--so-called viscous erosion~\\citep{metzger2009jet}. Ejected particles interact with turbulence in the shear layer and shock waves throughout the exhaust plume where the concentration is low ($\\phi_p\\ll1$) and the gas-phase Mach number, $\\Mac$, is high. Interphase coupling between particles and turbulence in high-speed shear layers are capable of altering pressure fluctuations radiating outwards from the plume~\\citep{krothapalli2003turbulence,buchta2019sound} and generating flow instabilities analogous to two-fluid flows \\citep{mcfarland2016computational}.\n\n\n\nThe physical mechanisms contributing to erosion vary depending on the rocket thrust, atmospheric conditions of the landing environment, and physical soil properties. Table~\\ref{table:params} lists atmospheric conditions at the surface of Earth, the Moon, and Mars. Regolith (the upper layer of soil) on the Moon is tightly packed due to its near-vacuum conditions, preventing the exhaust gas from penetrating deep within. Thus, erosion is primarily a consequence of shear induced by the viscous flow above the surface~\\citep{hutton1968comparison,metzger2009jet}, causing particles to spread far from the lander. Due to the lack of an atmosphere, the Moon is continually bombarded with small meteorites and its regolith exhibits a wide size distribution. In contrast, regolith on Mars is exposed to atmospheric wind and tends to be more spherical~\\citep{sullivan2005aeolian}. The stagnation pressure exerted by the jet exceeds the bearing capacity of martian soil and compresses/evacuates the upper layer to form a deep crater~\\citep{metzger2009jet}, which redirects the two-phase flow up towards the lander. \n\n\\begin{table}[ht!]\n\\centering\n\\caption{Typical atmospheric conditions at the surface of Earth, the Moon, and Mars.}\n\n\\begin{tabular}{llll}\n\\toprule\n & Earth & Moon & Mars \\\\ \n\\midrule\nGravitational acceleration [$\\rm{m/s^2}$] &$9.81$ & $1.62$ & $3.72$  \\\\\nPressure [$\\rm{mbar}$] &  $1013.25$ & $3\\times10^{-12}$ & $5$--$10$ \\\\\nDensity [$\\rm{kg/m^3}$] & $1.2$ & $\\approx 0$ & $1.66 \\times 10^{-2}$ \\\\\nDynamic viscosity [$\\rm{Ns/m^2}$] & $1.8 \\times 10^{-5}$ & N/A & $1.5 \\times 10^{-5}$  \\\\\nSound speed [$\\rm{m/s}$] &  $343$ & N/A &  $264$ \\\\\n\\bottomrule\n\\end{tabular}\n\\label{table:params}\n\\end{table}\n\n\\subsection{Modeling PSI}\nTheoretical and experimental studies of PSI date back to the early 1960s~\\citep{spady1962exploratory,roberts1963action,land1965experimental,roberts1966interface,hutton1969mars}. \\citet{roberts1963action} developed the first\nmodel to describe the erosion and subsequent transport of dust beneath a rocket for lunar environments. The erosion rate was determined from a balance between shear stress exerted by the plume and the threshold shear strength of the soil.\nHowever, the model neglected aerodynamic forces on the particles, and comparisons to laboratory-scale experiments showed only marginal agreement~\\citep{hutton1968comparison}. \\citet{metzger2008modification} modified Roberts' theory by incorporating the particle size distribution of lunar soil and imposing ejection angles inferred from Apollo landing videos. However, the authors conclude that a better understanding of the aerodynamic forces and erosion process in supersonic flow regimes are needed. In addition, recent laboratory experiments have revealed that the threshold friction velocity to initiate saltation (and ensuing erosion) differs substantially in low pressure environments like Mars, yet existing scaling laws are unable to capture this~\\citep{sullivan2017aeolian,andreotti2021lower}.\n\nParticle motion during PSI is a consequence of fluid-particle and particle-particle interactions at the \\textit{microscale}, i.e., fluid stresses and contact dynamics at the sub-particle level. Fluid forces acting on each particle depend non-linearly on the local Reynolds number, Mach number, and volume fraction. Inter-particle forces arising from collisions and sliding/rolling friction vary based on the physical properties of the regolith. In recent years, Eulerian-based two-fluid models~\\citep{gale2017gas,balakrishnan2018high,gale2020realistic,balakrishnan2019multi,chinnappan2021modeling,balakrishnan2021fluid} and particle-based methods~\\citep{he2012simulation,morris2015approach,rahimi2020near,shallcross2021modeling} of PSI have been conducted that explicitly account for these interactions. Yet, the underlying models these simulations are built upon (e.g., drag, sub-grid scale turbulence, etc.) were largely developed for incompressible flows. While the aerodynamics of high-speed projectiles have been studied for centuries (namely in the context of ballistics), there has been significant developments in recent years, largely owing to the advent of high-performance computing. This article focuses specifically on gas--particle interactions: the origins of existing models; progress in theoretical understanding over recent years; and perspectives for future model development.\n\n\n\\section{Scale separation}\nThe range of scales of motion in particle-laden flows are vast (see Fig.~\\ref{fig:length} for the simple case of an isolated particle). When the flow is incompressible, the smallest scales are typically on the order of the particle diameter, $d_p$. In turbulent flows, the smallest eddy size (the Kolmogorov, or dissipation length scale) is also important. Relevant time scales include the particle response time, $\\tau_p=\\rho_p d_p^2/(18\\mu_f)$, where $\\rho_p$ is the particle density and $\\mu_f$ is the fluid viscosity, in addition to time scales associated with the fastest turbulent velocity fluctuations.\nWhen gas-phase compressibility is important, information travels near or exceeds the sound speed, $c$. An acoustic time scale can be defined using $c$ and a relevant length scale, e.g., $d_p/c$, which is often much smaller than $\\tau_p$. For example, a solid particle with density $\\rho_p=3000$ kg/m$^3$ and diameter $d_p=100$~$\\upmu$m in air results in $\\tau_p\\approx 0.1$~s and $d_p/c\\approx 0.3$~$\\upmu$s! The interaction between shock waves and suspensions of particles involves length scales spanning the shock thickness (order of the mean free path of gas molecules), to wakes past particles at scales larger than the particle diameter. In the context of PSI, the largest scales of interest may include the full landing site, which can span kilometers. \n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.5\\textwidth]{Figs/length}\n\\caption{Schematic of a supersonic flow past a spherical particle (particle moving from left to right). Various length scales that are present (in approximately increasing magnitude): shock wave thickness $\\delta_s$; boundary layer thickness $\\delta_b$; Kolmogorov length scale $\\eta$; shock standoff distance $L_s$; particle diameter $d_p$; and wake length $L_w$.}\n\\label{fig:length}\n\\end{figure}\n\nA key challenge in modeling any multiphase flow system is properly capturing the length- and time-scales at play. The equation of motion for a particle with mass $m_p$ and velocity $\\bm{v}_p$ traveling through a viscous fluid can be expressed as\n\\begin{equation}\\label{eq:Newton}\n    m_p\\frac{{\\rm d}\\bm{v}_p}{{\\rm d}t}=\\int_S \\left(\\check{\\bm{\\sigma}}_f-\\check{p}_f\\mathbb{I}\\right)\\bcdot\\bm{n}\\,{\\rm d}S+\\bm{F}_{\\rm ext},\n\\end{equation}\nwhere $\\check{p}_f$ is the fluid pressure, $\\check{\\bm{\\sigma}}_f$ is the viscous stress tensor, $\\bm{n}$ is the unit normal vector outward from the particle surface $S$, $\\mathbb{I}$ is the identity tensor, and $\\bm{F}_{\\rm ext}$ are external body forces. Here, the $\\check{(\\cdot)}$ notation denotes a microscale quantity prior to any averaging. It becomes immediately apparent that a numerical solution to Eq.~\\eqref{eq:Newton} requires sub-particle scale resolution. Particle-resolved simulations for incompressible flows usually employ grid spacing 20--40 times smaller than the particle diameter~\\citep{tenneti2014particle}--even finer resolution is typically required for compressible flows--which becomes extremely computationally demanding when the system size of interest is much larger than the size of an individual particle. Thus, direct solutions to the conservation equations are rarely practical. Below we introduce the \\textit{averaged} equations of motion and the challenges that arise.\n\n\\subsection{Averaged equations}\\label{sec:average}\nObtaining a mathematical description that captures the multi-scale nature of two-phase flows typically involves ensemble averaging~\\citep{zhang1997momentum} or volume filtering~\\citep{anderson1967fluid, capecelatro2013euler}.\nThe main idea is to apply an averaging volume (or filter) with a characteristic size comparable to the inter-particle spacing to replace surface fluxes with volumetric source terms. This is analogous to large-eddy simulation (LES) of single-phase flows, and similarly results in unclosed terms that require models. Unlike in single-phase LES, the averaging procedure omits the volume occupied by particles, and consequently differentiation and filtering do not commute. This produces sub-filtered (or subgrid-scale) contributions at the surface of each particle.\n\nApplying such an averaging operation to the viscous compressible Navier--Stokes equations--recall the flow within the exhaust plume during PSI can be treated as continuum--yields equations for mass, momentum, and energy~\\citep{shallcross2020volume}\n\\begin{equation}\\label{eq:density}\n\\frac{\\partial \\phi_f\\rho_f}{\\partial t}+\\nabla\\bcdot\\left(\\phi_f\\rho_f \\bm{u}_f\\right)=0,\n\\end{equation}\n\\begin{equation}\\label{eq:momentum}\n\\frac{\\partial \\phi_f \\rho_f \\bm{u}_f}{\\partial t} + \\nabla \\bcdot (\\phi_f \\left\\{\\rho \\bm{u}_f\\bm{u}_f + \\bm{R}_f\\right\\})  =\\phi_f\\nabla\\bcdot \\left(\\bm{\\sigma}_f-p_f\\mathbb{I}\\right)+\\bm{F}_p,\n\\end{equation}\nand\n\\begin{equation}\\label{eq:energy}\n\\begin{aligned}\n\\frac{\\partial \\phi_f \\rho_f E_f}{\\partial t} & + \\nabla \\bcdot \\left( \\phi_f \\rho_f E_f \\bm{u}_f \\right) + \\nabla \\bcdot (\\phi_f (p_f \\bm{u}_f - \\bm{u}_f \\bcdot \\bm{\\sigma}_f)) + \\phi_f \\nabla \\bcdot \\bm{q}_f\\\\\n& = - p_f\\frac{\\partial\\phi_f}{\\partial t}+\\bm{\\sigma}_f\\bcol \\nabla \\left( \\phi_p \\bm{u}_p \\right)  + \\bm{u}_p \\bcdot \\bm{F}_p + Q_p - \\nabla \\bcdot \\left(\\phi_f \\{ \\bm{R}_{p} + \\frac{1}{2} \\bm{R}_{u} - \\bm{R}_{\\sigma} \\}  \\right),\n\\end{aligned}\n\\end{equation}\nwhere $\\phi_f=1-\\phi_p$ is the fluid-phase volume fraction, $\\rho_f$ is the fluid density, $\\bm{u}_f$ the fluid velocity, $\\bm{u}_p$ is the particle-phase velocity (in an Eulerian frame of reference), and $E_f$ the total energy. Interphase coupling takes place through momentum exchange, $\\bm{F}_p$, that requires models for drag and other forces acting on the particle (see Sec.~\\ref{sec:forces}), and heat exchange, $Q_p$, typically modeled using Nusselt-number correlations. The first term on the right-hand side of Eq.~\\eqref{eq:energy} can be thought of as a $pDV$ work term due to particles entering and leaving a control volume~\\citep{lhuillier2010multiphase}. By employing the particle-phase continuity equation and assuming constant particle density, this can be replaced with $-p_f\\nabla\\bcdot\\left(\\phi_p\\bm{u}_p\\right)$~\\citep{houim2016multiphase}, which may be more convenient in a numerical implementation. \n\nIn Eq.~\\eqref{eq:momentum}, $\\bm{R}_f$ is a residual stress that arises from filtering the non-linear convective term. Because the fluid velocity fluctuations may originate at the particle scale, the physics that govern this unclosed term differs significantly from the Reynolds stress appearing in classical single-phase flows. In fact, this term may be non-zero even in laminar flows (e.g., via steady wakes), and is therefore termed a \\textit{pseudo} turbulent Reynolds stress~\\citep{mehrabadi2015pseudo}. While it is typically neglected in incompressible flow models, recent work has shown that $\\bm{R}_f$ can contribute to a significant portion of the total kinetic energy during shock-particle interactions \\citep{hosseinzadeh2018investigation,sen2018role,mehta2019Pseudo,osnes2019computational,shallcross2020volume}. Special care needs to be taken when distinguishing the sub-filter scale velocity fluctuations originating from large-scale turbulent motion (via a classical energy cascade) and those induced by particles (pseudo turbulence).\nThis will be revisited in Sec.~\\ref{sec:PTKE}.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{Figs/multiscale}\n\\caption{Different modeling approached for simulating gas--particle flows. Left: Particle-resolved simulations directly solve the gas-phase equations with grid spacing $\\Delta x\\ll d_p$. Interphase coupling takes place through boundary conditions at the surface of each particle. At this scale, drag is an \\textit{output}. Middle: Euler--Lagrange (EL) methods track individual particles and solve the fluid on a grid with $\\Delta x\\approx d_p$ (highly-resolved EL) or $\\Delta x \\gg d_p$ (course EL). Requires closure models for drag and sub-grid scale turbulence. Right: Euler--Euler (EE) methods solve both phases on the grid and require closure models for gas-particle \\textit{and} particle-particle interactions.}\n\\label{fig:multiscale}\n\\end{figure}\n\nAdditional unclosed terms appear in Eq.~\\eqref{eq:energy} that account for work due to the subgrid-scale pressure and viscous stress ($\\bm{R}_{p}$ and $\\bm{R}_{\\sigma}$) and a triple product of subgrid-scale velocity fluctuations, $\\bm{R}_{u}$. Particle-resolved direct numerical simulations (PR--DNS) of shock waves traveling through arrays of randomly distributed particles have demonstrated that $\\bm{R}_{p}$ and $\\bm{R}_{\\sigma}$ are relatively small compared to the other terms in Eq.~\\eqref{eq:energy}~\\citep{mehta2019Pseudo,shallcross2020volume}, and \\citet{shallcross2020volume} showed that $\\bm{R}_{u}\\approx2\\bm{u}_f\\cdot\\bm{R}_f$. Thus, accurate solutions of the averaged compressible Navier--Stokes equations namely require validated models for the pseudo-turbulent Reynolds stress, $\\bm{R}_f$, and momentum exchange, $\\bm{F}_p$. Closure models have been proposed for these terms, but for most part are only valid in the incompressible or dilute limit. This is the focus of Secs.~\\ref{sec:forces} and~\\ref{sec:PTKE}.\n\nThe same averaging procedure could be extended to the particle phase as well, resulting in the so-called Euler--Euler (EE) or two-fluid method.\nThis gives rise to even more unclosed terms that require constitutive relations for the solid-phase rheology. While this is outside the scope of the present article, the full set of compressible EE equations can be found in \\citet{houim2016multiphase,balakrishnan2018high,fox2019kinetic}. Alternatively, Euler--Lagrange (EL) methods (also referred to as CFD--DEM in the chemical engineering literature) track individual particles and couple them to the averaged fluid equations. Figure~\\ref{fig:multiscale} shows an illustration of these different modeling approaches.\n\n\\subsection{Ill-posedness and hyperbolicity}\\label{sec:ill}\nIn addition to producing a number of terms that require closure, it is also well established that two-phase models with rigid particles can become ill-posed due to lack of hyperbolicity \\citep{lhuillier2013quest}. In general, the compressible EE equations have imaginary characteristics that can lead to unphysical instabilities. This has been shown to give rise to spurious volume fraction disturbances during shock-particle interactions~\\citep{theofanous2018shock}. EL methods generally remain hyperbolic due to the Lagrangian treatment of the particle phase, though they could be susceptible to similar numerical instabilities if inter-particle contact is not treated explicitly~\\citep{theofanous2017dynamics}. Numerous attempts have been made to restore hyperbolicity starting with \\citet{stuhmiller1977influence}, and seems only very recently to be properly resolved~\\citep{fox2020hyperbolic}.\n\nHyperbolicity depends strongly on the nature of the closure models.\nIt is well established that the pressure gradient force acting on the particles (termed the Archimedes force) is the source of ill-posedness. Until recently, many authors have resorted to neglecting the Archimedes force or adding an ad-hoc `turbulent dispersion' force to stabilize the solution, which compromises the physical accuracy~\\citep{lhuillier2013quest}. Because the Archimedes force is proportional to the density ratio, these issues are more prevalent in bubbly as compared to gas\u2013particle flows. \\citet{balakrishnan2021fluid} analyzed the hyperbolicity of a compressible two-fluid model for simulations of supersonic jet-induced cratering on a granular bed. It was found that the system is hyperbolic in most regions except in the vicinity of the crater, particularly where the particle compaction is high ($\\phi_p>0.6$). They conclude that high compaction alone is not a sufficient criterion for the system of equations to become degenerate.\n\nAn alternative approach to the ad-hoc corrections listed above is to take into account the velocity fluctuations of both phases, which necessitates additional closure for $\\bm{R}_f$~ \\citep{lhuillier2013quest}. \\citet{fox2019kinetic} showed that starting from a well-defined microscale description, it is possible to derive a hyperbolic two-fluid model wherein the fluxes and source terms have unambiguous definitions. Starting from the Boltzmann--Enskog kinetic equations for a binary system, a two-fluid model for fully compressible fluid--particle flows was derived without exclusion of the Archimdes force. A rigorous analysis of this model showed that it is hyperbolic for gas--particle flows with $\\rho_p/\\rho_f\\gg 1$. The following year, \\citet{fox2020hyperbolic} extended the model to arbitrary density ratios by including the added mass of the fluid on the particle in addition to fluid-mediated interactions between particles.\nAn additional equation to include $\\bm{R}_f$ in the fluid phase was also proposed. It was shown that pseudo turbulence has no effect on hyperbolicity but it is needed to ensure conservation. Details on the added mass treatment are given in Sec.~\\ref{sec:Fiu} and models for $\\bm{R}_f$ are given in Sec.~\\ref{sec:PTKE}.\n\n\\section{Forces acting on a particle}\\label{sec:forces}\nAs shown in Fig.~\\ref{fig:PSI2} and discussed in Sec.~\\ref{sec:intro}, PSI spans dilute to dense particle-laden flow regimes. In this section, we first review the state-of-the-art in modeling the forces acting on an \\textit{isolated} particle, then move on to extensions to multi-particle systems. In the case of an isolated particle moving through an incompressible fluid at low Reynolds numbers (Stokes flow), \\citet{basset1888treatise}, \\citet{boussinesq1885application}, and \\citet{oseen1927neuere} decomposed the hydrodynamic force (the first term on the right-hand side of Eq.~\\eqref{eq:Newton}) into separate contributions: the quasi-steady drag force $\\bm{F}_{qs}$; undisturbed flow forces $\\bm{F}_{un}$ (sometimes denoted as the pressure gradient or Archimedes force), inviscid unsteady force $\\bm{F}_{iu}$ (referred to as added-mass in the limit of incompressible flow), and viscous-unsteady force $\\bm{F}_{vu}$ (Basset history). This gives rise to the so-called BBO equation, given by\n\\begin{equation}\\label{eq:BBO}\n    m_p\\frac{{\\rm d}\\bm{v}_p}{{\\rm d}t}=\\underbrace{\\bm{F}_{qs}+\\bm{F}_{un}+\\bm{F}_{iu}+\\bm{F}_{vu}}_{\\int_S \\left(\\check{\\bm{\\sigma}}_f-\\check{p}_f\\mathbb{I}\\right)\\bcdot\\bm{n}\\,{\\rm d}S}+\\bm{F}_{\\rm ext}.\n\\end{equation}\n\n\\citet{parmar2011generalized,parmar2012equation} extended the BBO equations to viscous compressible flows, where the separate force contributions are given by\n\\begin{subequations}\\label{eq:forces}\n\\begin{align}\n\\bm{F}_{qs} & = 3\\pi\\mu_f d_p\\left(\\bm{u}_f-\\bm{v}_p\\right)F_D(\\Rey_p,\\Mac_p,\\phi_p),\\label{eq:Fqs} \\\\\n\\bm{F}_{un} & = V_p\\rho_f\\frac{{\\rm D}\\bm{u}_f}{{\\rm D}t}, \\\\\n\\bm{F}_{iu} & = V_p\\int_{-\\infty}^t K_{iu}\\left(t-\\chi;\\Mac_p\\right)\\left(\\frac{{\\rm D}(\\rho_f\\bm{u}_f)}{{\\rm D}t}-\\frac{{\\rm d}(\\rho_f\\bm{v}_p)}{{\\rm d}t}\\right)_{t=\\chi}{\\rm d}\\chi,\\label{eq:Fiu} \\\\\n\\bm{F}_{vu} & = \\frac{3}{2}d_p^2\\sqrt{\\pi\\rho_f\\mu_f}\\int_{-\\infty}^t K_{vu}\\left(t-\\chi;\\Rey_p,\\Mac_p\\right)\\left(\\frac{{\\rm D}(\\rho_f\\bm{u}_f)}{{\\rm D}t}-\\frac{{\\rm d}(\\rho_f\\bm{v}_p)}{{\\rm d}t}\\right)_{t=\\chi}{\\rm d}\\chi,\\label{eq:Fvu}\n\\end{align}\n\\end{subequations}\nwhere $K_{iu}$ is the inviscid unsteady force kernel that will be discussed in detail in Sec.~\\ref{sec:Fiu} and $K_{vu}$ is the viscous-unsteady force kernel. The expressions above are valid for an isolated particle in the limit of zero Reynolds number and Mach number.\nNevertheless, they provide a natural framework for empirical extensions to more realistic flow conditions (though an alternative strategy is proposed in the final section of this article). For example, the quasi-steady drag force includes a correction factor $F_D(\\Rey_p,\\Mac_p,\\phi_p)$ typically given by an empirical correlation based on far-field flow properties. This term is related to the usual definition of the drag coefficient, $C_D$, according to\n\\begin{equation}\n    F_D=\\frac{\\Rey_p}{24}C_D(\\Rey_p,\\Mac_p,\\phi_p).\n\\end{equation}\nThe relevant Reynolds and Mach numbers used in these correlations are based on the relative velocity between the phases, i.e., $\\Rey_p=\\rho_f\\phi_f |\\bm{u}_f-\\bm{v}_p|d_p/\\mu_f$ and $\\Mac_p=|\\bm{u}_f-\\bm{v}_p|/c$, where $\\phi_f |\\bm{u}_f-\\bm{v}_p|$ is the superficial velocity that characterizes the Reynolds number at finite volume fraction. However, it remains an open question how best to interpret the fluid quantities at the particle location in a numerical simulation, especially for flows involving shocks where the flow state can exhibit steep gradients. Simplifying assumptions can also be made to the unsteady force contributions and modeled using empirical correlations. In the following sections we summarize existing models for quasi-steady drag, the inviscid unsteady force, their origins, and extensions to multi-particle systems.\n\nWe briefly note that to date, little attention has been paid to finite Mach number and volume fraction corrections for $\\bm{F}_{vu}$, and its relative importance in high-speed gas--particle flows is not clear (and is likely small at high Reynolds numbers~\\citep{ling2011importance}). Thus, we refrain any further discussion towards this term.\n\n\n\n\n\\subsection{Quasi-steady drag on an isolated particle}\\label{sec:iso}\n\nThe reader already versed in multiphase flow will be familiar with the seminal work by George Stokes who derived an analytic solution to the drag force on a sphere in the limit $\\Rey_p\\rightarrow0$ in 1851~\\citep{stokes1851effect}. The resulting drag coefficient, $C_D=24/\\Rey_p$, acts as the basis for nearly all drag laws that have been proposed thereafter. Yet, the contributions by Francis Bashforth the following decade are far less known, despite its influence on modern day compressible drag formulations.\nThe astute reader might recognize the name, as he also had a hand in developing the Adams--Bashforth method in 1883~\\citep{bashforth1883attempt}, a class of multi-step methods commonly used today for numerical time integration. But it was his invention of the ballistic chronograph in 1864 that provided the first reliable data on the aerodynamics of high-speed projectiles.\n\nBashforth's experiments used artillery round shots (cannon fire) that allowed for up to 10 velocity measurements to be made per shot~\\citep{bashforth1870}. The size and speed of cannonballs offers a\nsurprisingly useful flow regime for studying the drag force. Typical velocities span $100-700$ m/s, corresponding to $0.3\\le\\Mac_p\\le2$ in air, over which $C_D$ changes sharply due to compressibility effects. In addition, the Reynolds numbers straddle the critical value ($\\Rey_p\\approx2\\times10^5$) where $C_D$ exhibits an abrupt drop. Bashforth showed that the drag force at moderate (subsonic) velocities is nearly proportional to the square of velocity, at greater velocities (subsonic to supersonic) it is nearly proportional to the cube of velocity, and at even higher velocities it is again closely proportional to the square of velocity~\\citep{gilman1905ballistic}. These experiments were some of the last to use round shots--artillery fire at that time transitioned to elongated cylindrical bullets--and consequently are among few measurements ever made of high-speed, relatively large ($74-225$ mm) spheres.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore than a century later, \\citet{miller1979sphere} compiled available data for the drag on\nspheres over a wide range of Reynolds and Mach numbers. Interestingly, the\nmost accurate high Reynolds number data for moderate to high Mach numbers turned out to be from Bashforth's experiments. They showed that his measurements scatter no more than modern data at that time, which was used to construct a comprehensive map of $C_D$ as a function of $\\Rey_p$ for $0.1\\le\\Mac_p\\le3$. However, they concluded that significant scatter exists among all the data collected. As stated in \\citet{clift2005bubbles}: \\textit{``Many of the data in the literature for $\\Mac>0.2$ are unreliable \\ldots because of high levels of freestream turbulence, interference by a support, wall effects, etc.\"}\nThis is not surprising, as much of this data is based on 18-th and 19-th century cannon firings!\n\nUsing available experimental and theoretical data at that time, \\citet{henderson1976drag} developed a Reynolds- and Mach-number dependent drag correlation as piecewise functions for subsonic, supersonic ($\\Mac_p>1.75$), and linearly interpolated transonic regimes (see \\ref{app:henderson}). It includes non-continuum effects when the mean-free path of the gas phase $\\lambda$ approaches the particle diameter, i.e., finite values of the Knudsen number $\\Kn_p=\\lambda/d_p$. For an ideal gas, the Knudsen number can be defined in terms of the Reynolds and Mach number according to \\citep{clift2005bubbles}\n\\begin{equation}\n    \\Kn_p=\\sqrt{\\frac{\\pi\\gamma}{2}}\\frac{\\Mac_p}{\\Rey_p},\n\\end{equation}\nwhere $\\gamma$ is the ratio of specific heats for the gas phase. When $\\Kn_p<10^{-3}$ the fluid can be treated as a continuum. At larger values of the Knudsen number, the collision rate of gas molecules with the surface of the particle is not high enough to satisfy the usual no-slip condition. At moderate but still small values, the flow exhibits a small departure from no-slip (slip regime). When $\\Kn_p>10$, gas molecules collide with the particle but inter-molecule interactions are rare (free molecular flow). These regimes are summarized in Table~\\ref{table:Kn}.\n\n\\begin{table}[h]\n\\centering\n\\caption{Flow regimes based on the Knudsen number~\\citep{schaaf1958flow}.}\n\\begin{tabular}{lr}\n\\toprule\nKnudsen number range & Flow regime\\\\\n\\midrule\n$\\Kn_p\\le0.01$ & Continuum regime\\\\\n$0.01<\\Kn_p\\le0.1$ & Slip flow\\\\\n$0.1<\\Kn_p\\le10$ & Transitional flow\\\\\n$\\Kn_p>10$ & Free molecular flow\\\\\n\\bottomrule\n\\end{tabular}\n\\label{table:Kn}\n\\end{table}\n\nIt was later shown by \\citet{loth2008compressibility} that the drag coefficient at finite Mach numbers can be separated into two regimes: a rarefaction-dominated regime at low Reynolds numbers ($\\Rey_p\\lessapprox45$); and a compression-dominated regime at higher Reynolds numbers ($\\Rey_p\\gtrapprox45$). In between, it was suggested that the competing effects cancel each other out, leading to a so-called nexus condition, where $C_D\\approx1.63$ is independent of $\\Mac_p$ and $\\Kn_p$. Two separate models were constructed for these regimes. The drag model is based on theoretical analysis and experimental data from \\citet{hoerner1958fluid} taken from ballistic ranges.  Similar to the model by \\citet{henderson1976drag}, it relies on interpolation and approximation due to a lack of reliable data in the transitional regime ($\\Mac_p\\approx0.9$).\n\n\\citet{parmar2010improved} assessed the models of \\citet{henderson1976drag} and \\citet{loth2008compressibility} using data collected by \\citet{bailey1976sphere}, and proposed an improved correlation for the drag coefficient. The model by \\citet{henderson1976drag} was found to exhibit a maximum deviation from the data of 16\\%. The correlation by \\citet{loth2008compressibility} was found to deviate by as much as 55\\% when $\\Mac_p\\approx 0.9$. The model proposed by \\citet{parmar2010improved} was developed for continuum flows ($\\Kn<0.01$) with $\\Rey_p\\le 2\\times 10^5$ and ${\\rm  Ma}_p\\le 1.75$. The drag coefficient is decomposed into three correlations for subcritical ($\\Mac_p<\\Mac_{\\rm cr}\\approx0.6$), intermediate, and supersonic Mach number regimes, according to\n\\begin{equation}\\label{eq:Parmar}\nC_D(\\Rey_p,\\Mac_p)=\n    \\begin{cases}\n      C_{D,{\\rm std}}(\\Rey_p)+\\left[C_{D,{\\rm cr}}(\\Rey_p)-C_{D,{\\rm std}}(\\Rey_p) \\right]\\frac{\\Mac_p}{\\Mac_{\\rm cr}} & \\text{if } \\Mac_p<\\Mac_{\\rm cr},\\\\\n      C_{D,{\\rm sub}}(\\Rey_p,\\Mac_p) & \\text{if } \\Mac_{\\rm cr}<\\Mac_p\\le 1,\\\\\n       C_{D,{\\rm sup}}(\\Rey_p,\\Mac_p) & \\text{if } 1<\\Mac_p\\le 1.75.\n    \\end{cases}\n\\end{equation}\nIn the limit of zero Mach number, the drag coefficient reduces to $C_{D,{\\rm std}}$, the standard correlation of \\citet{clift1970motion}  valid for $\\Rey_p<2\\times10^5$ (given in \\ref{app:CG}). \nThe Mach number-dependent correlations are provided in \\ref{app:parmar}. For subcritical Mach numbers, the drag coefficient is only weakly affected by compressibility effects due to the absence of any shock (or expansion) waves. For supercritical but still subsonic Mach numbers, $C_D$ becomes more strongly dependent on the Mach number due to the presence of a weak shock. For supersonic Mach numbers, a bow shock is formed (with a standoff distance that decreases with increasing Mach number) that leads to a large increase in $C_D$. Schlieren visualization of a sphere in free flight from recent experiments by \\citet{nagata2020experimental} are shown in Fig.~\\ref{fig:Nagata}, highlighting these flow regimes.\n\n\\begin{figure}[h]\n\\subfigure[$\\Mac_p=0.9$]{\\includegraphics[width=0.32\\textwidth]{Figs/Nagata1b}}\n\\subfigure[$\\Mac_p=1.21$]{\\includegraphics[width=0.32\\textwidth]{Figs/Nagata2b}}\n\\subfigure[$\\Mac_p=1.39$]{\\includegraphics[width=0.32\\textwidth]{Figs/Nagata3c}}\n\\centering\n\\caption{Schlieren visualization of flow past a sphere with $\\Rey_p=\\mathcal{O}\\left(10^5\\right)$. Adapted from \\citet{nagata2020experimental} with permission.}\n\\label{fig:Nagata}\n\\end{figure}\n\n\n\nWith the advent of high-performance computing, DNS of compressible particle-laden flows is beginning to shed new light on this topic. \\citet{nagata2016investigation,nagata2020direct} performed DNS of flow past an isolated sphere for $0.3\\le\\Mac_p\\le2$ and $250\\le\\Rey_p\\le1000$. They report shortcomings of existing drag correlations owing to the compromised accuracy in the transitional regime ($0.9<\\Mac_p<1$) due to lack of experimental data. They demonstrated that the effect of $\\Mac_p$ and $\\Rey_p$ on the flow structure and drag coefficient can be characterized by the position of the separation point. A rapid extension of the length of the recirculation region was observed in the transitional regime, where $C_D$ was shown to increase with increasing Mach number independently of the Reynolds number.\n\nShortly after, \\citet{loth2021supersonic} combined the DNS data of \\citet{nagata2016investigation,nagata2020direct} with rarefied-gas simulations and an expanded experimental dataset to develop new empirical models for the drag coefficient (given in \\ref{app:loth}). It is simpler than that originally proposed by \\citet{loth2008compressibility} and appears to be the most accurate and comprehensive model developed for compressible gas--particle flows to date (and the first to incorporate numerical data). It was shown to be approximately twice as accurate compared to the correlations of \\citet{loth2008compressibility} and \\citet{parmar2010improved} at moderate Mach numbers, and showed improvement to Loth's original model at higher Mach numbers ($\\Mac_p>2$) and for rarefied conditions. The overall trends in $C_D$ are shown in Fig.~\\ref{fig:drag}, highlighting the quasi-nexus that separates the rarefaction and compressibility flow regimes. It can be seen that the compression-dominated region ($\\Rey_p>60$) yields an increase in $C_D$ as $\\Mac_p$ increases, whereas in the rarefaction-dominated region ($\\Rey_p<30$), $C_D$ is inversely proportional to $\\Mac_p$. In between these regimes--the quasi-nexus--there exists a weak transonic bump that connects the two. While DNS was used to quantitatively describe this behavior, the authors suggest additional data is warranted to further refine the drag coefficient within the quasi-nexus region.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.72\\textwidth]{Figs/drag_MP}\n\\caption{Drag coefficient on a spherical particle showing rarefaction (shaded in red) and compression (shaded in yellow) dominated regimes using the new model proposed by \\citet{loth2021supersonic}. Multi-particle effects on the drag coefficient are shown in green.}\n\\label{fig:drag}\n\\end{figure}\n\n\n\\subsection{Drag in multi-particle systems}\nIn the presence of one or more neighbors, the drag force exerted on a particle can vary significantly compared to that of an isolated sphere. Multi-particle drag correlations are needed when the inter-particle distance becomes comparable to the particle diameter--typically when $\\phi_p\\gtrapprox 1\\times10^{-3}$. In a homogeneous fluidization system (see Fig.~\\ref{fig:PRDNS}), the single-particle drag coefficient can be corrected to take into account finite volume fraction effects. In general, the mean drag over a suspension of particles has a complex dependency on $\\Rey_p$ and $\\phi_p$ (and likely on $\\Mac_p$ as well, though this is far less studied).\n\nCorrelations used to account for the drag force in multi-particle systems can be classified into two types. In the first type, \\citet{ergun1952fluid} proposed a correlation based on the drag force in the limit of Stokes flow with an additional term that depends linearly on $\\Rey_p$, such that the drag correction in Eq.~\\eqref{eq:Fqs} is expressed as\n \\begin{equation}\\label{eq:Ergun}\n     F_D(\\Rey_p,\\phi_p)=F_D(0,\\phi_p)+\\alpha\\Rey_p.\n \\end{equation}\nOriginally, $\\alpha$ was only a function of volume fraction~\\citep{ergun1952fluid}. However, it was later shown that $\\alpha$ also depends non-linearly on $\\Rey_p$~\\citep{hill2001first,beetstra2007drag,tenneti2011drag,tang2015new}. Recently, \\citet{tang2016direct} proposed a drag law of this type for freely-evolving spherical particles by incorporating the granular temperature (a measure of the particle velocity variance) to account for particle mobility effects (see \\ref{app:tang}). They found that particle mobility increases the drag force compared to stationary particles at the same volume fraction, with greater effect at higher $\\Rey_p$.\n \nThe second correlation type originally proposed by \\citet{wen1966mechanics} takes the form\n\\begin{equation}\\label{eq:WenYu}\n    F_D(\\Rey_p,\\phi_p)=F_D(\\Rey_p,0)\\phi_f^{-\\beta},\n\\end{equation}\nwhere $F_D(\\Rey_p,0)$ is the finite Reynolds number drag correction for an isolated particle (see Sec.~\\ref{sec:iso}). The value for $\\beta$ was originally constant \\citep{wen1966mechanics}, but was later shown to depend on $\\Rey_p$ \\citep{di1994voidage}. More recently, \\citet{tenneti2011drag} proposed a model following this form based on PR--DNS of flow past random assemblies of fixed spherical particles valid for $0.01\\le\\Rey_p\\le 300$ and $0.1\\le\\phi_p\\le0.5$ (given in \\ref{app:tenneti}). This was later extended to freely evolving particles for $0.001\\le\\rho_p/\\rho_f\\le1000$~\\citep{tavanashad2021particle}, where it was shown that $\\beta$ depends on the density ratio. Contrary to what \\citet{tang2016direct} observed, this data showed that the drag force for freely-evolving particles approaches that of fixed particle assemblies when $\\rho_p/\\rho_f>100$. Recall $\\rho_p/\\rho_f=\\mathcal{O}\\left(10^5\\right)$ during PSI.\n\nCompared to incompressible flows, much less attention has been paid to multi-particle drag correlations for flows at finite Mach number. The presence of neighboring particles acts to increase the drag coefficient compared that of an isolated particle. At moderate Reynolds numbers, this increase in drag coefficient obfuscates the quasi-nexus region shown in Fig.~\\ref{fig:drag}. A third, multi-particle dominated regime exists in the upper-right quadrant of Fig.~\\ref{fig:drag}, located above the \\citet{clift1971motion} curve. This may intersect with the compression-dominated regime, though its precise behavior remains largely unknown.\n\nIn an attempt to incorporate volume fraction effects into compressible drag correlations, \\citet{ling2012interaction} combined the Mach number-dependent drag model of \\citet{parmar2010improved} with the (incompressible) volume fraction correction of \\citet{sangani1991added}. Note this follows the second correlation type given by Eq.~\\eqref{eq:WenYu}.\nThe corresponding drag coefficient is given by\n\\begin{equation}\nC_D(\\Rey_p,\\Mac_p,\\phi_p)=C_{D,{\\rm std}}(\\Rey_p)\\xi_1(\\Rey_p,\\Mac_p)\\xi_2(\\phi_p),\n\\end{equation}\nwhere $\\xi_1(\\Rey_p,\\Mac_p)$ is the Mach number correlation for an isolated particle that was provided in Eq.~\\eqref{eq:Parmar}, and the volume fraction correction is given by\n\\begin{equation}\n    \\xi_2(\\phi_p)=\\frac{1+2\\phi_p}{\\left(1-\\phi_p\\right)^2}.\n\\end{equation}\nThe above expression was analytically derived from periodic arrays of spherical particles in the zero Reynolds number limit for $\\phi_p\\le 0.3$ \\citep{sangani1991added}.\nHowever, such an approach does not guarantee it will capture the non-linear coupling between $\\Rey_p$, $\\Mac_p$, and $\\phi_p$ in general. Recent PR--DNS of shock-particle interactions have shown that particles experience a large variation in drag force due to fluid-mediated particle-particle interactions, which current models fail to capture\n\\citep{mehta2018propagation,mehta2019effect,osnes2021performance}. \n\n\n\n\\begin{figure}[h]\n\\subfigure[$\\Rey_p=20$, $\\phi_p=0.1$, $\\Mac_p=0$]{\\includegraphics[width=0.42\\textwidth]{Figs/PRDNS}}\\quad\\quad\n\\subfigure[$\\Rey_p=300$, $\\phi_p=0.1$, $\\Mac_p=0.8$]{\\includegraphics[width=0.39\\textwidth]{Figs/particles}}\n\\centering\n\\caption{Instantaneous snapshots from PR--DNS of a homogeneous suspension of particles. (a) Incompressible flow with $\\rho_p/\\rho_f=1000$ showing fluid velocity (red: high, blue: low)~\\citep{lattanzi2021fluid}. (b) Compressible flow past fixed spheres showing a 2D cross-section of the 3D flow highlighting fluid dilatation (black/white) and vorticity (red: positive, blue: negative)~\\citep{Khalloufi2021}.}\n\\label{fig:PRDNS}\n\\end{figure}\n\nWhile PR--DNS of multi-particle systems are now common for incompressible flow, only in recent years PR--DNS of compressible flows past assemblies of particles have come online \\citep[e.g.,][]{regele2014unsteady,mehta2016shock,mehta2018propagation,theofanous2018shock,hosseinzadeh2018investigation,osnes2019computational,shallcross2020volume}. However, these studies typically consider a shock wave traveling through an array of particles, which introduces challenges in developing drag correlations due to the lack of statistical stationarity and homogeneity. Figure~\\ref{fig:PRDNS}(b) shows an example of PR--DNS of a homogeneous flow with $\\phi_p=0.1$ and $\\Mac_p=0.8$ based on the mean slip velocity \\citep{Khalloufi2021}. The flow accelerates to supersonic speeds in the interstitial space between particles, resulting in large values of fluid dilatation (shocklets). These regions of high compression have important consequences on vorticity generation due to baroclinic torque ($\\nabla\\rho_f\\times\\nabla p_f$) at the vertex of curved shocks \\citep{kida1990enstrophy}, which is anticipated to alter the drag force. This work among other recent PR--DNS represent a promising new direction for developing improved drag correlations that span relevant values of $\\Rey_p$, $\\Mac_p$, and $\\phi_p$ for applications related to PSI and other compressible gas--particle flows.\n\n\n\n\n\n\\subsection{Unsteady inviscid force}\\label{sec:Fiu}\nCompared to the quasi-steady drag force, the unsteady forces appearing in Eq.~\\eqref{eq:forces} have received much less attention, especially in the context of compressible flows. It is typically assumed that these terms scale with the fluid-to-particle density ratio, and therefore can be neglected for gas--solid flows. This is generally true for the case of a particle accelerating in a quiescent fluid. However, when the surrounding fluid is accelerating, the order of magnitude of the inviscid unsteady terms ($\\bm{F}_{iu}$ and $\\bm{F}_{un}$) relative to the quasi-steady drag force scales like $\\Rey_p d_p/L_f$, where $L_f$ is the characteristic length scale of the background flow \\citep{bagchi2002steady}. Therefore, the motion of finite-size particles in a compressible flow undergoing strong acceleration (e.g., in the presence of a shock wave) can be greatly influenced by the unsteady forces, regardless of the density ratio. In fact, the peak drag coefficient of an isolated particle interacting with a shock wave can be more than 10 times larger than the value from the quasi-steady drag force depending on the shock Mach number, $\\Mac_s$, and $\\Rey_p$ \\citep{parmar2009modeling,ling2011importance}.\n\n\n\n\nThe inviscid unsteady force given in Eq.~\\eqref{eq:Fiu} is expressed in terms of a response kernel, $K_{iu}(\\tau;\\Mac_p)$, used to weigh the history of the particle's acceleration. The kernel decays over a non-dimensional acoustic time $ct/d_p$, which depends on both the particle's shape and Mach number. In an incompressible flow, the acoustics propagate infinitely fast and the kernel reduces to a Dirac delta function, i.e., $K_{iu}(\\tau;\\Mac_p=0)=\\delta(\\tau)/2$.\nFor a spherical particle in an incompressible flow, integration of the kernel results in an added mass coefficient $C_m=0.5$. However, when the flow is compressible, the Mach number delays the approach to steady state and the force no longer takes the form of a constant mass multiplied by the instantaneous acceleration. Because of this, \\citet{miles1951virtual,longhorn1952unsteady} and later reiterated by \\citet{parmar2008unsteady}, emphasized that reference to this force as a `virtual' or `added' mass is only applicable to incompressible flows.\n\nA popular expression for $K_{iu}$ was obtained by \\citet{longhorn1952unsteady} for a spherical particle in the limit of zero Mach number as $K_{iu}(\\tau;\\Mac_p=0)=\\exp(-\\tau)\\cos(\\tau)$. More than half a century later, \\citet{parmar2008unsteady} extended this to finite (but subcritical) Mach numbers. They demonstrated that $\\Mac_p$ has a pronounced effect on both the peak value of the unsteady force and the effective added-mass coefficient. By employing the compressible form of the Bernoulli equation, a Mach number expansion for pressure was derived and integrated around a sphere to obtain the force.\nAt a Mach number of $0.5$, the effective added-mass coefficient was found to be $C_m(\\Mac_p=0.5)\\approx 1$, approximately twice as large as the incompressible value for a sphere.\n\nIn general, $K_{iu}$ becomes negligibly small after a few acoustic time scales. If the relative acceleration changes slowly over this duration, it can be taken outside of the integral and Eq.~\\eqref{eq:Fiu} can be rewritten as\n\\begin{equation}\\label{eq:Fiu2}\n    \\bm{F}_{iu}  \\approx V_p C_m(\\Mac_p,\\phi_p)\\left(\\frac{{\\rm D}(\\rho_f\\bm{u}_f)}{{\\rm D}t}-\\frac{{\\rm d}(\\rho_f\\bm{v}_p)}{{\\rm d}t}\\right),\n\\end{equation}\nwhere $V_p$ is the particle volume. \\citet{ling2012interaction} proposed a model for the added mass coefficient, given by\n\\begin{equation}\nC_m(\\Mac_p,\\phi_p)=C_{M,{\\rm std}}\\eta_1(\\Mac_p)\\eta_2(\\phi_p),\n\\end{equation}\nwhich reduces to the usual value $C_{M,{\\rm std}}=0.5$ in the limit of zero Mach number and volume fraction. The Mach number correction (valid in the subcritial regime) is given by~\\citep{parmar2008unsteady}\n\\begin{equation}\n\\eta_1(\\Mac_p)=\n    \\begin{cases}\n      1+1.8\\Mac_p+7.6\\Mac_p^4 & \\text{if } \\Mac_p<0.6,\\\\\n      2.633 & \\text{otherwise}.\n    \\end{cases}\n\\end{equation}\nThe volume fraction correction is taken from the expression by \\citet{sangani1991added} as\n\\begin{equation}\n    \\eta_2(\\phi_p)=\\frac{1+2\\phi_p}{1-\\phi_p},\n\\end{equation}\nwhich is valid for random arrays of spheres in incompressible low Reynolds number flow when $\\phi_p<0.3$. Such a simple expression for the inviscid unsteady force has received mixed success when applied to shock-particle interactions \\citep{parmar2009modeling,koneru2021assessment,osnes2021performance}.\n\nAn interesting alternative to modeling the inviscid unsteady force was recently proposed by \\citet{fox2020hyperbolic}, and demonstrated to be a key ingredient in restoring hyperbolicity of the compressible two-fluid equations (refer back to Sec.~\\ref{sec:ill}). In-lieu of treating the force as a separate term in Eq.~\\eqref{eq:BBO}, the added mass is transported with the particle velocity, which implicitly captures the force history without the need for a response kernel. This is based on the formulation of \\citet{cook1984virtual}, but generalized to a compressible fluid and a non-constant added-mass function to capture volume fraction effects. The mass appearing in the conservation equations are augmented to include the portion of the wake moving with the particle. In an EE framework, this involves replacing $\\phi_p$ in the particle-phase equations with $\\phi_p^\\star=\\phi_p+\\phi_a$, such that the particles carry an added mass $m_p=(\\rho_p\\phi_p+\\rho_f\\phi_a)V_p=\\rho_e\\phi_p^\\star V_p$, where $\\alpha_a$ is the added volume fraction of the surrounding fluid and $\\rho_e$ is the effective density of the particle.\nA simple model was proposed based on a mass exchange function that acts as a source term for $\\phi_p^\\star$, given by\n\\begin{equation}\n    \\frac{\\partial \\rho_e\\phi_p^\\star}{\\partial t}+\\nabla\\bcdot\\left(\\rho_e\\phi_p^\\star\\bm{u}_p\\right)=\\frac{\\rho_f\\phi_f\\phi_p}{\\tau_a}\\left(C_m^\\star-C_m \\right),\n\\end{equation}\nwhere $\\tau_a$ is a time scale that characterizes the expansion/contraction/formation of particle wakes, and $C_m^\\star$ is the added mass function that should depend on $\\Rey_p$, $\\Mac_p$, and $\\phi_p$. When a particle moves from a region of high $\\phi_p$ to low $\\phi_p$ (i.e., into a region with larger inter-particle spacing), $C_m<C_m^\\star$ and the wake will grow by entraining the surrounding fluid. In a EL framework, this would require replacing $m_p$ in Eq.~\\eqref{eq:BBO} with an effective mass $m_p^\\star$ carried by each particle that evolves according to\n\\begin{equation}\n    \\frac{{\\rm d}m_p^\\star}{{\\rm d}t} = \\frac{1}{\\tau_a}\\rho_f \\phi_f V_p(C_m^\\star-C_m).\n\\end{equation}\nSuch a treatment for the added mass has advantages over the more traditional approach given by Eq.~\\eqref{eq:Fiu}, as the instantaneous acceleration of each phase need not be evaluated at the particle location, and the time-history of the response kernel is not required. Models for $\\tau_a$ and $C_m^\\star$ are needed for relevant values of $\\Rey_p$, $\\Mac_p$, and $\\phi_p$, which could be evaluated from PR--DNS \\citep[e.g., see][for a recent evaluation of $\\tau_a$]{moore2019lagrangian}.\n\n\n\\section{Pseudo-turbulence in multi-particle systems}\\label{sec:PTKE}\nFlow through a collection of particles gives rise to wakes that interact non-linearly with each other and with surrounding particles. The collective effect of these flow disturbances acts as a source of fluid-phase turbulence. Because such fluctuations exist even in laminar flow (e.g., via steady wakes), and originate at small scales, it is often termed \\textit{pseudo}-turbulence. This is in contrast to classical turbulence that involves a transfer of energy from large scales of motion to small scales. Models for velocity fluctuations originating from pre-existing background turbulence (i.e., from the classical turbulence cascade) are not considered here as they have already received significant attention for single-phase flow \\citep[e.g.,][]{garnier2009large}. Instead, we focus on models for the pseudo-turbulent Reynolds stress, $\\bm{R}_f$ in Eqs.~\\eqref{eq:momentum}--\\eqref{eq:energy},  that is expected to play a role when the inter-particle spacing is comparable to the size of the wakes (i.e., at moderate values of $\\phi_p$).\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.9\\textwidth]{Figs/PTKE}\n\\caption{A two-dimensional simulation showing gas-phase vorticity (color) and dilatation (black/white) shortly after a planar shock wave passes over a suspension of particles with $\\Mac_s=1.22$ and $\\phi_p=0.17$. Adapted from \\citet{shallcross2020volume}.}\n\\label{fig:PTKE}\n\\end{figure}\n\nWhen a shock wave passes over a cloud of particles at moderate volume fractions, small-scale velocity fluctuations are produced in particle interstitial sites and advected downstream with the mean flow (see Fig.~\\ref{fig:PTKE}).\n\\citet{theofanous2018shock} performed one of the first three-dimensional particle-resolved simulations of shock-induced dispersion. The gas phase was shown to choke near the downstream edge of the particle cloud due to the abrupt change in volume fraction, resulting in supersonic expansion that significantly increases particle acceleration. \\citet{shallcross2020volume} later showed that including $\\bm{R}_f$ in the definition of total fluid energy is critical for capturing this choking behavior in simulations based on averaged equations. Near the edge of the particle cloud, large gradients in $\\bm{R}_f$ can be on the same order as the forces acting on the particles \\citep{osnes2019computational}. The corresponding root-mean-square velocity fluctuations have been observed to be as much as 60\\% of the mean flow. Its strength increases with increasing values of $\\phi_p$ and incident shock Mach number \\citep{mehta2019Pseudo,osnes2019computational}.\n\nModeling the pseudo-turbulent Reynolds stress has only gained considerable attention in recent years. An algebraic model for the pseudo-turbulent kinetic energy (PTKE), $k_f={\\rm tr}(\\bm{R}_f)/2$, was first proposed by \\citet{mehrabadi2015pseudo} for incompressible homogeneous gas--solid flows, given by\n\\begin{equation}\\label{eq:k1}\n    \\frac{k_f}{K_f}=2\\phi_p+2.5\\phi_p\\phi_f^3\\exp\\left(-\\phi_p\\Rey_p^{1/2}\\right),\n\\end{equation}\nwhere $K_f=\\bm{u}_f\\bcdot\\bm{u}_f/2$ is the kinetic energy of the resolved flow field. The correlation was developed in the range $0.1\\le\\phi_p\\le0.5$ and $0.01\\le\\Rey_p\\le300$ and tends to appropriate values in the zero volume fraction limit (i.e., $k_f\\rightarrow 0$ when $\\phi_p\\rightarrow 0$). The pseudo-turbulent Reynolds stress tensor is reconstructed from the PTKE according to\n\\begin{equation}\\label{eq:Rij}\n\\bm{R}_f' = 2 \\rho_f k_f \\left( \\bm{b} +\\frac{1}{3} \\mathbb{I} \\right),\n\\end{equation}\nwhere $\\bm{R}_f'$ is the Reynolds stress aligned with the local velocity difference between the phases and $\\bm{b}$ is the anisotropic stress tensor that depends on $\\Rey_p$ and $\\phi_p$~\\citep{mehrabadi2015pseudo}. $\\bm{R}_f$ is then obtained by rotating $\\bm{R}_f'$ to align with the Cartesian coordinate system. Details on the implementation of the rotation matrix in an EE framework can be found in \\citet{peng2019implementation}.\n\nUsing PR--DNS of shock-induced flow through random arrays of particles, \\citet{osnes2019computational} proposed a model for the streamwise component of the Reynolds stress (denoted here as direction $1$) according to\n\\begin{equation}\n    R_{f,11}=u_{f,1}^2\\frac{\\phi_{\\rm sep}}{\\phi_f-\\phi_{\\rm sep}},\n\\end{equation}\nwhere $\\phi_{\\rm sep}$ represents the volume fraction of separated flow in particle wakes, which increases significantly as $\\phi_p$ is reduced as a result of increasing inter-particle separation. The authors proposed a simple model of the form $\\phi_{\\rm sep}\\left(\\phi_f,\\Rey_p\\right)=\\phi_p C\\left(\\Rey_p\\right)$, where $C\\left(\\Rey_p\\right)\\approx 1.5$ was determined for incident shock Mach numbers between $2.2\\le \\Mac_s\\le 3$ and volume fractions $0.05\\le\\phi_p\\le0.1$.\nThe authors note that it would be appropriate to introduce a time-dependency for $\\phi_{\\rm sep}$, since particle wakes and fluctuations are not generated instantaneously after the shock wave passes over a particle. This is conceptually similar to the added mass formulation proposed by \\citet{fox2020hyperbolic} reported in Sec.~\\ref{sec:Fiu}. However, such models do not yet exist.\n\nA key drawback with the algebraic models proposed by \\citet{mehrabadi2015pseudo} and \\citet{osnes2019computational} is that they only predict finite PTKE in the regions where particles are present ($\\phi_p>0$). While these fluctuations originate in the vicinity of particles, they can be transported with the mean flow into regions devoid of particles where $\\phi_p=0$ (see Fig.~\\ref{fig:PTKE}). Another challenge is distinguishing the sub-grid scale velocity fluctuations originating from pre-existing turbulence (via a classical energy cascade) and those induced by particles (pseudo turbulence). With this in mind, \\citet{shallcross2020volume} rigorously derived a transport equation for the PTKE based on volume filtering, given by\n\\begin{equation}\\label{eq:k}\n\\frac{\\partial \\phi_f \\rho_f k_f}{\\partial t} + \\nabla \\bcdot \\left(\\phi_f \\rho_f \\bm{u}_f k_f \\right) = -\\phi_f \\bm{R}_f \\bcol\\nabla \\bm{u}_f + \\left(\\bm{u}_p - \\bm{u}_f \\right) \\bcdot \\bm{F}_p   - \\phi_f \\rho_f \\varepsilon_{PT},\n\\end{equation}\nwhere the viscous and sub-filtered contributions are absorbed into $\\varepsilon_{PT}$, which represents dissipation of PTKE. The first term on the right-hand side of Eq.~\\eqref{eq:k} represents production due to mean shear--the usual turbulence production term in single phase flow. The second term on the right-hand side represents production due to particle drag that is only active when $\\phi_p>0$.\nIt was found that reconstructing $\\bm{R}_f$ from the transported PTKE via Eq.~\\eqref{eq:Rij} yields accurate predictions of the anisotropy for shock-particle interactions, despite the coefficients being derived for steady, incompressible flows. However, the results were shown to depend strongly on $\\varepsilon_{PT}$.\n\nFollowing what is typically done in single-phase turbulence modeling \\citep{vassilicos2015dissipation}, the dissipation rate can be modeled as $\\varepsilon_{PT}\\propto k_f/\\tau_\\varepsilon$, where $\\tau_\\varepsilon$ is a dissipation time scale that requires modeling. \\citet{vartdal2018using} proposed a simple model of the form $\\tau_\\varepsilon=l/\\sqrt{k_f}$, where the length scale $l$ can be related to $d_p$ or the inter-particle spacing. \\citet{shallcross2020volume} proposed a model that blends two time scales: one similar to the model by \\citet{vartdal2018using} away from particles; and another based on the relative velocity between the phases in the vicinity of particles. However, the model has only been tested under limited flow conditions. For broader applicability, a transport equation for the dissipation rate could be derived as well. Such an approach would of course lead to additional terms that require models. The increase in available compressible DNS data in recent years~\\citep[e.g.,][]{theofanous2017dynamics, theofanous2018shock, mehta2019Pseudo,shallcross2020volume} represent an exciting opportunity for developing such models valid across relevant two-phase flow conditions.\n\n\\section{Conclusions}\nAbove we present a survey of available models for high-speed gas--particle flows, with an emphasis on fluid-particle interactions. This class of flows gives rise to rich multiphase flow physics that pose significant challenges in predicting its behavior. While such flows are present in numerous applications, this article was primarily motivated by PSI during planetary and lunar landings. With future planned sample-return missions from Mars and eventual crewed missions to the Moon, Mars, and beyond, the risks associated with PSI are paramount. Simulating the harsh environment during touchdown using physics-based (EE and EL) approaches offers many advantages over the use of empirical correlations for macroscopic behavior like crater depth and erosion rate. However, the subgrid-scale models they rely on are largely based on data from incompressible flows or outdated experimental measurements.\n\n\\begin{figure}[h]\n\\subfigure[]{\\includegraphics[width=0.47\\textwidth]{Figs/3DPSI}\\label{fig:3DPSI}}\n\\subfigure[]{\\includegraphics[width=0.49\\textwidth]{Figs/jpdf}\\label{fig:jPDF}}\n\\centering\n\\caption{EL simulation of an underexpanded jet impinging on a granular bed highlighting the surface (I), crater (II), and ejecta (III). (a) Visualization of the granular surface ($\\phi_p=0.6$ contour shown in brown) and local gas-phase Mach number (red/yellow). (b) Joint-PDF (in log scale) of particle Mach number and volume fraction at the same instant. Adapted from \\citet{shallcross2021modeling}.}\n\\label{fig:PSI-EL}\n\\end{figure}\n\n\nIt should be noted that when the particle phase is densely packed (i.e., near close packing $\\phi_p>0.6$), a significant amount of power would be required to sustain a mean velocity difference between the phases at moderate Mach numbers. To highlight this, Fig.~\\ref{fig:PSI-EL} shows results from a recent EL simulation of an underexpanded jet impinging on a bed of monodisperse spherical particles \\citep{shallcross2021modeling}. The joint probability density function (PDF) of $\\Mac_p$ and $\\phi_p$ reveals high probability events of moderate Mach numbers at low $\\phi_p$ (corresponding to the ejecta above the crater), whereas $\\phi_p>0.4$ predominantly occurs when $\\Mac_p<0.1$. Thus, it \\textit{might} be possible to restrict model development at finite Mach numbers to dilute and moderately dense concentrations only.\n\nFinally, it should be noted that much of the flow physics discussed throughout this review are also present in many systems on Earth. For example, pyroclastic density currents are dangerous multiphase flows emanating from volcanic eruptions that operate under a wide range of Mach numbers and volume fractions \\citep{lube2020multiphase}. In addition, the detonation of a heterogeneous explosive results in rapid dispersal of high-speed solid particles \\citep{zhang2001explosive}, which can have tragic consequences as seen with the 2020 Beirut explosion \\citep{guglielmi2020beirut}.\n\n\n\\begin{tcolorbox}[arc=0mm,colback=lyellow,colframe=lyellow]\n\\subsection*{Summary points}\n\\begin{enumerate}\n\\item Compared to its incompressible counterpart, compressible gas--particle flows introduce new scales of motion (e.g., the shock wave thickness and acoustic time scale) that pose significant modeling challenges.\n    \\item The two-fluid equations for disperse multiphase systems are well known to be ill-posed. After more than four decades of attempts to remedy this, it was only recently rigorously resolved by \\citet{fox2020hyperbolic}, who showed that inclusion of the added mass and a fluid-mediated contribution to the particle-phase pressure tensor are needed to ensure hyperbolicity for arbitrary density ratios.\n    \\item The pseudo-turbulent Reynolds stress plays an important role at moderate volume fractions, and can contribute to a significant portion of the total kinetic energy during shock-particle interactions. It is also needed to ensure conservation.\n    \\item Existing models for the quasi-steady drag force can be broadly categorized into (i) single-particle correlations across Reynolds and Mach numbers; and (ii) multi-particle correlations exclusively developed for incompressible flow. The intersection between these two regimes, illustrated in Fig.~\\ref{fig:drag}, remain elusive.\n    \\item Mach number dependent drag laws used today have surprising origins from 18th- and 19th-century cannon fire experiments.\n    \\item \\citet{loth2021supersonic} was the first to incorporate data from numerical simulations to refine the $\\Mac_p-\\Rey_p$ drag coefficient for an isolated sphere where large experimental uncertainty exists.\n   \n   \n    \\item Unlike in incompressible gas--solid flows, particle motion in a compressible flow undergoing strong acceleration (e.g., in the presence of a shock wave) can be greatly influenced by unsteady forces.\n\\end{enumerate}\n\\end{tcolorbox}\n\n\\begin{tcolorbox}[arc=0mm,colback=lred,colframe=lred]\n\\subsection*{Future issues}\n\\begin{enumerate}\n    \\item With the advent of PR--DNS, the past two decades have seen an explosion of multi-particle drag correlations for incompressible flows. With PR--DNS of compressible gas--particle flows starting to come online,\n    We anticipate similar progress to be made for drag models at finite $\\Rey_p$, $\\phi_p$, \\textit{and} $\\Mac_p$.\n    \\item Following the form given in Eq.~\\eqref{eq:WenYu}, a natural choice for developing improved multi-particle compressible drag correlations would be to use the recent Reynolds and Mach number-dependent drag coefficient of \\citet{loth2021supersonic} for the drag acting on an isolated particle, and augment it with the Reynolds- and volume fraction-dependent correlation of \\citet{tenneti2011drag}. Its utility would need to assessed by PR--DNS.\n    \\item It is now recognized that flow past a collection of particles exhibits a distribution in drag forces with significant variance \\citep{akiki2016force,mehta2019effect,lattanzi2020stochastic}. Various models have recently been proposed to capture higher-order drag force statistics arising from neighbor-induced flow perturbations in EL \\citep{akiki2017pairwise,esteghamatian2018stochastic,seyed2020microstructure,lattanzi2021stochastic} and EE \\citep{lattanzi2021fluid} methods. Future research should leverage and extend these models to the compressible flow regime.\n    \\item Treating the hydrodynamic force as a stochastic variable offers some potential advantages over the classical BBO treatment of modeling each force contribution separately. Rather than attempting to tease out how each pair-wise neighbor interaction contributes to the hydrodynamic force on a given particle,  \\citet{lattanzi2020stochastic} demonstrated that the statistics obtained from treating the force as a stochastic variable are reconcilable with PR--DNS.\n    \\item While models for disperse two-phase flows have traditionally focused on hydrodynamic interactions, many applications of compressible gas--particle flows operate at elevated temperatures where thermal effects become important. Rapid changes in particle temperature can result in melting or initiate chemical reactions. Thus, improved models for inter-phase heat transfer are needed. Progress in this area can be found in \\citet{ling2016inter}.\n    \\item This article focused exclusively on models for \\textit{spherical} particles. Lunar and martian regolith is often polydisperse, and can be highly non spherical. This has important consequences on fluid-particle interactions (e.g., drag and lift) in addition to particle-particle interactions (e.g., internal friction, angle of repose). Special care needs to be taken to incorporate such effects in a manner that does not result in introducing `tuning' parameters that can be used to fit data.\n    \\item EE methods rely on models for particle-phase rheology obtained from kinetic theory of granular flows. Future research is warranted to validate or extend these models to compressible flows.\n\\end{enumerate}\n\\end{tcolorbox}\n\n\\section*{Acknowledgements} \nThe author thanks Professor Jason Rabinovitch for fruitful discussions and comments related to PSI, and Drs. Magnus Vartdal and Andreas Osnes for providing critical feedback on various modeling aspects. This work was supported by the National Aeronautics and Space Administration (NASA) under grant number 80NSSC20K1868.\n\n", "meta": {"timestamp": "2021-09-09T02:24:19", "yymm": "2109", "arxiv_id": "2109.02523", "language": "en", "url": "https://arxiv.org/abs/2109.02523"}}
{"text": "\\section{Introduction}\nIn this paper we prove new results in two areas of theoretical computer science that have received a lot of attention recently: \\emph{streaming algorithms} and \\emph{locally decodable codes}.\n\n\\emph{Streaming algorithms} is a model of computation introduced by Alon, Matias and Szegedy~\\cite{alon1999space} (for which they won the G{\\\"o}del Prize in 2005) in order to understand the space complexity of approximation algorithms to solve problems. In the last decade, there have been several results in the direction of proving upper and lower bounds for streaming algorithms for combinatorial optimization problems~\\cite{verbin2011streaming,goel2012communication,kapralov2014streaming,guruswami2017streaming,kapralov20171+,kapralov2019optimal,DBLP:conf/approx/GuruswamiT19,DBLP:conf/focs/ChouGV20,chou2021approximabilityA,chou2021approximabilityB,assadi2021simple,chen2021almost}. \nThe  goal here is to obtain a  $1/\\gamma$ approximation (for some $\\gamma \\leq 1$) of the optimum value of the combinatorial optimization problem with as little space as possible.\nOne favourite problem considered by many works is the well-known \\emph{Max-Cut}, or its generalization over large alphabets $\\ensuremath{\\mathbb{Z}}_r$, \\emph{Unique Games}. Here, giving a $2$-approximation algorithm for Max-Cut on $n$ vertices can be done in logarithmic space, while a sequence of works~\\cite{kapralov2014streaming,kapralov20171+,kapralov2019optimal} showed that getting a $(2-\\varepsilon)$-approximation requires linear space, matching the upper bound by~\\cite{ahn2012graph}. A similar, but less optimized, scenario was observed for Unique Games, i.e., there is a threshold behaviour in complexity going from $r$ to $(r-\\varepsilon)$-approximation. Curiously, many of these lower bounds were proven via variants of a problem called \\emph{Boolean Hidden Matching} $(\\ensuremath{\\mathsf{BHM}}$) and it is well known that $\\ensuremath{\\mathsf{BHM}}$ can be solved using logarithmic \\emph{quantum space}, so a natural question is, could quantum space help solving these combinatorial optimization problems? One corollary from~\\cite{kapralov2014streaming,shi2012limits} is that obtaining the \\emph{strong} $(1+\\varepsilon)$-approximation factor for Max-Cut and Unique Games streaming algorithms is quantum-hard.  However, understanding the space complexity of streaming in the widely-studied, weaker regime of $(2-\\varepsilon)$-approximation (for Maxcut) or $(r-\\varepsilon)$-approximation (for Unique games over $\\ensuremath{\\mathbb{Z}}_r$) algorithms, it is still unclear whether there could be any savings in the quantum regime.\n\n\\emph{Locally decodable codes} ($\\mathsf{LDC}$s) are error correcting codes $C:\\Sigma^n\\rightarrow\\Gamma^N$ (for alphabets $\\Sigma,\\Gamma$) that allow transmission of information over noisy channels. By querying a few locations of a noisy codeword $\\tilde{C}(x)$, one needs to reconstruct an arbitrary coordinate of $x\\in \\Sigma^n$ with probability at least $1/|\\Sigma|+\\varepsilon$. The main goal in this field is to understand trade-offs between $N$ and $n$. \n$\\mathsf{LDC}$s have found several applications in pseudorandom generators, hardness\namplification, private information retrieval schemes, cryptography, complexity theory (refer to~\\cite{yekhanin2012locally,gopi2018locality} for a detailed exposition). Despite their ubiquity, $\\mathsf{LDC}$s are not well understood, even with the simplest of case of \\emph{$2$-query $\\mathsf{LDC}$s}. \nFor the case when $\\Sigma=\\Gamma=\\{0,1\\}$, exponential lower bounds of $N=2^{\\Omega(n)}$  were established over two decades back~\\cite{goldreich2002lower,DBLP:journals/jcss/KerenidisW04,dvir2007locally}. In contrast, a breakthrough result of Dvir and Gopi~\\cite{dvir20162} in 2015 showed how to construct $2$-query $\\mathsf{LDC}$s with \\emph{subexponential} length in the regime when $\\Sigma=\\{0,1\\}$ and $\\Gamma$ is a finite field $\\ensuremath{\\mathbb{F}}_N$. Despite these results, our knowledge of such $N$ and $n$ trade-offs for $2$-query $\\mathsf{LDC}$s is still lacking, specially for the not very well studied case when $\\Sigma=\\Gamma=\\ensuremath{\\mathbb{Z}}_r$.\n\nPrior works that handled simpler versions of the questions above used one technical tool successfully: \\emph{hypercontractivity} for real-valued functions over the Boolean cube. Since we are concerned with  proving quantum lower bounds for streaming algorithms and establishing lower bounds for $\\mathsf{LDC}$s when the input alphabet is over $\\ensuremath{\\mathbb{Z}}_r$, it leads us to the following main question: \n    \\emph{Is there a version of hypercontractivity for matrix-valued functions over~$\\ensuremath{\\mathbb{Z}}_r$?}\n\n\\subsection{Our results} \n\nSummarizing our main contributions, we first prove a version of hypercontractivity for matrix-valued functions $f:\\ensuremath{\\mathbb{Z}}_r^n\\rightarrow\\mathbb{C}^{m\\times m}$. The proof of this  crucially relies on proving uniform convexity for trace norms of $r$ matrices, which in turn {generalizes} the powerful $2$-uniform convexity by Ball, Carlen and Lieb~\\cite{ball1994sharp}. Using this new hypercontractivity theorem, we prove our two applications.\n\nFirst, we prove a quantum space lower bound for streaming algorithms.  It is easy to see that obtaining a $2$-approximation algorithm for Max-$k$-Cut on $n$ vertices in the classical streaming model can  be done in $O(\\log n)$ space, and we show that obtaining a $1.99$-approximation algorithm in the adversarial model requires $\\Omega(n^{1-2/t})$ quantum space or $\\Omega(n^{1-1/t})$ classical space. As far as we are aware, this is the first quantum space lower bound for an optimization problem. Although our lower bounds apply to the adversarial model, while prior works of Kapralov, Khanna and Sudan~\\cite{kapralov2014streaming} and the mathematical tour-de-force result of Kapralov and Krachun~\\cite{kapralov2019optimal} obtained an $\\Omega(n)$ classical space lower bound for $(2-\\varepsilon)$-approximation in the \\emph{random} model, our proofs are significantly simpler. We further generalize our results to the case of $t$-hyperedge hypergraphs with vertices taking values over $\\ensuremath{\\mathbb{Z}}_r$. These hypergraphs can naturally be viewed as instances of Unique Games wherein the constraints are over $\\ensuremath{\\mathbb{Z}}_r$. Here again, we prove that obtaining an $r$-approximation algorithm requires $O(\\log n)$ classical space and obtaining a $(r-\\varepsilon)$-approximation algorithm requires $\\Omega(n^{1-1/t})$ classical space or $\\Omega(n^{1-2/t})$ quantum space. \n\n\nSecond, we show an $N= 2^{\\Omega(n/r^4)}$ lower bound for (even non-linear) $\\mathsf{LDC}$s over $\\ensuremath{\\mathbb{Z}}_r$. In particular, for all $r$ smaller than $n^{1/4}$, we prove an exponential in $n$ lower bound for $\\mathsf{LDC}$s over $\\ensuremath{\\mathbb{Z}}_r$. Previous main results in this direction were by Goldreich et al.~\\cite{goldreich2002lower} for $r=2$ and linear $\\mathsf{LDC}$s, Kerenidis and de Wolf~\\cite{DBLP:journals/jcss/KerenidisW04} for $r=2$ and \\emph{non-linear} $\\mathsf{LDC}$s, Wehner and de Wolf~\\cite{wehner2005improved} for non-linear $\\mathsf{LDC}$s from $\\{0,1\\}^n\\to\\ensuremath{\\mathbb{Z}}_r^N$ and finally by Dvir and Shpilka~\\cite{dvir2007locally} for $r>2$ but linear $\\mathsf{LDC}$s. Apart from the result of~\\cite{dvir2007locally}, we are not aware of any lower bounds for non-linear $\\mathsf{LDC}$s  from $\\ensuremath{\\mathbb{Z}}_r^n\\to\\ensuremath{\\mathbb{Z}}_r^N$, even though it is a very natural question with connections to other fundamental problems, such as polynomial identity testing~\\cite{dvir2007locally}, private information retrieval~\\cite{katz2000efficiency,goldreich2002lower}, additive combinatorics~\\cite{briet2016outlaw} and quantum complexity theory~\\cite{aaronson2018pdqp}, to cite a few. Furthermore, we are not aware of a formal reduction between $\\mathsf{LDC}$s with $\\Sigma=\\{0,1\\}$ and $\\Sigma=\\ensuremath{\\mathbb{Z}}_r$, specially with recovery probability $1/|\\Sigma| + \\varepsilon$.\nMoreover, some past works define $\\mathsf{LDC}$s over general $\\Sigma$ with success probability $\\geq \\operatorname{Pr}[\\text{wrong output}] + \\varepsilon$~\\cite{gopi2018locality}, $\\geq 1/2 + \\varepsilon$~\\cite{goldreich2002lower} or $\\geq 1-\\varepsilon$~\\cite{dvir2011matrix}. These alternative definitions are encompassed by ours by considering $\\varepsilon$ a constant large enough. In the remaining part of the introduction, we describe these contributions in more detail. \n\n\n\n\n\\subsection{Matrix hypercontractive inequality (over large alphabets)}\n\\paragraph{Fourier analysis on the Boolean cube.} We first discuss the basics of Fourier analysis before stating our result. Let $f:\\{0,1\\}^n\\rightarrow \\ensuremath{\\mathbb{R}}$ be a function, then the Fourier decomposition of $f$ is\n$$\nf(x)=\\sum_{S\\in\\{0,1\\}^n} \\widehat{f}(S)(-1)^{S\\cdot x},\n$$\nwhere $S\\cdot x = \\sum_{i=1}^n S_ix_i$ (where the sum is over $\\{0,1\\}$) and the \\emph{Fourier coefficients} of $f$ are defined as $\\widehat{f}(S)=\\mathbb{E}_x[f(x) (-1)^{S\\cdot x}]$, the expectation taken over uniformly random $x\\in\\{0,1\\}^n$. One of the technical tools in the area of theoretical computer science is the hypercontractivity theorem proven by Bonami and Beckner~\\cite{bonami1970etude,beckner1975inequalities}. In order to understand the hypercontractivity theorem, we first need to define the noise operator: for a noise parameter $\\rho \\in [-1,1]$, let $\\operatorname{T}_\\rho$ be an operator on the space of functions $f:\\{0,1\\}^n\\rightarrow \\ensuremath{\\mathbb{R}}$ defined as \n$$\n    ({\\operatorname{T}}_\\rho f)(x)=\\operatorname*{\\mathbb{E}}_{y\\sim \\mathcal{N}_\\rho(x)}[f(y)],\n$$\nwhere $y\\sim \\mathcal{N}_\\rho(x)$ denotes that the random string $y\\in\\{0,1\\}^n$ is drawn as $y_i=x_i$ with probability $\\frac{1}{2} + \\frac{1}{2}\\rho$ and as $y_i=x_i\\oplus 1$ with probability $\\frac{1}{2} - \\frac{1}{2}\\rho$ for each $i\\in[n]$ independently. One can show that the Fourier expansion of ${\\operatorname{T}}_\\rho f$ can be written as\n$$\n({\\operatorname{T}}_\\rho f)(x)=\\sum_{S\\in\\{0,1\\}^n}\\rho^{|S|}\\widehat{f}(S)(-1)^{S\\cdot x}.\n$$\nOne way to intuitively view this expression is that ``large-weight\" Fourier coefficients are reduced by an exponential factor while ``small-weight\" Fourier coefficients remain approximately the same. Consequently, it is not hard to see that $\\|{\\operatorname{T}}_\\rho f\\|_p\\leq \\|f\\|_p$ for every $p\\geq 1$, where $\\|f\\|_p:=\\big(\\mathbb{E}_x[|f(x)|^p]\\big)^{1/p}$ is the standard normalized $p$-norm of the function $f$. The main hypercontractivity theorem states that the previous inequality holds true even if we increase the left-hand size by a larger norm (meaning that norms under the noise operator are not just contractive, but \\emph{hypercontractive}), i.e., for every $p\\in [1,2]$ and $\\rho \\leq \\sqrt{p-1}$, we have that $\\|{\\operatorname{T}}_\\rho f\\|_2\\leq \\|f\\|_p$,\\footnote{The hypercontractivity theorem can be stated for arbitrary $1\\leq p \\leq q$ and $\\rho \\leq \\sqrt{(p-1)/(q-1)}$, here we state it for $q=2$ since we will be concerned with this setting.} which can alternatively be written as\n\\begin{align}\n\\label{eq:basichypercontractivity}\n    \\left(\\sum_{S\\in \\{0,1\\}^n} \\rho^{2|S|} \\widehat{f}(S)^2\\right)^{1/2} \\leq \\left(\\frac{1}{2^n}\\sum_{x\\in\\{0,1\\}^n} |f(x)|^p\\right)^{1/p}.\n\\end{align}\nThis inequality has found several applications in approximation theory~\\cite{khot2007optimal,dinur2005hardness}, expander graphs~\\cite{hoory2006expander}, circuit complexity~\\cite{linial1993constant}, coding theory~\\cite{carlen1993optimal}, quantum computing~\\cite{gavinsky2007exponential,DBLP:journals/qic/Montanaro11} (for more applications we refer the reader to~\\cite{de2008brief,o2014analysis,montanaro2012some}). All these applications deal with understanding the effect of noise on real-valued functions on the Boolean cube. \n\n\n\n\\noindent \\textbf{Generalizations of hypercontractivity.} There are two natural generalizations of hypercontractivity: $(i)$ a hypercontractivity statement for arbitrary product probability spaces. In this direction, it is possible to prove a similar hypercontractive inequality: for every $p\\in [1,2]$ and $f\\in L^2(\\Omega_1\\times\\cdots\\times\\Omega_n, \\pi_1\\otimes\\cdots\\otimes\\pi_n)$, we have\n\\begin{align}\n\\label{eq:genhyperFr}\n    \\|{\\operatorname{T}}_\\rho f\\|_2 \\leq \\|f\\|_p \\hspace{1mm}\\text{ for  }\\hspace{1mm} \\rho \\leq \\sqrt{p-1}\\cdot\\lambda^{1/p-1/2},\n\\end{align} \nwhere $\\lambda$ is the smallest probability in any of the finite probability spaces $(\\Omega_i,\\pi_i)$ (see~\\cite[Chapter~10]{o2014analysis}). As a corollary, one gets a hypercontractive inequality for $f:\\ensuremath{\\mathbb{Z}}_r^n\\to\\mathbb{R}$; $(ii)$ a hypercontractivity statement for matrix-valued functions $f:\\{0,1\\}^n\\rightarrow \\mathbb{C}^{m\\times m}$, where the Fourier coefficients $\\widehat{f}(S)=\\mathbb{E}_x[f(x)(-1)^{S\\cdot x}]$ are now $m\\times m$ complex matrices. This was considered by Ben-Aroya, Regev and de Wolf~\\cite{ben2008hypercontractive}, who proved a hypercontractivity statement by using the powerful inequality of Ball, Carlen and Lieb~\\cite{ball1994sharp}.\n\nHowever, is there a generalization of hypercontractivity in both directions, i.e., a matrix-valued hypercontractivity for functions over $\\ensuremath{\\mathbb{Z}}_r$? This is  open as far as we are aware and is our first main technical result.\n\\begin{result}\n\\label{result:hyper}\n    For any $f:\\ensuremath{\\mathbb{Z}}_r^n\\rightarrow \\mathbb{C}^{m\\times m}$, $p\\in[1,2]$ and \n    $\\rho\\leq \\sqrt{\\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)}}$,\n    %\n    \\begin{align}\n    \\label{eq:resulthypereq}\n        \\left(\\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^n} \\rho^{2|S|} \\|\\widehat{f}(S)\\|_p^2\\right)^{1/2} \\leq \\left(\\frac{1}{r^n}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\|f(x)\\|_p^p\\right)^{1/p},\n    \\end{align}\n    %\n    where $\\|M\\|_p :=\\big(\\sum_i\\sigma_i(M)^p\\big)^{1/p}$ is the Schatten $p$-norm defined from the singular values $\\{\\sigma_i(M)\\}_i$ of the matrix $M$ and $|S|:=|\\{i\\in[n]: S_i\\neq 0\\}|$ is the Hamming weight of $S\\in\\ensuremath{\\mathbb{Z}}_r^n$.\n\\end{result}\nThe above result can be seen as an analogue of Eq.~\\eqref{eq:basichypercontractivity} where the absolute values are replaced with Schatten norms. We now make a couple of remarks. First, when $m=1$ our result compares to the one in Eq.~\\eqref{eq:genhyperFr} for $f:\\ensuremath{\\mathbb{Z}}_r^n\\to\\mathbb{R}$, but with a slightly worse $\\rho$ parameter compared to the $(1/r)^{1/p-1/2}$ factor.  Second, for $r=2$ we recover the same inequality from~\\cite{ben2008hypercontractive}.  The proof of this result is rather mathematical and not-so-intuitive. To this end, as in the proof of hypercontractive inequalities~\\cite{o2014analysis,ben2008hypercontractive}, our result follows by induction on $n$. It so happens that the base case is the most non-trivial step in the proof. So for now, let us assume $n=1$, i.e., our goal is to prove Eq.~\\eqref{eq:resulthypereq} for $n=1$.  We now consider  two special \\emph{simple} cases of the~inequality.\n\n\\emph{(i)} $r=2$ and $\\mathbb{C}^{m\\times m}$ is replaced with real numbers: in this case, this is the well-known two-point inequality by Gross~\\cite{gross1975logarithmic} used for understanding the Logarithmic Sobolev inequalities. A proof of this inequality can also be easily viewed from a geometric perspective. As far as we are aware, there is no generalized $r$-point inequality for $r>2$.\n    \n    \\emph{(ii)} $r=2$ and $\\mathbb{C}^{m\\times m}$ are arbitrary matrices: in this case, we only need to deal with two matrices $f(0),f(1)$ and Eq.~\\eqref{eq:resulthypereq} is exactly a powerful inequality in functional analysis, called the \\emph{$2$-uniform convexity} of trace norms,\n    $$\n        \\left(\\frac{\\|X+Y\\|_p^p + \\|X-Y\\|_p^p}{2}\\right)^{2/p} \\geq \\|X\\|_p^2 + (p-1)\\|Y\\|_p^2.\n    $$\n    This inequality was first proven for certain values of $p$ by Tomczak-Jaegermann~\\cite{tomczak1974moduli} before being extended for all $p\\in[1,2]$ by Ball, Carlen and Lieb~\\cite{ball1994sharp} in 1994. Since then it has found several applications, e.g.\\ an optimal hypercontractivity inequality for Fermi fields~\\cite{carlen1993optimal}, regularized convex optimization~\\cite{duchi2010composite} and metric embedding~\\cite{lee2004embedding,naor2016spectral}. {$2$-uniform convexity can also be used to prove a variety of other inequalities, for example, Khintchine inequality~\\cite{tomczak1974moduli,davis1984complex}, Hoeffding and Bennett-style bounds~\\cite{pinelis1994optimum,howard2020time}}. Moreover, the above result could be seen as a corollary of Hanner's inequality for matrices (originally proven for Lebesgue spaces $L_p$~\\cite{hanner1956uniform}), but, unfortunately, Hanner's inequality for Schatten trace ideals are only proven for $p\\leq 4/3$ (see more in~\\cite{ball1994sharp}). As far as we are aware, a generalization of the above inequality when considering $r$ matrices was unknown.\n\n\n\n\nOne contribution in our work is the following generalization of a  result from Ball, Carlen and Lieb~\\cite{ball1994sharp} (note it also implies a generalization of the two-point inequality), which we believe may be of independent interest.\n\\begin{result}\n\\label{res:bcl}\n    Let $r\\in\\mathbb{Z}$, $r\\geq 2$. Let $\\omega_r := e^{2i\\pi/r}$, $A_0,\\ldots,A_{r-1}\\in \\ensuremath{\\mathbb{C}}^{n\\times n}$ and $p\\in [1,2]$, then\n    %\n    \\begin{align}\n\t\t\\left(\\frac{1}{r} \\sum_{k=0}^{r-1}\\left\\|\\sum_{j=0}^{r-1} \\omega_r^{jk} A_j\\right\\|_p^p\\right)^{2/p} \\geq\\left\\|A_0 \\right\\|_p^2 + \\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)}\\sum_{k=1}^{r-1} \\left\\| A_k\\right\\|_p^2.\\label{eq:eqball-intro}\n\t\\end{align}\n\\end{result}\n\nNow that we have established Result~\\ref{res:bcl}, the proof of Result~\\ref{result:hyper} is a simple induction argument on $n$: for the base case $n=1$, Result~\\ref{result:hyper} is exactly Result~\\ref{res:bcl}, and proving the induction step requires an application of Minkowski inequality. Since this proof is very similar to the one in~\\cite{ben2008hypercontractive}, we omit the details here. \n\n\n\n\n\\subsection{Application 1: Streaming algorithms}\nApproximation algorithms for combinatorial optimization problems have been a rich area of study in theoretical computer science. One of the most famous approximation algorithms is by Goemans and Williamson~\\cite{goemans1995improved} who proved that one can obtain a $1/0.878$-approximation algorithm in \\emph{polynomial time} for Max-Cut using semi-definite programming and this is believed to be optimal assuming the Unique Games conjecture is true~\\cite{khot2007optimal}. In the past few years there has been a sequence of works~\\cite{kapralov2014streaming,guruswami2017streaming,DBLP:conf/approx/GuruswamiT19,kapralov2019optimal,DBLP:conf/focs/ChouGV20,chou2021approximabilityA,chou2021approximabilityB} that tried to prove \\emph{unconditional} hardness of combinatorial optimization (e.g.\\ the Max-Cut problem) in the well-known streaming model of computation by Alon, Matias and Szegedy~\\cite{alon1999space}.\n\nIn the streaming model, the goal is to optimize the amount of \\emph{space} needed to solve a problem rather than time, and output a value which is at least a fraction $1/\\gamma$ of the optimum value with high probability. Many recent works referenced above have shown interesting threshold theorems, for example, for the Max-Cut problem, getting a $2$-approximation algorithm using $O(\\log n)$ classical space is easy: one simply counts the number of edges in the graph (which requires only a counter of size $2\\log n$) and outputs half this count. Moreover, one can obtain a graph sparsifier using $O(n/\\varepsilon^2)$ space~\\cite{ahn2012graph} and, from it, a $(1+\\varepsilon)$-approximation for the Max-Cut value. On the other hand, Kapralov et al.~\\cite{kapralov2014streaming} initiated the study of proving streaming lower bounds for Max-Cut in the random-edge model (where inputs arrive randomly, and not necessarily adversarially), and in this work they showed that one requires $\\Omega(\\sqrt{n})$ space for $(2-\\varepsilon)$-approximations in an $n$-vertex graph, together with a classical lower bound $\\Omega(n^{1-\\varepsilon})$ for $(1+\\varepsilon)$-approximations in the adversarial model (their proof, together with Result~\\ref{res:lowerHHM} below, immediately implies a similar quantum lower bound for $(1+\\varepsilon)$-approximation). After a sequence of works, Kapralov and Krachun~\\cite{kapralov2019optimal} finally obtained an $\\Omega(n)$ space lower bound for $(2-\\varepsilon)$-approximations even in the random-edge model.\n\nA common technique to prove streaming lower bounds is via communication complexity.  To see this, suppose a problem $P$ has inputs $(X,Y)$ and the goal is to find space-efficient streaming algorithms to compute $P(X,Y)$, when $X,Y$ are presented in a stream (i.e., presented bit-by-bit). Then, one way to lower bound the \\emph{space complexity} is to prove lower bounds on the following problem: consider the one-way communication problem where Alice gets the input $X$, Bob gets the input $Y$, their goal is to compute $P(X,Y)$ and only Alice is allowed to communicate to Bob. Then one can show that any lower bound for randomized one-way communication implies an equivalent lower bound for streaming algorithms. This technique has been used by a sequence of papers to prove lower bounds on space complexity of Max-Cut~\\cite{kapralov2014streaming,kapralov2019optimal,DBLP:conf/approx/GuruswamiT19}, matching~\\cite{goel2012communication}, Max-CSP~\\cite{guruswami2017streaming,DBLP:conf/focs/ChouGV20,chou2021approximabilityA,chou2021approximabilityB} and counting cycles~\\cite{verbin2011streaming,assadi2021simple}. One problem that is used often in this direction is a variant of the Boolean Hidden Matching. \n\n\\subsubsection{Hidden Matching and its variants}\nThe Boolean Hidden Matching ($\\ensuremath{\\mathsf{BHM}}$) problem was introduced  by Bar-Yossef et al.~\\cite{bar2004exponential} (which was in turn inspired by Kerenidis and de Wolf~\\cite{DBLP:journals/jcss/KerenidisW04} for proving $\\mathsf{LDC}$ lower bounds) in order to prove exponential separations between quantum and classical one-way communication complexities. Below we described the generalized Hidden Matching problem over larger alphabets and hypermatching. \nThe $r$-ary Hidden Hypermatching ($r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$) problem is a two-party communication problem between Alice and Bob:  Alice is given $x\\in \\ensuremath{\\mathbb{Z}}_r^n$ and Bob is given a string $w\\in \\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}$ and $\\alpha n/t$-many disjoint $t$-tuples (for $\\alpha\\in(0,1]$), i.e., hyperedges of an $\\alpha$-partial hypermatching, which can also be viewed as an incident matrix $M\\in \\{0,1\\}^{\\alpha n/t\\times n}$ (each row corresponding to a hyperedge). In the $\\ensuremath{\\mathsf{YES}}$ instance it is promised that $w=Mx$ (over $\\ensuremath{\\mathbb{Z}}_r$), while in the $\\ensuremath{\\mathsf{NO}}$ instance it is promised that $w$ is uniformly random, and the goal is to decide which is the case using a message sent from Alice to Bob.\n\nThere have been a few lines of work in understanding the problem of Hidden Hypermatching: $(i)$ the seminal work of Bar-Yossef et al.~\\cite{bar2004exponential} and Gavinsky et al.~\\cite{gavinsky2007exponential} showed that, for $r=t=2$, $\\ensuremath{\\mathsf{BHM}}$ can be solved using $O(\\log n)$ qubits but requires $\\Omega(\\sqrt{n})$ classical bits of communication; $(ii)$ Verbin and Yu~\\cite{verbin2011streaming} considered the problem where $r=2$ and $t\\geq 2$ (which in fact inspired many follow-up works on using hypermatching for classical streaming lower bounds) and showed a classical lowed bound of $\\Omega(n^{1-1/t})$, which was subsequently generalized to a $\\Omega(n^{1-2/t})$ quantum lower bound by Shi, Wu and Yu~\\cite{shi2012limits}; {$(iii)$ Guruswami and Tao~\\cite{DBLP:conf/approx/GuruswamiT19} studied the problem for when $t=2$ and $r\\geq 2$, proving a classical $\\Omega(\\sqrt{n})$ lower bound.} A natural question is then, what is the quantum and classical communication complexities for $r,t\\geq2$? In this paper, we give both upper and lower bounds for the Hidden Hypermatching problem for every $r$ and $t$.\n\n\n\n\\paragraph{Upper bounds on Hidden Hypermatching.} For a given $t\\geq2$, the same classical communication protocol for $r=2$ can be used for general $r>2$. The idea is that Alice picks $O((n/\\alpha)^{1-1/t})$ entries of $x$ uniformly at random to send to Bob. By the Birthday Paradox, with high probability Bob will obtain all the values from one of his hyperedges $i$, and thus can compare $(Mx)_i$ with the corresponding $w_i$. If they are equal, he outputs $\\ensuremath{\\mathsf{YES}}$, otherwise he outputs $\\ensuremath{\\mathsf{NO}}$, which leads to an one-side error of $O(1/r)$. The total amount of communication is $O(\\log{(rn)}(n/\\alpha)^{1-1/t})$ bits.\\footnote{ One can  further improve this complexity to $O(\\log{(n\\log{r})} + (\\log r)\\cdot (n/\\alpha)^{1-1/t})$ by Newman's theorem~\\cite{newman1991private}.} The situation is more interesting in the quantum setting. For $t=2$, we prove that Hidden Hypermatching can be solved using only a logarithmic amount of qubits for every $r=\\poly(n)$.\n\\begin{result}\n\\label{res:upperBHM}\n    There is a protocol for $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,2,n)$ with one-sided error $1/3$ using $O(\\log{(nr)}/\\alpha)$~qubits.\n\\end{result}\nThe above bound uses a non-trivial procedure that allows to learn the sum of two numbers modulo $r$ by using just one ``query'' and crucially uses the knowledge of the string $w$: given a suitable superposition of two numbers, one can obtain their sum with one-sided error by using one measurement. As far as we are aware, such a statement was not known prior to our work. However, the knowledge of $w$ is vital, which means that the protocol does not work for more general settings where there is no promise on the inputs (e.g.\\ a relational version of the $r$-ary Hidden Hypermatching problem where Bob must output one hyperedge $i$ and its corresponding value $(Mx)_i$), and it also cannot be used as a building block for the general case $t,r>2$. The current upper bound on the quantum communication complexity of the $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ problem with $t,r> 2$ thus matches the classical one. In view of the lower bounds stated below, we hence make the following conjecture. \n\\begin{conjecture}\n    If $t,r>2$, there is a protocol for $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ using $O(\\log{(rn)}(n/\\alpha)^{1-1/\\lceil t/2\\rceil})$~qubits.\n\\end{conjecture}\n\n\\paragraph{Lower bounds on Hidden Hypermatching.} The standard approach for proving a lower bound on the amount of communication required to solve the Hidden Hypermatching problem is via Fourier analysis. In the classical proofs of Gavinsky et al.~\\cite{gavinsky2007exponential}, Verbin and Yu~\\cite{verbin2011streaming} and Guruswami and Tao~\\cite{DBLP:conf/approx/GuruswamiT19}, the total variation distance between the probability distributions arising from the $\\ensuremath{\\mathsf{YES}}$ and $\\ensuremath{\\mathsf{NO}}$ instances is bounded using the inequality of Kahn, Kalai and Linial~\\cite{kahn1989influence} (which can be seen as a corollary of the hypercontractivity inequality). On the other hand, Shi, Wu and Yu~\\cite{shi2012limits} obtained a quantum lower bound by bounding the Schatten 1-norm between the possible density matrices received by Bob in both $\\ensuremath{\\mathsf{YES}}$ and $\\ensuremath{\\mathsf{NO}}$ instances via the matrix-valued hypercontractivity from Ben-Aroya, Regev and de Wolf~\\cite{ben2008hypercontractive}. We follow a similar approach by using our \\emph{generalized matrix-valued hypercontractive} inequality from Result~\\ref{result:hyper} in order to obtain the following lower bound (note that, for $r=2$, our lower bound is exponential better in $\\alpha$ compared to~\\cite{shi2012limits}).\n\n\\begin{result}\n    \\label{res:lowerHHM}\n    Every constant-bias protocol for the $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ problem with $t,r\\geq 2$ requires at least $\\Omega((n/t)^{1-2/t}/\\alpha^{2/t})$ qubits of communication or $\\Omega((n/t)^{1-1/t}/\\alpha^{1/t})$ bits of communication.\n\\end{result}\n\n\n\n\\subsubsection{Relation to streaming lower bounds}\nAs mentioned at the start of this section, using one-way communication complexity lower bounds has been a common technique used by several recent works~\\cite{verbin2011streaming,kapralov2014streaming,guruswami2017streaming,DBLP:conf/approx/GuruswamiT19,DBLP:conf/focs/ChouGV20} to prove streaming lower bounds. Using our classical and quantum communication lower bound  we present two lower bounds for streaming problems. \n\nThere are a few natural generalizations to Max-Cut. One is Max-$k$-Cut, i.e., finding the maximum cut value on a hypergraph with $k$-sized hyperedges. Clearly, the lower bound of~\\cite{kapralov2019optimal} holds true for Max-$k$-Cut, but could one prove better lower bound depending on $k$? Another is the Unique Games problem, a constraint satisfaction problem defined on a graph, where a linear constraint (a permutation) over $\\ensuremath{\\mathbb{Z}}_r$ is specified on each edge and the goal is to find a vertex assignment over $\\ensuremath{\\mathbb{Z}}_r$ that maximizes the number of satisfied constraints. When $r=2$, Unique Games reduces to Max-Cut. Guruswami and Tao~\\cite{DBLP:conf/approx/GuruswamiT19} studied the streaming complexity of the Unique Games problem and proved a lower bound of $\\Omega(\\sqrt{n})$ in the adversarial model by using a reduction to Hidden Matching over $\\ensuremath{\\mathbb{Z}}_r$  and the same bound was obtained in~\\cite{chou2021approximabilityB} for a larger set of problems including Unique Games.\n\nHere we join both directions, i.e., Max-$k$-Cut and the standard Unique Games problem, into a generalized version of Unique Games defined on a hypergraph and obtain streaming classical and quantum lower bounds in the adversarial model for any value $k,r\\geq 2$. \n\\begin{result}\n    Every streaming algorithm giving a $(r-\\varepsilon)$-approximation for Unique Games on $k$-hyperedge $n$-vertex hypergraphs over $\\ensuremath{\\mathbb{Z}}_r$ uses $\\Omega(n^{1-2/k})$ quantum space or $\\Omega(n^{1-1/k})$ classical space.\n\\end{result}\nThe above result clearly generalizes the work of Guruswami and Tao~\\cite{DBLP:conf/approx/GuruswamiT19}. Compared to Kapralov and Krachun~\\cite{kapralov2019optimal}, on the one hand our results are for the weaker adversarial model, and they obtained a stronger linear lower bound, but on the other hand, their result does not immediately generalize for $\\ensuremath{\\mathbb{Z}}_r$ and is a purely classical proof (in fact we remark that our classical lower bound is significantly simpler than their work). \nAs far as we are aware, these are the first quantum lower bounds for Unique Games and Max-$k$-Cut in the streaming model.\n\n\n\\subsection{Application 2: Locally decodable codes}\n\nA locally decodable  code ($\\mathsf{LDC}$) is an error correcting code that allows to retrieve a single bit of the original message (with high probability) by only examining a few bits in a corrupted codeword. More formally, a $(q,\\varepsilon,\\delta)$-$\\mathsf{LDC}$ was defined by Katz and Trevisan~\\cite{katz2000efficiency} as a function $C:\\ensuremath{\\mathbb{Z}}_r^n\\rightarrow \\ensuremath{\\mathbb{Z}}_r^N$ that satisfies the following: for all $x\\in \\ensuremath{\\mathbb{Z}}_r^n$, $i\\in [n]$ and $y\\in\\ensuremath{\\mathbb{Z}}_r^N$ that satisfies $d(C(x),y)\\leq \\delta$ (i.e., a $\\delta$-fraction of the elements of $C(x)$ are corrupted), there exists an algorithm $\\ensuremath{\\mathcal{A}}$ that makes $q$ queries to $y$ non-adaptively and outputs $\\mathcal{A}^{y}(i)\\in \\ensuremath{\\mathbb{Z}}_r$ such that $\\Pr[\\mathcal{A}^{y}(i)=x_i]\\geq 1/r+\\varepsilon$ (where the probability is over the randomness of $\\ensuremath{\\mathcal{A}}$). Over $\\{0,1\\}$, $\\mathsf{LDC}$s have found several applications in private information retrieval~\\cite{chor1995private}, multiparty computation~\\cite{ishai2004hardness}, data structures~\\cite{chen2009efficient} and average-case complexity theory~\\cite{trevisan2004some}.\n\nThe natural question in constructing $\\mathsf{LDC}$s is the trade-off between $N$ and $n$. A well-known $2$-query $\\mathsf{LDC}$ is the Hadamard encoding that maps $x\\in \\ensuremath{\\mathbb{Z}}_r^n$ into the string $C(x)=(\\langle x,y\\rangle)_{y\\in \\{0,1\\}^n}$: on input $i\\in [n]$, a decoding algorithm queries $C(x)$ at a uniformly random $y$ and $y+e_i$ and retrieves $C(x)=\\langle x,y\\oplus e_i\\rangle-\\langle x,y\\rangle$, where $e_i = 0^{i-1}10^{n-i}$. Here the encoding length is $N=2^n$, and an important question is, are there $2$-query $\\mathsf{LDC}$s with $N\\ll 2^n$? For the case $r=2$, Goldreich et al.~\\cite{goldreich2002lower} showed a lower bound $N= 2^{\\Omega(n)}$ for \\emph{linear codes}, which was later improved by Obata~\\cite{obata2002optimal}. Later, Kerenidis and de Wolf~\\cite{DBLP:journals/jcss/KerenidisW04} proved an exponential lower bound for \\emph{non-linear codes} using a quantum argument!\\footnote{For simplicity in exposition, we omit the dependence on $\\delta,\\varepsilon$ in these lower bounds.} \n\nThis left open the setting where $r>2$. Following these works, {for $2$-query \\emph{non-linear} $\\mathsf{LDC}$s $C:\\{0,1\\}^n\\to\\ensuremath{\\mathbb{Z}}_r^N$ (note the inputs are over $\\{0,1\\}$ and not $\\ensuremath{\\mathbb{Z}}_r$), Wehner and de Wolf~\\cite{wehner2005improved} proved the lower bound $N=2^{\\Omega(n/r^2)}$.} On the other hand, Dvir and Shpilka~\\cite{dvir2007locally} showed a lower bound of $N= 2^{\\Omega(n)}$ for every $2$-query \\emph{linear} $\\mathsf{LDC}$ $C:\\ensuremath{\\mathbb{Z}}_r^n\\to\\ensuremath{\\mathbb{Z}}_r^N$, even independent of the field size. To prove their result, they crucially observed that, given a linear $\\mathsf{LDC}$ over $\\ensuremath{\\mathbb{Z}}_r$, one can construct a linear $\\mathsf{LDC}$ over $\\{0,1\\}$ (with almost the same parameters) and then invoked the result of Goldreich et al.~\\cite{goldreich2002lower}. This reduction, however, fails for non-linear codes and motivates if there are\n\\emph{non-linear} $\\mathsf{LDC}$s $C:\\ensuremath{\\mathbb{Z}}_r^n\\to\\ensuremath{\\mathbb{Z}}_r^N$ with $N\\ll 2^n$? \n\nThe main contribution here is a lower bound for \\emph{non-linear} $\\mathsf{LDC}$s over $\\ensuremath{\\mathbb{Z}}_r$ that scale as $2^{\\Omega(n/r^4)}$, and which gives a super-polynomial lower bound for $r=o(n^{1/4})$. Our lower bound comes from using our hypercontractive inequality in Result~\\ref{result:hyper}. The idea is similar to the one from~\\cite{ben2008hypercontractive}, but more technical as a result of optimizing the dependence on $r$. A large $r^2N\\times r^2N$ matrix with rank $1$ is constructed from a given $2$-query $\\mathsf{LDC}$.\nBy considering its Fourier transform over $\\ensuremath{\\mathbb{Z}}_r$, there exist various entries of the form $\\mathbb{E}_x\\big[\\omega_r^{k_1C(x)_{j_1} + k_2C(x)_{j_2} - x_i}\\big]$, whose absolute values are bounded by a technical result generalizing a few different ideas from~\\cite{katz2000efficiency,DBLP:journals/jcss/KerenidisW04,ben2008hypercontractive}. It is possible then to lower bound the Schatten norm of the Fourier transformed matrix. On the other hand, its rank $1$ implies a simple expression for the original matrix's Schatten norm. The hypercontractive inequality connects both quantities and leads to the following final result. \n\\begin{result}\n\\label{res:ldchyper}\n\tIf $C:\\ensuremath{\\mathbb{Z}}_r^n\\to\\ensuremath{\\mathbb{Z}}_r^N$ is a $(2,\\delta,\\varepsilon)$-$\\mathsf{LDC}$, then $N=2^{\\Omega(\\delta^2\\varepsilon^4 n/r^4)}$.\n\\end{result}\nWe briefly mention that, if one requires the success probability to be larger than, for example, $1/2+\\varepsilon$ instead of $1/r+\\varepsilon$, so that plurality vote can be used and the success probability amplified, then $\\varepsilon$ becomes a constant bounded away from $1/r$ (if $r>2$) and our lower bound is no longer dependent on $\\varepsilon$.\n\n\\paragraph{Further applications (Private information retrieval)} Katz and Trevisan~\\cite{katz2000efficiency}, and Goldreich et al.~\\cite{goldreich2002lower} established a nice connection between $\\mathsf{LDC}$s and private information retrieval (\\textsf{PIR}) protocols. We do not define these $\\textsf{PIR}$ schemes here and refer the reader to Section~\\ref{sec:PIR}. Almost as a black-box, using Result~\\ref{res:ldchyper}, we get the following lower bound for $\\textsf{PIR}$ schemes over $\\ensuremath{\\mathbb{Z}}_r$. \n\\begin{result}\n    A classical $2$-server PIR scheme with query size $t$, answer size $a$ and recovery probability $1/r + \\varepsilon$, satisfies $t= \\Omega\\big(\\delta^2\\varepsilon^4 n/r^4 - a)$.\n\\end{result}\n\n\\paragraph{After completion of this work.}\nAfter completing this work, Chou et al.~\\cite{chou2021linear} put up an online preprint in which they improve our classical streaming lower bound to $\\Omega(n)$ for a broad class of problems, including Unique Games. As far as we are aware, our quantum streaming lower bound is the first for hypergraphs over $\\ensuremath{\\mathbb{Z}}_r$. Additionally, after completion, Jop Bri\\\"et (private communication) gave an alternate proof of  $N=2^{\\Omega(n/r^2)}$ for $2$-query LDCs over $\\ensuremath{\\mathbb{Z}}_r$ using the non-commutative Khintchine~inequality.\n\n\\subsection{Future work} Our work open up a few directions of research. \n\n\\emph{\\textbf{1.}} \\emph{Proving $\\mathsf{LDC}$ lower bounds.} The first natural open question is, can we prove a lower bound of $N= 2^{\\Omega(n/r)}$ for $\\mathsf{LDC}$s over $\\ensuremath{\\mathbb{Z}}_r$, or, more ambitiously, prove that $N= 2^{\\Omega(n)}$? As far as we are aware, there are no super-polynomial lower bounds for $N$ even for $r=\\omega(\\sqrt{n})$. \nSimilarly, can one also prove a lower bound of $N= 2^{\\Omega(n\\log r)}$ for  \\emph{non-linear} locally-\\emph{correctable} codes over $\\ensuremath{\\mathbb{Z}}_r$ (thereby matching a similar lower bound for linear case~\\cite{bhattacharyya2011tight}).\n\n\\emph{\\textbf{2.}} \\emph{Communication complexity of $r$-ary Hidden Hypermatching.} Our communication protocol behind Result~\\ref{res:upperBHM} relies on the promise on the inputs, i.e., on the string $w\\in\\mathbb{Z}_r^{\\alpha n/t}$ that either satisfies $w=Mx$ or is uniformly random. Is there a protocol with the same complexity which does not explicitly use $w$? More generally, what is the communication complexity of a relational version of the $r$-$\\ensuremath{\\mathsf{HH}}(\\alpha,2,n)$ problem in which Bob outputs a hyperedge and the corresponding entry of $Mx$? Moreover, is it possible to match the quantum lower bounds from Result~\\ref{res:lowerHHM}?\n\n\\emph{\\textbf{3.}} \\emph{Better bounds on streaming algorithms.} What is the quantum space complexity of approximating Max-Cut or Unique Games? Is it possible to obtain some saving in space complexity, e.g.\\ an upper bound of $O(n^{1-2/t})$ that matches our lower bound, or is the quantum space complexity $\\Omega(n)$? The former would be interesting because advantage in quantum space complexity are only handful (for contrived problems) and the latter would require proving new quantum lower bounds for the communication problems introduced in~\\cite{kapralov20171+,kapralov2019optimal,chou2021approximabilityA,chou2021approximabilityB,chou2021linear}.\n\n\\emph{\\textbf{4.}} \\emph{Generalized hypercontractivity.} Another open question is regarding our main Result~\\ref{result:hyper}, which shows a form of $(2,q)$-hypercontractivity, since the result works for all Schatten $p$-norms with $p\\in [1,2]$. Can we prove a general $(q,p)$-hypercontractive statement for matrices, firstly for matrix-valued functions over $\\{0,1\\}$, and then further generalize that to functions over $\\ensuremath{\\mathbb{Z}}_r$? Proving this might also require a generalization of the powerful inequality of Ball, Carlen and Lieb~\\cite{ball1994sharp} in a different direction.  \n\n\n\\paragraph{Acknowledgements.} SA firstly thanks T.S.\\ Jayram for introducing him to this problem (and several discussions thereafter) on proving quantum bounds for streaming algorithms while participating in the program ``Quantum Wave in Computing\u201d held at Simons Institute for the Theory for Computing. We thank Jop Bri\\\"et and Ronald de Wolf for many clarifications and discussions regarding hypercontractivity and $\\mathsf{LDC}$s, and Mario Szegedy for discussions during the initial stages of this project. We are also very thankful to Keith Ball and Eric Carlen for the help in understanding their proof of uniform convexity for trace ideals. JFD was supported by the Singapore National Research Foundation, the Prime Minister's Office, Singapore and the Ministry of Education, Singapore under the Research Centres of Excellence programme under research grant R 710-000-012-135.\n\n\n\n\\section{Preliminaries}\n\\label{sec:sec2}\n\nLet $[n]:=\\{1,\\ldots,n\\}$. For $r\\in\\ensuremath{\\mathbb{Z}}$, $r\\geq 2$, we let $\\ensuremath{\\mathbb{Z}}_r:=\\{0,\\ldots,r-1\\}$ be the ring with addition and multiplication modulo $r$, and let $\\omega_r := e^{2\\pi i/r}$.\nGiven $S\\in\\ensuremath{\\mathbb{Z}}_r^n$, we write $|S|:= |\\{i\\in[n]: S_i\\neq 0\\}|$ for its Hamming weight.\nLet $\\operatorname{D}(\\mathbb{C}^m)$ be the set of all quantum states over $\\mathbb{C}^m$, i.e., the set of positive semi-definite matrices with trace $1$. For a matrix $M\\in \\ensuremath{\\mathbb{C}}^{m\\times m}$, the (unnormalized) Schatten $p$-norm is defined as $\\|M\\|_p := (\\operatorname{Tr}|M|^p)^{1/p} = \\big(\\sum_i\\sigma_i(M)^p\\big)^{1/p}$, where $\\{\\sigma_i(M)\\}_i$ are the singular values of $M$, i.e., the eigenvalues of the positive semi-definite operator $|M|:=\\sqrt{M^\\dagger M}$. We also define the normalized Schatten $p$-norm as $\\|M\\|_p := \\big(\\frac{1}{m}\\operatorname{Tr}|M|^p\\big)^{1/p} = \\big(\\frac{1}{m}\\sum_i\\sigma_i(M)^p\\big)^{1/p}$. Throughout the paper we shall use the unnormalized Schatten norm, unless stated otherwise.\nGiven a vector $v\\in\\mathbb{C}^m$, its $p$-norm is $\\|v\\|_p := \\big(\\sum_{i=1}^m |v_i|^p\\big)^{1/p}$. Given two probability distributions $P$ and $Q$ on the same finite set, their total variation distance is $\\|P-Q\\|_{\\text{tvd}} := \\sum_i |P(i) - Q(i)|$ (we might abuse notation and use random variables instead of their probability distributions in $\\|\\cdot\\|_{\\text{tvd}}$). For a probability $p = 1/r + \\varepsilon$ with fixed $r\\in\\ensuremath{\\mathbb{Z}}$, we refer to $\\varepsilon$ as its \\emph{advantage}, and to $2\\varepsilon$ as its \\emph{bias}. \n\nThe Fourier transform of a matrix-valued function $f:\\ensuremath{\\mathbb{Z}}_r^n\\to\\mathbb{C}^{m\\times m}$ is a function $\\widehat{f}:\\ensuremath{\\mathbb{Z}}_r^n\\to\\mathbb{C}^{m\\times m}$ defined by\n\\begin{align*}\n    \\widehat{f}(S) = \\frac{1}{r^n}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}f(x)\\omega_r^{-S\\cdot x},\n\\end{align*}\nwhere $S\\cdot x = \\sum_{i=1}^n S_ix_i$  is a sum over $\\ensuremath{\\mathbb{Z}}_r$. Here the Fourier coefficients $\\widehat{f}(S)$ are $m\\times m$ complex matrices and we can write $f:\\ensuremath{\\mathbb{Z}}_r^n\\to\\mathbb{C}^{m\\times m}$ as\n\\begin{align*}\n    f(x) = \\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n}\\widehat{f}(S)\\omega_r^{S\\cdot x}.\n\\end{align*}\n\nWe will need the Holevo-Helstrom theorem~\\cite{helstrom1976quantum} which characterizes the optimal success probability of distinguishing between two quantum states.\n\\begin{fact}[{\\cite[Theorem~3.4]{watrous2018theory}}]\n    \\label{lem:lem3.5.c3}\n    Let $\\rho_0,\\rho_1$ be two quantum states that appear with probability $p$ and $1-p$, respectively. The optimal success probability of predicting which state it is by a POVM~is\n    %\n    \\begin{align*}\n        \\frac{1}{2} + \\frac{1}{2}\\|p\\rho_0 - (1-p)\\rho_1\\|_1.\n    \\end{align*}\n\\end{fact}\n\n\n\\section{Hypercontractive Inequality}\nIn this section we prove our main result, a hypercontractive inequality for matrix-valued functions over $\\ensuremath{\\mathbb{Z}}_r$, generalizing a result from~\\cite{ben2008hypercontractive}. The proof is by induction on $n$ and the base case $n=1$ is proven in Section~\\ref{sec:sec3.1}, which is a generalization of Ball, Carlen and Lieb~\\cite{ball1994sharp} when considering $r$ matrices. After this, the induction is fairly straightforward and is described in Section~\\ref{sec:sec3.2}.\n\n\n\\subsection{Generalizing Ball, Carlen and Lieb}\n\\label{sec:sec3.1}\nWe first state the powerful inequality of Ball, Carlen and Lieb~\\cite[Theorem~1]{ball1994sharp}.\n\\begin{theorem}[Optimal 2-uniform convexity]\n    \\label{thr:thr1}\n    Let $A,B\\in\\mathbb{C}^{n\\times n}$, and $p\\in[1,2]$. Then\n    %\n    $$\n        \\left(\\frac{\\|A+B\\|_p^p + \\|A-B\\|_p^p}{2}\\right)^{2/p} \\geq \\|A\\|_p^2 + (p-1)\\|B\\|_p^2.\n    $$\n\\end{theorem}\n\n\nAs previously mentioned in the introduction, this inequality was first proven by Tomczak-Jaegermann~\\cite{tomczak1974moduli} for $p\\leq 4/3$, before being generalized by Ball, Carlen and Lieb~\\cite{ball1994sharp} for all $p\\in [1,2]$ in 1994. Since then it has found several applications~\\cite{carlen1993optimal,duchi2010composite,lee2004embedding,naor2016spectral}. The above result can be recast in a slightly different way.\n\\begin{theorem}\n    \\label{thr:alternativeBCL}\n    Let $p\\in[1,2]$ and $Z,W\\in\\mathbb{C}^{n\\times n}$ such that $\\operatorname{Tr}[|Z|^{p-1}ZW^\\dagger] =\\operatorname{Tr}[|Z|^{p-1}WZ^\\dagger] = 0$ (where $|Z|^{p-1}=(ZZ^\\dagger)^{(p-1)/2}$).~Then\n    %\n    \\begin{align*}\n        \\|Z+W\\|_p^2 \\geq \\|Z\\|_p^2 + (p-1)\\|W\\|_p^2.\n    \\end{align*}\n\\end{theorem}\nTheorem~\\ref{thr:alternativeBCL} is implicit in the proof of~\\cite[Theorem~1]{ball1994sharp}, and it is where most of the difficulty lies, while the reduction from Theorem~\\ref{thr:thr1} to Theorem~\\ref{thr:alternativeBCL} is done by defining\n\\begin{align*}\n    Z = \\begin{bmatrix} A & 0 \\\\ 0 & A \\end{bmatrix}, \\qquad W = \\begin{bmatrix} B & 0 \\\\ 0 & -B \\end{bmatrix}.\n\\end{align*}\nNonetheless, Theorem~\\ref{thr:alternativeBCL} holds more generally for any $Z,W\\in\\mathbb{C}^{n\\times n}$ that satisfy $\\operatorname{Tr}[|Z|^{p-1}ZW^\\dagger] =\\operatorname{Tr}[|Z|^{p-1}WZ^\\dagger]=0$. By using this result, we can prove the following generalization of Theorem~\\ref{thr:thr1}.\n\\begin{theorem}[A generalization of~\\cite{ball1994sharp}]\n\\label{eq:conjectureballgen}\n    Let $r\\in\\mathbb{Z}$, $r\\geq 2$. Let $\\omega_r = e^{2i\\pi/r}$, $A_0,\\ldots,A_{r-1}\\in \\ensuremath{\\mathbb{C}}^{n\\times n}$ and $p\\in [1,2]$, then\n    %\n    \\begin{subequations}\n\t\\begin{align}\n\t\t\\left(\\frac{1}{r} \\sum_{j=0}^{r-1}\\|A_j\\|_p^p\\right)^{2/p} &\\geq \\left\\| \\frac{1}{r}\\sum_{j=0}^{r-1} A_j \\right\\|_p^2 + \\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)}\\sum_{k=1}^{r-1} \\left\\| \\frac{1}{r}\\sum_{j=0}^{r-1} \\omega_r^{-jk}A_j \\right\\|_p^2,\\\\\n\t\t\\left(\\frac{1}{r} \\sum_{k=0}^{r-1}\\left\\|\\sum_{j=0}^{r-1} \\omega_r^{jk} A_j\\right\\|_p^p\\right)^{2/p} &\\geq\\left\\|A_0 \\right\\|_p^2 + \\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)}\\sum_{k=1}^{r-1} \\left\\| A_k\\right\\|_p^2.\\label{eq:genball}\n\t\\end{align}\n\t\\end{subequations}\n\\end{theorem}\nNotice that for $r=2$ we recover Theorem~\\ref{thr:thr1}, since $\\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)} = p-1$.\n\\begin{proof}\n\tIn order to prove this theorem, first note that both inequalities are equivalent: just define $A'_k = \\frac{1}{r}\\sum_{j=0}^{r-1} \\omega_r^{-jk}A_j \\iff A_k = \\sum_{j=0}^{r-1}\\omega_r^{jk}A'_j$. Therefore we shall focus on Eq.~(\\ref{eq:genball}).\tIn order to prove it, let us first define the $rn\\times rn$ matrices\n\t%\n\t\\begin{align}\n\t    \\label{eq:matricesY}\n\t\tZ_j := \\operatorname{diag}(\\{\\omega_r^{jk}A_j\\}_{k=0}^{r-1}) = \\begin{bmatrix}\n\t\t\tA_j & 0 & 0 & \\dots & 0\\\\\n\t\t\t0 & \\omega_r^{j}A_j & 0 & \\dots & 0\\\\\n\t\t\t0 & 0 & \\omega_r^{2j}A_j & \\dots & 0\\\\\n\t\t\t\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n\t\t\t0 & 0 & 0 & \\dots & \\omega_r^{(r-1)j}A_{j}\n\t\t\\end{bmatrix}\n\t\\end{align}\n\t%\n\tfor $j\\in\\{0,\\dots,r-1\\}$. Now, since the trace is additive for block matrices, we have\n\t%\n\t\\begin{align}\n\t    \\label{eq:YandAmatrices}\n\t\t\\operatorname{Tr}\\left|\\sum_{j=0}^{r-1}Z_j\\right|^p =  \\sum_{k=0}^{r-1}\\operatorname{Tr}\\left|\\sum_{j=0}^{r-1} \\omega_r^{jk} A_j\\right|^p.\n\t\\end{align}\n\t%\n\tMoreover, observe that\n\t%\n\t\\begin{align*}\n\t    \\|Z_j\\|_p^2 = \\left(\\sum_{k=0}^{r-1}\\operatorname{Tr}|\\omega_r^{jk}A_j|^p\\right)^{2/p} = (r\\operatorname{Tr}|A_j|^p)^{2/p} = r^{2/p}\\|A_j\\|_p^2.\n\t\\end{align*}\n\t%\n\tTherefore we can rewrite Eq.~(\\ref{eq:genball}) as\n\t%\n\t\\begin{align*}\n\t\t\\left\\|\\sum_{j=0}^{r-1}Z_j\\right\\|^2_p \\geq \\|Z_0\\|_p^2 + \\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)}\\sum_{j=1}^{r-1}\\|Z_j\\|_p^2.\n\t\\end{align*}\n\t%\n\tThe above can be proven by repeated applications of Theorem~\\ref{thr:alternativeBCL} as follows: consider a permutation of $[r-1]$ given by $(k_1,\\ldots,k_{r-1})$. Since $\\operatorname{Tr}[|Z_j|^{p-1}Z_jZ_k^\\dagger] = \\operatorname{Tr}[|Z_j|^{p-1}Z_kZ_j^\\dagger] = 0$ for any $j\\neq k$, then (define $k_0:=0$)\n\n\t\\begin{align*}\n\t    \\operatorname{Tr}\\left[|Z_{k_j}|^{p-1}Z_{k_j}\\left(\\sum_{l=j+1}^{r-1}Z_{k_l}\\right)^\\dagger\\right] = \\operatorname{Tr}\\left[|Z_{k_j}|^{p-1}\\left(\\sum_{l=j+1}^{r-1}Z_{k_l}\\right)Z_{k_j}^\\dagger\\right] = 0\n\t\\end{align*}\n\t%\n\tfor every $j\\in\\{0,1,\\dots,r-2\\}$, meaning that Theorem~\\ref{thr:alternativeBCL} can be applied, which implies\n\t%\n\t\\begin{align*}\n\t    \\left\\|\\sum_{j=0}^{r-1}Z_j\\right\\|^2_p &\\geq \\|Z_0\\|_p^2 + (p-1)\\left\\| \\sum_{j=1}^{r-1}Z_j\\right\\|^2_p\\\\\n\t    &\\geq \\|Z_0\\|_p^2 + (p-1)\\|Z_{k_1}\\|_p^2 + (p-1)^2\\left\\| \\sum_{j=2}^{r-1}Z_{k_j}\\right\\|^2_p\\geq \\|Z_0\\|_p^2 + \\sum_{j=1}^{r-1}(p-1)^j\\|Z_{k_j}\\|_p^2.\n\t\\end{align*}\n\t%\n\tAveraging the above inequality over all the $(r-1)!$ permutations of the set $[r-1]$, we obtain\n\t%\n\t\\begin{align*}\n\t    \\left\\|\\sum_{j=0}^{r-1}Z_j\\right\\|^2_p &\\geq \\|Z_0\\|_p^2 + \\frac{1}{(r-1)!}\\sum_{j=1}^{r-1}\\|Z_j\\|_p^2\\sum_{k=1}^{r-1}(r-2)!(p-1)^k\\\\\n\t    &= \\|Z_0\\|_p^2 + \\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)}\\sum_{j=1}^{r-1}\\|Z_j\\|_p^2,\n\t\\end{align*}\n\t%\nproving our theorem statement.\n\\end{proof}   \n\\begin{remark}\n    \\label{rem:1}\n    It is not hard to see that $\\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)} \\geq \\frac{p-1}{r-1}$ and $\\lim_{p\\to 2}\\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)} = 1$.\n\\end{remark}\n\\noindent Observe that $t\\mapsto t^{p/2}$ is concave for $p\\in[1,2]$, hence Theorem~\\ref{eq:conjectureballgen} implies the seemingly weaker\n\\begin{align}\n    \\label{eq:BCLweaker}\n    \\frac{1}{r} \\sum_{k=0}^{r-1}\\left\\|\\sum_{j=0}^{r-1} \\omega_r^{jk} A_j\\right\\|_p^2 &\\geq \\left\\|A_0 \\right\\|_p^2 + \\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)}\\sum_{k=1}^{r-1} \\left\\| A_k\\right\\|_p^2\n\\end{align}\nfor $p\\in[1,2]$. Nonetheless, the above inequality also implies Theorem~\\ref{eq:conjectureballgen} (this fact was already pointed out for $r=2$ by~\\cite{ball1994sharp}). Indeed, consider again the $rn\\times rn$ matrices $Z_j$ from Eq.~\\eqref{eq:matricesY}. Then, similar to Eq.~\\eqref{eq:YandAmatrices} (which only considered the $\\ell=0$ case below), for any $\\ell\\in\\ensuremath{\\mathbb{Z}}_r$ we have\n\\begin{align*}\n    \\operatorname{Tr}\\left|\\sum_{j=0}^{r-1}\\omega_r^{j\\ell} Z_j\\right|^p = \\sum_{k=0}^{r-1}\\operatorname{Tr}\\left|\\sum_{j=0}^{r-1} \\omega_r^{jk} A_j\\right|^p \\implies \\left\\|\\sum_{j=0}^{r-1}\\omega_r^{j\\ell} Z_j\\right\\|_p^2 = \\left(\\sum_{k=0}^{r-1}\\left\\|\\sum_{j=0}^{r-1} \\omega_r^{jk} A_j\\right\\|^p_p\\right)^{2/p}.\n\\end{align*}\n Since $\\|Z_j\\|^2_p = r^{2/p}\\|A_j\\|^2_p$ for $j\\in\\ensuremath{\\mathbb{Z}}_r$, Eq.~\\eqref{eq:BCLweaker} implies (define $\\zeta := \\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)}$ for simplicity)\n\\begin{align*}\n    \\left\\|A_0 \\right\\|_p^2 + \\zeta\\sum_{k=1}^{r-1} \\left\\| A_k\\right\\|_p^2 = \\frac{\\left\\|Z_0 \\right\\|_p^2}{r^{2/p}} + \\zeta\\sum_{k=1}^{r-1} \\frac{\\left\\| Z_k\\right\\|_p^2}{r^{2/p}}\n    \\leq \\frac{r^{-2/p}}{r} \\sum_{\\ell=0}^{r-1}\\left\\|\\sum_{j=0}^{r-1} \\omega_r^{j\\ell} Z_j\\right\\|_p^2\n    = \\left(\\frac{1}{r}\\sum_{k=0}^{r-1}\\left\\|\\sum_{j=0}^{r-1} \\omega_r^{jk} A_j\\right\\|^p_p\\right)^{2/p},\n\\end{align*}\nwhich is exactly Theorem~\\ref{eq:conjectureballgen}.\n\n\\subsection{Proving $(2,p)$-hypercontractive inequality over $\\ensuremath{\\mathbb{Z}}_r$}\n\\label{sec:sec3.2}\nHaving proven the base case of our main theorem statement, we are now ready to prove our hypercontractivity theorem for matrix-valued functions over $\\ensuremath{\\mathbb{Z}}_r$.\n\\begin{theorem}\n    \\label{thm:genmathypercontractivity}\n    Let $p\\in [1,2]$. For every $f:\\ensuremath{\\mathbb{Z}}_r^n\\rightarrow \\mathbb{C}^{m\\times m}$ and\n    %\n    \\begin{align*}\n        \\rho \\leq \\sqrt{\\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)}},\n    \\end{align*}\n    %\n    we have\n    $$\n    \\left(\\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^n} \\rho^{2|S|} \\|\\widehat{f}(S)\\|_p^2\\right)^{1/2} \\leq \\left(\\frac{1}{r^n}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\|f(x)\\|_p^p\\right)^{1/p},\n    $$\n    where $|S| := |\\{i\\in[n]:S_i\\neq 0\\}|$.\n\\end{theorem} \n\\begin{proof}\nFor ease of notation, define $\\zeta := \\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)}$. It suffices to prove the inequality for $\\rho = \\sqrt{\\zeta}$. Our proof closely follows the one in~\\cite{ben2008hypercontractive} and is by induction on $n$. For $n=1$, the desired statement is\n\\begin{align}\n    \\label{eq:basecaseinduction}\n    \\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r} \\zeta^{|S|} \\|\\widehat{f}(S)\\|_p^2\\leq  \\left(\\frac{1}{r}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r}\\|f(x)\\|_p^p\\right)^{2/p}.\n\\end{align}\nConsider the matrices $A_0,\\dots,A_{r-1}$ such that $f(k)=\\sum_{j=0}^{r-1}\\omega_r^{j k}A_j$ for all $k\\in\\ensuremath{\\mathbb{Z}}_r$, so that Eq.~\\eqref{eq:basecaseinduction} can be written as\n\\begin{align*}\n    \\left\\|A_0 \\right\\|_p^2 + \\zeta\\sum_{k=1}^{r-1} \\left\\| A_k\\right\\|_p^2 \\leq \\left(\\frac{1}{r}\\sum_{k=0}^{r-1}\\left\\|\\sum_{j=0}^{r-1}\\omega_r^{j k}A_j\\right\\|_p^p\\right)^{2/p},\n\\end{align*}\nusing the fact that $\\widehat{f}(j) = \\frac{1}{r}\\sum_{k=0}^{r-1}f(k)\\omega_r^{-jk} = A_j$, which is precisely Theorem~\\ref{eq:conjectureballgen}.\n\nWe now assume the inequality holds for $n$ and prove it for $n+1$. Let $f:\\ensuremath{\\mathbb{Z}}_r^{n+1}\\rightarrow \\mathbb{C}^{m\\times m}$ and $g_i=f\\vert_{x_{n+1}=i}$ for $i\\in \\{0,\\ldots,r-1\\}$ be the function obtained by fixing the last bit of $f(\\cdot)$ to $i$. By the induction hypothesis we have that, for every $i\\in \\{0,\\ldots,r-1\\}$ and $p\\in [1,2]$,\n\\begin{align*}\n    \\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^n} \\zeta^{|S|} \\|\\widehat{g_i}(S)\\|_p^2\\leq  \\left(\\frac{1}{r^n}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\|g_i(x)\\|_p^p\\right)^{2/p}.\n\\end{align*}\nWe now take the $\\ell_p$ average of each of these $r$ inequalities to obtain\n\\begin{align}\n    \\label{eq:ellqaverage}\n    \\left(\\frac{1}{r}\\sum_{i=0}^{r-1}\\left(\\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^n} \\zeta^{|S|} \\|\\widehat{g_i}(S)\\|_p^2\\right)^{p/2}\\right)^{2/p}&\\leq  \\left( \\frac{1}{r}\\sum_{i=0}^{r-1}\\frac{1}{r^n}\\sum_{x\\in \\ensuremath{\\mathbb{Z}}_r^n}\\|g_i(x)\\|_p^p\\right)^{2/p}\n    &= \\left(\\frac{1}{r^{n+1}}\\sum_{x\\in \\ensuremath{\\mathbb{Z}}_r^{n+1}} \\|f(x)\\|_p^p\\right)^{2/p}.\n\\end{align}\nThe right-hand side of the inequality above is exactly the right-hand side of the conjectured hypercontractive inequality. Below, we show how to lower bound the left-hand side of the inequality above by the desired left-hand side of the conjectured statement. To do so, we will need the following Minkowski's inequality.\n\\begin{lemma}[{Minkowski's inequality, \\cite[Theorem 26]{hardy1952j}}]\n    \\label{lem:minkowski}\n    For any $r_1\\times r_2$ matrix whose rows are given by $u_1,\\dots,u_{r_1}$ and whose columns are given by $v_1,\\dots,v_{r_2}$, and any $1\\leq q_1\\leq q_2\\leq \\infty$,\n    %\n    \\begin{align*}\n        \\left\\|\\left(\\|v_1\\|_{q_2},\\dots,\\|v_{r_2}\\|_{q_2}\\right)\\right\\|_{q_1} \\leq \\left\\|\\left(\\|u_1\\|_{q_1},\\dots,\\|u_{r_2}\\|_{q_1}\\right)\\right\\|_{q_2}.\n    \\end{align*}\n\\end{lemma}\nNow, consider the $r^n\\times r$ matrix whose entries are given by\n$\nc_{S,i}=r^{n/2}\\big\\|\\zeta^{|S|/2} \\widehat{g_i}(S)\\big\\|_p\n$\nfor every $i\\in \\{0,\\ldots,r-1\\}$ and $S\\in \\ensuremath{\\mathbb{Z}}_r^n$. Then the left-hand side of Eq.~\\eqref{eq:ellqaverage} can be written as\n\\begin{align}\n    \\label{eq:afterminkowski}\n    \\left(\\frac{1}{r}\\sum_{i=0}^{r-1}\\left(\\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^n} \\zeta^{|S|} \\|\\widehat{g_i}(S)\\|_p^2\\right)^{p/2}\\right)^{1/p}&=    \\left(\\frac{1}{r}\\sum_{i=0}^{r-1}\\left(\\frac{1}{r^n}\\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^n} c_{S,i}^2\\right)^{p/2}\\right)^{1/p}\\nonumber\\\\\n    &\\geq \\left(\\frac{1}{r^n}\\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^n}\\left(\\frac{1}{r}\\sum_{i=0}^{r-1} c_{S,i}^p\\right)^{2/p}\\right)^{1/2}\\nonumber\\\\\n    &= \\left(\\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^n}\\zeta^{|S|}\\left(\\frac{1}{r}\\sum_{i=0}^{r-1} \\|\\widehat{g_i}(S)\\big\\|_p^p\\right)^{2/p}\\right)^{1/2},\n\\end{align}\nwhere the first inequality follows from Lemma~\\ref{lem:minkowski} with $q_1 = p$ and $q_2=2$.\n\nNow, for a fixed $S\\in\\ensuremath{\\mathbb{Z}}_r^n$, we use the base case $n=1$, i.e., Eq.~\\eqref{eq:basecaseinduction}, on the functions $h(i)=\\widehat{g_i}(S)$ in order to get\n$$\n\\left(\\frac{1}{r}\\sum_{i=0}^{r-1}\\|\\widehat{g_i}(S)\\|_p^p\\right)^{2/p}\\geq \\sum_{i=0}^{r-1} \\zeta^{|i|} \\left\\|\\frac{1}{r}\\sum_{j=0}^{r-1}h(j)\\omega_r^{-ij}\\right\\|_p^2 = \\sum_{i=0}^{r-1} \\zeta^{|i|} \\left\\|\\frac{1}{r}\\sum_{j=0}^{r-1}\\widehat{g_j}(S)\\omega_r^{-ij}\\right\\|_p^2.\n$$\nPlugging this back into Eq.~\\eqref{eq:afterminkowski}, we have\n\\begin{align*}\n    \\left(\\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^n}\\zeta^{|S|}\\left(\\frac{1}{r}\\sum_{i=0}^{r-1} \\|\\widehat{g_i}(S)\\|_p^p\\right)^{2/p}\\right)^{1/2} &\\geq \\left(\\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^{n}}\\sum_{i=0}^{r-1}\\zeta^{|S|+|i|}\\left\\|\\frac{1}{r}\\sum_{j=0}^{r-1}\\widehat{g_j}(S)\\omega_r^{-i j}\\right\\|_p^2\\right)^{1/2} \\\\\n    &= \\left(\\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^{n+1}}\\zeta^{|S|}\\|\\widehat{f}(S)\\|_p^2\\right)^{1/2},\n\\end{align*}\nwhere we used the fact that $g_j=f\\vert_{x_{n+1}=j}$, so, for every $i\\in\\ensuremath{\\mathbb{Z}}_r$ and $S\\in\\ensuremath{\\mathbb{Z}}_r^n$, we have that $\\widehat{f}(S,i)=\\frac{1}{r}\\sum_{j=0}^{r-1}\\widehat{g_j}(S)\\omega_r^{-i j}$. The lower bound we obtained above is exactly the left-hand side of the conjectured hypercontractive inequality, which proves the theorem statement. \n\\end{proof}\n\\begin{remark}[Comparison with hypercontractivity for real numbers]\n    \\label{rem:rem_realfunctions}\n    For real functions $f:\\ensuremath{\\mathbb{Z}}_r^n\\rightarrow \\ensuremath{\\mathbb{R}}$, it is known that~\\cite{latala2000between,wolff2007hypercontractivity} (see also~\\cite[Theorem~10.18]{o2014analysis})\n    $$\n    \\left(\\sum_{S\\in \\ensuremath{\\mathbb{Z}}_r^n} \\rho^{2|S|}|\\widehat{f}(S)|^2\\right)^{1/2} \\leq \\left(\\frac{1}{r^n}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}|f(x)|^p\\right)^{1/p},\n    $$\n    where $\\rho \\leq \\sqrt{\\frac{(r-1)^{1-1/p} - (r-1)^{-(1-1/p)}}{(r-1)^{1/p} - (r-1)^{-1/p}}}$. Moreover, this bound on $\\rho$ is perfectly sharp, meaning that our bound $\\rho \\leq \\sqrt{\\frac{(p-1)(1-(p-1)^{r-1})}{(r-1)(2-p)}}$ in Theorem~\\ref{thm:genmathypercontractivity} can possibly be improved.\n\\end{remark} \n\n\n\n\\section{Hidden Hypermatching Problem}\n\nThe Boolean Hidden Matching ($\\mathsf{BHM}$) problem is a canonical problem in one-way communication complexity. Here, Alice is given a string $x\\in\\{0,1\\}^n$, while Bob is given a string $w\\in\\{0,1\\}^{\\alpha n/2}$ and a sequence of $\\alpha n/2$ disjoint pairs $(i_1,j_1),\\dots,(i_{\\alpha n/2}, j_{\\alpha n/2})\\in[n]^2$ (called $\\alpha$-partial matching), where $\\alpha\\in(0,1]$. Let $z\\in\\{0,1\\}^{\\alpha n/2}$ be the string defined as $z_\\ell = x_{i_\\ell}\\oplus x_{j_\\ell}$ for $\\ell\\in[\\alpha n/2]$. It is promised that $z\\oplus w = b^{\\alpha n/2}$ for some $b\\in\\{0,1\\}$. By sending a single message from Alice to Bob, their task is to output $b$, i.e., to decide whether $z\\oplus w$ equals the all $0$ string or the all $1$ string.\n\nThe $\\mathsf{BHM}$ problem was proposed by Bar-Yossef \\emph{et al.}~\\cite{bar2004exponential}, where they showed a simple quantum protocol using only $O(\\log{n})$ qubits of communication. Later, Gavinsky \\emph{et al.}~\\cite{gavinsky2007exponential}, by using Fourier techniques, specially the inequality of Kahn, Kalai and Linial~\\cite{kahn1989influence}, proved that any classical protocol needs to communicate $\\Omega(\\sqrt{n})$ bits in order to solve the problem. Since then, many generalizations of the $\\mathsf{BHM}$ problem were proposed. Verbin and Yu~\\cite{verbin2011streaming} extended the $\\alpha n/2$ disjoint pairs received by Bob to $\\alpha n/t$ disjoint $t$-tuples $(M_{1,1},\\dots,M_{1,t}),\\dots,(M_{\\alpha n/t,1},\\dots,M_{\\alpha n/t,t})$ (called ``hypermatching''). The main task now is to compute the parity $z_\\ell = \\bigoplus_{k=1}^t x_{M_{\\ell,k}}$ of a ``hyperedge''. Verbin and Yu named the resulting problem Boolean Hidden Hypermatching ($2\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$),\\footnote{We use the notation $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ for simplicity in exposition throughout.} proved a lower bound $\\Omega(n^{1-1/t})$ on any classical communication protocol and used this to bound the amount of space required in streaming algorithms. A quantum lower bound $\\Omega(n^{1-2/t})$ on the $2\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ problem was later proven by Shi, Wu and Yu~\\cite{shi2012limits}.\n\nSubsequently, Kapralov, Khanna and Sudan~\\cite{kapralov2014streaming} proposed the Boolean Hidden Partition, where Bob does not receive a matching anymore, but the edges of any graph $G$. It is promised that either $Mx=w$, where $M$ is the edge incidence matrix of $G$, or $w$ is taken uniformly at random independently on $x$, and Alice and Bob's task is to decide which is the correct case. \nIn another line, Guruswami and Tao~\\cite{DBLP:conf/approx/GuruswamiT19} introduced the $r$-ary Hidden Matching ($r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,2,n)$) problem, where now $x$ and $w$ are over $\\ensuremath{\\mathbb{Z}}_r$ instead of $\\{0,1\\}$, Bob receives a matching $M$ (and not a general graph), and either $Mx=w$ or $w$ is drawn uniformly at random.\nFinally, Doriguello and Montanaro~\\cite{doriguello2020exponential} expanded the $2\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ problem to computing a fixed Boolean function on the hyperedges of Bob's hypermatching instead of the Parity function. Here we shall consider the standard Hidden Hypermatching problem over a larger alphabet. \n\nIn the following, an $\\alpha$-partial $t$-hypermatching $M\\in\\mathcal{M}_{t,n}^\\alpha$ on $n$ vertices is defined as a sequence of $\\alpha n/t$ disjoint hyperedges $(M_{1,1},\\dots,M_{1,t}),\\dots,(M_{\\alpha n/t, 1},\\dots, M_{\\alpha n/t, t})\\in[n]^t$ with $t$ vertices each, where $\\mathcal{M}_{t,n}^\\alpha$ is the set of all such hypermatchings. If $\\alpha = 1$, we shall write $\\mathcal{M}_{t,n}$.\n\n\\begin{definition}\n\\label{def:hypermatching}\n    Let $n,t\\in\\mathbb{N}$ be such that $t|n$ and $\\alpha\\in(0,1]$. In the $r$-ary Hidden Hypermatching $(r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n))$ problem, Alice gets $x\\in\\ensuremath{\\mathbb{Z}}_r^n$, Bob gets an $\\alpha$-partial $t$-hypermatching $M\\in\\mathcal{M}_{t,n}^\\alpha$ and a string $w\\in\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}$. The hyperedges of $M$ are $(M_{1,1},\\dots,M_{1,t}),\\dots,(M_{\\alpha n/t, 1},\\dots, M_{\\alpha n/t, t})$. Let $M\\in\\{0,1\\}^{\\alpha n/t \\times n}$ also be the incident matrix of Bob's hypermatching.  Consider the~distributions:\n   \n    %\n    \\begin{enumerate}\n        \\item  $\\ensuremath{\\mathsf{YES}}$ distribution $\\mathcal{D}^{\\ensuremath{\\mathsf{YES}}}$, let $w=Mx$ (where the matrix product $Mx$ is over $\\ensuremath{\\mathbb{Z}}_r$);\n       \n        \\item  $\\ensuremath{\\mathsf{NO}}$ distribution $\\mathcal{D}^{\\ensuremath{\\mathsf{NO}}}$, $w$ is uniformly random in $\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}$.\n    \\end{enumerate}\n    %\n    In the $r$-ary Hidden Hypermatching problem, Alice sends a message to Bob who needs to decide with high probability if $w$ is drawn from $\\mathcal{D}^{\\ensuremath{\\mathsf{YES}}}$ or $\\mathcal{D}^{\\ensuremath{\\mathsf{NO}}}$. \n\\end{definition}\n\n\n\\subsection{Quantum protocol for $r$-ary Hidden Hypermatching}\n\nFor $t=2$, we obtain a efficient quantum communication protocol to solve the $r$-ary Hidden Hypermatching problem.\n\\begin{theorem}\n    \\label{thr:rary-upper}\n    Given $\\varepsilon > 0$, there is a protocol for the $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,2,n)$ problem with one-sided error $\\varepsilon$ and $O(\\frac{1}{\\alpha}\\log{(nr)}\\log(1/\\varepsilon))$ qubits of communication from Alice to Bob.\n\\end{theorem}\n\\begin{proof}\n    Let $M\\in\\mathcal{M}^\\alpha_{2,n}$ be Bob's matching with edges $(M_{1,1},M_{1,2}),\\dots,(M_{\\alpha n/2,1},M_{\\alpha n/2,2})$. Alice sends the following state to Bob,\n\t%\n\t\\begin{align*}\n\t\t\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n} |x_i,i\\rangle,\n\t\\end{align*}\n\t%\n\twho measures it with the POVM $\\{E_1,\\dots,E_{\\alpha n/2}, \\mathbb{I}-\\sum_{i=1}^{\\alpha n/2} E_i\\}$, where\n\t%\n\t\\begin{align*}\n\t\tE_i := |M_{i,1}\\rangle\\langle M_{i,1}| + |M_{i,2}\\rangle\\langle M_{i,2}|\n\t\\end{align*}\n\t%\n\tfor $i\\in \\{1,\\dots,\\alpha n/2\\}$. With probability $1-\\alpha$ the POVM outputs the final outcome, and with probability $\\alpha$ he will obtain a measurement outcome $E_i$ with $i\\in[\\alpha n/2]$ and get the state\n\t%\n\t\\begin{align*}\n\t\t|\\psi\\rangle := \\frac{1}{\\sqrt{2}}(|x_{M_{i,1}},M_{i,1}\\rangle +  |x_{M_{i,2}},M_{i,2}\\rangle).\n\t\\end{align*}\n\t%\n\tBy repeating the procedure $O(1/\\alpha)$ times, Bob obtains an outcome $i\\in[\\alpha n/2]$ with high probability.\n\t\n\tFor the ease of notation, we can write $M_{i,1} = 0$ and $M_{i,2} = 1$ (note that Bob knows the values of both $M_{i,1},M_{i,2}$ explicitly). Bob now attaches a $\\lceil\\log_2{r}\\rceil$-qubit register in the state $|0\\rangle$ to $|\\psi\\rangle$ and applies a Fourier transform $Q_r$ over $\\ensuremath{\\mathbb{Z}}_r$ to it to obtain\n\t\\begin{align*}\n\t\t|0\\rangle|\\psi\\rangle \\to \\frac{1}{\\sqrt{r}}\\sum_{k=0}^{r-1} |k\\rangle|\\psi\\rangle.\n\t\\end{align*}\n\n\tFrom now on we shall consider a parameter $\\ell\\in\\mathbb{Z}_r$ to be determined later. Let $X$ be the usual Pauli operator and let $S_\\ell$ and $P$ be the shift and phase operators over $\\ensuremath{\\mathbb{Z}}_r$ defined as $S_\\ell|k\\rangle = |\\ell-k\\rangle$ and $P|k\\rangle = \\omega_r^{k}|k\\rangle$ for $k\\in\\mathbb{Z}_r$. Let $C_\\ell := PS_\\ell P\\otimes X$. Bob applies the controlled unitary $U_\\ell$ defined as $U_\\ell|k\\rangle|\\psi\\rangle = |k\\rangle C_\\ell^k|\\psi\\rangle$ on his state, followed by an inverse Fourier transform $Q^\\dagger_r$ on his first register to get\n\t%\n\t\\begin{align*}\n\t\t\\frac{1}{\\sqrt{r}}\\sum_{k=0}^{r-1} U_\\ell|k\\rangle|\\psi\\rangle = \\frac{1}{\\sqrt{r}}\\sum_{k=0}^{r-1} |k\\rangle C_\\ell^k|\\psi\\rangle \\stackrel{Q^\\dagger_r\\otimes \\ensuremath{\\mathbb{I}}}{\\longrightarrow} \\frac{1}{r}\\sum_{j=0}^{r-1}\\sum_{k=0}^{r-1}\\omega_r^{-jk} |j\\rangle C_\\ell^k|\\psi\\rangle.\n\t\\end{align*}\n\t%\n\tLet us calculate $C_\\ell|\\psi\\rangle$ and $C_\\ell^2|\\psi\\rangle$. We have\n\t%\n\t\\begin{align}\n\t\tC_\\ell|\\psi\\rangle &= \\frac{1}{\\sqrt{2}}(PS_\\ell P \\otimes X)(|x_0,0\\rangle +  |x_1,1\\rangle)\\nonumber\\\\\n\t\t&= \\frac{1}{\\sqrt{2}}(PS_\\ell \\otimes \\mathbb{I})(\\omega_r^{x_0}|x_0,1\\rangle +  \\omega_r^{x_1}|x_1,0\\rangle)\\nonumber\\\\\n\t\t&= \\frac{1}{\\sqrt{2}}(P \\otimes \\mathbb{I})(\\omega_r^{x_0}|\\ell -x_0,1\\rangle +  \\omega_r^{x_1}|\\ell -x_1,0\\rangle)\\nonumber\\\\\n\t\t&= \\frac{\\omega_r^{\\ell}}{\\sqrt{2}}(|\\ell -x_1,0\\rangle + |\\ell -x_0,1\\rangle)\\label{eq:bobstate}\n\t\\end{align}\n\t%\n    and\n\t%\n\t\\begin{align*}\n\t    C_\\ell^2|\\psi\\rangle &= \\frac{\\omega_r^{\\ell}}{\\sqrt{2}}(PS_\\ell P \\otimes X)(|\\ell -x_1,0\\rangle + |\\ell -x_0,1\\rangle)\\\\\n\t    &= \\frac{\\omega_r^{\\ell}}{\\sqrt{2}}(PS_\\ell \\otimes \\mathbb{I})(\\omega_r^{\\ell - x_1}|\\ell -x_1,1\\rangle + \\omega_r^{\\ell - x_0}|\\ell -x_0,0\\rangle)\\\\\n\t    &= \\frac{\\omega_r^{\\ell}}{\\sqrt{2}}(P \\otimes \\mathbb{I})(\\omega_r^{\\ell - x_1}|x_1,1\\rangle + \\omega_r^{\\ell - x_0}|x_0,0\\rangle)\\\\\n\t    &= \\omega_r^{2\\ell}|\\psi\\rangle.\n\t\\end{align*}\n\t%\n\tWe can see from the above that $C_\\ell^{2k}|\\psi\\rangle = \\omega_r^{2k\\ell}|\\psi\\rangle$. By defining $\\Delta_\\ell := \\ell - (x_0+x_1)$ and $\\delta_k =1$ if $k$ is odd and $0$ otherwise, Bob's final state is\n\t%\n\t\\begin{align}\n\t\t\\frac{1}{r}\\sum_{j=0}^{r-1}\\sum_{k=0}^{r-1}\\omega_r^{k(\\ell-j)}|j\\rangle\\frac{1}{\\sqrt{2}}(|x_0 + \\Delta_\\ell\\delta_k,0\\rangle + |x_1 + \\Delta_\\ell\\delta_k,1\\rangle).\\label{eq:bobfinal}\n\t\\end{align}\n\tNow observe that, if $\\ell=x_0+x_1$, then $C_\\ell|\\psi\\rangle = \\omega_r^{\\ell}|\\psi\\rangle$ in Eq.~\\eqref{eq:bobstate}. This means that Bob's state in Eq.~\\eqref{eq:bobfinal} becomes $|x_0+x_1\\rangle|\\psi\\rangle$, and if he measures his first register, he obtains $x_0+x_1~(\\text{mod}~r)$ with certainty.\n\t\n\tOn the other hand, if $\\ell\\neq x_0+x_1$, then the probability of measuring the first register and obtaining the outcome $m\\in\\mathbb{Z}_r$ is\n\t%\n\t\\begin{align*}\n\t    \\operatorname{Pr}[m] &= \\frac{1}{2r^2}\\sum_{k_1,k_2=0}^{r-1}\\omega_r^{(\\ell-m)(k_1-k_2)}(\\langle x_0 + \\Delta_\\ell\\delta_{k_2}|x_0 + \\Delta_\\ell\\delta_{k_1}\\rangle + \\langle x_1 + \\Delta_\\ell\\delta_{k_2}|x_1 + \\Delta_\\ell\\delta_{k_1}\\rangle)\\\\\n\t\t&= \\frac{1}{r^2}\\sum_{k_1,k_2~\\text{even}}^{r-1}\\omega_r^{(\\ell-m)(k_1-k_2)} + \\frac{1}{r^2}\\sum_{k_1,k_2~\\text{odd}}^{r-1}\\omega_r^{(\\ell-m)(k_1-k_2)}\\\\\n\t\t&= \\left|\\frac{1}{r}\\sum_{k~\\text{even}}^{r-1}\\omega_r^{k(\\ell-m)}\\right|^2 + \\left|\\frac{1}{r}\\sum_{k~\\text{odd}}^{r-1}\\omega_r^{k(\\ell-m)}\\right|^2.\n\t\\end{align*}\n    %\n    It is not hard to see that the above probability is maximum for when $m = \\ell$, in which case\n\t%\n\t\\begin{align*}\n\t\t\\operatorname{Pr}[m=\\ell] = \\frac{1}{r^2}\\left\\lfloor\\frac{r+1}{2}\\right\\rfloor^2 + \\frac{1}{r^2}\\left\\lfloor\\frac{r}{2}\\right\\rfloor^2 = \\begin{cases}\n\t\t\t\\frac{1}{2} \\qquad &r~\\text{even},\\\\\n\t\t\t\\frac{1}{2} + \\frac{1}{2r^2} \\qquad &r~\\text{odd}.\n\t\t\\end{cases}\n\t\\end{align*}\n\t%\n\tGiven the considerations above, Bob uses the following strategy: he picks $\\ell$ as the corresponding entry $w_i$ from $w\\in\\mathbb{Z}_r^{\\alpha n/2}$ given the measured hyperedge $(M_{i,1},M_{i,2})$. If the outcome $m$ from measuring his final state in Eq.~\\eqref{eq:bobfinal} equals $w_i$, then he outputs $\\mathsf{YES}$, otherwise he outputs $\\mathsf{NO}$. Indeed, in the $\\mathsf{YES}$ instance, $w_i = x_{M_{i,1}}+x_{M_{i,2}}$ and so $m$ equals $w_i$ with probability $1$, while in the $\\mathsf{NO}$ instance, $m$ equals $w_i$ with probability at most $\\frac{1}{2}+\\frac{1}{2r^2}$. Thus the communication protocol has one-sided error at most $\\frac{1}{2}+\\frac{1}{2r^2}$, i.e., $\\operatorname{Pr}[\\text{error}|\\mathsf{YES}] = 0$ and $\\operatorname{Pr}[\\text{error}|\\mathsf{NO}] \\leq \\frac{1}{2}+\\frac{1}{2r^2}$. By repeating the whole protocol $O(\\log(1/\\varepsilon))$ more times, the one-sided error probability can be decreased to $\\varepsilon$: if in any of the repetitions the final measurement outcome is different from $w_i$, then Bob knows that $\\mathsf{NO}$ is the correct answer.\n\\end{proof}\n\n\\subsection{Quantum lower bound on $r$-ary Hidden Hypermatching}\n\nIn this section we shall turn our attention to proving quantum and classical lower bounds on the amount of communication required by the $r$-$\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ problem, but first we need the following~lemma.\n\\begin{lemma}\n    \\label{lem:hypercontractive}\n    Let $f:\\ensuremath{\\mathbb{Z}}_r^n \\to \\operatorname{D}(\\mathbb{C}^{2^m})$ be any mapping from an $n$-bit alphabet to $m$-qubit density matrices. Then for any $\\delta\\in[0,1/(r-1)]$, we have\n    %\n    \\begin{align*}\n        \\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n} \\delta^{|S|}\\|\\widehat{f}(S)\\|^2_1 \\leq 2^{2(r-1)\\delta m}.\n    \\end{align*}\n\\end{lemma}\n\\begin{proof}\n    Let $p := 1+(r-1)\\delta$. First note that, given the eigenvalues $\\sigma_1,\\dots,\\sigma_{2^m}$ from $f(x)$, which are non-negative reals that sum to $1$, we have\n    %\n    \\begin{align*}\n        \\|f(x)\\|_p^p = \\sum_{i=1}^{2^m}\\sigma_i^p \\leq \\sum_{i=1}^{2^m}\\sigma_i = 1.\n    \\end{align*}\n    %\n    Using Theorem~\\ref{thm:genmathypercontractivity} and Remark~\\ref{rem:1}, we now get\n    %\n    \\begin{align*}\n        \\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n}\\left(\\frac{p-1}{r-1}\\right)^{|S|}\\|\\widehat{f}(S)\\|_p^2 \\leq \\left(\\frac{1}{r^n}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\|f(x)\\|_p^p\\right)^{2/p} \\leq \\left(\\frac{1}{r^n}\\cdot r^n\\right)^{2/p} = 1.\n    \\end{align*}\n    %\n    On the other hand, the normalized Schatten norm $2^{-m/p}\\|\\widehat{f}(S)\\|_p$ is non-decreasing with $p$, since $p \\leq q \\implies \\left(\\frac{1}{2^m}\\sum_{i=1}^{2^m}\\sigma_i^p\\right)^{1/p} \\leq \\left(\\frac{1}{2^m}\\sum_{i=1}^{2^m}\\sigma_i^q\\right)^{1/q}$ by H\\\"older's inequality, hence\n    %\n    \\begin{align*}\n        \\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n}\\left(\\frac{p-1}{r-1}\\right)^{|S|}2^{-2m/p}\\|\\widehat{f}(S)\\|_p^2 \\geq \\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n}\\left(\\frac{p-1}{r-1}\\right)^{|S|}2^{-2m}\\|\\widehat{f}(S)\\|_1^2.\n    \\end{align*}\n    %\n    Rearranging the inequalities leads to\n    %\n    \\[\n        \\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n}\\left(\\frac{p-1}{r-1}\\right)^{|S|}\\|\\widehat{f}(S)\\|_1^2 \\leq 2^{2m(1-1/p)} \\leq 2^{2m(p-1)}. \\qedhere\n    \\]\n\\end{proof}\n\nWe are now ready to state and prove our main quantum communication complexity lower bound for the $r$-ary Hidden Hypermatching problem. \n\\begin{theorem}\n\\label{thm:hypermatchinglowerbound}\n    Any quantum protocol that achieves advantage $\\varepsilon>0$ for the $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ problem with $t\\geq 3$ and $\\alpha \\leq \\min(1/2, (r-1)^{-1/2})$ requires at least $m = \\Omega(r^{-(1+1/t)}(\\varepsilon^2/\\alpha)^{2/t}(n/t)^{1-2/t})$ qubits of communication from Alice to Bob.\n\\end{theorem}\nNotice that for $r=2$ our lower bound reads $\\Omega(\\alpha^{-2/t}(n/t)^{1-2/t})$, which has a better dependence on $\\alpha$ compared to the lower bound $\\Omega(\\log(1/\\alpha)(n/t)^{1-2/t})$ from~\\cite{shi2012limits}. Also, see Remark~\\ref{rem:alphadependence} at the end of the section for an improvement on the requirement $\\alpha \\leq \\min(1/2, (r-1)^{-1/2})$.\n\\begin{proof}\n    Consider an $m$-qubit communication protocol. An arbitrary $m$-qubit protocol can be viewed as Alice sending an encoding of her input $x\\in \\ensuremath{\\mathbb{Z}}_r^n$ into a quantum state so that Bob can distinguish if his $w$ was drawn from $\\mathcal{D}^{\\ensuremath{\\mathsf{YES}}}$ or $\\mathcal{D}^{\\ensuremath{\\mathsf{NO}}}$. \n   \n    Let $\\rho:\\ensuremath{\\mathbb{Z}}_r^n\\to \\operatorname{D}(\\mathbb{C}^{2^m})$ be Alice's encoding function. For our `hard' distribution, Alice and Bob receive $x\\in\\ensuremath{\\mathbb{Z}}_r^n$ and $M\\in\\mathcal{M}_{t,n}^\\alpha$, respectively, uniformly at random, while Bob's input $w\\in\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}$ is drawn from the distribution $\\mathcal{D} := \\frac{1}{2}\\mathcal{D}^{\\ensuremath{\\mathsf{YES}}} + \\frac{1}{2}\\mathcal{D}^{\\ensuremath{\\mathsf{NO}}}$, i.e., with probability $1/2$ is comes from $\\mathcal{D}^{\\ensuremath{\\mathsf{YES}}}$, and with probability $1/2$ it comes from $\\mathcal{D}^{\\ensuremath{\\mathsf{NO}}}$. Let $p_x := r^{-n}$, $p_M := |\\mathcal{M}^{\\alpha}_{t,n}|^{-1}$ and $p_w := r^{-\\alpha n/t}$, then our hard distribution $\\mathcal{P}$ is\n\t%\n\t\\begin{align}\n\t\\label{eq:eq3.5.c3}\n\t\t\\operatorname{Pr}[x,\\ensuremath{\\mathsf{YES}},M,w] = \\frac{1}{2}p_x\\cdot p_M \\cdot [Mx = w], \\quad \t\t\\operatorname{Pr}[x,\\ensuremath{\\mathsf{NO}},M,w] = \\frac{1}{2}p_x\\cdot p_M\\cdot  p_w.\n\t\\end{align}\n\t    \n\tConditioning on Bob's input $(M, w)$, from his perspective, Alice sends the message $\\rho_x$ with probability $\\operatorname{Pr}[x|M,w]$. Therefore, conditioned on an instance of the problem ($\\ensuremath{\\mathsf{YES}}$ or $\\ensuremath{\\mathsf{NO}}$), Bob receives one of the following two quantum states $\\rho_\\ensuremath{\\mathsf{YES}}^{M,w}$ and $\\rho_\\ensuremath{\\mathsf{NO}}^{M,w}$, each appearing with probability $\\operatorname{Pr}[\\ensuremath{\\mathsf{YES}}|M,w]$ and $\\operatorname{Pr}[\\ensuremath{\\mathsf{NO}}|M,w]$, respectively,\n\t%\n\t\\begin{align}\n\t\\label{eq:rhoyesrhono}\n\t\\begin{aligned}\n\t\t\\rho_{\\ensuremath{\\mathsf{YES}}}^{M,w} &= \\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n} \\operatorname{Pr}[x|\\ensuremath{\\mathsf{YES}},M,w]\\cdot\\rho_x = \\frac{1}{\\operatorname{Pr}[\\ensuremath{\\mathsf{YES}},M,w]} \\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n} \\operatorname{Pr}[x,\\ensuremath{\\mathsf{YES}},M,w] \\cdot \\rho_x,\\\\ \n\t    \\rho_{\\ensuremath{\\mathsf{NO}}}^{M,w} &= \\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n} \\operatorname{Pr}[x|\\ensuremath{\\mathsf{NO}},M,w]\\cdot\\rho_x = \\frac{1}{\\operatorname{Pr}[\\ensuremath{\\mathsf{NO}},M,w]} \\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n} \\operatorname{Pr}[x,\\ensuremath{\\mathsf{NO}},M,w] \\cdot \\rho_x.\n\t\\end{aligned}\n\t\\end{align}\n\t%\n\tBob's best strategy to determine the distribution of $w$ conditioning on his input $(M,w)$ is no more than the chance to distinguish between these two quantum states $\\rho_\\ensuremath{\\mathsf{YES}}^{M,w}$ and $\\rho_\\ensuremath{\\mathsf{NO}}^{M,w}$.\n\t\n\tNow let $\\varepsilon_{bias}$ be the bias of the protocol that distinguishes between $\\rho_\\ensuremath{\\mathsf{YES}}^{M,w}$ and $\\rho_\\ensuremath{\\mathsf{NO}}^{M,w}$. According to Lemma~\\ref{lem:lem3.5.c3}, the bias $\\varepsilon_{bias}$ of any quantum protocol for a fixed $M$ and $w$ can be upper bounded as\n\t%\n\t\\begin{align*}\n\t\t\\varepsilon_{bias} \\leq \\big\\|{\\operatorname{Pr}}[\\ensuremath{\\mathsf{YES}}|M,w]\\cdot\\rho_\\ensuremath{\\mathsf{YES}}^{M,w} - \\operatorname{Pr}[\\ensuremath{\\mathsf{NO}}|M,w]\\cdot\\rho_\\ensuremath{\\mathsf{NO}}^{M,w}\\big\\|_1.\n\t\\end{align*}\n\t%\n\tWe prove in Theorem~\\ref{thr:raryhidden} below that, if $m \\leq \\frac{\\gamma}{r^{1+1/t}}(\\frac{\\varepsilon^2}{\\alpha})^{2/t} (n/t)^{1-2/t}$ for a universal constant $\\gamma$, then the average bias over $M$ and $w$ is at most $\\varepsilon^2$, i.e.,\n\t%\n\t\\begin{align*}\n\t\t\\operatorname*{\\mathbb{E}}_{(M,w)\\sim\\mathcal{P}_{M,w}}[\\varepsilon_{bias}] \\leq \\varepsilon^2,\n\t\\end{align*}\n\t%\n\twhere $\\mathcal{P}_{M,w}$ is the marginal distribution of $\\mathcal{P}$. Therefore, by Markov's inequality, for at least a $(1-\\varepsilon)$-fraction of $M$ and $w$, the bias in distinguishing between $\\rho_\\ensuremath{\\mathsf{YES}}^{M,w}$ and $\\rho_\\ensuremath{\\mathsf{NO}}^{M,w}$ is $\\varepsilon$ small. Therefore, Bob's advantage over randomly guessing the right distribution will be at most $\\varepsilon$ (for the event that $M$ and $w$ are such that the distance between $\\rho_\\ensuremath{\\mathsf{YES}}^{M,w}$ and $\\rho_\\ensuremath{\\mathsf{NO}}^{M,w}$ is more than $\\varepsilon$) plus $\\varepsilon/2$ (for the advantage over random guessing when $\\varepsilon_{bias} \\leq \\varepsilon$), and so $m = \\Omega(r^{-(1+1/t)}(\\varepsilon^2/\\alpha)^{2/t}(n/t)^{1-2/t})$.\n\\end{proof}\n\n\\begin{theorem}\n\t\\label{thr:raryhidden}\n    For $x\\in\\ensuremath{\\mathbb{Z}}_r^n$, $M\\in\\mathcal{M}_{t,n}^\\alpha$, $w\\in\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}$ and $b\\in\\{\\ensuremath{\\mathsf{YES}},\\ensuremath{\\mathsf{NO}}\\}$, consider the probability distribution $\\mathcal{P}$ defined in Eq.~(\\ref{eq:eq3.5.c3}). Given an encoding function $\\rho:\\ensuremath{\\mathbb{Z}}_r^n\\to \\operatorname{D}(\\mathbb{C}^{2^m})$, consider the quantum states $\\rho_\\ensuremath{\\mathsf{YES}}^{M,w}$ and $\\rho_\\ensuremath{\\mathsf{NO}}^{M,w}$ from Eq.~(\\ref{eq:rhoyesrhono}). If $\\alpha \\leq \\min(1/2,(r-1)^{-1/2})$, there is a universal constant $\\gamma>0$ (independent of $n$, $t$, $r$ and $\\alpha$), such that, for all $\\varepsilon > 0$, if $m \\leq \\frac{\\gamma}{r^{1+1/t}}(\\frac{\\varepsilon^2}{\\alpha})^{2/t} (n/t)^{1-2/t}$,~then\n    %\n    \\begin{align*}\n        \\operatorname*{\\mathbb{E}}_{(M,w)\\sim\\mathcal{P}_{M,w}}\\left[\\big\\|\\operatorname{Pr}[\\ensuremath{\\mathsf{YES}}|M,w]\\cdot\\rho_\\ensuremath{\\mathsf{YES}}^{M,w} - \\operatorname{Pr}[\\ensuremath{\\mathsf{NO}}|M,w]\\cdot\\rho_\\ensuremath{\\mathsf{NO}}^{M,w}\\big\\|_1\\right] \\leq \\varepsilon^2.\n    \\end{align*}\n\\end{theorem}\n\\begin{proof}\n\tFor the ease of notation, we shall write\n\t%\n\t\\begin{align*}\n\t    \\varepsilon_{bias} := \\operatorname*{\\mathbb{E}}_{(M,w)\\sim\\mathcal{P}_{M,w}}\\left[\\big\\|\\operatorname{Pr}[\\ensuremath{\\mathsf{YES}}|M,w]\\cdot\\rho_\\ensuremath{\\mathsf{YES}}^{M,w} - \\operatorname{Pr}[\\ensuremath{\\mathsf{NO}}|M,w]\\cdot\\rho_\\ensuremath{\\mathsf{NO}}^{M,w}\\big\\|_1\\right].\n\t\\end{align*}\n\t%\n\tTherefore, we have that\n\t%\n\t\\begin{align*}\n\t\t\\displaybreak[0]\\varepsilon_{bias} &= \\sum_{M\\in\\mathcal{M}_{t,n}^\\alpha}\\sum_{w\\in\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}}\\operatorname{Pr}[M,w]\\cdot \\big\\|\\operatorname{Pr}[\\ensuremath{\\mathsf{YES}}|M,w]\\cdot\\rho_\\ensuremath{\\mathsf{YES}}^{M,w} - \\operatorname{Pr}[\\ensuremath{\\mathsf{NO}}|M,w]\\cdot\\rho_\\ensuremath{\\mathsf{NO}}^{M,w}\\big\\|_1\\displaybreak[0]\\\\\n\t\t&= \\sum_{M\\in\\mathcal{M}_{t,n}^\\alpha}\\sum_{w\\in\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}}\\Big\\|\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\Big(\\operatorname{Pr}[x,\\ensuremath{\\mathsf{YES}},M,w]\\cdot\\rho_x - \\operatorname{Pr}[x,\\ensuremath{\\mathsf{NO}},M,w]\\cdot\\rho_x\\Big)\\Big\\|_1\\displaybreak[0]\\\\\n\t\t&= \\sum_{M\\in\\mathcal{M}_{t,n}^\\alpha}\\sum_{w\\in\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}}\\Big\\|\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\frac{1}{2}p_x\\cdot p_M \\left(\\big[Mx = w\\big] - p_w\\right)\\rho_x\\Big\\|_1 && \\tag{By Eqs.~\\eqref{eq:eq3.5.c3},~\\eqref{eq:rhoyesrhono}}\\displaybreak[0]\\\\\n\t\t&= \\sum_{M\\in\\mathcal{M}_{t,n}^\\alpha}\\sum_{w\\in\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}}\\Big\\|\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\frac{1}{2}p_x\\cdot p_M \\left(\\big[Mx = w\\big] - p_w\\right)\\cdot \\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n}\\widehat{\\rho}(S)\\omega_r^{S\\cdot x}\\Big\\|_1\\tag{Fourier decomposition of $\\rho$}\\displaybreak[0]\\\\\n\t\t&= \\sum_{M\\in\\mathcal{M}_{t,n}^\\alpha}\\sum_{w\\in\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}} \\Big\\|\\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n} u(M,w,S) \\widehat{\\rho}(S)\\Big\\|_1\\displaybreak[0]\\\\\n\t\t&\\leq \\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n}\\sum_{M\\in\\mathcal{M}_{t,n}^\\alpha}\\sum_{w\\in\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}}|u(M,w,S)|\\cdot \\|\\widehat{\\rho}(S)\\|_1,\n\t\\end{align*}\n\t%\n\twhere we defined\n\t%\n\t\\begin{align}\n\t\t\\label{eq:defnofu}\n\t\t\tu(M,w,S) := \\frac{1}{2}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}p_x\\cdot p_M \\cdot \\omega_r^{S\\cdot x} \\left(\\big[Mx = w\\big] - p_w\\right).\n\t\t\\end{align}\n\t\n\tNext, we upper bound the quantity $u(M,w,S)$ using the lemma below. In the following lemma, given an $\\alpha$-partial hypermatching $M\\in\\mathcal{M}_{t,n}^\\alpha$, we can, without lost of generality, complete $M$ with $(1-\\alpha)n/t$ remaining hyperedges and turn it into a perfect hypermatching, i.e., we can assume that $M\\in\\mathcal{M}_{t,n}$. Moreover, we shall write $S|_{M_i} = S_{M_{i,1}} S_{M_{i,2}}\\dots S_{M_{i,t}}\\in\\ensuremath{\\mathbb{Z}}_r^t$ to denote the string $S$ restricted to the hyperedge $M_i = (M_{i,1},\\dots,M_{i,t})$, where $S_{M_{i,j}}$ is the $M_{i,j}$-th entry of $S$. The same applies to $x\\in\\ensuremath{\\mathbb{Z}}_r^n$.\n\t\\begin{lemma}\n\t\\label{lem:understandingu}\n\t    Let $M\\in\\mathcal{M}_{t,n}$, $w\\in\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}$ and $S\\in\\ensuremath{\\mathbb{Z}}_r^n$. Define the set \n\t    \\begin{align*}\n\t    \\Delta(M) = \\{S\\in\\ensuremath{\\mathbb{Z}}_r^n\\setminus\\{0^n\\} ~|~ S_{M_{i,1}} = S_{M_{i,2}} = \\cdots = S_{M_{i,t}} &\\text{ for every } i\\in[\\alpha n/t]\\\\\n\t    \\text{and}~ S|_{M_i} = 0^t &\\text{ for every } i>\\alpha n/t\\}.\n\t    \\end{align*}\n\t    Given $u(M,w,S)$ as defined in Eq.~(\\ref{eq:defnofu}), we have\n\t    $\n\t    u(M,w,S)=\\frac{1}{2}\\cdot r^{-\\alpha n/t}\\cdot p_M\n\t    $\n\t    if $S\\in\\Delta(M)$ and 0 if $S\\notin\\Delta(M)$.\n\t\\end{lemma}\n\t\n\t\\begin{proof}\n    Recall the definition of $u$:\n    $$\n    u(M,w,S) = \\frac{1}{2}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}p_x\\cdot p_M \\cdot \\omega_r^{S\\cdot x} \\left(\\big[Mx = w\\big] - p_w\\right).\n    $$\n    In order to understand this expression, we start with the following:\n\t%\n\t\\begin{align*}\n\t\t\\displaybreak[0]\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\omega_r^{S\\cdot x}\\big[Mx = w\\big] &= \\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\omega_r^{S\\cdot x} \\prod_{i=1}^{\\alpha n/t} \\left[(Mx)_i = w_i\\right]\\displaybreak[0]\\\\\n\t\t&= \\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\omega_r^{S\\cdot x} \\prod_{i=1}^{\\alpha n/t} \\left[\\sum_{j=1}^t x_{M_{i,j}} \\equiv w_i~(\\text{mod}~r)\\right]\\displaybreak[0]\\\\\n\t\t&= \\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\omega_r^{\\sum_{i=1}^{n/t}\\sum_{j=1}^t S_{M_{i,j}} x_{M_{i,j}}} \\prod_{i=1}^{\\alpha n/t} \\left[\\sum_{j=1}^t x_{M_{i,j}} \\equiv w_i~(\\text{mod}~r)\\right]\\displaybreak[0]\\\\\n\t\t&= \\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\omega_r^{\\sum_{i=1}^{n/t}\\sum_{j=1}^t S_{M_{i,j}} x_{(i-1)t+j}} \\prod_{i=1}^{\\alpha n/t} \\left[\\sum_{j=1}^t x_{(i-1)t+j} \\equiv w_i~(\\text{mod}~r)\\right],\n\t\\end{align*}\n\t%\n\twhere we reordered $x\\in\\ensuremath{\\mathbb{Z}}_r^n$ in the last step. Therefore\n\t%\n\t\\begin{align*}\n\t    \\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\omega_r^{S\\cdot x}\\big[Mx = w\\big] &= \\left(\\prod_{i=1}^{\\alpha n/t}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^{t}}\\omega_r^{\\sum_{j=1}^t S_{M_{i,j}}x_j}\\left[\\sum_{j=1}^t x_{j} \\equiv w_i~(\\text{mod}~r)\\right] \\right)\\left(\\prod_{i>\\alpha n/t}^{n/t}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^{t}}\\omega_r^{\\sum_{j=1}^t S_{M_{i,j}}x_j}\\right)\\\\\n\t    &= r^{n(1-\\alpha)}\\prod_{i=1}^{\\alpha n/t}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^t}\\omega_r^{S|_{M_i}\\cdot x}\\left[\\sum_{j=1}^t x_j \\equiv w_i~(\\text{mod}~r)\\right],\n\t\\end{align*}\n\twhere $S|_{M_i} = 0^t$ for all $i > \\alpha n/t$, otherwise the expression above is $0$. Now we use that\n\t%\n\t\\begin{align*}\n\t\t\\sum_{j=1}^t x_j \\equiv w_i~(\\text{mod}~r) \\implies x_t \\equiv w_i - \\sum_{j=1}^{t-1} x_j ~(\\text{mod}~r),\n\t\\end{align*}\n\t%\n\tand so\n\t%\n\t\\begin{align*}\n\t\tS|_{M_i}\\cdot x = \\sum_{j=1}^t S_{M_{i,j}}x_i = \\sum_{j=1}^{t-1} S_{M_{i,j}}x_j + S_{M_{i,t}}\\left(w_i - \\sum_{j=1}^{t-1}x_j\\right) = S_{M_{i,t}}w_i + \\sum_{j=1}^{t-1}(S_{M_{i,j}} - S_{M_{i,t}})x_j\n\t\\end{align*}\n\t%\n\tmodulo $r$. This leads to\n\t%\n\t\\begin{align}\n\t    \\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\omega_r^{S\\cdot x}\\big[Mx = w\\big] &= r^{n(1-\\alpha)}\\prod_{i=1}^{\\alpha n/t}\\omega_r^{S_{M_{i,t}}w_i}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^{t-1}}\\omega_r^{\\sum_{j=1}^{t-1}(S_{M_{i,j}} - S_{M_{i,t}})x_j}= \\frac{r^{n}}{r^{\\alpha n/t}}\\prod_{i=1}^{\\alpha n/t}\\omega_r^{S_{M_{i,t}}w_i}\\label{eq:rary1}\n\t\\end{align}\n\t%\n\tif, for all $i\\in[\\alpha n/t]$, $S_{M_{i,j}}$ is constant for all $j\\in[t]$, i.e., if $S_{M_{i,1}} = S_{M_{i,2}} = \\dots = S_{M_{i,t}}$ for any $i\\in[\\alpha n/t]$. Otherwise the above expression is $0$. \n\t%\n\tThus, if $S_{M_{i,1}} = S_{M_{i,2}} = \\dots = S_{M_{i,t}}$ for any $i\\in[\\alpha n/t]$ and $S|_{M_i}=0^t$ for $i>\\alpha n/t$, then we can use Eq.~(\\ref{eq:rary1}) to get (remember that $p_x := r^{-n}$ and $p_w := r^{-\\alpha n/t}$)\n\t%\n\t\\begin{align*}\n   \t\t\\displaybreak[0]|u(M,w,S)| = \\frac{1}{2}\\left|\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}p_xp_M \\omega_r^{S\\cdot x} \\left(\\big[Mx = w\\big] - p_w\\right)\\right| &= \\frac{1}{2}\\frac{1}{r^{\\alpha n/t}}p_M\\left|\\prod_{i=1}^{\\alpha n/t}\\omega_r^{S_{M_{i,t}}w_i} - [S=0^n]\\right|\\displaybreak[0] \\\\\n\t\t&= \\begin{cases}\n\t\t\t0 &~\\text{if}~ S=0^n,\\\\\n\t\t\t\\frac{1}{2} r^{-\\alpha n/t} p_M &~\\text{if}~S\\neq 0^n.\n\t\t\\end{cases}\n\t\\end{align*}\n\t%\n\tHence, we have\n\t%\n\t\\begin{align*}\n\t    |u(M,w,S)| = \\begin{cases}\n\t\t\t0 &\\text{if}~ S=0^n,\\\\\n\t\t\t\\frac{1}{2} r^{-\\alpha n/t} p_M &\\text{if}~S_{M_{i,1}} = S_{M_{1,2}} = \\dots= S_{M_{i,t}}~ \\forall i\\in[\\alpha n/t] ~\\text{and}~ S|_{M_i} = 0^t~\\forall i>\\alpha n/t,\\\\\n\t\t\t0 &\\text{otherwise},\n\t\t\\end{cases}\n\t\\end{align*}\n\tproving the lemma statement\n    \\end{proof}\n    \tWe now proceed to upper bound $\\varepsilon_{bias}$ using the expression for $|u(M,w,S)|$ from Lemma~\\ref{lem:understandingu}. For $S\\in\\ensuremath{\\mathbb{Z}}_r^n$, let $|S| := |\\{i\\in[n]: S_i\\neq 0\\}|$. Notice that, if $S\\in\\Delta(M)$, then $|S| = kt$ for some $k\\in[\\alpha n/t]$. Hence, we have that\n\t%\n\t\\begin{align*}\n\t\t\\varepsilon_{bias} \\leq \\frac{1}{2}\\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n}\\sum_{\\substack{M\\in\\mathcal{M}^\\alpha_{t,n} \\\\ S\\in\\Delta(M)}}p_M \\sum_{w\\in\\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}}\\frac{1}{r^{\\alpha n/t}}\\|\\widehat{\\rho}(S)\\|_1 &= \\frac{1}{2}\\sum_{k=1}^{\\alpha n/t}\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S| = kt}}\\sum_{\\substack{M\\in\\mathcal{M}^\\alpha_{t,n} \\\\ S\\in\\Delta(M)}} p_M\\|\\widehat{\\rho}(S)\\|_1\\\\\n\t\t&= \\frac{1}{2}\\sum_{k=1}^{\\alpha n/t}\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S| = kt}}\\operatorname*{Pr}_{M\\sim\\mathcal{M}_{t,n}^\\alpha}[S\\in\\Delta(M)]\\cdot\\|\\widehat{\\rho}(S)\\|_1,\n\t\\end{align*}\n\t%\n\tusing that \n\t%\n\t\\begin{align*}\n\t    \\sum_{\\substack{M\\in\\mathcal{M}^\\alpha_{t,n} \\\\ S\\in\\Delta(M)}} p_M = \\operatorname*{Pr}_{M\\sim\\mathcal{M}^{\\alpha}_{t,n}}[S\\in\\Delta(M)].\n\t\\end{align*}\n\t%\n\tWe now upper bound this probability using the following lemma.  \n\t%\n\t\\begin{lemma}\n\t    \\label{lem:probcomb}\n\t    Let $t\\in\\ensuremath{\\mathbb{Z}}$. Let $S\\in\\ensuremath{\\mathbb{Z}}_r^n$ with $k_j := \\frac{1}{t}\\cdot |\\{i\\in[n]:S_i = j\\}|\\in\\ensuremath{\\mathbb{Z}}$ \n\t    for $j\\in\\{1,\\dots,r-1\\}$. Let $k := \\sum_{j=1}^{r-1} k_j$. For any $M\\in\\mathcal{M}_{t,n}^\\alpha$, let $\\Delta(M)$ be the set from Lemma~\\ref{lem:understandingu}. Then\n\t    %\n\t    \\begin{align*}\n\t        \\operatorname*{Pr}_{M\\sim\\mathcal{M}_{t,n}^\\alpha}[S\\in\\Delta(M)] = \\frac{\\binom{\\alpha n/t}{k}}{\\binom{n}{kt}}\\frac{k!}{(kt)!}\\prod_{j=1}^{r-1}\\frac{(k_jt)!}{k_j!}.\n\t    \\end{align*}\n\t\\end{lemma}\n\t\\begin{proof}\n\t    We can assume without lost of generality that $S = 1^{k_1t}2^{k_2t}\\dots (r-1)^{k_{r-1}t}0^{n-kt}$. First note that the total number $|\\mathcal{M}_{t,n}^\\alpha|$ of $\\alpha$-partial hypermatchings is $n!/\\big((t!)^{\\alpha n/t}(\\alpha n/t)!(n-\\alpha n)!\\big)$. This can be seen as follows: pick a permutation of $n$, view the first $\\alpha n/t$ tuples of length $t$ as $\\alpha n/t$ hyperedges, and ignore the ordering within each hyperedge, the ordering of the $\\alpha n/t$ hyperedges and the ordering of the last $n-\\alpha n$ vertices. Now, given our particular $S$, notice that $S\\in\\Delta(M)$ if, for $j\\in[r-1]$, $M$ has exactly $k_j$ hyperedges in \n\t    $$\n\t    \\left\\{1 + t\\sum_{i=1}^{j-1}k_i,~2 + t\\sum_{i=1}^{j-1}k_i,~3 + t\\sum_{i=1}^{j-1}k_i,\\ldots,(k_j-1) + t\\sum_{i=1}^{j-1}k_i,~t\\sum_{i=1}^{j}k_i\\right\\},\n\t    $$ \n\t    i.e., $k_1$ hyperedges in $\\{1,\\dots,k_1t\\}$, $k_2$ hyperedges in $\\{k_1t+1,\\dots,(k_2+k_1)t\\}$, etc., and also $\\alpha n/t-k$ hyperedges in $[n]\\setminus[kt]$. The number of ways to pick $k_j$ hyperedges in $\\left\\{1 + t\\sum_{i=1}^{j-1}k_i,\\dots,t\\sum_{i=1}^{j}k_i\\right\\}$ is $(k_jt)!/((t!)^{k_j}k_j!)$. The number of ways to pick the remaining $\\alpha n/t - k$ hyperedges in $[n]\\setminus[kt]$ is $(n-kt)!/((t!)^{\\alpha n/t - k} (\\alpha n/t - k)! (n-\\alpha n)!)$. Hence $\\operatorname*{Pr}_{M\\sim\\mathcal{M}_{t,n}^\\alpha}[S\\in\\Delta(M)]$ equals\n\t    \\[\n\t        \\frac{\\frac{(n-kt)!}{(t!)^{\\alpha n/t - k} (\\alpha n/t - k)! (n-\\alpha n)!}}{\\frac{n!}{(t!)^{\\alpha n/t}(\\alpha n/t)!(n-\\alpha n)!}}\\prod_{j=1}^{r-1}\\frac{(k_jt)!}{(t!)^{k_j}k_j!} = \\frac{(n-kt)!(\\alpha n/t)!}{n!(\\alpha n/t - k)!}\\prod_{j=1}^{r-1}\\frac{(k_jt)!}{k_j!} = \\frac{\\binom{\\alpha n/t}{k}}{\\binom{n}{kt}}\\frac{k!}{(kt)!}\\prod_{j=1}^{r-1}\\frac{(k_jt)!}{k_j!}. \\qedhere\n        \\]\n\t\\end{proof}\n\t%\n\tBy using Lemma~\\ref{lem:probcomb} and the notation $|S|_i := |\\{j\\in[n]:S_j=i\\}|$, we continue upper bounding $\\varepsilon_{bias}$ as follows\n\t%\n\t\\begin{align}\n\t\t\\displaybreak[0]\\varepsilon_{bias} &\\leq \\frac{1}{2}\\sum_{k=1}^{\\alpha n/t}\\frac{\\binom{\\alpha n/t}{k}}{\\binom{n}{kt}}\\sum_{\\substack{k_1,\\dots,k_{r-1}\\geq 0 \\\\ \\sum_{j=1}^{r-1} k_j = k}}\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S|_i = k_it, ~i\\in[r-1]}}\\frac{k!}{(kt)!}\\left(\\prod_{j=1}^{r-1}\\frac{(k_jt)!}{k_j!}\\right) \\|\\widehat{\\rho}(S)\\|_1\\nonumber\\displaybreak[0]\\\\\n\t\t&\\leq \\frac{1}{2}\\sum_{k=1}^{\\alpha n/t}\\frac{\\binom{\\alpha n/t}{k}}{\\binom{n}{kt}}\\sqrt{\\sum_{\\substack{k_1,\\dots,k_{r-1}\\geq 0 \\\\ \\sum_{j=1}^{r-1} k_j = k}}\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S|_i = k_it, ~i\\in[r-1]}}\\frac{k!^2}{(kt)!^2}\\prod_{j=1}^{r-1}\\frac{(k_jt)!^2}{k_j!^2}} \\sqrt{\\sum_{\\substack{k_1,\\dots,k_{r-1}\\geq 0 \\\\ \\sum_{j=1}^{r-1} k_j = k}}\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S|_i = k_it, ~i\\in[r-1]}}\\|\\widehat{\\rho}(S)\\|_1^2}\\label{eq:new_proof1}\\displaybreak[0]\\\\\n\t\t&\\leq \\frac{1}{2}\\sum_{k=1}^{\\alpha n/t}\\frac{\\binom{\\alpha n/t}{k}}{\\binom{n}{kt}}\\sqrt{\\sum_{\\substack{k_1,\\dots,k_{r-1}\\geq 0 \\\\ \\sum_{j=1}^{r-1} k_j = k}}\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S|_i = k_it, ~i\\in[r-1]}}\\frac{k!^2}{(kt)!^2}\\prod_{j=1}^{r-1}\\frac{(k_jt)!^2}{k_j!^2}} \\sqrt{\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S| = kt}}\\|\\widehat{\\rho}(S)\\|_1^2}\\nonumber\\displaybreak[0]\\\\\n\t\t&= \\frac{1}{2}\\sum_{k=1}^{\\alpha n/t}\\frac{\\binom{\\alpha n/t}{k}}{\\sqrt{\\binom{n}{kt}}}\\sqrt{\\sum_{\\substack{k_1,\\dots,k_{r-1}\\geq 0 \\\\ \\sum_{j=1}^{r-1} k_j = k}} \\frac{k!^2}{(kt)!}\\prod_{j=1}^{r-1}\\frac{(k_jt)!}{k_j!^2}} \\sqrt{\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S| = kt}}\\|\\widehat{\\rho}(S)\\|_1^2},\\label{eq:new_proof2}\n\t\\end{align}\n\t%\n\twhere Eqs.~\\eqref{eq:new_proof1} and~\\eqref{eq:new_proof2} used Cauchy-Schwarz inequality and $\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S|_i = k_it}} 1 = \\binom{n}{kt}(kt)!\\prod_{j=1}^{r-1}\\frac{1}{(k_jt)!}$, respectively. We now use the multinomial theorem in\n\t%\n\t\\begin{align}\n\t    \\label{eq:alphadependence}\n\t\t\\sum_{\\substack{k_1,\\dots,k_{r-1}\\geq 0 \\\\ \\sum_{j=1}^{r-1} k_j = k}} \\frac{k!^2}{(kt)!}\\prod_{j=1}^{r-1}\\frac{(k_jt)!}{k_j!^2} &= \\sum_{\\substack{k_1,\\dots,k_{r-1}\\geq 0 \\\\ \\sum_{j=1}^{r-1} k_j = k}} \\frac{\\binom{k}{k_1,\\dots,k_{r-1}}^2}{\\binom{kt}{k_1t,\\dots,k_{r-1}t}} \\leq \\sum_{\\substack{k_1,\\dots,k_{r-1}\\geq 0 \\\\ \\sum_{j=1}^{r-1} k_j = k}} \\binom{k}{k_1,\\dots,k_{r-1}} = (r-1)^k,\n\t\\end{align}\n\t%\n\twhich leads to\n\t%\n\t\\begin{align*}\n\t\t\\varepsilon_{bias} \\leq \\frac{1}{2}\\sum_{k=1}^{\\alpha n/t}\\alpha^k\\frac{\\binom{ n/t}{k}}{\\sqrt{\\binom{n}{kt}}}(r-1)^{k/2} \\sqrt{\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S| = kt}}\\|\\widehat{\\rho}(S)\\|_1^2},\n\t\\end{align*}\n\t%\n\twhere we also used that $\\binom{\\alpha n/t}{k} \\leq \\alpha^k \\binom{n/t}{k}$ for $\\alpha\\in[0,1]$. In order to compute the above sum, we shall split it into two parts: one in the range $1\\leq k < 4rm$, and the other in the range $4rm \\leq k \\leq \\alpha n/t$. \n\n\t\\textbf{Sum I} ($1\\leq k < 4rm$): in order to upper bound each term, pick $\\delta = k/(4rm)$ in Lemma~\\ref{lem:hypercontractive}, so\n\t%\n\t\\begin{align*}\n\t\t\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S| = kt}} \\|\\widehat{\\rho}(S)\\|_1^2 \\leq \\frac{1}{\\delta^{kt}}\\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n}\\delta^{|S|} \\|\\widehat{f}(S)\\|_1^2 \\leq \\frac{1}{\\delta^{kt}}2^{2r\\delta m} = \\left(\\frac{2^{1/(2t)}4rm}{k}\\right)^{kt}.\n\t\\end{align*}\n\t%\n\tTherefore, and by using that $m \\leq \\frac{\\gamma}{r^{1+1/t}} (\\frac{\\varepsilon^2}{\\alpha})^{2/t} (n/t)^{1-2/t}$ and $\\binom{q}{s}^2\\binom{\\ell q}{\\ell s}^{-1} \\leq (\\frac{s}{q})^{(\\ell-2)s}$ (see~\\cite[Appendix~A.5]{shi2012limits}) for $q=n/t,s=k,\\ell=t$, we have\n\t%\n\t\\begin{align*}\n\t\t\\frac{1}{2}\\sum_{k=1}^{4rm-1}\\alpha^k\\frac{\\binom{n/t}{k}}{\\sqrt{\\binom{n}{kt}}}(r-1)^{k/2}\\sqrt{\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S| = kt}} \\|\\widehat{\\rho}(S)\\|_1^2} &\\leq \\frac{1}{2}\\sum_{k=1}^{4rm-1} \\alpha^k (r-1)^{k/2}\\left(\\frac{kt}{n}\\right)^{(1-2/t)kt/2}\\left(\\frac{2^{1/(2t)}4rm}{k}\\right)^{kt/2}\\\\\n\t\t&\\leq \\frac{1}{2}\\sum_{k=1}^{4rm-1} \\alpha^k (r-1)^{k/2}\\left(\\frac{2^{1/(2t)}4\\gamma \\varepsilon^{4/t}}{\\alpha^{2/t}r^{1/t}k^{2/t}}\\right)^{kt/2}\\\\\n\t\t&\\leq \\frac{1}{2}\\sum_{k=1}^{4rm-1}\\left(\\frac{2^{1/4}(4\\gamma)^{t/2} \\varepsilon^2 }{k}\\right)^{k} \\leq \\frac{\\varepsilon^2}{2}\n\t\\end{align*}\n\t%\n\tfor sufficiently small $\\gamma$. \n\t\n\t\\textbf{Sum II} ($4rm \\leq k \\leq \\alpha n/t$): first we note that the function $g(k) := \\alpha^k (r-1)^{k/2}\\binom{n/t}{k}/\\sqrt{\\binom{n}{kt}}$ is non-increasing in the interval $1\\leq k \\leq \\alpha n/t \\leq n/(2t)$. That is because $\\alpha \\sqrt{r-1} \\leq 1$, and so\n\t%\n\t\\begin{align*}\n\t\t\\displaybreak[0]\\frac{g(k-1)}{g(k)} \\geq \\frac{\\binom{n/t}{k-1}}{\\sqrt{\\binom{n}{kt-t}}}\\frac{\\sqrt{\\binom{n}{kt}}}{\\binom{n/t}{k}} = \\sqrt{\\frac{kt}{n-kt+t}\\prod_{j=1}^{t-1}\\frac{n-kt+j}{kt-j}} &\\geq \\sqrt{\\frac{kt}{n-kt+t}\\prod_{j=1}^{t-1}\\frac{n-kt+j+1}{kt-j+1}}\\displaybreak[0]\\\\\n\t\t&= \\sqrt{\\prod_{j=1}^{t-2}\\frac{n-kt+j+1}{kt-j}} \\geq 1,\n\t\\end{align*}\n\t%\n\twhere we used that $\\frac{a}{b} \\geq \\frac{a+s}{b+s}$ for all $a,b,s>0$ with $a\\geq b$. Hence, and with the aid once more of Lemma~\\ref{lem:hypercontractive} with $\\delta=1$ and the inequality $\\binom{q}{s}^2\\binom{\\ell q}{\\ell s}^{-1} \\leq (\\frac{s}{q})^{(\\ell-2)s}$ (for $q=n/t,s=2m,\\ell=t$) in order to bound $g(4rm)$,\n\t%\n\t\\begin{align}\n\t\t\\frac{1}{2}\\sum_{k=4rm}^{\\alpha n/t}\\alpha^k\\frac{\\binom{n/t}{k}}{\\sqrt{\\binom{n}{kt}}}(r-1)^{k/2}\\sqrt{\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S| = kt}} \\|\\widehat{\\rho}(S)\\|_1^2} &\\leq \\frac{1}{2}g(4rm) \\sum_{k=4rm}^{\\alpha n/t}\\sqrt{\\sum_{\\substack{S\\in\\ensuremath{\\mathbb{Z}}_r^n \\\\ |S| = kt}} \\|\\widehat{\\rho}(S)\\|_1^2}\\nonumber\\\\\n\t\t&\\leq \\frac{1}{2}g(4rm)\\sqrt{\\frac{\\alpha n}{t}}\\sqrt{\\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^n} \\|\\widehat{\\rho}(S)\\|_1^2}\\label{eq:eqfinalhidden}\\\\\n\t\t&\\leq \\frac{1}{2}\\left(\\alpha \\sqrt{r-1}\\right)^{4rm}\\left(\\frac{4rm}{n/t}\\right)^{2(t-2)rm}\\sqrt{\\frac{\\alpha n}{t}}2^{(r-1)m}\\nonumber\\\\\n\t\t&\\leq \\frac{1}{2}\\left(2^{1/4}\\alpha \\sqrt{r-1}\\right)^{4rm} \\left(\\frac{(4\\gamma)^{t/2}\\varepsilon^2}{\\alpha \\sqrt{r}(n/t)}\\right)^{4(1-2/t)rm}\\sqrt{\\frac{\\alpha n}{t}}\\nonumber\\\\\n\t\t&\\leq \\frac{\\varepsilon^2}{2},\\nonumber\n\t\\end{align}\n\t%\n\twhere Eq.~(\\ref{eq:eqfinalhidden}) comes from Cauchy-Schwarz, and in the last step we used that $m\\geq 1 \\implies 4(1-2/t)m \\geq 1$ (so $n$ is in the denominator and $\\varepsilon^{4(1-2/t)m} \\leq \\varepsilon$) and picked $\\gamma$ sufficiently small. \n\t\n\tFinally, merging both results, we get that, if $m \\leq \\frac{\\gamma}{r^{1+1/t}}(\\frac{ \\varepsilon^2}{\\alpha})^{2/t} (n/t)^{1-2/t}$, then $\\varepsilon_{bias} \\leq \\varepsilon^2$.\n\\end{proof}\n\nA very similar classical communication lower bound for the $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ problem can be proven.\n\\begin{theorem}\n    \\label{thr:thr_classicalhh}\n    Any one-way classical protocol that achieves advantage $\\varepsilon>0$ for the $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ problem with $t\\geq 2$ and $\\alpha \\leq 1/2$ requires at least $\\Omega(r^{-1}(\\varepsilon^4/\\alpha)^{1/t}(n/t)^{1-1/t})$ bits of communication.\n\\end{theorem}\nThe proof is very similar to that of past works~\\cite{gavinsky2007exponential,verbin2011streaming,DBLP:conf/approx/GuruswamiT19} and we include it in Appendix~\\ref{app:appB} for completeness. We now conclude this section with a remark that improves the $r$ dependence of the $\\alpha$ parameter.\n\\begin{remark}\n    \\label{rem:alphadependence}\n    The dependence of $\\alpha$ on $r$ can be improved. For example, we can improve the bound in Eq.~(\\ref{eq:alphadependence}) by observing that $\\binom{k}{k_1,\\dots,k_{r-1}}^2\\binom{kt}{k_1t,\\dots,k_{r-1}t}^{-1} \\leq 1$, which can be seen from the identity $\\binom{k}{k_1,\\dots,k_{r-1}} = \\binom{k_1}{k_1}\\binom{k_1+k_2}{k_2}\\cdots\\binom{k_1+k_2+\\cdots+k_{r-1}}{k_{r-1}}$ and the inequality $\\binom{q}{s}^2\\binom{\\ell q}{\\ell s}^{-1} \\leq (\\frac{s}{q})^{(\\ell-2)s} \\leq 1$. Hence\n    %\n    \\begin{align*}\n        \\sum_{\\substack{k_1,\\dots,k_{r-1}\\geq 0 \\\\ \\sum_{j=1}^{r-1} k_j = k}} \\frac{k!^2}{(kt)!}\\prod_{j=1}^{r-1}\\frac{(k_jt)!}{k_j!^2} &= \\sum_{\\substack{k_1,\\dots,k_{r-1}\\geq 0 \\\\ \\sum_{j=1}^{r-1} k_j = k}} \\frac{\\binom{k}{k_1,\\dots,k_{r-1}}^2}{\\binom{kt}{k_1t,\\dots,k_{r-1}t}} \\leq \\sum_{\\substack{k_1,\\dots,k_{r-1}\\geq 0 \\\\ \\sum_{j=1}^{r-1} k_j = k}} 1 = \\binom{k+r-2}{k},\n    \\end{align*}\n    %\n    which is better than $(r-1)^{k}$. By bounding\n    %\n    \\begin{align*}\n        \\binom{k+r-2}{k} \\leq e^k\\left(1 + \\frac{r-2}{k}\\right)^{k},\n    \\end{align*}\n    %\n    the new function $g(k) := \\alpha^k \\sqrt{\\binom{k+r-2}{k}}\\binom{n/t}{k}/\\sqrt{\\binom{n}{kt}}$ is still non-increasing in the interval $4rm \\leq k \\leq \\alpha n/t \\leq n/(2t)$ if now\n    %\n    \\begin{align*}\n        \\alpha \\leq e^{-1/2}\\operatorname*{\\min}_{4rm\\leq k \\leq \\alpha n/t}\\sqrt{\\frac{k}{k+r-2}} = e^{-1/2}\\sqrt{\\frac{4rm}{4rm+r-2}}.\n    \\end{align*}\n    %\n    For $m\\gg 1$, $\\alpha$ is essentially independent of $r$, and hence $\\alpha \\leq \\min(1/2,e^{-1/2}) = 1/2$.\n\\end{remark}\n \n\n\\subsection{Quantum streaming lower bound for Unique Games on hypergraphs}\n\nThe Unique Games problem is a generalization of the classical Max-Cut and can in fact be viewed as constraint satisfaction problems on a graph but over a larger alphabet. Consider a graph on $n$ vertices $x_1,\\ldots,x_n$ and edges in $E$. The constraint on an arbitrary edge $(i,j)\\in E$ is specified by a permutation $\\pi_{i,j}:\\ensuremath{\\mathbb{Z}}_r\\rightarrow \\ensuremath{\\mathbb{Z}}_r$ and the goal is to find an assignment of $x_1,\\ldots,x_n\\in \\ensuremath{\\mathbb{Z}}_r$ that maximizes \n$$\n\\sum_{(i,j)\\in E} [\\pi_{i,j}(x_i)=x_j].\n$$ \nIn this section, we consider a generalization of Unique Games to hypergraphs.\n\\begin{definition}[Unique Games instance on hypergraphs]\n\\label{def:UGhyper}\n    A hypergraph $H=(V,E)$ is defined on a vertex set $V$ of size $n$ with $t$-sized hyperedges $E$ (i.e., $t$-sized subsets of $V$). Given a linear constraint on a hyperedge $e\\in E$, i.e., a linear function $\\pi_e:\\ensuremath{\\mathbb{Z}}_r^t\\rightarrow \\{0,1\\}$, the goal is to compute\n    $$\n        \\max_{x\\in \\ensuremath{\\mathbb{Z}}_r^n}\\sum_{e\\in E} \\pi_{e}(x_e),\n    $$\n    where $x_e$ corresponds to the set of vertex-assignment in the hyperedge $e\\in E$.    \n\\end{definition}\n\\begin{definition}\n    Let $H=(V,E)$ be a hypergraph and let $\\operatorname{OPT}$ be the optimal value of the Unique Games on $H$. A randomized algorithm gives a $\\gamma$-approximation to a Unique Games instance with failure probability $\\delta\\in[0,1/2)$ if, on any input hypergraph $H$, it outputs a value in the interval $[\\operatorname{OPT}/\\gamma, \\operatorname{OPT}]$ with probability at least $1-\\delta$.\n\\end{definition}\nA uniformly random assignment of $x\\in \\ensuremath{\\mathbb{Z}}_r^n$ to the vertex set $V$ will satisfy a $1/r$-fraction of the hyperedges, since each linear constraint $\\pi_e(x_e)$ is satisfied with probability $1/r$. This gives a trivial $r$-approximation algorithm for the problem above. Below we show that any better than trivial approximation requires space that scales as $n^\\beta$ for constant $\\beta>0$.\n\\begin{theorem}\n\\label{thm:streaminguglowerbound}\n    Let $r,t\\geq 2$ be integers. Every {quantum} streaming algorithm giving a $(r-\\varepsilon)$-approximation for Unique Games on hypergraphs (as in Definition~\\ref{def:UGhyper}) with at most $t$-sized hyperedges with alphabet size $r$ and success probability at least $2/3$ over its internal randomness, needs $\\Omega((n/t)^{1-2/t})$ space (which hides dependence on $r,\\varepsilon$). \n\\end{theorem}\n The proof of this theorem combines techniques used by Guruswami and Tao~\\cite{DBLP:conf/approx/GuruswamiT19} and Kapralov, Khanna and Sudan~\\cite{kapralov2014streaming}. Akin to these works, based on the Hidden Matching problem, we will construct instances of the hypergraph for which a Unique Games instance is hard to solve space-efficiently in the streaming model. \n\n\\paragraph{Input distributions.} To this end, we construct two distributions $\\ensuremath{\\mathcal{Y}}$ and $\\ensuremath{\\mathcal{N}}$ such that $\\ensuremath{\\mathcal{Y}}$ is supported on satisfiable Unique Games instances and $\\ensuremath{\\mathcal{N}}$ is supported on instances for which at most an $O(1/r)$-fraction of the constraints is satisfied. We now define these instances in a multi-stage way (using $k$ stages). First, sample $k$ independent $\\alpha$-partial $t$-hypermatchings on $n$ vertices and then construct a hypergraph $G$ by putting together all the hyperedges from these $k$ stages. Note that $G$ still has $n$ vertices, while the number of hyperedges is $k\\cdot \\alpha n /t$ (since each stage has $\\alpha n/t$ many hyperedges and we allow multiple hyperedges should they be sampled). Now we specify the constraints $\\pi_e$ in Definition~\\ref{def:UGhyper} for the $\\ensuremath{\\mathcal{Y}},\\ensuremath{\\mathcal{N}}$ distributions:\n\\begin{itemize}\n    \\item $\\ensuremath{\\mathcal{Y}}$ distribution: sample $z\\in \\ensuremath{\\mathbb{Z}}_r^n$ and for each $e\\in E$, let $\\pi_e(x_e)=\\big[\\sum_{i\\in e}x_i=\\sum_{i\\in e}z_i\\big]$ (where by $i\\in e$ we mean all the vertices in the hyperedge $e$).\n        \\item $\\ensuremath{\\mathcal{N}}$ distribution: for each $e\\in E$, pick a uniform $q\\in \\ensuremath{\\mathbb{Z}}_r$ and let $\\pi_e(x_e)=\\big[\\sum_{i\\in e}x_i=q\\big]$.\n\\end{itemize}\nIt is clear that, in the $\\ensuremath{\\mathcal{Y}}$ distribution, the optimal solution is when all the $x_1,\\ldots,x_n$ are just set to $z_1,\\ldots,z_n$. Below we show that for the $\\ensuremath{\\mathcal{N}}$ distribution, the value of the optimal solution is at most $(1+\\varepsilon)/r$ with high probability.\n\n\\begin{lemma}\n    \\label{lem:NOdistribution}\n    Let $\\varepsilon\\in (0,1)$. If $k=O(r(\\log r)t/(\\alpha\\varepsilon^2))$, then for the Unique Games instance sampled from $\\ensuremath{\\mathcal{N}}$ distribution above, the optimal fraction of satisfiable constraints  (i.e., number of hyperedges $e\\in E$ for which $\\pi_e(\\cdot)$ evaluates to $1$) over all possible vertex labelling is at most $(1+\\varepsilon)/r$ with high probability.\n\\end{lemma}\n\\begin{proof}\n    The proof of this lemma is similar to the proof in~\\cite[Lemma~4.1]{DBLP:conf/approx/GuruswamiT19}. Fix an assignment $x\\in \\ensuremath{\\mathbb{Z}}_r^n$. Let $X^\\ell_{e}$ be the random variable that indicates that the hyperedge $e\\in E$ appears in $\\ell$-th stage and is satisfied by $x$. Let $S=\\sum_{\\ell,e}X^{\\ell}_e$. The expectation of $S$ is $k\\alpha n/t \\cdot 1/r$, since the total number of hyperedges is $\\alpha n/t$ for each of the $k$ stages and the probability that a uniform $x$ satisfies a $t$-hyperedge (i.e., probability that $\\sum_{e\\in E} x_e=q$ for some fixed $q$) is $1/r$. Using the same analysis in~\\cite{DBLP:conf/approx/GuruswamiT19}, we can show that the variables $X^\\ell_{e}$ are negatively correlated. Indeed, first note that hyperedges from different stages are independent. Now suppose we know that the random variables $X^{\\ell}_{e_1},\\dots,X^{\\ell}_{e_s}$ have value $1$, and we also know a hyperedge $e\\in E$. If $e\\cap e_u \\neq \\emptyset$ for some $u\\in[s]$, then $X^\\ell_e=0$, since the hyperedges of a given stage form a matching. Otherwise, the conditional expectation of $X^\\ell_e$ (conditioned on $e\\cap e_u = \\emptyset$ for all $u\\in[s]$) is $\\frac{\\alpha n/t - s}{r}\\binom{n-ts}{t}^{-1}$, which is less than its unconditional expectation of $\\frac{\\alpha n/t}{r}\\binom{n}{t}^{-1}$. Therefore, in all cases one has $\\mathbb{E}[X^\\ell_e|X^\\ell_{e_1}=\\cdots=X^\\ell_{e_s} = 1] \\leq \\mathbb{E}[X^\\ell_e]$, which means negative correlation.\n    \n    Hence, using a Chernoff bound for negative-correlated variables leads to\n    $$\n        \\Pr[S\\geq (1+\\varepsilon)(k\\alpha n/t)/r]\\leq \\exp(-\\varepsilon^2 k\\alpha n /(3rt))=\\exp(-O(n\\log r)),\n    $$\n    where the inequality used the choice of $k$. Applying a union bound over the set of $x\\in \\ensuremath{\\mathbb{Z}}_r^n$ concludes the proof of the lemma.\n\\end{proof}\n\n\n\\paragraph{Reduction to Hypermatching.} The reduction to $r$-ary Hidden Hypermatching is similar to the analysis used by Guruswami and Tao~\\cite{DBLP:conf/approx/GuruswamiT19}, but now it is from quantum streaming algorithms to one-way quantum communication complexity. The main lemma that we need is the following. \n\\begin{lemma}\n\\label{lem:reductionfromyesnotobhm}\n    Let $\\varepsilon>0$. If there is a streaming algorithm using at most $c$ qubits of space that distinguishes between the $\\ensuremath{\\mathcal{Y}}$ and $\\ensuremath{\\mathcal{N}}$ distributions on Unique Games instances (with $k$ stages) with bias $1/3$, then there is a $c$-qubit protocol that distinguish between the $\\mathsf{YES}$ and $\\mathsf{NO}$ distributions of $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ with bias $\\Omega(1/k)$. \n\\end{lemma}\n\nIn order to prove this lemma we need a few definitions and facts. First, towards proving the lemma above, let us assume there is a $c$-qubit streaming $\\ensuremath{\\mathcal{A}}$ for Lemma~\\ref{lem:reductionfromyesnotobhm}. During the execution of the streaming protocol on instances from the $\\ensuremath{\\mathcal{Y}}$ and $\\ensuremath{\\mathcal{N}}$ distributions, let the memory content after receiving the $i$th stage constraints be given by the $c$-qubit quantum states $\\ket{\\phi_i^\\ensuremath{\\mathcal{Y}}}$ and $\\ket{\\phi_i^\\ensuremath{\\mathcal{N}}}$, respectively.\\footnote{Without loss of generality, we assume they are pure states-- this only affects the cost of the protocol by a constant factor (since one can always purify mixed quantum states by doubling the dimension).} Assume that $|\\phi_0^\\ensuremath{\\mathcal{Y}}\\rangle = |\\phi_0^\\ensuremath{\\mathcal{N}}\\rangle = 0$. Using the notion of informative index from~\\cite[Definition~6.2]{kapralov2014streaming}, we say an index $j\\in \\{0,\\ldots,k-1\\}$ is $\\delta$-informative if \n$$\n    \\big\\|\\ket{\\phi^\\ensuremath{\\mathcal{Y}}_{j+1}}-\\ket{\\phi^\\ensuremath{\\mathcal{N}}_{j+1}}\\big\\|_1\\geq \\big\\|\\ket{\\phi^\\ensuremath{\\mathcal{Y}}_{j}}-\\ket{\\phi^\\ensuremath{\\mathcal{N}}_{j}}\\big\\|_1+\\delta.\n$$\nWith this definition it is not hard to see the following fact, which follows from a simple triangle inequality.\n\\begin{fact}\n    \\label{fact:informativeindex}\n    Suppose there exists a streaming protocol for distinguishing the $\\ensuremath{\\mathcal{Y}},\\ensuremath{\\mathcal{N}}$ distributions with advantage $\\geq 1/3$, then there exists a $\\Omega(1/k)$-informative index.\n\\end{fact}\nSuppose $j^*$ is an $\\Omega(1/k)$-informative index for the streaming protocol $\\ensuremath{\\mathcal{A}}$. Using this we devise a communication protocol for $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ with bias $\\Omega(1/k)$ as follows: suppose Alice has a string $x\\in \\ensuremath{\\mathbb{Z}}_r^n$ and Bob has $w\\in \\ensuremath{\\mathbb{Z}}_r^{\\alpha n/t}$ and a hypermatching $M\\in\\mathcal{M}_{t,n}^\\alpha$.\n\\begin{enumerate}\n    \\item Alice samples $j^*$ many $\\alpha$-partial $t$-hypermatchings and runs the streaming algorithm $\\ensuremath{\\mathcal{A}}$ on Unique Games constraints for the first $j^*$ stages that follow the $\\ensuremath{\\mathcal{Y}}$ distribution with $z=x$. She then sends the memory contents after these $j^\\ast$ stages to Bob.\n    \\item Bob assigns the constraints $\\sum_{i\\in e}x_i=w_e$, where $e\\in M$, according to his inputs $w,M$. He then continues running $\\ensuremath{\\mathcal{A}}$ on these constraints as the $(j^*+1)$th~stage. \n\n    Let $\\ket{s}$ be the quantum state that Bob gets after running $\\mathcal{A}$.\n    \\item Let $\\ket{\\phi^\\ensuremath{\\mathsf{YES}}}$ and $\\ket{\\phi^\\ensuremath{\\mathsf{NO}}}$ be the resulting quantum states under the two cases, depending on $w$'s distribution (these can be computed by Bob since $\\mathcal{A}$ is known). Bob can distinguish between $\\ket{\\phi^\\ensuremath{\\mathsf{YES}}}$ and $\\ket{\\phi^\\ensuremath{\\mathsf{NO}}}$ with bias $\\frac{1}{2}\\big\\||\\phi^\\ensuremath{\\mathsf{YES}}\\rangle-|\\phi^\\ensuremath{\\mathsf{NO}}\\rangle\\big\\|_1$ by measuring the state $|s\\rangle$ with a suitable POVM, according to Lemma~\\ref{lem:lem3.5.c3}.\n\\end{enumerate}\n\nWe are now ready to prove Lemma~\\ref{lem:reductionfromyesnotobhm}.\n\\begin{proof}[Proof of Lemma~\\ref{lem:reductionfromyesnotobhm}]\n    We argue that the above protocol achieves a $\\Omega(1/k)$ bias in distinguishing between the $\\mathsf{YES}$ and $\\mathsf{NO}$ distributions from $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$. To this end, let $U$ be the unitary that maps the quantum state after stage $j^*$ and constraints of stage $j^*+1$ (which is classical) to the quantum state after $j^*+1$. Thus we have $\\ket{\\phi^\\ensuremath{\\mathsf{YES}}}=\\ket{\\phi^\\ensuremath{\\mathcal{Y}}_{j^*+1}}=U\\ket{\\phi^\\ensuremath{\\mathcal{Y}}_{j^*},C^\\ensuremath{\\mathcal{Y}}}$ and $\\ket{\\phi^\\ensuremath{\\mathsf{NO}}}=U\\ket{\\phi^\\ensuremath{\\mathcal{Y}}_{j^*},C^\\ensuremath{\\mathcal{N}}}$, where $C^\\ensuremath{\\mathcal{Y}}$ and $C^\\ensuremath{\\mathcal{N}}$ are the constraints corresponding to the $\\ensuremath{\\mathsf{YES}}$ and $\\ensuremath{\\mathsf{NO}}$ distributions, respectively, and, similarly, we have $\\ket{\\phi^\\ensuremath{\\mathcal{N}}_{j^*+1}}=U\\ket{{\\phi^\\ensuremath{\\mathcal{N}}_{j^*}},C^\\ensuremath{\\mathcal{N}}}$. Then, we have\n    \\begin{align*}\n        \\big\\|\\ket{\\phi^\\ensuremath{\\mathsf{YES}}}-\\ket{\\phi^\\ensuremath{\\mathsf{NO}}}\\big\\|_1 &\\geq      \\big\\|\\ket{\\phi^\\ensuremath{\\mathcal{Y}}_{j^*+1}}-\\ket{\\phi^\\ensuremath{\\mathcal{N}}_{j^*+1}}\\big\\|_1-     \\big\\|\\ket{\\phi^\\ensuremath{\\mathsf{NO}}}-\\ket{\\phi^\\ensuremath{\\mathcal{N}}_{j^*+1}}\\big\\|_1\\\\\n        &\\geq \\big\\|\\ket{\\phi^\\ensuremath{\\mathcal{Y}}_{j^*+1}}-\\ket{\\phi^\\ensuremath{\\mathcal{N}}_{j^*+1}}\\big\\|_1 -     \\big\\|\\ket{\\phi^\\ensuremath{\\mathcal{Y}}_{j^*}}-\\ket{\\phi^\\ensuremath{\\mathcal{N}}_{j^*}}\\big\\|_1 =\\Omega(1/k),\n    \\end{align*}\n    %\n    where the second inequality used that  $\\big\\|\\ket{\\phi^\\ensuremath{\\mathsf{NO}}}-\\ket{\\phi^\\ensuremath{\\mathcal{N}}_{j^*+1}}\\big\\|_1 = \\big\\|U\\ket{\\phi^\\ensuremath{\\mathcal{Y}}_{j^*},C^\\ensuremath{\\mathcal{N}}}-U\\ket{\\phi^\\ensuremath{\\mathcal{N}}_{j^*},C^\\ensuremath{\\mathcal{N}}}\\big\\|_1\\leq \\|\\ket{\\phi^\\ensuremath{\\mathcal{Y}}_{j^*}}-\\ket{\\phi^\\ensuremath{\\mathcal{N}}_{j^*}}\\|_1$ (since unitaries preserve norms) and the third inequality is because $j^*$ is an informative index. Hence in Step (3) of the procedure above, the bias of Bob in obtaining the right outcome is~$\\Omega(1/k)$.\n\\end{proof}\n\n\\begin{proof}[Proof of Theorem~\\ref{thm:streaminguglowerbound}] Finally, by picking $k=O(r(\\log r)t/(\\alpha\\varepsilon^2))$ in order to invoke Lemma~\\ref{lem:NOdistribution} and using our lower bound in Theorem~\\ref{thm:hypermatchinglowerbound} with $\\alpha= O(1)$, we get our desired lower bound of\n\\[\n\\Omega(r^{-(1+1/t)}(k^2\\alpha)^{-2/t}(n/t)^{1-2/t})=\\Omega((n/t)^{1-2/t}). \\qedhere\n\\]\n\\end{proof}\n\nIt is possible to prove a classical version of Theorem~\\ref{thm:streaminguglowerbound}.\n\\begin{theorem}\n\\label{thm:streaminguglowerbound2}\n    Let $r,t\\geq 2$ be integers. Every classical streaming algorithm giving an $(r-\\varepsilon)$-approximation for Unique Games on hypergraphs (as in Definition~\\ref{def:UGhyper}) with at most $t$-sized hyperedges with alphabet size $r$ and success probability at least $2/3$ over its internal randomness, needs $\\Omega((n/t)^{1-1/t})$ space (which hides dependence on $r,\\varepsilon$).\n\\end{theorem}\n\\begin{proof}\n    Since the proof is very similar to the one of Theorem~\\ref{thm:streaminguglowerbound}, we shall just point out the few required modifications. The main idea is still to reduce a streaming algorithm for Unique Games to a communication protocol for $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$. The distributions $\\ensuremath{\\mathcal{Y}}$ and $\\ensuremath{\\mathcal{N}}$ on the Unique Games inputs are the same. Let $S_i^\\ensuremath{\\mathcal{Y}}$ and $S_i^\\ensuremath{\\mathcal{N}}$ be the memory after receiving the $i$th stage constraints. The notion of information index is similarly defined for $S_i^\\ensuremath{\\mathcal{Y}}$ and $S_i^\\ensuremath{\\mathcal{N}}$, i.e., an index $j\\in \\{0,\\ldots,k-1\\}$ is $\\delta$-informative if \n    $$\n        \\big\\|S_{j+1}^\\ensuremath{\\mathcal{Y}} - S_{j+1}^\\ensuremath{\\mathcal{N}}\\big\\|_{\\text{tvd}}\\geq \\big\\|S_j^\\ensuremath{\\mathcal{Y}}-S_j^\\ensuremath{\\mathcal{Y}}\\big\\|_{\\text{tvd}}+\\delta.\n    $$\n    \n    The communication protocol for $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ is basically the same as in the quantum case, using an $\\Omega(1/k)$-informative index $j^\\ast$. At the end of Step (2), Bob gets the memory $s$. Let $S^\\ensuremath{\\mathsf{YES}}$ and $S^\\ensuremath{\\mathsf{NO}}$ be the resulting memory distributions under the two cases depending on $w$'s distribution. Bob outputs $1$ if $\\operatorname{Pr}[S^\\ensuremath{\\mathsf{YES}} = s] \\geq \\operatorname{Pr}[S^\\ensuremath{\\mathsf{NO}} = s]$, and $0$ otherwise. The bias of distinguishing between $S^\\ensuremath{\\mathsf{YES}}$ and $S^\\ensuremath{\\mathsf{NO}}$ is $\\frac{1}{2}\\|S^\\ensuremath{\\mathsf{YES}} - S^\\ensuremath{\\mathsf{NO}}\\|_{\\text{tvd}}$, which can be shown to be at least $\\Omega(1/k)$, similarly to the quantum case. Indeed, let $f$ be the function that maps the memory after stage $j^\\ast$ and constraints $C^\\ensuremath{\\mathcal{Y}}$ or $C^\\ensuremath{\\mathcal{N}}$ of stage $(j^\\ast+1)$ to the memory after stage $(j^\\ast+1)$. Then $S^\\ensuremath{\\mathsf{YES}} = S^\\ensuremath{\\mathcal{Y}}_{j^\\ast+1} = f(S^\\ensuremath{\\mathcal{Y}}_{j^\\ast},C^\\ensuremath{\\mathcal{Y}})$ and $S^\\ensuremath{\\mathsf{NO}} = f(S^\\ensuremath{\\mathcal{Y}}_{j^\\ast},C^\\ensuremath{\\mathcal{N}})$. By using Lemma~\\ref{lem:functionbias} below, we can show that\n    %\n    \\begin{align*}\n        \\big\\|S^\\ensuremath{\\mathsf{YES}} - S^\\ensuremath{\\mathsf{NO}}\\big\\|_{\\text{tvd}} &\\geq \\big\\|S^\\ensuremath{\\mathcal{Y}}_{j^\\ast+1} - S^\\ensuremath{\\mathcal{N}}_{j^\\ast+1}\\big\\|_{\\text{tvd}} - \\big\\|S^\\ensuremath{\\mathsf{NO}} - S^\\ensuremath{\\mathcal{N}}_{j^\\ast+1}\\big\\|_{\\text{tvd}}\\\\\n        &= \\big\\|S^\\ensuremath{\\mathcal{Y}}_{j^\\ast+1} - S^\\ensuremath{\\mathcal{N}}_{j^\\ast+1}\\big\\|_{\\text{tvd}} - \\big\\|f(S^\\ensuremath{\\mathcal{Y}}_{j^\\ast},C^\\ensuremath{\\mathcal{Y}}) - f(S^\\ensuremath{\\mathcal{N}}_{j^\\ast},C^\\ensuremath{\\mathcal{N}})\\big\\|_{\\text{tvd}}\\\\\n        &\\geq \\big\\|S^\\ensuremath{\\mathcal{Y}}_{j^\\ast+1} - S^\\ensuremath{\\mathcal{N}}_{j^\\ast+1}\\big\\|_{\\text{tvd}} - \\big\\|S^\\ensuremath{\\mathcal{Y}}_{j^\\ast} - S^\\ensuremath{\\mathcal{N}}_{j^\\ast}\\big\\|_{\\text{tvd}} \\geq \\Omega(1/k).\n    \\end{align*}\n    %\n    \\begin{lemma}[{\\cite[Claim 6.5]{kapralov2014streaming}}]\n        \\label{lem:functionbias}\n        Let $X,Y$ be two random variables and $W$ be independent of $(X,Y)$. Then, for any function $f$,\n        %\n        \\begin{align*}\n            \\|f(X,W) - f(Y,W)\\|_{\\operatorname{tvd}} \\leq \\|X-Y\\|_{\\operatorname{tvd}}.\n        \\end{align*}\n    \\end{lemma}\n    \n    Therefore Bob can distinguish between $S^\\ensuremath{\\mathsf{YES}}$ and $S^\\ensuremath{\\mathsf{NO}}$ with bias at least $\\Omega(1/k)$, meaning that there is a $c$-bit protocol that distinguish between the $\\mathsf{YES}$ and $\\mathsf{NO}$ distributions of $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$ with bias $\\Omega(1/k)$. By picking $k=O(r(\\log r)t/(\\alpha\\varepsilon^2))$ in order to invoke Lemma~\\ref{lem:NOdistribution} and using the classical lower bound on $r\\text{-}\\ensuremath{\\mathsf{HH}}(\\alpha,t,n)$, we get our desired lower bound of\n    \\[\n        \\Omega(r^{-1}(k^4\\alpha)^{-1/t}(n/t)^{1-1/t})=\\Omega((n/t)^{1-1/t}). \\qedhere\n    \\]\n\\end{proof}\n\n\n\\section{Locally Decodable Codes}\nIn this section we prove our lower bound on locally decodable codes over $\\ensuremath{\\mathbb{Z}}_r$. Before that, let us first formally define an $\\mathsf{LDC}$.\n\\begin{definition}[Locally decodable code]\n    A $(q,\\delta,\\varepsilon)$-locally decodable code over $\\ensuremath{\\mathbb{Z}}_r$ is a function $C:\\ensuremath{\\mathbb{Z}}_r^n\\rightarrow \\ensuremath{\\mathbb{Z}}_r^N$ that satisfies the following: for every $x\\in \\ensuremath{\\mathbb{Z}}_r^n$ and $i\\in [n]$, there exists a (randomized) algorithm $\\ensuremath{\\mathcal{A}}$ that, on any input $y\\in \\ensuremath{\\mathbb{Z}}_r^N$ that satisfies $d(y,C(x))\\leq \\delta N$, makes $q$ queries to $y$ non-adaptively and outputs a number $\\ensuremath{\\mathcal{A}}^{y}(i)\\in \\ensuremath{\\mathbb{Z}}_r$ that satisfies $\\Pr[\\ensuremath{\\mathcal{A}}^{y}(i)=x_i]\\geq 1/r+\\varepsilon$ (where the probability is only taken over the randomness of $\\ensuremath{\\mathcal{A}}$).\n\\end{definition}\nAs is often the case when proving $\\mathsf{LDC}$ lower bounds, we use the useful fact proven by Katz and Trevisan~\\cite{katz2000efficiency} that, without loss of generality, one can assume that an $\\mathsf{LDC}$ is smooth, i.e., the queries made by $\\ensuremath{\\mathcal{A}}$ have ``reasonable\" probability over all indices, and that $\\ensuremath{\\mathcal{A}}$ makes queries to a codeword (and not a corrupted codeword). We first formally define a smooth code below. \n\\begin{definition}[Smooth code]\n    We say $C:\\ensuremath{\\mathbb{Z}}_r^n\\rightarrow \\ensuremath{\\mathbb{Z}}_r^N$ is a $(q,c,\\varepsilon)$-smooth code if there exists a decoding algorithm $\\ensuremath{\\mathcal{A}}$ that satisfies the following: for every $x\\in \\ensuremath{\\mathbb{Z}}_r^n$ and $i\\in [n]$, $\\ensuremath{\\mathcal{A}}$ makes at most $q$ non-adaptive queries to $C(x)$ and outputs $\\ensuremath{\\mathcal{A}}^{C(x)}(i)\\in \\ensuremath{\\mathbb{Z}}_r$ such that $\\Pr[\\ensuremath{\\mathcal{A}}^{C(x)}(i)=x_i]\\geq 1/r+\\varepsilon$ (where the probability is only taken over the randomness of $\\ensuremath{\\mathcal{A}}$). Moreover, for every $x\\in \\ensuremath{\\mathbb{Z}}_r^n$, $i\\in [n]$ and $j\\in [N]$, on input $i$, the probability that $\\ensuremath{\\mathcal{A}}$ queries the index $j$ in $C(x)\\in \\ensuremath{\\mathbb{Z}}_r^N$ is at most $c/N$.\n\\end{definition}\nCrucially note that smooth codes only require a decoder to recover $x_i$ when given access to an actual codeword, unlike the standard definition of $\\mathsf{LDC}$ where a decoder is given a noisy codeword. With this definition in hand, we state a theorem of Katz and Trevisan.\n\\begin{theorem}[\\cite{katz2000efficiency}]\n    \\label{thr:ldc-smoothcode-equivalence}\n    A $(q,\\delta,\\varepsilon)$-$\\mathsf{LDC}$ $C:\\ensuremath{\\mathbb{Z}}_r^n\\rightarrow \\ensuremath{\\mathbb{Z}}_r^N$ is a $(q,q/\\delta,\\varepsilon)$-smooth code.\n\\end{theorem}\n\\noindent We remark that a converse to this theorem holds: a $(q, c, \\varepsilon)$-smooth code is a $(q,\\delta,\\varepsilon - c\\delta)$-$\\mathsf{LDC}$, since the probability that the decoder queries one of $\\delta N$ corrupted positions is at most $(c/N)(\\delta N) = c\\delta$.\n\n\n\n\\subsection{Smooth codes over large alphabets}\n\nKatz and Trevisan~\\cite{katz2000efficiency} observed that a $(q,c,\\varepsilon)$-smooth code over $\\{0,1\\}$ is a $(q,q,\\varepsilon^2/2c)$-smooth code that is good on \\emph{average}, i.e., that there is a decoder $\\ensuremath{\\mathcal{A}}$ such that, for all $i\\in[n]$,\n\\begin{align*}\n    \\frac{1}{2^n}\\sum_{x\\in\\{0,1\\}^n}\\operatorname{Pr}[\\ensuremath{\\mathcal{A}}^{C(x)}(i)=x_i] \\geq \\frac{1}{2} + \\frac{\\varepsilon^2}{2c}.\n\\end{align*}\nThis comes from the observation that a $q$-decoder can partition the set $[N]$ into $q$-tuples, pick one of such tuples uniformly at random and continue as the original decoder by querying the elements of the picked tuple at the cost of a slightly worse success probability. These ideas are formally explained in the result below, where we already generalize them to large alphabets $\\ensuremath{\\mathbb{Z}}_r$ (the overall presentation is inspired in~\\cite[Theorem~15]{ben2008hypercontractive}).\n\\begin{theorem}\n\t\\label{thr:ldc1}\n\tSuppose $C: \\ensuremath{\\mathbb{Z}}_r^n \\to \\ensuremath{\\mathbb{Z}}_r^N$ is a $(q,c,\\varepsilon)$-smooth code. Then for every $i\\in[n]$, there exists a set $M_i$ consisting of at least $\\varepsilon N/(2cq)$ disjoint sets of at most $q$ elements of $[N]$ each such that, for every $Q\\in M_i$, there exists a function $f_Q : \\ensuremath{\\mathbb{Z}}_r^{|Q|} \\to \\ensuremath{\\mathbb{Z}}_r$ with the property\n\t%\n\t\\begin{align*}\n\t\t\\sum_{k=1}^{r-1}\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\left[\\omega_r^{k(f_Q(C(x)_Q) - x_i)}\\right] \\geq \\frac{r}{2}\\varepsilon.\n\t\\end{align*}\n\t%\n\tHere $C(x)_Q$ is the restriction of $C(x)$ to the bits in $Q$.\n\\end{theorem}\n\\begin{proof}\n\tFix some $i\\in[n]$. In order to decode $x_i$, we can assume, without lost of generality, that the decoder $\\ensuremath{\\mathcal{A}}$ picks some set $Q\\subseteq [N]$ (of at most $q$ indices) with probability $p(Q)$, queries those bits, and then outputs a random variable (not yet a function) $f_Q(C(x)_Q)\\in\\ensuremath{\\mathbb{Z}}_r$ that depends on the query-outputs. Call such a $Q$ ``good'' if\n\t%\n\t\\begin{align*}\n\t\t\\frac{1}{r} + \\frac{\\varepsilon}{2} \\leq \\operatorname*{Pr}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}[f_Q(C(x)_Q) = x_i] = \\operatorname*{\\mathbb{E}}_{\\substack{x\\sim\\ensuremath{\\mathbb{Z}}_r^n \\\\ k\\sim\\mathbb{Z}_r}}\\left[\\omega_r^{k(f_Q(C(x)_Q) - x_i)}\\right] \\iff \\frac{r}{2}\\varepsilon \\leq \\sum_{k=1}^{r-1}\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\left[\\omega_r^{k(f_Q(C(x)_Q) - x_i)}\\right].\n\t\\end{align*}\n\t%\n\tNow construct the hypergraph $H_i = (V, E_i)$ with $V = [N]$ and edge-set $E_i$ consisting of all good sets $Q$. The probability that the decoder queries any $Q\\in E_i$ is $p(E_i) := \\sum_{Q\\in E_i} p(Q)$. If it queries some $Q\\in E_i$, then \n\t%\n\t\\begin{align*}\n\t\t\\operatorname*{Pr}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}[f_Q(C(x)_Q) = x_i] \\leq 1 \\iff \\sum_{k=1}^{r-1}\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\left[\\omega_r^{k(f_Q(C(x)_Q) - x_i)}\\right] \\leq r-1,\n\t\\end{align*}\n\t%\n\tand if it queries some $Q\\notin E_i$, then $\\sum_{k=1}^{r-1}\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\big[\\omega_r^{k(f_Q(C(x)_Q) - x_i)}\\big] < \\frac{r}{2}\\varepsilon$. Given the smooth code property of outputting $x_i$ with probability at least $\\frac{1}{r} + \\varepsilon$ for every $x$, we have\n\t%\n\t\\begin{align*}\n\t\tr\\varepsilon \\leq \\sum_{k=1}^{r-1}\\operatorname*{\\mathbb{E}}_{x,Q}\\left[\\omega_r^{k(f_Q(C(x)_Q) - x_i)}\\right] < p(E_i)(r-1) + (1-p(E_i))\\frac{r}{2}\\varepsilon = \\frac{r}{2}\\varepsilon + p(E_i)\\left(r-1-\\frac{r}{2}\\varepsilon\\right),\n\t\\end{align*}\n\t%\n\thence\n\t%\n\t\\begin{align*}\n\t\tp(E_i) > \\frac{\\varepsilon}{2-2/r-\\varepsilon} \\geq \\frac{\\varepsilon}{2}.\n\t\\end{align*}\n\t%\n\tSince $C$ is also smooth, for every $j\\in[N]$ we have\n\t%\n\t\\begin{align*}\n\t\t\\sum_{Q\\in E_i:j\\in Q} p(Q) \\leq \\sum_{Q:j\\in Q}p(Q) = \\operatorname{Pr}[\\ensuremath{\\mathcal{A}}~\\text{queries}~j] \\leq \\frac{c}{N}.\n\t\\end{align*}\n\t%\n\tLet $M_i$ be a matching in $H_i$ of maximal size. We want to show that $|M_i|\\geq \\varepsilon N/(2cq)$. To do so, define $T := \\bigcup_{Q\\in M_i} Q$. Observe that the set $T$ has at most $q|M_i|$ elements, and intersects each $Q\\in E_i$ (otherwise $M_i$ would not be maximal). The size of $M_i$ can be lower bounded as follows:\n\t%\n\t\\begin{align*}\n\t\t\\frac{\\varepsilon}{2} < p(E_i) = \\sum_{Q:Q\\in E_i}p(Q) \\overset{(a)}{\\leq} \\sum_{j\\in T}\\sum_{Q\\in E_i: j\\in Q} p(Q) \\leq \\frac{c|T|}{N} \\leq \\frac{cq|M_i|}{N},\n\t\\end{align*}\n\t%\n\twhere (a) holds because each $Q\\in E_i$ is counted exactly once on the left and at least once on the right (since $T$ intersects each $Q\\in E_i$). Hence $|M_i| \\geq \\varepsilon N/(2cq)$. Finally, the random variables $f_Q(C(x)_Q)$ can be fixed in $\\ensuremath{\\mathbb{Z}}_r$ without reducing the probability $\\operatorname{Pr}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}[f_Q(C(x)_Q) = x_i]$.\n\\end{proof}\nAs previously mentioned, the above result tells us that a decoder can focus its queries on one of the $q$-tuples $Q$. We can go one step further and show that the decoder can restrict itself to computing a linear function of the queried bits while still maintaining a good correlation with the target bit $x_i$ at the cost of decreasing the average success probability.\n\\begin{theorem}\n    \\label{thr:ldc2}\n\tSuppose $C: \\ensuremath{\\mathbb{Z}}_r^n \\to \\ensuremath{\\mathbb{Z}}_r^N$ is a $(q,c,\\varepsilon)$-smooth code. Then for every $i\\in[n]$, there exists a set $M_i$ consisting of at least $\\varepsilon N/(2cq)$ disjoint sets of at most $q$ elements of $[N]$ each such that, for every $Q\\in M_i$,\n\t%\n\t\\begin{align*}\n\t\t\\sum_{k=1}^{r-1}\\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^{|Q|}}\\left|\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\left[\\omega_r^{S\\cdot C(x)_Q - kx_i}\\right]\\right|  \\geq \\frac{\\varepsilon r}{2}.\n\t\\end{align*}\n\t%\n\tHere $C(x)_Q$ is the restriction of $C(x)$ to the bits in $Q$.\n\\end{theorem}\n\\begin{proof}\n\tFix $i\\in[n]$ and take the set $M_i$ produced by Theorem~\\ref{thr:ldc1}. For every $Q\\in M_i$ we have\n\t%\n\t\\begin{align*}\n\t\t\\sum_{k=1}^{r-1}\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\left[\\omega_r^{k(f_Q(C(x)_Q) - x_i)}\\right] \\geq \\frac{\\varepsilon r}{2}.\n\t\\end{align*}\n\t%\n\tFor $k\\in\\{1,\\dots,r-1\\}$, define the function $h_{Q,k}:\\ensuremath{\\mathbb{Z}}_r^{|Q|}\\to\\mathbb{C}$ by $h_{Q,k}(x) = \\omega_r^{kf_Q(x)}$. Consider its Fourier transform $\\widehat{h}_{Q,k}: \\ensuremath{\\mathbb{Z}}_r^{|Q|}\\to\\mathbb{C}$. Hence we can write\n\t%\n\t\\begin{align*}\n\t\th_{Q,k}(x) = \\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^{|Q|}}\\widehat{h}_{Q,k}(S)\\omega_r^{S\\cdot x}.\n\t\\end{align*}\n\t%\n\tFinally, using that $|\\widehat{h}_{Q,k}(S)|\\in[0,1]$ for all $S\\in\\ensuremath{\\mathbb{Z}}_r^{|Q|}$, we can upper bound $\\varepsilon r/2$ by\n\t%\n\t\\[\n\t\t\\sum_{k=1}^{r-1}\\operatorname*{\\mathbb{E}}_{x}\\left[\\omega_r^{k(f_Q(C(x)_Q) - x_i)}\\right] = \\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^{|Q|}}\\sum_{k=1}^{r-1}\\widehat{h}_{Q,k}(S)\\operatorname*{\\mathbb{E}}_{x}\\left[\\omega_r^{S\\cdot C(x)_Q-kx_i}\\right] \\leq \\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^{|Q|}}\\sum_{k=1}^{r-1}\\left|\\operatorname*{\\mathbb{E}}_{x}\\left[\\omega_r^{S\\cdot C(x)_Q-kx_i}\\right]\\right|.\\qedhere\n\t\\]\n\\end{proof}\n\n\\subsection{An exponential lower bound for $\\mathsf{LDC}$s}\n\nIn this section, we use our results from matrix-valued hypercontractivity to obtain our lower bound for $\\mathsf{LDC}$s over $\\ensuremath{\\mathbb{Z}}_r$.\n\\begin{theorem}\n\\label{thm:2querylowerboundhyper}\n\tIf $C:\\ensuremath{\\mathbb{Z}}_r^n\\to\\ensuremath{\\mathbb{Z}}_r^N$ is a $(2,\\delta,\\varepsilon)$-$\\mathsf{LDC}$, then $N=2^{\\Omega(\\delta^2\\varepsilon^4 n/r^4)}$.\n\\end{theorem}\n\\begin{proof}\n    In this proof we shall use the normalized Schatten norm. Fix $x\\in \\ensuremath{\\mathbb{Z}}_r^n$. Define the vector $v_x\\in \\ensuremath{\\mathbb{C}}^{r^2N}$ \n    $$\n    v_x=\\big(1,\\dots,1,\\omega_r^{C(x)_1},\\ldots,\\omega_r^{C(x)_N}, \\omega_r^{2 C(x)_1},\\ldots,\\omega_r^{2 C(x)_N},\\ldots,\\omega_r^{(r-1) C(x)_1},\\ldots,\\omega_r^{(r-1) C(x)_N} \\big),\n    $$\n    where each sequence $\\omega_r^{jC(x)_1},\\dots,\\omega_r^{jC(x)_N}$ is repeated $r$ times consecutively. Let $R:=r^2N$ and define the $R\\times R$ symmetric matrix $f(x):=v_x^{\\operatorname{T}}\\cdot v_x$ whose $(N(r j_1+m_1)+\\ell_1,N(rj_2+m_2)+\\ell_2)$-entry is $\\omega_r^{j_1C(x)_{\\ell_1} + j_2C(x)_{\\ell_2}}$, where $j_1,j_2,m_1,m_2\\in\\ensuremath{\\mathbb{Z}}_r$ and $\\ell_1,\\ell_2\\in[N]$ (note that there are $r$ repeated entries $\\omega_r^{j_1C(x)_{\\ell_1} + j_2C(x)_{\\ell_2}}$ in each row and column). Since $f(x)$ has rank $1$ and its $R^2$ entries have absolute value $1$, its only non-zero singular value is $R$. Hence $\\|f(x)\\|_p^p = R^{p-1}$ for every $x\\in\\ensuremath{\\mathbb{Z}}_r^n$.\n\t\n\tFix $i\\in[n]$. For every $k\\in\\{1,\\dots,r-1\\}$ consider the $R\\times R$ matrices $\\widehat{f}(0^{i-1}k0^{n-i})$ that are the Fourier transform of $f$ at the strings in $\\ensuremath{\\mathbb{Z}}_r^n$ which are zero in all but the $i$th coordinate:\n\t%\n\t\\begin{align*}\n\t\t\\widehat{f}(0^{i-1}k0^{n-i}) = \\frac{1}{r^n}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}f(x)\\omega_r^{-kx_i}.\n\t\\end{align*}\n\t%\n\tWe shall lower bound $\\sum_{k=1}^{r-1}\\big\\|\\widehat{f}(0^{i-1}k0^{n-i})\\big\\|_p^p$.\n\t\n\tBy Theorem~\\ref{thr:ldc2}, there is a set $M_i$ consisting of at least $\\delta \\varepsilon N/8$ disjoint sets of indices in $[N]$, each with cardinality at most $2$,\\footnote{Here we used Theorem~\\ref{thr:ldc-smoothcode-equivalence} in order to invoke Theorem~\\ref{thr:ldc2} with $c=q/\\delta$.} such that $\\sum_{k=1}^{r-1}\\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^{|Q|}}\\big|{\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}}\\big[\\omega_r^{S\\cdot C(x)_Q - kx_i}\\big]\\big| \\in[\\varepsilon r/2,r^{|Q|}(r-1)]$. Given $S=(S_1,S_2)$, consider $Q=(Q_1,Q_2)\\in M_i$\\footnote{If $Q$ is a singleton, take $Q=(Q_1,Q_1)$ and $S=(S_1,0)$.} and the following $2\\times 2$ submatrix in $f(x)$ \n\t%\n\t\\begin{align*}\n\t\t\\begin{pmatrix}\n\t\t\t\\omega_r^{2S_1 C(x)_{Q_1}} & \\omega_r^{S_1 C(x)_{Q_1} + S_2 C(x)_{Q_2}}\\\\\n\t\t\t\\omega_r^{S_1 C(x)_{Q_1} + S_2 C(x)_{Q_2}} & \\omega_r^{2S_2 C(x)_{Q_2}}\n\t\t\\end{pmatrix}.\n\t\\end{align*}\n\t%\n\tObserve that this submatrix clearly exists in $f(x)$, and comes from the rows and columns $N(rS_1 +m_1) + Q_1$ and $N(rS_2 +m_2) + Q_2$ for any $m_1,m_2\\in\\ensuremath{\\mathbb{Z}}_r$. In particular, we can take $m_1=S_2$ and $m_2=S_1$, so that such submatrix does not have overlapping rows or columns with any other submatrix similarly defined from different $S'$ or $Q'$. Hence the corresponding $2\\times 2$ submatrix of $\\widehat{f}(0^{i-1}k0^{n-i})$ is\n\t%\n\t\\begin{align*}\n\t\t\\begin{pmatrix}\n\t\t\t\\alpha & \\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\big[\\omega_r^{S\\cdot C(x)_Q - kx_i}\\big]\\\\\n\t\t\t\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\big[\\omega_r^{S\\cdot C(x)_Q - kx_i}\\big] & \\beta\n\t\t\\end{pmatrix},\n\t\\end{align*}\n\t%\n\tfor some $\\alpha,\\beta\\in\\mathbb{C}$ (in this proof we will not be concerned with the value of $\\alpha,\\beta$). Let $P$ be the $R\\times R$ permutation matrix that, for every $Q=(Q_1,Q_2)$ and $S = (S_1,S_2)$, swaps rows $N(rS_1 + S_2) + Q_1$ and $N(rS_2 + S_1) + Q_2$. We define the matrices $F_i(k) := P\\widehat{f}(0^{i-1}k0^{n-i})$ for $k\\in\\{1,\\dots,r-1\\}$. Because we previously chose $m_1=S_2$ and $m_2=S_1$, for each of the at least $\\delta\\varepsilon N/8$ sets $Q\\in M_i$, $F_i(k)$ has diagonal entries $\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\big[\\omega_r^{S\\cdot C(x)_Q - kx_i}\\big]$ for all $S\\in\\ensuremath{\\mathbb{Z}}_r^{|Q|}$ (each entry is repeated twice). In other words, $F_i(k)$ has at least $\\delta\\varepsilon Nr^2/4$ entries $\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\big[\\omega_r^{S\\cdot C(x)_Q - kx_i}\\big]$ for $Q\\in M_i$ and $S\\in\\ensuremath{\\mathbb{Z}}_r^{|Q|}$.\n\t\n\tThe Schatten norm $\\|\\cdot\\|_p$ is \\emph{unitarily invariant}: $\\|UAV\\|_p = \\|A\\|_p$ for every matrix $A$ and unitaries $U,V$. We shall use the following lemma. Its proof is left to the end of the section.\n\t%\n\t\\begin{lemma}[{\\cite[Eq. (IV.52)]{bhatia2013matrix}}]\n\t    \\label{lem:ldc3}\n\t\tLet $\\|\\cdot\\|$ be a unitarily-invariant norm on $\\mathbb{C}^{d\\times d}$. If $A\\in\\mathbb{C}^{d\\times d}$ and $\\operatorname{diag}(A)$ is the matrix obtained from $A$ by setting its off-diagonal entries to $0$, then $\\|{\\operatorname{diag}}(A)\\| \\leq \\|A\\|$.\n\t\\end{lemma}\n\t\n\tUsing this lemma, we obtain\n\t%\n\t\\begin{align*}\n\t\t\\sum_{k=1}^{r-1}\\left\\|\\widehat{f}(0^{i-1}k0^{n-i})\\right\\|_p^p = \\sum_{k=1}^{r-1}\\|F_i(k)\\|_p^p &\\geq \\sum_{k=1}^{r-1}\\|\\text{diag}(F_i(k))\\|_p^p \\geq \\frac{2}{R}\\sum_{k=1}^{r-1}\\sum_{Q\\in M_i}\\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^{|Q|}}\\left|\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\big[\\omega_r^{S_Q\\cdot C(x)_Q - kx_i}\\big]\\right|^p,\n\t\\end{align*}\n\t%\n\tbut, by H\\\"{o}lder's inequality,\n\t%\n\t\\begin{align*}\n\t    \\sum_{k=1}^{r-1}\\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^{|Q|}}\\left|\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\big[\\omega_r^{S_Q\\cdot C(x)_Q - kx_i}\\big]\\right|^p \\geq \\frac{1}{r^{3(p-1)}} \\left(\\sum_{k=1}^{r-1}\\sum_{S\\in\\ensuremath{\\mathbb{Z}}_r^{|Q|}}\\left|\\operatorname*{\\mathbb{E}}_{x\\sim\\ensuremath{\\mathbb{Z}}_r^n}\\big[\\omega_r^{S_Q\\cdot C(x)_Q - kx_i}\\big]\\right|\\right)^p \\geq \\frac{1}{r^{3(p-1)}}\\left(\\frac{\\varepsilon r}{2}\\right)^{p},\n\t\\end{align*}\n\t%\n\thence\n\t%\n\t\\begin{align*}\n\t    \\sum_{k=1}^{r-1}\\left\\|\\widehat{f}(0^{i-1}k0^{n-i})\\right\\|_p^p \\geq \\frac{2}{R}\\frac{\\delta\\varepsilon N}{8}\\frac{1}{r^{3(p-1)}}\\left(\\frac{\\varepsilon r}{2}\\right)^{p} = \\frac{\\delta\\varepsilon}{4r^{2p-1}}\\left(\\frac{\\varepsilon}{2}\\right)^p,\n\t\\end{align*}\n\t%\n\twhich implies \n\t%\n\t\\begin{align*}\n\t\t\\sum_{k=1}^{r-1}\\left\\|\\widehat{f}(0^{i-1}k0^{n-i})\\right\\|_p^2 \\geq \\frac{1}{r^{2/p-1}}\\left(\\sum_{k=1}^{r-1}\\left\\|\\widehat{f}(0^{i-1}k0^{n-i})\\right\\|_p^p\\right)^{2/p} \\geq \\frac{1}{r^3}\\left(\\frac{\\delta \\varepsilon}{4}\\right)^{2/p} \\left(\\frac{\\varepsilon}{2}\\right)^2,\n\t\\end{align*}\n\t%\n\twhere we used H\\\"{o}lder's inequality again. Now, using the hypercontractive inequality, we have for any $p\\in[1,2]$ that\n\t%\n\t\\begin{align*}\n\t\tn(p-1)\\frac{1}{r^4}\\left(\\frac{\\delta \\varepsilon}{4}\\right)^{2/p}\\left(\\frac{\\varepsilon}{2}\\right)^2 \\leq \\sum_{i=1}^n\\sum_{k=1}^{r-1} \\frac{p-1}{r-1}\\left\\|\\widehat{f}(0^{i-1}k0^{n-i})\\right\\|_p^2 \\leq \\left(\\frac{1}{r^n}\\sum_{x\\in\\ensuremath{\\mathbb{Z}}_r^n}\\|f(x)\\|_p^p\\right)^{2/p} = R^{2(p-1)/p}.\n\t\\end{align*}\n    %\n    Choosing $p=1+1/\\log{R}$ gives us\n\t%\n\t\\begin{align*}\n\t\t\\frac{n}{\\log{R}}\\frac{1}{r^4}\\left(\\frac{\\delta \\varepsilon}{4}\\right)^{2}\\left(\\frac{\\varepsilon}{2}\\right)^2 \\leq R^{2/(1+\\log{R})} = 4^{\\log{R}/(1+\\log{R})} \\implies R \\geq \\frac{2^{\\delta^2\\varepsilon^4 n/(2^{6}r^4)}}{2^{4^{\\log{R}/(1+\\log{R})}}} = 2^{\\Omega(\\delta^2\\varepsilon^4 n/r^4)}.\n\t\\end{align*}\n\tSince $R=r^2N$, we have the desired lower bound by adjusting the constant in the $\\Omega(\\cdot)$ in the exponent.\n\\end{proof}\n\\begin{myproof}{Lemma~\\ref{lem:ldc3}}\n    The proof sets the off-diagonal entries of $A$ to $0$ recursively without increasing its norm. Start with the off-diagonal entries in the $d$th row and column. Define $D_d$ be the diagonal matrix by $D_{d,d} = -1$ and $D_{i,i}=1$ for $i<d$. Note that $D_dAD_d$ is the same as $A$, except that the off-diagonal entries of the $d$th row and column are multiplied by $-1$. Hence $A_{d-1} := (A+D_dAD_d)/2$ is the matrix obtained from $A$ by setting those entries to $0$ (this does not affect the diagonal). Since $D_d$ is unitary, by the triangle inequality\n    %\n    \\begin{align*}\n        \\|A_{d-1}\\| = \\|(A+D_dAD_d)/2\\| \\leq \\frac{1}{2}(\\|A\\| + \\|D_dAD_d\\|) = \\|A\\|.\n    \\end{align*}\n    %\n    Continuing in this manner for $i=1,\\dots,d-1$, we can set the off-diagonal entries in the $(d-i)$th row and column of $A_{d-i}$ to $0$ by using the diagonal matrix $D_{d-i}$ which has a $-1$ only on its $(d-i)$th position and without increasing its norm.\n\\end{myproof}\n\n\n\n\n\\section{2-server private information retrieval}\n\\label{sec:PIR}\n\nAs mentioned in the introduction, the connection between $\\mathsf{LDC}$s and \\textsf{PIR} is well known since the results of~\\cite{katz2000efficiency,goldreich2002lower}. In general, upper bounds on $\\mathsf{LDC}$s  are derived via \\textsf{PIR} schemes, which in turn means that our $\\mathsf{LDC}$ lower bounds translate to $\\textsf{PIR}$ lower bounds, which we illustrate below. We first define the notion of private information retrieval.\n\\begin{definition} \n    A one-round, $(1-\\delta)$-secure, $k$-server private information retrieval $\\mathsf{(PIR)}$ scheme with recovery probability $1/r+\\varepsilon$, query size $t$ and answer size $a$, consists of a randomized user and $k$ deterministic algorithms $S_1,\\ldots,S_k$ (the servers) that satisfy the following:\n    %\n    \\begin{enumerate}\n        \\item On input $i\\in [n]$, the user produces $k$ queries $q_1,\\ldots,q_k\\in \\ensuremath{\\mathbb{Z}}_r^t$ and sends them to the $k$ servers respectively. The servers reply back with a string $a_j=S_j(x,q_j)\\in\\ensuremath{\\mathbb{Z}}_r^a$, and based on $a_1,\\ldots,a_k$ and $i$, the user outputs $b\\in \\ensuremath{\\mathbb{Z}}_r$.\n        \\item For every $x\\in \\ensuremath{\\mathbb{Z}}_r^n$ and $i\\in [n]$, the output $b$ of the user satisfies $\\Pr[b=x_i]\\geq 1/r+\\varepsilon$.\n        \\item For every $x\\in \\ensuremath{\\mathbb{Z}}_r^n$ and $j\\in[k]$, the distributions over $q_j$ (over the user's randomness) are $\\delta$-close for different $i\\in[n]$.\n    \\end{enumerate}\n    %\n    We crucially remark that for the lower bounds that we present below, the function $S_j$ could be an arbitrary (not necessarily linear) function over $x_1,\\ldots,x_n \\in \\ensuremath{\\mathbb{Z}}_r$. \n\\end{definition}\n\nOur \\textsf{PIR} lower bound follows almost immediately from the following immediate consequence of Goldreich et al.~\\cite[Lemma~5.1]{goldreich2002lower}. In the following we shall assume $\\delta = 0$.\n\\begin{lemma}[\\cite{goldreich2002lower}]\n    \\label{lem:goldreich}\n    If there is a classical $2$-server $\\mathsf{PIR}$ scheme with query size $t$, answer size $a$ and recovery probability $1/r+\\varepsilon$, then there is a $(2, 3, \\varepsilon)$-smooth code $C:\\ensuremath{\\mathbb{Z}}_r^n\\rightarrow (\\ensuremath{\\mathbb{Z}}_r^a)^m$ with $m \\leq 6r^t$.\n\\end{lemma}\nWe remark that Goldreich et al.~\\cite{goldreich2002lower} state the lemma above only for $r=2$, but the exact same analysis carries over to the large alphabet case. We now  get the following main theorem.\n\\begin{theorem}\n    A classical $2$-server $\\mathsf{PIR}$ scheme with query size $t$, answer size $a$ and recovery probability $1/r + \\varepsilon$ satisfies $t\\geq \\Omega\\big((\\delta^2\\varepsilon^4 n/r^4 - a)/\\log{r}\\big)$. \n\\end{theorem}\n\\begin{proof}\n    By using Lemma~\\ref{lem:goldreich}, there is a $(2,3,\\varepsilon)$-smooth code $C:\\ensuremath{\\mathbb{Z}}_r^n\\rightarrow (\\ensuremath{\\mathbb{Z}}_r^a)^m$ with $m\\leq 6r^t$. In order to apply Theorem~\\ref{thm:2querylowerboundhyper}, we form a new code $C'$ by transforming each old string $C(x)_j\\in\\mathbb{F}_r^a$ using the Hadamard code into $C'(x)_j\\in\\{0,1\\}^{2^a}\\subseteq\\mathbb{Z}_r^{2^a}$. The total length of $C'$ is $m'=m 2^a$. By using Theorem~\\ref{thm:2querylowerboundhyper} on $C'$ (note that the theorem can be applied directly to a smooth code), this gives us\n    $$\n        m'a\\geq 2^{\\Omega(\\delta^2\\varepsilon^4 n/r^4)},\n    $$\n    and since $m=O(r^t)$, we get the desired lower bound in the theorem statement.\n\\end{proof}\n\n\\DeclareRobustCommand{\\DE}[3]{#3}\n\\bibliographystyle{alpha}\n", "meta": {"timestamp": "2021-09-07T02:38:36", "yymm": "2109", "arxiv_id": "2109.02600", "language": "en", "url": "https://arxiv.org/abs/2109.02600"}}
{"text": "\\section{Introduction}\n\nHand-drawn animation has been around for over 100 years and is one of the most popular mediums of digital entertainment today. Though the advent of drawing tablets and digital software have made the process of creating hand-drawn animation substantially easier, it is still a highly manual process that involves drawing and editing each individual frame. Many of these tasks lie in the grey area between repetitively algorithmic processes and artistic choices, opening the door for new assistive tools that augment artists\u2019 workflows.\n\nExisting commercial tools have applied heuristic algorithms in this domain with limited results, usually requiring artists to work in vector format or use complex character rigging that removes the hand-drawn feel of the final product. Deep learning approaches, on the other hand, can act directly on top of raw pixel input but cannot scale easily to HD resolutions and fail to properly exploit the structure of hand-drawn animation drawings -- specifically, the smaller line enclosures (segments) which can be extracted by a flood-fill or morphological algorithm.\n\nIn this paper, we focus on the task of learning visual correspondence across sequences of  raster animation line drawings. This is a fundamental building block for building assistive animation tools for tasks such as coloring, in-betweening, and texturing which make up a large portion of the tedious, non-creative work in the animation pipeline. With correspondence information an animator can color or texture a few frames in a sequence and propagate the colors through the rest of the images, saving hours of manual labor. New in-between frames can be generated by morphing neighboring frames with correspondence information, which can reduce the amount of line drawings needed to make smooth looking motion.\n\nDespite demand for a data-driven solution to the correspondence problem, little progress has been made because of the difficult design requirements and lack of available data with correspondence labels. Suitable approaches should (i) operate on raster input and scale to HD (1920\u00d71080) and above resolutions; (ii) produce correspondences on the level of segments; (iii) be able to handle complex real-world animation; (iv) be trainable using colorized images as supervision; (v) be fast enough for interactive applications.\n\n\n\n\nIn this paper, we propose the Animation Transformer (AnT) to address these issues. Unlike pixel-based video tracking methods which suffer from the intractability of computing attention over a large number of pixels, AnT operates over the line-enclosed segments (see Figure \\ref{fig/crops}) in the line image and uses a Transformer-based architecture to learn the spatial and visual relationships between segments. By operating on this representation AnT avoids the need to directly process HD images in their entirety and is both compute and memory efficient, scaling to 4K images and beyond. We optimize AnT with a forward matching loss and a cycle consistency loss that enables it to be trained on real-world animation datasets without full ground-truth correspondence labels. \n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figures/crops}\n\\vspace{-0.1in}\n\\caption{ Given an input image $I$ each crop $C_{i}$ is obtained by placing a bounding box around the center of each enclosure of $I$ and resizing it to a common size. }\n\\vspace{-0.1in}\n\\label{fig/crops}\n\\end{figure}\n\nWe conduct extensive experiments to show our model\u2019s effectiveness in a variety of settings. When trained on ground-truth correspondence labels generated from 3D rendering software, AnT demonstrates a large improvement over a strong pixel-based baseline even after domain specific improvements are added to the baseline. When AnT is trained solely on colorized images from a real-world animation dataset, its performance approaches that of the model trained on ground-truth correspondence labels -- showing that AnT is not bounded by the availability of large datasets with correspondence labels. While AnT has broad applicability in animation, we highlight its potential as a creative tool through showcasing results on guided colorization from reference images.\n\n\\section{Related work}\n\n\\PAR{Correspondence Matching:}\nOur paper builds off of a growing body of research that learns correspondence by matching features extracted from a deep neural network across images. A common approach is to extract high-level activation maps from the image and match corresponding regions in feature space. This framework has been applied to video tracking \\cite{vondrick2018tracking, lai2019self} and exemplar-based colorization \\cite{meyer2018color, zhang2019deep} in the photo-realistic domain as well as exemplar-based colorization in the line image animation domain \\cite{shi2020deep,zhang2021lineart}. However, representations learned in this way are inherently limited by the memory complexity of computing dense pixel attention maps. Even with multi-scale techniques \\cite{li2020correspondence} or local attention \\cite{lai2019self}, it is computationally infeasible to use these techniques for HD and above resolutions. In contrast, our approach computes attention over the line-enclosed segments in the images, which makes the attention operation bounded by the number of segments rather than pixels in the input image.\n\nResearch has also explored using the feature matching framework in combination with different ways of representing image regions, such as patches \\cite{jabri2020spacetime,bertasius2021spacetime} and local descriptors \\cite{superglue, luo2019contextdesc}. Of particular interest to our approach is the line of research that learns multi-view correspondence across sketch images with local descriptors \\cite{navarro2021sketchzooms,deng2020sketchdesc}. However, our domain necessitates we learn correspondence at the level of segments so that we can train on real-world animation datasets with segment-level color labels and use the learned correspondences as an assistive tool for coloring.\n\n\\PAR{Segment-based methods:} Segments offer a natural way to decompose line images into a useful representation for learning tasks such as correspondence. Relevant to our approach is the work of Zhu et al. \\cite{zhu2016toontrack}  which formulates segment-level correspondence matching across a sequence of images as a network flow graph problem and solves for the global optimum using the k-shortest path algorithm with Shape Context \\cite{shape_context} features. Other work in this direction adopt a similar graph matching approach and apply spectral matching \\cite{maejima2019graph} and quadratic programming \\cite{sato2014reference} on top of non-learned segment features. Recent work from Dang et al. \\cite{dang2020correspondence} proposes using a U-Net to extract local features and optimize for correspondence matches with a triplet loss that minimizes the distances between matching segments and penalizes low distances between non-corresponding segments. Similar to these approaches, AnT uses global feature aggregation across segments to learn correspondences. However, we are the first to explore using a  to aggregate segment features and do not require ground truth correspondences or hard example mining as input data.\n\n\\PAR{Transformers:} Transformers have been shown to be highly effective at learning a wide range of tasks in domains such as language\nmodeling \\cite{attention-is-all-you-need}, image recognition \\cite{vit}, object detection \\cite{detr}, and protein folding \\cite{senior2020improved}. Transformers introduced self-attention layers, which, similarly to Non-Local Neural\nNetworks \\cite{wang2018nonlocal}, scan through each element of a sequence and update it by aggregating information from the whole sequence. Recent applications of Transformers to computer vision use image patches \\cite{vit, detr, bertasius2021spacetime} to break up the image into a tractable sequence length that avoids the quadratic complexity of computing attention over every pixel. Sarlin et al. \\cite{superglue} propose using a Transformer-based architecture to match sets of local feature descriptors where the matching assignments are estimated by solving a differentiable optimal transport problem. We design our Transformer architecture in a similar fashion, but use a different matching and loss formulation to handle the fact that one-to-none and one-to-many correspondences can occur in our domain.\n\\PAR{Cycle consistency:} Cycle-consistency has been applied as a learning objective for 3D shape matching, image alignment, depth estimation, and image-to-image-translation \\cite{zhu2017cyclegan}. In the context of temporal domains, it can be a rich source of learning signal because the visual world is continuous and smoothly-varying. Recent work has shown that cycle-consistency is useful for learning visual tracking in the photo-realistic domain \\cite{lai2019self, jabri2020spacetime} by learning to propagate labels in a forward-backward fashion. Our work applies this idea in the context of segment labels which allow us to train on datasets without ground-truth correspondence labels.\n\\PAR{Sketch-oriented Deep Learning:} Our work is also tangentially related to the broader area of sketch-oriented deep learning. Research has investigated methods for a variety of tasks, such as single-image colorization from hints \\cite{zhang2018twostage,yonetsuji2017paintschainer,ci2018user}, sketch clean-up \\cite{simo2018realtime,simo2016learning}, sketch generation \\cite{ha2017neural,ge2021creative}, sketch shadowing \\cite{zheng2020learning}, and synthesis of vector graphics \\cite{reddy2021im2vec}.\n\\PAR{Assistive Animation Tools:} Finally, we take inspiration from a variety of creative tools that aim to augment the animation pipeline. LazyBrush by S\u00fdkora et al. \\cite{sykora2009lazybrush} paints hand-made cartoon drawings from imprecise color strokes. EBSynth by Jamri\u0161ka et al. \\cite{jamriska2019ebsynth} uses patch-based synthesis to paint over photo-realistic video from exemplar images with texture coherence, contrast and high frequency details. BetweenIT by Whited et al. \\cite{brian2010betweenit} use stroke interpolation from keyframes for smooth in-betweening of vectorized animation. Zhang et al. \\cite{zhang2021lineart} propose a system for colorizing in-between frames from line frames and colorized keyframes using a deep neural network. This work shares a similar goal to AnT but operates on the level of pixels instead of segments.\n\n\\section{Method}\n\n\n\\PAR{Motivation:}\nOur goal is to estimate visual correspondence across a sequence of animation frames at the level of the line-enclosed segments in the line images. By using this naturally occurring structure (see Figure \\ref{fig/crops}), we learn the spatial and positional relationships between segments; for example, a hand will have segments for each finger which are all connected to a segment for the palm. As the character moves throughout the sequence, we can expect the structure to hold; if we see several finger shaped segments we know we will see a round palm segment or small fingernail segments nearby (see Figure 3). However, due to occlusion and motion, a segment may completely go out of frame or be split into smaller sub-parts in the next frame that both correspond to the same segment in the earlier frame (see Figure 3). Thus, we formulate AnT as a segment matching problem where segments can match to 0, 1, or multiple segments in the other frames.\n\n\n\n\n\\PAR{Data:} The architecture for AnT is motivated by the structure of data it operates on as well as the availability of two types of labels: correspondence labels that assign each segment a consistent, \\textit{unique} ID throughout the sequence and color labels that assign each segment a consistent, but possibly \\textit{non-unique} color. Correspondence labels offer the cleanest, most direct form of supervision for our task; but they come at the expense of not existing in the real-world -- in order to obtain these we use 3D rendering software to generate the realistic looking line images with unique segment IDs (for more details see Section \\ref{dataset}). On the other hand, colorized animation is plentiful in the real-world but offers a weaker form of supervision for our task; multiple segments often share the same color across the sequence, so color labels only tell the model that a segment in one frame corresponds to something in the set of segments in the other frame that share the same color. Our architecture is able to operate on and learn effectively from both forms of supervision.\n\n\\PAR{Formulation:} Consider two line images $A \\in \\mathbb{R}^{H\\times{W}\\times{1}}$ and $B \\in \\mathbb{R}^{H\\times{W}\\times{1}}$ which have $M$ and $N$ segments and are indexed by $\\mathcal{A} := \\{1,..., M\\}$ and $\\mathcal{B} := \\{1,..., N\\}$, respectively.  We extract segments from the line images using a trapped-ball filling algorithm where each line enclosed region is a separate segment. We divide the image into a set of smaller cropped images using the bounding box coordinates of each segment and then resize each cropped image to a smaller resolution $H_{c},{W_c}$. Each segment has positional information $\\mathbf{p}_i = (x_i,y_i,h_i,w_i)$ in the form of its bounding box coordinates and visual information $\\mathbf{d}_i \\in \\mathbb{R}^{H_{c}\\times{W_c}\\times{2}}$ in the form of the concatenated line image and binary segmentation mask crops. We refer to these features $\\mathbf{x}_i$ jointly as the local segment features. \n\n\n\n\\begin{figure*}[h]\n\\centering\n\\includegraphics[width=\\linewidth]{figures/architecture}\n\\vspace{-0.1in}\n\\caption{\\textbf{AnT Architecture.} Given reference and target line images, the backbone module extracts visual and positional information for each segment. The per-segment features are passed through a multiplex transformer architecture that aggregates information across segments and frames, yielding a similarity matrix between the reference and target segments. The final color predictions are computed via\na linear combination of the color labels in the reference frame.}\n\\label{fig/arch}\n\\vspace{-0.1in}\n\\end{figure*}\n\n\n\\subsection{AnT Architecture}\n\nAs shown in Figure \\ref{fig/arch}, our model consists of three main modules: the CNN backbone network to extract visual features for each segment, the bounding box encoder to extract positional embeddings for each segment, and a multiplex transformer which learns the global structure across segments and frames and predicts the final match matrix. \n\nThe multiplex transformer architecture is inspired by \\cite{superglue} and we encourage readers to refer to the original SuperGlue paper for additional details. While the positional and visual features are an important foundation for estimating segment correspondences, there are often visual ambiguities that arise which cannot be solved by looking at local features alone. For example, in Figure \\ref{fig/occlusion} we see examples of cases that would make matching on local features alone impossible: an occlusion or deformation can disfigure an individual segment or there may be multiple segments such as eyes that are indistinguishable from one another if viewed in isolation. Additionally, animation line drawings often contain groups of neighboring segments that pertain to the same semantic part but are split into multiple segments because the artist has drawn an object in the foreground whose contour lines intersect with that an object behind it (see Figure \\ref{fig/grouping}).\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=\\linewidth]{figures/occlusion}\n\\vspace{-0.1in}\n\\caption{An example of occlusion in our evaluation data. We show results of our model's performance on this sequence in the middle row.}\n\\label{fig/occlusion}\n\\vspace{-0.1in}\n\\end{figure}\n\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{figures/grouping}\n\\vspace{-0.1in}\n\\caption{An example of grouping in animation: if you zoom in on the image on the left, you will see many smaller segments generated from the shadow pass that pertain to the same semantic group.}\n\\label{fig/grouping}\n\\vspace{-0.2in}\n\\end{figure}\n\n\nThese challenges motivate the need for an architecture that can aggregate global feature information across segments within the individual images as well as integrate segment information across images. We describe this in more detail in the following sections.\n\n\n\\PAR{CNN Backbone:}\nStarting from the cropped images $\\mathbb{R}^{H_{c}\\times{W_c}\\times{2}}$, a conventional CNN backbone generates high-level activation maps for each segment crop. A $1\\times{1}$ convolution squashes the spatial dimensions of the high-level activation maps yielding $D$ dimensional feature vectors. In our experiments we use $D = 256$ and $H_c, W_c = 32$.\n\n\\PAR{Positional Encoder:}\nWe combine the visual features from the CNN backbone with positional information from the bounding box coordinates to get the final local features $\\mathbf{x}_i$ for each segment. We embed the bounding box coordinates into a $D$ dimensional vector with a multilayer perceptron (MLP) and add these into the visual features:\n\\begin{equation}\n    \\label{eq:keypoint-encoder}\n        \\mathbf{x_i} = \\text{CNN}_{\\text{enc}}(\\mathbf{d}_i ) + \\text{MLP}_{\\text{enc}}\\left(\\mathbf{p}_i\\right).\n    \\end{equation}\nUnlike \\cite{superglue}, we train both the CNN backbone and positional encoder end-to-end with the multiplex transformer.\n\n\\PAR{Multiplex Transformer:} As in \\cite{superglue}, we adopt a multiplex transformer architecture, which has two modes of information aggregation: it connects segments to all the other segments within the same image (self-attention) and connects segments to all the segments in the other image (cross-attention). In self-attention, features are aggregated at the level of segments within each individual image yielding features $\\mathbf{z}_{\\mathcal{A}}^\\ell,  \\mathbf{z}_{\\mathcal{B}}^\\ell$ for input images $\\mathcal{A}, \\mathcal{B}$, respectively. Cross-attention operates over the output of the last self-attention step but aggregates information across images, yielding a new set of features  $\\mathbf{z}_{\\mathcal{A}}^{\\ell+1},  \\mathbf{z}_{\\mathcal{B}}^{\\ell+1}$.\n\nIn query, key, value notation our attention function can be described as a variant of the classic formulation:\n\\begin{equation}\n   \\mathrm{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\mathrm{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{D}})\\mathbf{V}\n\\end{equation}\nwhere in the cross-attention layers, the keys and values originate from the aggregated features $\\mathbf{z}_j$ of a \\textit{target} image and the queries originate from the aggregates features originating from a \\textit{reference} image $\\mathbf{z}_i$. In the self-attention layers, the queries, keys, and values all originate from the same source features $\\mathbf{z}_i$. It is important to note that \\textit{reference} and \\textit{target} are relative terms -- \\textit{target} just denotes the other image with respect to the \\textit{reference} image. In our architecture, both directions of cross-attention are happening simultaneously. From the perspective of image $\\mathcal{A}$, image $\\mathcal{B}$ is the \\textit{target} and from the perspective of image $\\mathcal{B}$, $\\mathcal{A}$ is the target. An overview of the self and cross-attention blocks is illustrated in Figure \\ref{fig/multiplex}.\n\n\\begin{figure}[h]\n\\centering\n\\setlength\\fboxsep{0pt}\n\\setlength\\fboxrule{0.2pt}\n\\includegraphics[width=0.9\\linewidth]{figures/multiplex}\n\\vspace{-0.1in}\n\\caption{The blue and purple circles in \\textit{a)}, represent the center of the segments in each line image. The \\textit{self-} and \\textit{cross-attention} blocks in \\textit{b)} show how attention can be computed between segment features from the same image or across images. In our architecture, we have used an interleaved approach of \\textit{self-} and \\textit{cross-attention} combined with skip connections between each transformer block as it is depicted in \\textit{c)}.} \n\\label{fig/multiplex}.\n\\vspace{-0.2in}\n\\end{figure}\n\nSimilar to the original transformer implementation, the multiplex transformer is made up of stacked transformer blocks that each consists of a multi-headed attention layer followed by a point-wise fully connected layer. We alternate between self and cross-attention in the transformer blocks and add residual connections between each block. The final matching features are computed by the output of the last transformer block and a final linear projection layer, yielding final features $\\mathbf{z}_\\mathcal{A}^L \\in \\mathbb{R}^{M\\times{D}}$ and $\\mathbf{z}_\\mathcal{B}^L \\in \\mathbb{R}^{N\\times{D}}$.\n\n\\subsection{Matching}\n\nAnT learns a similarity matrix between the aggregated reference and target features from the multiplex transformer and then predicts the target label with a weighted sum of all the labels in the reference frame.  We compute the predicted target labels $\\mathbf{\\hat{c}}_j \\in \\mathbb{R}^N$ as a linear combination of the labels $\\mathbf{c}_i \\in \\mathbb{R}^M$ in the reference frame:\n\\begin{equation}\n  \\mathbf{\\hat{c}}_j = \\sum_{i=1}^{M} \\mathcal{S}_{ij} \\mathbf{c}_i\n  \\label{eq:fwd_nonlocal}\n\\end{equation}\nwhere $\\mathcal{S}_{ij}$ is a similarity matrix between the target and reference frame such that the rows sum to one. As in \\cite{deep-exemplar-video-colorization}, we use inner product similarity normalized by softmax:\n\\begin{equation}\n    \\mathcal{S}_{ij} = \\frac{\\exp\\left(\\mathbf{f}_i^T \\mathbf{f}_j\\right)}{\\sum_{i=1}^{M} \\exp\\left(\\mathbf{f}_i^T \\mathbf{f}_j\\right)}\n    \\label{eqn:sim}\n\\end{equation}\nwhere $\\mathbf{f}_i \\in \\mathbb{R}^D$ is the feature vector corresponding to the segment at index $i$ in  $\\mathbf{z}_\\mathcal{A}^L$ and $\\mathbf{f}_j \\in \\mathbb{R}^D$ is the feature vector corresponding to the segment at index $j$ in  $\\mathbf{z}_\\mathcal{B}^L$.\n\n\\subsection{Loss}\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{figures/cycle_consistency}\n\\vspace{-0.1in}\n\\caption{ The cycle consistency loss allows the model to utilize real world animation data without ground truth correspondences. An example scenario is shown in which the second target segment is incorrectly matched in the forward propagation but the model is not penalized by the color matching loss because both the predicted segment color label and the ground truth color label have the same color. To solve this, we propagate unique segment IDs through the forward pass and then back again to the reference image segments, enabling our cycle consistency loss to penalize the model according to whether the propagated IDs match their original values.}\n\\label{fig/cycle}\n\\vspace{-0.1in}\n\\end{figure}\n\n\nIn order to be able to learn from both correspondence and color labels, AnT employs two loss functions that can be used independently or averaged together depending on the label source.\n\n\\PAR{Forward match loss:}\nTo encourage the model to directly use the correspondence or color labels, we use categorical cross-entropy loss between the predicted target labels $\\mathbf{\\hat{c}}_j$ and the $\\mathbf{c}_j$ ground truth target labels from the dataset. In cases where we have correspondence labels, both the target labels $\\mathbf{c}_j$ and the reference labels $\\mathbf{c}_i$ used as input to the weighted average calculation are unique and thus the model directly minimizes incorrect correspondences. However, in the case of color labels, $\\mathbf{c}_i$ and $\\mathbf{c}_j$ are non-unique and the model only minimizes incorrect color assignments. This leads to the model learning to shortcut and find matches that yield the correct color assignments but are incorrect correspondences (see Figure \\ref{fig/cycle}).\n\n\\PAR{Cycle consistency loss:} \nIn order to solve the previously mentioned issues, we employ a cycle consistency loss that prevents the model from learning to shortcut in cases where we have non-unique color labels. Instead of using the reference labels from the dataset, we initialize a random vector of unique segment IDs $\\mathbf{r}_i \\in \\mathbb{R}^M$ and use these in place of $\\mathbf{c}_i$ for the weighted label aggregation:\n\\begin{equation}\n  \\mathbf{\\hat{r}}_j = \\sum_{i=1}^{M} \\mathcal{S}_{ij} \\mathbf{r}_i\n  \\label{eq:fwd_nonlocal}\n\\end{equation}\nWe then propagate the predicted target labels $\\mathbf{\\hat{r}}_j$ in the backward direction:\n\\begin{equation}\n  \\mathbf{\\hat{r}}_i = \\sum_{j=1}^{N} \\mathcal{T}_{ij} \\mathbf{\\hat{r}}_j\n  \\label{eq:fwd_nonlocal}\n\\end{equation}\nwhere $\\mathcal{T}$ is the backward correlation matrix computed by:\n\\begin{equation}\n    \\mathcal{T}_{ij} = \\frac{\\exp\\left(\\mathbf{f}_j^T \\mathbf{f}_i\\right)}{\\sum_{j=1}^{N} \\exp\\left(\\mathbf{f}_j^T \\mathbf{f}_i\\right)}\n    \\label{eqn:fwd_sim}\n\\end{equation}\nAs with the forward match loss, we use categorical cross-entropy loss between the randomly initialized segment IDs $\\mathbf{r}_i$ and the predicted segment IDs $\\mathbf{\\hat{r}}_i$ propagated over the entire cycle. Our final loss term with both losses is:\n \\begin{equation}\n    \\mathcal{L} = \\sum_{j=1}^{N} \\mathcal{L}_{fwd}\\left(\\mathbf{\\hat{c}}_j, \\mathbf{c_j}\\right) +  \\alpha \\sum_{i=1}^{Q} \\mathcal{L}_{cyc}\\left(\\mathbf{\\hat{r}}_i, \\mathbf{r}_i\\right)\n   \\label{eqn:objective}\n\\end{equation}\nwhere $\\mathcal{L}_{fwd}$ is the forward matching loss, $\\mathcal{L}_{cyc}$ is the cycle consistency loss, and $\\alpha$ is a hyper-parameter that weights the cycle consistency loss. In our experiments we use both losses with $\\alpha = 0.25$.\n\n\n\n\n\n\n\n\n\\section{Experiments}\n\n\n\\subsection{Dataset details}\n\\label{dataset}\n\n\\PAR{Synthetic Dataset:}\nTo train AnT with ground truth segment correspondence labels, we generate a synthetic dataset in Cinema4D using freely available 3D models. We render realistic looking line images using a toon shader and generate the segment correspondence labels by assigning unique IDs to individual meshes. The characters are rigged with different movements, deformations, and rotations to simulate actual animation. We use 11 3D character models from TurboSquid and generate 1000 frames at 1500x1500 pixel resolution for each character, yielding 11000 frames in total. During training, we apply random frame skipping and other augmentation techniques such as cropping, jittering, and shearing. The characters range in complexity from some characters with as few as 10 segments to others with as high as 50. We create our evaluation set by randomly selecting sequences for a total of 1100 frames (10\\% of the dataset), uniformly split across each character.\n\n\\PAR{Real Dataset:} As a medium, hand-drawn animation is much more diverse and expressive than its 3D counterpart. Since animators are not confined to the limits of a 3D program, hand-drawn animation encompasses a much broader set of animation styles and character designs. For any visual correspondence model to work in the wild on a variety of animation styles, it cannot only be trained on synthetic data from 3D programs. To solve this, we collect a dataset of high resolution hand-drawn animation from 17 different real-world animation productions, totalling 3578 frames. The animation style of each production varies greatly, although the style is closer to U.S. and European animation. The dataset is extremely diverse, with hundreds of different characters. Importantly, the real dataset does not have unique correspondence labels; we use the segment colors in the colorized images to extract labels. In contrast with the synthetic dataset, this yields \\textit{non-unique} numeric segment labels. We create our evaluation set by randomly selecting sequences 25 variable-length sequences uniformly across each production. For 5 sequences in the evaluation set, no training data from the originating production exists in the training set at all.\n\n\\subsection{Implementation details}\n\n\\PAR{Training details:} We train AnT using the AdamW optimizer with and a learning rate of 5e-4, weight decay of 1e-4, gradient clipping at global norm 1. We use a learning rate warmup of 1K steps, and train for 100k iterations with no learning rate decay. AnT is trained with an effective batch size of 64 using gradient accumulation over 4 batches of 16 image pairs each. The transformer has input and attention dropout of 0.1, which we found helpful for regularization. Unless otherwise specified, we train AnT with $L=9$ layers of alternating multi-head self- and cross-attention with 4 heads each and $D=256$ dimensional local features.\n\n\\PAR{Time and memory complexity:} A single forward pass of AnT takes on average 76ms (13 FPS) on a Nvidia Tesla V100 GPU. Using $M$ and $N$ to denote the number of reference and target segments, each cross attention layer AnT has to make $\\mathcal{O}(MN)$ comparisons and each self attention layer AnT has to make $\\mathcal{O}(M^2 + N^2)$ comparisons. By comparison, a forward pass of DEVC takes on average 147ms (6 FPS). Memory-wise, DEVC has to make $\\mathcal{O}((HW)^2)$ comparisons, where $H$,$W$ are the spatial dimensions of the CNN features. We were limited to using a batch size of 3 for DEVC, whereas we could use a batch size of 64 for AnT, yielding much faster training. Our leak-proof filling method implemented in OpenGL takes on average 1.4s on the same hardware, yielding a total inference speed of 2.16s for AnT or 2.87s for DEVC. \n\n\\subsection{Comparisons}\n\n\\PAR{Baselines:} We compare the performance of AnT to both the vanilla implementation of Deep Exemplar Video Colorization (DEVC) \\cite{deep-exemplar-video-colorization} as well as variants of DEVC with domain specific modifications. DEVC is a state-of-the-art video colorization network that operates on the pixel-level and matches features with a deep neural network. To use DEVC in our tasks, we use only the correspondence subnet and use the categorical cross-entropy loss on the colorized warped image. We then generate a predicted segment label for each segment by a non-learned post-processing step: we take the maximally occurring color in each of the segment locations on the warped image.\n\nSince DEVC is a pixel-based approach, we create two variants with domain specific enhancements that take advantage of the problem structure. Since small segments are the hardest to predict, we weight the loss of each pixel in the warped image output inversely proportional to the size of the segment corresponding to that pixel location. This helps prevent the network from unknowingly focusing on large segment areas while ignoring smaller ones. We refer to this network as DEVC (Weighted Loss). \n\nWe also observed that high resolution is important for performance. We introduce the local attention mechanism used in \\cite{lai2019self} in place of global attention to enable training at higher resolutions. This model is referred to as DEVC (Local Attention). We train DEVC and DEVC (Weighted Loss) at 512x512 pixel resolution with batch size of 2 until convergence. We train DEVC (Local Attention) at 640x640 pixel resolution also with a batch size of 2 until convergence.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{figures/recursive}\n\\vspace{-0.1in}\n\\caption{ Starting from a reference color and line images i.e. $c_{ref}$ and $l_{ref}$, we recursively propagate the colors of each generated image $c_{i}$ to colorize every incoming line image $l_{i+1}$. }\n\\label{fig/recursive}\n\\vspace{-0.2in}\n\\end{figure}\n\n\\PAR{Metrics:} To measure correspondence across sequences, we recursively propagate segment labels over 10 frames as illustrated in Figure \\ref{fig/recursive}, using a single ground truth reference frame to seed the colors for the rest of the predictions. We use per-segment label accuracy and mean Intersection-Over-Union averaged over the label classes as our evaluation metrics.\n\n\\PAR{Results:} We show qualitative results in Figure \\ref{fig/wide} and results of comparing AnT to DEVC in Table \\ref{tab:comp} on both the synthetic and real datasets. The synthetic column is evaluated on the ground truth segment correspondence labels, while the real dataset is evaluated on the non-unique color labels.\n\n\\begin{table}[h]\n  \\centering\n  \\resizebox{\\columnwidth}{!}{%\n  \\begin{tabular}{r  c c c c}\n    \\toprule\n    & \\multicolumn{2}{c}{\\bf Synthetic} & \\multicolumn{2}{c}{\\bf Real} \\\\\n  & Accuracy & Mean IoU & Accuracy & Mean IoU \\\\\n  \\midrule\n    DEVC & 66.19 & 43.17 & 42.86 & 29.37 \\\\\n    DEVC (Weighted Loss) & 79.92 & 55.98 & 61.86 & 38.05 \\\\\n    DEVC (Local Attention) & 84.11 & 62.60 & 57.34 & 32.98 \\\\\n    \\fontseries{b}\\selectfont{AnT (Ours)} & \\fontseries{b}\\selectfont{92.17} &  \\fontseries{b}\\selectfont{72.90} & \\fontseries{b}\\selectfont{79.38} & \\fontseries{b}\\selectfont{45.38}  \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n  \\caption{\\label{tab:comp}\n    {\\bf Evaluation on Correspondence (Synthetic) and Colorization (Real).} AnT strictly outperforms all the baselines, even after segment-specific modifications are added. The real dataset contains chunkier motion that moves outside the field of view in DEVC (Local Attention)\n    }\n    \\vspace{-0.2in}\n\\end{table}\n\n\n\\begin{figure*}[h]\n\\centering\n\\includegraphics[width=\\linewidth]{figures/wide}\n\\vspace{-0.3in}\n\\caption{AnT is effective at colorizing complex scenes with occlusion, small segments, and complex deformations. In the bottom left example, AnT fails to colorize the yellow sleeve because it was not present in the reference line image. We encourage readers to look at the Appendix for additional results.}\n\\label{fig/wide}\n\\vspace{-0.1in}\n\\end{figure*}\n\n\n\n\\subsection{Model Ablation Study}\nWe pull apart several key components of AnT to show how performance changes when these components are removed (see Table \\ref{tab:ablation}). The transformer is highly correlated with performance, which shows that the global feature aggregation helps learn effective representations. Similarly, spatial information is also necessary for AnT to reason about the segment structures effectively. When cycle-consistency is removed in the model trained on the real dataset, the model avoids learning generalizable correspondences -- it \"cheats\" by matching non-corresponding segments with the same color.\n\n\\begin{table}[h]\n  \\centering\n\n  \\resizebox{\\columnwidth}{!}{%\n  \\begin{tabular}{r  c c c c}\n    \\toprule\n    & \\multicolumn{2}{c}{\\bf Synthetic} & \\multicolumn{2}{c}{\\bf Real} \\\\\n  & Accuracy & Mean IoU & Accuracy & Mean IoU \\\\\n  \\midrule\n    No transformer & 78.56 & 67.82 & 65.91 & 39.53 \\\\\n    No positional embedding & 81.88 & 67.23 & 68.23 & 40.20  \\\\\n    No cycle consistency & 91.49 & 71.01 & 68.48 & 41.10 \\\\\n    Smaller (3 layers) & 88.03 & 69.90 & 76.09 & 44.02 \\\\  \n    Full (9 layers) & \\fontseries{b}\\selectfont{92.17} &  \\fontseries{b}\\selectfont{72.90} & \\fontseries{b}\\selectfont{79.38} & \\fontseries{b}\\selectfont{45.38} \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n    \\caption{\\label{tab:ablation}\n    {\\bf Model Ablation study.} Comparison of different model variants in AnT.\n  }\n  \\vspace{-0.2in}\n\\end{table}\n\n\\subsection{Training Data Ablation Study}\nIn order to assess AnT's ability to learn without ground-truth correspondence labels, we perform an ablation study (see Table \\ref{tab:training}) across 3 different training sets: synthetic, real, and mixed (which is simply the sum of synthetic and real). As in the earlier section, the synthetic and real columns denote the evaluation set with the same metrics as before.\nNotably, we see that when AnT is trained on the real dataset, its performance on the synthetic correspondence dataset approaches that of when it has access to correspondences at training time. The real dataset is much more challenging and diverse, leading to a more robust model that can predict correspondences on the less challenging synthetic dataset. The inverse is not true; when the model trained on only synthetic data is evaluated on real, a bigger performance differential exists. We hypothesize this is because the synthetic dataset lacks diversity and challenge. \n\n\\begin{table}[h]\n  \\centering\n\n  \\resizebox{\\columnwidth}{!}{%\n  \\begin{tabular}{r  c c c c}\n    \\toprule\n    & \\multicolumn{2}{c}{\\bf Synthetic} & \\multicolumn{2}{c}{\\bf Real} \\\\\n  & Accuracy & Mean IoU & Accuracy & Mean IoU \\\\\n  \\midrule\n    Synthetic & 92.17 & 72.90 & 72.55 & 39.93 \\\\\n    Real & 89.46 & 70.20 & 79.38 & 45.38 \\\\\n    Mixed & \\fontseries{b}\\selectfont{94.25} & \\fontseries{b}\\selectfont{77.27} & \\fontseries{b}\\selectfont{79.84} & \\fontseries{b}\\selectfont{51.64} \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n  \\caption{\\label{tab:training}\n    {\\bf Training Data Ablation Study.} While the best results on synthetic come from the mixed training set, when AnT is only trained on the real dataset its performance approaches that of the model trained on correspondence labels.\n  }\n  \\vspace{-0.2in}\n\\end{table}\n\n\\section{Conclusion}\n\nIn this paper, we have shown that segment is an effective structure for learning visual correspondence on hand-drawn images. Our results show our method's ability to leverage real-world animation datasets that are crucial for learning accurate correspondences on a wide variety of animation styles.\n\nWe hope this work encourages more research into practical, data-driven creative tools for animation. Although we focused on flat-filled animation in this work, our method can be extended to other tasks such as propagating shadows and texture or predicting optical flow.\n\n\\section{Acknowledgements}\n\nWe'd like to thank Andrew Drozdov, Xavier Snelgrove, James MacGlashan, Eric Jang and everyone at ML Collective for helpful conversations that helped inform the design of AnT. We would also like to thank Paul-Edouard Sarlin for the SuperGlue code release and answering questions about hyperparameter settings and model scaling. We also thank Masha Shugrina for answering many questions about the CreativeFlow dataset. Finally, we are grateful to the engineering team at Spell for their work on training infrastructure that this project used. \n\n\\section*{Author Contributions}\n\\noindent\n\\textbf{Evan Casey} led the research and technical direction and wrote the majority of this manuscript.\n\n\\noindent\n\\textbf{Evan Casey and V\u00edctor P\u00e9rez} implemented the models, training infrastructure, and conducted all the experiments used in the paper.\n\n\\noindent\n\\textbf{V\u00edctor P\u00e9rez} helped come up with the transformer-based architecture, wrote early versions of the manuscript, and created most of the visualizations and figures used in the paper.\n\n\\noindent\n\\textbf{Zhuoru Li} contributed to many of the ideas in AnT from his parallel research on applying transformers to colorization. He advised on some critical pieces of the research and wrote parts of the manuscript.\n   \n\\noindent\n\\textbf{Harry Teitelman} managed the non-synthetic (real) dataset curation and labeling pipeline.\n   \n\\noindent\n\\textbf{Harry Teitelman and Nick Boyajian} developed the synthetic data generation pipeline.\n\n\\noindent\n\\textbf{Tim Pulver, Harry Teitelman, and Nick Boyajian} built the Cadmium desktop app.\n\n\\noindent\n\\textbf{Mike Manh} developed the OpenGL segmentation algorithm.\n\n\\noindent\n\\textbf{William Grisaitis} contributed to early versions of the training infrastructure and was helpful in early research discussions.\n    \n\n    \n\n    \n\n    \n\n\n\n{\\small\n\\bibliographystyle{ieee_fullname}\n", "meta": {"timestamp": "2021-09-10T15:09:53", "yymm": "2109", "arxiv_id": "2109.02614", "language": "en", "url": "https://arxiv.org/abs/2109.02614"}}
