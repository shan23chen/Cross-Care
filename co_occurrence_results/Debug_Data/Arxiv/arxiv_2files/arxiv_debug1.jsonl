{"text": "\\section{GAN-based Abstraction}\\label{sec:abstraction}\n\n\\subsection{Model Abstraction}\n\nThe underlying idea is the following: given a stochastic process $\\{\\eta_{t}\\}_{t\\ge 0}$ with transition probabilities $\\mathbb{P}_{s_0}(\\eta_{t}=s) = \\mathbb{P}(\\eta_{t}=s\\mid \\eta_{t_0}=s_0)$, we aim at finding another stochastic process whose trajectories are faster to simulate but similar to the original ones.\nTime has to be discretized, meaning we fix an initial time $t_0$ and a time step $\\Delta t$ that suits our problem. We define $\\tilde{\\eta}_i := \\eta_{t_0+i\\cdot\\Delta t}$, $\\forall i\\in\\mathbb{N}$. In addition, \ngiven a fixed time horizon $H$, we define time-bounded trajectories as $\\tilde{\\eta}_{[1,H]} = s_1 s_2\\cdots s_H\\in S^H\\subseteq\\mathbb{N}^{H\\times n}$. Given a state $s_0$ and a set of parameters $\\theta$, we can represent a trajectory of length $H$ as a realization of a random variable over the state space $S^H$.\nThe probability distribution for such random variable is given by the product of the transition probabilities at each time step: \n${ \\mathbb{P}_{s_0,\\theta}(\\tilde{\\eta}_{[1,H]}=s_1s_2\\cdots s_H)=\\prod_{i=1}^H\\mathbb{P}_{s_{i-1}, \\theta}(\\tilde{\\eta}_{i}=s_{i})}$.\nThe CTMC, $\\{\\eta_{t}\\}_{t\\ge 0}$, is now expressed as a time-homogeneous Discrete Time Markov Chain $\\{\\tilde{\\eta}_i\\}_i$. \nAn additional approximation has to be made: the abstract model takes values in $S'\\subseteq\\mathbb{R}_{\\geq 0}^n$, a continuous space in which the state space $S\\subseteq\\mathbb{N}^n$ is embedded.\nIn constructing the approximate probability distribution for trajectories we can decide to restrict our attention to arbitrary aspects of the process, rather than trying to preserve the full behavior. A \\emph{projection} $\\pi$ from $S^H$ to an arbitrary space $U^H$ can be used to reach this purpose,  for instance, to monitor the number of molecules belonging to a certain subset of chemical species, i.e., $U\\subseteq S$. \nNote that $\\pi(\\tilde{\\eta}_{[0,H]})$ is a random variable over $U^H$. \nSuch flexibility could be extremely helpful in capturing the dynamics of systems in which some species are not observable.\n\n\\paragraph{Abstraction accuracy.} Another important ingredient is a meaningful quantification of the error introduced by the abstraction procedure, i.e., the reconstruction accuracy. Such quantification must be based on a distance, $d$, among distributions. \nWe choose the Wasserstein distance, together with the absolute and relative difference among means and variances of the histograms.\nGiven a distribution over initial states $s_0$ and a distribution over parameters $\\theta$, we would like to measure the expected error at every time instant $t_i = t_0+i\\cdot\\Delta t$ with $i\\in\\{1,\\dots,H\\}$. Formally, we want to measure $\\mathbb{E}_{s_0,\\theta}\\left[d\\big(\\pi(\\eta_{[1,H]})\\big |_i, \\pi'(\\eta'_{[1,H]})\\big |_i\\big)\\right]$ where $\\pi(\\eta_{[1,H]})\\big |_i$ denotes the $i$-th time components of the projected trajectory $\\pi(\\eta_{[1,H]})\\in U^H$. \nTo estimate such quantity we use a well-known unbiased estimator, which is the average over the distances computed over a large sample set of  initial settings. Computing the distance among SSA and abstract distributions at each time step quantifies how small the expected error is and, more importantly, \nhow it evolves in time. As a matter of fact, it shows whether the error tends to propagate or not and how much each species contributes to the abstraction error. \nIn practice, we compute $H \\cdot n$ distances among distributions over $\\mathbb{N}$ as we want to know how each species contributes in the reconstruction error. \n\n\\vspace{-0.2cm}\n\n\\subsection{Dataset Generation}\n\n\\paragraph{Training set.} \nChoose a set of $N_{train}$ initial settings and for each setting simulate $k_{train}$ SSA trajectory of length $H$. The training set is composed of $N_{train}\\cdot k_{train}$ pairs initial setting-trajectory, i.e. pairs $(\\theta^i,s_0^i,\\eta^{ij}_{[1,H]})$ for $i=1, \\ldots,N_{train}$ and $j= 1,\\ldots, k_{train}$.\n\n\\paragraph{Test set.} \n\nChoose a set of $N_{test}$ initial settings and for each setting simulate a large number, $k_{test}\\gg k_{train}$, of SSA trajectory of length $H$. The test set is composed of $N_{test}\\cdot k_{test}$ pairs initial setting-trajectory, i.e. pairs $(\\theta^i,s_0^i,\\eta^{ij}_{[1,H]})$ for $i=1, \\ldots,N_{test}$ and $j= 1,\\ldots, k_{test}$. \n\n\\paragraph{Partial observability.} In case of partial observability, $U\\subseteq S$,  we fix an initial condition for species in $U$, and simulate a pool of trajectories each time sampling the initial value of species in $S\\smallsetminus U$. As a result, we are learning and abstract distribution that marginalizes over unobserved variables.\n\n\\vspace{-0.2cm}\n\n\\subsection{cWCGAN-GP architecture} \n\nThe critic $C_{w_c}$ takes as input a batch of initial states, $s_0^1,\\dots , s_0^b$, a batch of parameters, $\\theta_1, \\dots , \\theta_b$, and a batch of subsequent trajectories, $\\eta_{[1,H]}^1,\\dots , \\eta_{[1,H]}^b$. For each $i\\in\\{ 1,\\dots , b\\}$ the inputs, $\\eta_{[1,H]}^i$, $s_0^i$ and $\\theta_i$, are concatenated to form an input with dimension $b\\times (H+1)\\times (n+m)$.\nFormally, $C_{w_c}: S^{H+1}\\times\\Theta\\rightarrow \\mathbb{R}$.  To enforce the Lipschitz property over $C_ {w_c}$ we add a gradient penalty term over $\\mathbb{P}_{\\hat{x}}$. Samples of $\\mathbb{P}_{\\hat{x}}$ are generated by sampling uniformly along straight lines connecting points coming from a batch of real trajectories and points coming from a batch of generated trajectories.\n\nOn the other hand, the generator $G_{w_g}$ takes as input a batch of initial states, $s_0^1,\\dots , s_0^b$, a batch of parameters, $\\theta_1, \\dots , \\theta_b$, and a batch of random noise, $z^1,\\dots , z^b$, with dimension $k$, a user-defined hyper-parameter. For each $i\\in\\{ 1,\\dots , b\\}$ the two inputs are, once again, concatenated to form an input with dimension $b\\times (n+m+k)$. The generator outputs a batch of generated trajectories $\\hat{\\eta}_{[1,H]}^1,\\dots\\hat{\\eta}_{[1,H]}^b$. Formally, $G_{w_g}:S\\times\\Theta\\times Z\\rightarrow S^H$, such that $G_{w_g}(s_0,\\theta,z) = \\hat{\\eta}_{[1,H]} = s_1\\cdots s_H$. See the pseudocode for the algorithm in Appendix~\\ref{sec:algorithm} of [XXX].\n\n\\vspace{-0.2cm}\n\n\\subsection{Model Training}\nThe cWCGAN-GP-based model abstraction framework consists in training two different CNNs.\nThe loss function, introduced in Eq.~\\eqref{eq:wassdist_gp}, is a parametric function depending both on the generator weights $w_g$ and the critic weights $w_c$. \nWhen training the critic, we keep the generator weights constant $\\overline{w}_g$, and we maximize $\\mathcal{L}(w_c, \\overline{w}_g)$ w.r.t. $w_c$.  \nFormally, we solve the problem \n\\begin{equation*}\n   \\small\n   w_c^* = \\underset{w_c}{\\mbox{argmax}}\\Big\\{\\mathcal{L}(w_c, \\overline{w}_g)\\Big\\}. \n\\end{equation*}\nOn the other hand, in training the generator, we keep the critic weights constant $\\overline{w}_c$, \nand we minimize $\\mathcal{L}(\\overline{w}_c, w_g)$ w.r.t. $w_g$. \nFormally, we solve the problem \n\\begin{equation*}\n    \\small\n    w_g^* = \\underset{w_g}{\\mbox{argmin}} \\Big\\{\\mathcal{L}(\\overline{w}_c,w_g)\\Big\\} =  \\underset{w_g}{\\mbox{argmin}}  \\Big\\{ -\\mathbb{E}_{z,(s_0,\\theta)}\\Big[C_{\\overline{w}_c}\\big(G_{w_g}(z,s_0,\\theta),s_0,\\theta)\\Big]\\Big\\}.\n\\end{equation*}\nAs mentioned in Section \\ref{sec:background}, the loss function derives from the Wasserstein distance between the real and generated distributions, see \\cite{arjovsky2017wasserstein,gulrajani2017improved} for the mathematical details.\n\nIntuitively, the generator generates a batch of samples, and these, along with real examples from the dataset, are provided to the critic, which is then updated to get better at estimating the distance between the real and the abstract distribution. The generator is then updated based on scores obtained by the generated samples from the critic. An important collateral advantage is that WGANs have a loss function that correlates with the quality of generated examples.\n\nTraining the cWCGAN-GP has a cost. Nonetheless, once it has been trained, its evaluation is extremely fast. Details about training and evaluation costs are discussed in Section~\\ref{sec:experiments}. \n \n\n\\paragraph{Abstract Model Simulation.}\nOnce the training is over, we can discard the critic and focus only on the trained generator $G$. In order to generate an abstract trajectory starting from a state $s_0^*$ with parameters $\\theta^*$, we just have to sample a value $z$ from the random noise variable $Z$ and evaluate the generator on the pair $(s_0^*, \\theta^*, z)$. The output is a stochastic trajectory of length $H$: $G(s_0^*, \\theta^*, z) = \\hat{\\eta}_{[1,H]}$. The stochasticity is provided by the random noise variable, de facto the generator acts as a distribution transformer that maps a simple random variable  into a complex distribution. In order to generate a pool of $p$ trajectories, we simply sample $p$ different values from the random noise variable: $z_1, \\dots,z_p$.\nTherefore, the generation of a trajectory has a fixed computational cost. \n\\section{Case Studies}\\label{sec:casestudies}\n\\begin{itemize}\n\\item \\textbf{SIR Model (Absorbing state).} \nThe SIR epidemiological model describes a population divided in three mutually exclusive groups: susceptible (S), infected (I) and recovered (R). The system state at time $t$ is $\\eta_t= (S_t, I_t, R_t)$. The possible reactions, given by the interaction of individuals (representing the molecules of a CRN), are the following:\n\\begin{itemize}\n    \\item $R_1: S+I\\xrightarrow{\\theta_1\\cdot I_tS_t/(S_t+I_t+R_t)} 2I$ (infection),\n    \\item $R_2: I\\xrightarrow{\\theta_2\\cdot I_t} R$  (recovery).\n\\end{itemize}\nThe model describes the spread, in a population, of an infectious disease that grants immunity to those who recover from it. As the SIR model is well-known and stable, we use it as a testing ground for our GAN-based abstraction procedure. The ranges for the initial state are $S_0, I_0, R_0 \\in [30, 200]$.\nAn important aspect of the SIR model is the presence of an absorbing states. In fact, when $I = 0$ or when $R=N$ no more reaction can take place. \n\n\\item \\textbf{Ergodic SIRS Model.} Small perturbations of the SIR model force the system to be ergodic. We called this revised version ergodic SIRS (eSIRS). This model has no absorbing state. In particular, we assume that the population is not perfectly isolated, meaning there is always a chance of getting infected from some external individuals. In addition, we also assume that immunity is only temporary.\nThe possible reactions are now the following:\n\\begin{itemize}\n    \\item $R_1: S+I\\xrightarrow{\\theta_1\\cdot I_tS_t/(S_t+I_t+R_t)+\\theta_2\\cdot S_t} 2I$ (infection),\n    \\item $R_2: I\\xrightarrow{\\theta_3\\cdot I_t} R$  (recovery),\n    \\item $R_3: R\\xrightarrow{\\theta_4\\cdot R_t} S$  (immunity loss),\n\\end{itemize}\nBoth epidemiological models are essentially unimodal.\nThe ranges for the initial state are $S_0, I_0, R_0 \\in [0, N]$ such that $S_0 + I_0 + R_0 = N$. In our experiments $N= 100$. The range for parameter $\\theta_1$ is $[0.5,5]$.\n\n\\item \\textbf{Genetic Toggle Switch Model (Bistability).} \nThe toggle switch is a well-known bistable biological circuit. Briefly, this system consists of two genes, $G_1$ and $G_2$, that mutually repress each other. The system displays two stable equilibrium states in which either of the two gene products represses the expression of the other gene.\nThe possible reactions are:\n\\begin{itemize}\n    \\item $prod_i: G_i^{on}\\xrightarrow{kp_i\\cdot G_i^{on}} G_i^{on}+P_i$, for $i=1,2$;,\n    \\item $bind_i: 2P_j+G_i^{on}\\xrightarrow{kb_i\\cdot G_i^{on}\\cdot P_j\\cdot(P_j-1)} G_i^{off}$, for $i=1,2$ and $j=2,1$ resp.;\n    \\item $unbind_i: G_i^{off}\\xrightarrow{ku_i\\cdot G_i^{off}} G_i^{on}+2P_j$, for $i=1,2$ and $j=2,1$ resp.;\n    \\item $deg_i: P_i\\xrightarrow{kd_i\\cdot Pi} \\emptyset$, for $i = 1,2$.\n\\end{itemize}\nThe ranges for the initial state are $G_{1,0}, G^{on}_{2,0}\\in\\{0,1\\}$ and  $P_{1,0}, P_{2,0} \\in [5, 20]$.\n\n\\item \\textbf{Oscillator Model.} \nThe oscillator circuit consists of three species A, B and C and three reactions, in which A converts B to itself, B converts C to itself, and C converts A to itself.\nThe three species regulate each other in a cyclic manner. This circuit was found to exhibit oscillations in the concentrations of the three species. \n\\begin{itemize}\n    \\item $R_1: A+B\\xrightarrow{{\\tiny \\theta\\cdot\\frac{A\\cdot B}{A+B+C}}} 2A$ (B transformation),\n    \\item $R_2: B+C\\xrightarrow{{\\tiny \\theta\\cdot\\frac{B\\cdot C}{A+B+C}}} 2B$ (C transformation),\n    \\item $R_3: C+A\\xrightarrow{{\\tiny \\theta\\cdot\\frac{C\\cdot A}{A+B+C}}} 2C$ (A transformation).\n\\end{itemize}  \nThe ranges for the initial state are $A_0, B_0, C_0 \\in [20, 100]$.\n\n\n\\item \\textbf{MAPK Model.}\n\nMitogen-activated protein kinase cascade is a particular type of signal transduction into protein phosphorylation (PP) whose function is the amplification of a signal. The sensitivity increases with the number of cascade levels, such that a small change in a stimulus results in a large change in the response.\nNegative feedback from MAPK-PP to the MAKKK activating reaction with ultra-sensitivity to a input stimulus, governed by parameter $V_1$.\n\n\\begin{itemize}\n\\item $\\it R_1: MKKK \\xrightarrow{{\\tiny V_1\\cdot MKKK/( (1+(MAPK\\_PP/K_l)^n)\\cdot (K_1+MKKK) )}} MKKK\\_P$,\n\t\n\\item $\\it R_2:  MKKK\\_P \\xrightarrow{{\\tiny V_2\\cdot MKKK\\_P/(K_2+MKKK\\_P)}} MKKK $,\n\t\n\\item $\\it R_3:\tMKK \\xrightarrow{{\\tiny k_3\\cdot MKKK\\_P\\cdot MKK/(K_3+MKK)}} MKK\\_P $,\n\t\n\\item $\\it R_4:  MKK\\_P  \\xrightarrow{{\\tiny k_4\\cdot MKKK\\_P\\cdot MKK\\_P/(K_4+MKK\\_P)  }}     MKK\\_PP$,\n\t\n\\item $\\it R_5: \tMKK\\_PP\n\\xrightarrow{{\\tiny V_5\\cdot MKK\\_PP/(K_5+MKK\\_PP) }} \nMKK\\_P $,\n\t\n\\item $\\it R_6:\n\tMKK\\_P \n\t\\xrightarrow{{\\tiny V_6\\cdot MKK\\_P/(K_6+MKK\\_P)}}\n\tMKK$,\n\t\n\\item $\\it R_7:\n\tMAPK \n\t\\xrightarrow{{\\tiny k_7\\cdot MKK\\_PP\\cdot MAPK/(K_7+MAPK)}}\n   MAPK-P$,\n\\item $\\it R_8:\n\tMAPK\\_P \n\t\\xrightarrow{{\\tiny k_8\\cdot MKK-PP\\cdot MAPK\\_P/(K_8+MAPK\\_P)}}\n    MAPK\\_PP$,\n    \n\\item $\\it R_9:\n\tMAPK\\_PP \n\t\\xrightarrow{{\\tiny V_9\\cdot MAPK\\_PP/(K_9+MAPK\\_PP)}}\n    MAPK\\_P$,\n    \n\\item $\\it R_{10}:\n\tMAPK\\_P \n\t\\xrightarrow{{\\tiny V_{10}\\cdot MAPK\\_P/(K_{10}+MAPK\\_P)}}\n\tMAPK$.\n\n\\end{itemize}\n\n\\end{itemize}\nThe ranges for the initial state are: $MKKK_0, MKKK_0\\_P \\in [0, 100]$ such that $MKKK_0 + MKKK_0\\_P = 100$; $MKK_0, MKK_0\\_P, MKK_0\\_PP \\in [0, 300]$ such that $MKK_0 + MKK_0\\_P+ MKK_0\\_PP = 300$; $MAPK_0, MAPK_0\\_P, MAPK_0\\_PP \\in [0, 300]$ such that $MAPK_0 + MAPK_0\\_P+ MAPK_0\\_PP = 300$.\n\\section{Additional Plots}\n\\label{sec:plots}\nHere we present the remaining plots showing a qualitative evaluation of the performances of the abstract models.\nFor each model, we present a small batch of trajectories, both real and abstract (plots on the left column). From the plots of such trajectories we can appreciate if the abstract trajectories are similar to real ones and if they capture the most important macroscopic behaviors. We also show the histograms of the empirical distributions at time $t_H$ for each species (plots on the right column) to quantify the behavior over all the $2$K trajectories present in the test set. In particular, Fig. \\ref{fig:sir_trajectories} shows the results for the SIR case study, Fig. \\ref{fig:esir_trajectories_one_par} shows the results for the e-SIRS model with one varying parameter, Fig. \\ref{fig:ts_trajectories} shows the results for the Toggle Switch model and, finally, Fig. \\ref{fig:clock_trajectories} shows the results for the Oscillator model.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[scale=0.25]{imgs/SIR/SIR_Trajectories5.png}\n    \\includegraphics[scale=0.25]{imgs/SIR/SIR_hist_comparison_last_timestep_5.png}\n    \n    \\caption{SIR\n    model: \\textbf{(left)} comparison of trajectories generated with a cWCGAN-GP (orange) and the trajectories generated with the SSA algorithm (blue); \\textbf{(right)} comparison of the real and generated histogram at the last timestep. Performance on a randomly chosen test point represented by three trajectories: the top one (species S), the central one (species I) and the bottom one (species R).\\vspace{-0.5cm}}\n    \\label{fig:sir_trajectories_extra}\n    \\end{figure}\n    \n    \n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[scale=0.25]{imgs/eSIRS/eSIRS_Rescaled_Trajectories2_tight.png}\n    \\includegraphics[scale=0.25]{imgs/eSIRS/eSIRS_rescaled_hist_comparison_-1th_timestep_2_tight.png}\n    \\includegraphics[scale=0.25]{imgs/eSIRS/eSIRS_Rescaled_Trajectories16_tight.png}\n    \\includegraphics[scale=0.25]{imgs/eSIRS/eSIRS_rescaled_hist_comparison_-1th_timestep_16_tight.png}\n    \\caption{e-SIRS\n    model: \\textbf{(left)} comparison of trajectories generated with a cWCGAN-GP (orange) and the trajectories generated with the SSA algorithm (blue); \\textbf{(right)} comparison of the real and generated histogram at the last timestep. Performance for two, randomly chosen, test points. Each point is represented by a pair of trajectories: the top one (species S) and the bottom one is for  (species I).\\vspace{-0.5cm}}\n    \\label{fig:esir_trajectories}\n    \\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n\n    \\includegraphics[scale=0.25]{imgs/TS/ToggleSwitch_Rescaled_Trajectories18_tight.png}\n    \\includegraphics[scale = 0.25]{imgs/TS/ToggleSwitch_rescaled_hist_comparison_-1th_timestep_18_tight.png}\n    \n    \\caption{Toggle Switch model: \\textbf{(left)} comparison of trajectories generated with a cWCGAN-GP (orange) and the trajectories generated with the SSA algorithm (blue); \\textbf{(right)} comparison of the real and generated histogram at the last timestep. Performance for a randomly chosen, test point represented by a pair of trajectories: the top one (species P1) and the bottom one (species P2).\\vspace{-0.5cm}}\n    \\label{fig:ts_trajectories_extra}\n\\end{figure}\n\n\\begin{figure}[ht]\n   \\centering\n   \n   \\includegraphics[scale=0.25]{imgs/Oscillator/Oscillator_Rescaled_Trajectories4.png}\n   \\includegraphics[scale=0.25]{imgs/Oscillator/Oscillator_rescaled_hist_comparison_-1th_timestep_4.png}\n    \\caption{Oscilator model: \\textbf{(left)} comparison of trajectories generated with a cWCGAN-GP (orange) and the trajectories generated with the SSA algorithm (blue);\\textbf{(right)} comparison of the real and generated histogram at the last timestep. Performance on a randomly chosen test point represented by three trajectories: the top one (species A), the central one (species B) and the bottom one (species C). \\vspace{-0.5cm}\n   }\\label{fig:clock_trajectories_extra}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \n    \\subfigure[eSIRS]{  \\includegraphics[scale=0.23]{imgs/eSIRS/eSIRS_avg_mean_distance_1000epochs_32steps.png}}\n    \\subfigure[eSIRS-1P]{  \n    \\includegraphics[scale=0.23]{imgs/eSIRS_1P/Scaled_avg_mean_distance_1epochs_32steps.png}}\n    \\subfigure[SIR]{  \n    \\includegraphics[scale=0.23]{imgs/SIR/Scaled_avg_mean_distance_500epochs_16steps.png}}\n    \\subfigure[Oscillator]{  \n    \\includegraphics[scale=0.23]{imgs/Oscillator/Oscillator_avg_mean_distance_1000epochs_32steps.png}}\n    \\subfigure[Toggle Switch]{  \n    \\includegraphics[scale=0.23]{imgs/TS/ToggleSwitch_avg_mean_distance_2000epochs_32steps.png}}\n     \\subfigure[MAPK]{  \n    \\includegraphics[scale=0.23]{imgs/MAPK/MAPK_avg_mean_distance_1500epochs_32steps_scaling.png}}\n\n    \\caption{Plots of the average difference in the means over time for each model and each species. Errors are computed over the entire test set. Generated trajectories have been keep scaled to the interval $[-1,1]$ so that the scale of the system does not affect the scale of the error measure.}\\label{fig:avg_means_errors}\n\n    \n    \\subfigure[eSIRS]{  \\includegraphics[scale=0.23]{imgs/eSIRS/eSIRS_avg_mean_relative_distance_1000epochs_32steps.png}}\n    \\subfigure[eSIRS-1P]{  \n    \\includegraphics[scale=0.23]{imgs/eSIRS_1P/eSIRS_1P_avg_mean_relative_distance_1epochs_32steps.png}}\n    \\subfigure[SIR]{  \n    \\includegraphics[scale=0.23]{imgs/SIR/SIR_avg_mean_relative_distance_500epochs_16steps.png}}\n    \\subfigure[Oscillator]{  \n    \\includegraphics[scale=0.23]{imgs/Oscillator/Oscillator_avg_mean_relative_distance_999epochs_32steps.png}}\n    \\subfigure[Toggle Switch]{  \n    \\includegraphics[scale=0.23]{imgs/TS/ToggleSwitch_avg_mean_relative_distance_2000epochs_32steps.png}}\n     \\subfigure[MAPK]{  \n    \\includegraphics[scale=0.23]{imgs/MAPK/MAPK_avg_mean_relative_distance_1500epochs_32steps_scaling.png}}\n\n    \\caption{Plots of the average relative difference in the means over time for each model and each species. Errors are computed over the entire test set. Generated trajectories have been keep scaled back to $\\mathbb{N}$.}\\label{fig:avg_means_rel_errors}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\subfigure[eSIRS]{  \n    \\includegraphics[scale=0.23]{imgs/eSIRS/eSIRS_avg_var_distance_1000epochs_32steps.png}}\n    \\subfigure[eSIRS-1P]{  \n    \\includegraphics[scale=0.23]{imgs/eSIRS_1P/Scaled_avg_var_distance_1epochs_32steps.png}}\n    \\subfigure[SIR]{  \n    \\includegraphics[scale=0.23]{imgs/SIR/Scaled_avg_var_distance_500epochs_16steps.png}}\n    \\subfigure[Oscillator]{  \n    \\includegraphics[scale=0.23]{imgs/Oscillator/Oscillator_avg_var_distance_1000epochs_32steps.png}}\n    \\subfigure[Toggle Switch]{  \n    \\includegraphics[scale=0.23]{imgs/TS/ToggleSwitch_avg_var_distance_2000epochs_32steps.png}}\n    \\subfigure[MAPK]{  \n    \\includegraphics[scale=0.23]{imgs/MAPK/MAPK_avg_var_distance_1500epochs_32steps_scaling.png}}\n\n    \\caption{Plots of the average difference in the variances over time for each model and each species. Errors are computed over the entire test set. Generated trajectories have been keep scaled to the interval $[-1,1]$ so that the scale of the system does not affect the scale of the error measure.}\\label{fig:avg_vars_errors}\n    \n    \\subfigure[eSIRS]{  \\includegraphics[scale=0.23]{imgs/eSIRS/eSIRS_avg_var_relative_distance_1000epochs_32steps.png}}\n    \\subfigure[eSIRS-1P]{  \n    \\includegraphics[scale=0.23]{imgs/eSIRS_1P/eSIRS_1P_avg_var_relative_distance_1epochs_32steps.png}}\n    \\subfigure[SIR]{  \n   \\includegraphics[scale=0.23]{imgs/SIR/SIR_avg_var_relative_distance_500epochs_16steps.png}}\n    \\subfigure[Oscillator]{  \n    \\includegraphics[scale=0.23]{imgs/Oscillator/Oscillator_avg_var_relative_distance_999epochs_32steps.png}}\n    \\subfigure[Toggle Switch]{  \n    \\includegraphics[scale=0.23]{imgs/TS/ToggleSwitch_avg_var_relative_distance_2000epochs_32steps.png}}\n     \\subfigure[MAPK]{  \n    \\includegraphics[scale=0.23]{imgs/MAPK/MAPK_avg_var_relative_distance_1500epochs_32steps_scaling.png}}\n\n    \\caption{Plots of the average relative difference in the variances over time for each model and each species. Errors are computed over the entire test set. Generated trajectories have been keep scaled back to $\\mathbb{N}$.}\\label{fig:avg_vars_rel_errors}\n\\end{figure}\n\n\n\\begin{figure}[ht]\n    \\centering\n    \n    \\subfigure[Means abs. err.]{\n    \\includegraphics[scale=0.12]{imgs/eSIRS/eSIRS_mean_distance.png}}\n        \\subfigure[Means rel. err.]{\n    \\includegraphics[scale=0.12]{imgs/eSIRS/eSIRS_mean_relative_distance.png}}\n    \\subfigure[Variances abs. err.]{\n    \\includegraphics[scale=0.12]{imgs/eSIRS/eSIRS_var_distance.png}}\n    \\subfigure[Variances rel. err.]{\n    \\includegraphics[scale=0.12]{imgs/eSIRS/eSIRS_var_relative_distance.png}}\n    \\subfigure[Wass. dist.]{\n    \\includegraphics[scale=0.12]{imgs/eSIRS/eSIRS_wass_distance.png}}\n    \\caption{Histogram distance landscapes for the two-dimensional \\textbf{eSIRS} model.}\n    \\label{fig:eSIRS_distance_landscapes}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \n    \\subfigure[Means abs. err.]{\n    \\includegraphics[scale=0.12]{imgs/TS/ToggleSwitch_mean_distance.png}}\n            \\subfigure[Means rel. err.]{\n    \\includegraphics[scale=0.12]{imgs/TS/ToggleSwitch_mean_relative_distance.png}}\n    \\subfigure[Variances abs. err.]{\n    \\includegraphics[scale=0.12]{imgs/TS/ToggleSwitch_var_distance.png}}\n    \\subfigure[Variances rel. err.]{\n    \\includegraphics[scale=0.12]{imgs/TS/ToggleSwitch_var_relative_distance.png}}\n    \\subfigure[Wass. dist.]{\n    \\includegraphics[scale=0.12]{imgs/TS/ToggleSwitch_wass_distance.png}}\n    \\caption{Histogram distance landscapes for the two-dimensional \\textbf{Toggle Switch} model.}\n    \\label{fig:TS_distance_landscapes}\n\\end{figure}\n    \n\n\\begin{figure}[ht]\n    \\centering\n    \\subfigure[Wass. dist.]{\n    \\includegraphics[scale = 0.24]{imgs/MAPK/MAPK_wass_distance.png}}\n    \\subfigure[Means abs. err.]{\n    \\includegraphics[scale = 0.24]{imgs/MAPK/MAPK_mean_distance.png}}\n    \\subfigure[Means rel. err.]{\n    \\includegraphics[scale = 0.24]{imgs/MAPK/MAPK_mean_relative_distance.png}}\n    \\subfigure[Variances abs. err.]{\n    \\includegraphics[scale = 0.24]{imgs/MAPK/MAPK_var_distance.png}}\n    \\subfigure[Variances rel. err.]{\n    \\includegraphics[scale = 0.24]{imgs/MAPK/MAPK_var_relative_distance.png}}\n    \n    \\caption{Histogram distance landscapes for the \\textbf{MAPK} model.}\n    \\label{fig:mapk_distance_landscapes}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.245]{imgs/eSIRS_1P/analysis_avg_wass_distance_1epochs_32steps.png}\n    \\includegraphics[scale=0.245]{imgs/eSIRS_1P/analysis_avg_wass_distance_1epochs_32steps_100in.png}\n    \\includegraphics[scale=0.245]{imgs/eSIRS_1P/analysis_avg_wass_distance_1epochs_32steps_100par.png}\n    \\caption{Analysis of the generalization capabilities of the abstract model on various test sets: 100 different pairs $(s_0, \\theta)$ \\textbf{(left)}, fixed parameter and 100 different initial states \\textbf{(middle)} and a fixed initial state with 100 different parameters \\textbf{(right)}. For each test set we compute the mean  and the standard deviation of the distribution of Wasserstein distances over such sets.}\n    \\label{fig:esirs_1p_analysis}\n\\end{figure}\n\n\n\\section{Satisfaction probability}\\label{sec:satisf}\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[scale=0.17]{imgs/eSIRS/esirs_satisfability.png}\n     \\includegraphics[scale=0.17]{imgs/SIR/sir_sanity_check_absorption.png}\n     \\includegraphics[scale=0.17]{imgs/TS/ts_satisfability.png}\n    \\caption{\\textbf{(eSIRS)} Given the property ``eventually the number of infected stays below a threshold of 25 individual\", we check for each test point (x axis) the percentage of SSA (orange) and abstract (blue) trajectories that satisfy such property.\n    \\textbf{(SIR)} For abstract trajectories of the SIR model we check, for each test point, the percentage of valid trajectories, i.e. such that the state $I=0$ is absorbing.\n    \\textbf{(Toggle Switch)} Given the property ``eventually the level of protein P2 stays above a threshold of 50\", we check for each test point (x axis) the percentage of SSA (orange) and abstract (blue) trajectories that satisfy such property.\n    \\vspace{-0.25cm}}\n    \\label{fig:satisf}\n\\end{figure}\n\nWe seek a formal way to quantify whether the abstract model captures and preserves the emergent macroscopic behaviors of the original system. In order to do so, we can resort to formal languages, such as Signal Temporal Logic (STL) \\cite{maler2004monitoring}. The first step is to express formally the property that we would like abstract trajectories to preserve. Then we can measure the satisfaction probability of such property for both real and abstract trajectories and check if it is similar on a large pool of initial settings. Examples are shown in Fig. \\ref{fig:satisf}. For the e-SIRS model we consider the property ``eventually the number of infected remains below a threshold of 25 individual\". For abstract trajectories of the SIR model we check, for each test point, the percentage of valid trajectories, i.e. such that the state $I=0$ is absorbing. Finally,  for the Toggle Switch model we check the property ``eventually the level of protein $P_2$ stays above a threshold of $50$\", meaning we check for each test point the percentage of SSA and abstract trajectories that satisfy such property. It can be written as $\\Diamond_{[0,H]}\\square (P_2 > 50)$. These comparisons produce a measurable qualitative estimate of how good the reconstruction is. As future work, we intend to use such qualitative measure as a query strategy for an active learning approach, so that the obtained abstract model is driven in the desired direction.\n\n\n\n\\section{Statistical tests}\\label{sec:stat_test}\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[scale=0.245]{imgs/SIR/SIR_pvalues_no_rescaled_Energy.png}\n    \\includegraphics[scale=0.245]{imgs/eSIRS/eSIRS_pvalues_no_rescaled_Energy.png}\n    \\includegraphics[scale=0.245]{imgs/TS/ToggleSwitch_pvalues_no_rescaled_Energy.png}\n    \\includegraphics[scale=0.245]{imgs/Oscillator/Oscillator_pvalues_no_rescaled_Energy.png}\n    \\includegraphics[scale=0.245]{imgs/MAPK/MAPK_pvalues_no_rescaled_Energy_1500epochs.png}\n    \\caption{Average over the initial setting of the p-values (with confidence interval) for each species computing by the two-sample statistical test w.r.t. the number of samples present in the empirical distributions.}\n    \\label{fig:stat_test}\n\\end{figure}\nIn this section we show the results of a two-sample statistical test over all the cases studies. In particular, we use a statistical test based on the energy distance among distributions \\cite{szekely2013energy}. For each initial setting and for each species we compute the distance statistics and the p-value among the empirical approximation of the SSA distribution and the empirical approximation of the abstract distribution over the trajectory space, i.e. a $H$-dimensional space. In Fig. \\ref{fig:stat_test} we report the mean and the standard deviation of p-values over the initial settings present in the test set. Clearly the p-value decreases as the number of samples used to approximate the distributions increases. Fig. \\ref{fig:stat_test} shows how the p-values for each species varies according to the number of samples used.\nThese results come with no surprise as the abstract model was trained having only $10$ observations for each initial setting. It is interesting to observe how the Energy test is passed by a large percentage of points when the number of samples is around $10$. In order to enhance the resilience of the abstract model to such statistical tests we should increase the number of samples per point in the training set. This comes at the cost of reducing the number of initial setting, so that the resulting training set is not too large. In this regard, the active learning technique proposed in Section \\ref{sec:satisf} can be extremely beneficial.\n\n\\section{cWCGAN-GP Algorithm}\\label{sec:algorithm}\n\n\n\\begin{algorithm}[ht]\n\n \\KwData{The gradient penalty coefficient $\\lambda$, the number of epochs $n_{epochs}$, the number of critic iterations per generator iteration $n_{critic}$, the batch size $m$, Adam hyper-parameters $\\alpha, \\beta_1, \\beta_2$.}\n \\For{$e=1\\dots, n_{epochs}$}{\n  \\For{$t=1\\dots, n_{critic}$}{\n  \\For{$i=1\\dots, m$}{\n        Sample real data $(y, x)\\sim P_r$, latent variable $z \\sim p(z)$, a random number $\\epsilon\\sim U[0, 1]$\\;\n        $\\tilde{x}\\leftarrow G_{w_g}(z, y)$\\;\n        \n        $\\hat{x}\\leftarrow \\epsilon x +(1-\\epsilon)\\tilde{x}$\\;\n        $L^{(i)}\\leftarrow D_{w_c}(\\tilde{x}, y)-D_{w_c}(x, y)+\\lambda(\\parallel \\nabla_{\\hat{x}} D_{w_c}(\\hat{x},y)\\parallel_2-1)^2$\\; \n        }\n        {\n        $w_c\\leftarrow$ Adam $\\left(\\nabla_{w_c} \\tfrac{1}{m}\\sum_{i=1}^m L^{(i)}, w_c, \\alpha, \\beta_1, \\beta_2\\right)$\\;\n        }\n   }\n   {\n   Sample a batch of latent variables $\\{z^{(i)}\\}_{i=1}^m\\sim p(z)$\\ and a batch of random conditions $\\{y^{(i)}\\}_{i=1}^m\\sim p(y)$\\;\n   $\\theta\\leftarrow $ Adam $\\left(\\nabla_{w_g} \\tfrac{1}{m}\\sum_{i=1}^m -D_{w_c}(G_{w_g}(z^{(i)}, y^{(i)}), y^{(i)}), w_g, \\alpha, \\beta_1, \\beta_2 \\right)$\n   }\n   \n   {}\n }\n \\caption{Conditional WGAN with gradient penalty. Default values used for hyper-parameters: $\\lambda = 10$, $n_{critic} = 5$, $\\alpha = 0.0001$, $\\beta_1 = 0.5$, $\\beta_2 = 0.9$. Variable $x$ denotes the trajectories of length $H$ ($\\eta_{[1,H]}$), whereas variable $y$ denotes the condition, i.e., the initial setting of the system (pairs $(s_0, \\theta)$. }\n\\end{algorithm}\n\\section{Background}\\label{sec:background}\n\n\\subsection{Chemical Reaction Networks}\n \nConsider a system with $n$ species\nevolving according to a stochastic model defined as a Chemical Reaction Network. Under the well-stirred assumption, the time evolution can be modelled as a Continuous Time Markov\nChain (CTMC) on a discrete state space. \nThe vector $\\eta_t = (\\eta_{t,1},\\dots,\\eta_{t,n})\\in S\\subseteq\\mathbb{N}^n$ denotes the state vector at time $t$, where $\\eta_{t,i}$ is the number of individuals in species $i$ at time $t$. The dynamics is encoded by a set of $m$ reactions with parametric propensity functions that depends on the state of the system.\nDue to the memoryless property of CTMC, \nthe probability of finding the\nsystem in state $s$ at time $t$ given that it was in state $s_0$ at time $t_0$ can be expressed as a system of ODEs known as Chemical Master Equation (CME).\nSince in general the CME is a system with countably many differential equations, its analytic or numeric solution is almost always unfeasible. An alternative\ncomputational approach is to generate trajectories using stochastic algorithms\nfor simulation, like the well-known Gillespie\u2019s SSA~\\cite{gillespie1977exact} which produces statistically correct trajectories, i.e., sampled according to the stochastic process described by the CME.\n\n\\vspace{-0.2cm}\n\n\\subsection{Generative Adversarial Nets}\\label{sec:gan_intro}\n\nEvery dataset can be considered as a set of observations drawn from an unknown distribution $\\mathbb{P}_r$. Generative models aim at learning a model that mimics this unknown distribution as closely as possible, i.e., learn a distribution $\\mathbb{P}_{w_g}$ as similar as possible to $\\mathbb{P}_r$, in order to then get samples from it that are new but look as if they could have belonged to the original dataset.\nGenerative Adversarial Nets (GANs)~\\cite{goodfellow2014generative} are deep learning-based generative models, that, given a dataset, are capable of generating new random but plausible examples. \n\n\\paragraph{Wasserstein GAN.} In this work we consider the Wasserstein version of GAN (WGAN)~\\cite{arjovsky2017wasserstein,gulrajani2017improved} as it is known to be more stable and less sensitive to the choice of model architecture and hyperparameters compared to a traditional GAN.\nWGANs use the Wasserstein distance (also known as Earth-Mover's distance), rather than the Jensen Shannon divergence, to measure the difference between the model distribution $\\mathbb{P}_{w_g}$ and the target distribution $\\mathbb{P}_r$. Because of  Kantorovich-Rubinstein duality \\cite{villani2008optimal} such distance can be computed as the supremum over all the 1-Lipschitz functions $f : S \\rightarrow \\mathbb{R}$:\n\\begin{equation}\\label{eq:wassdist}\n\\small \nW(\\mathbb{P}_r,\\mathbb{P}_{w_g}) =  \\sup_{||f||_L\\le 1} \\left(\n\\mathbb{E}_{x\\sim\\mathbb{P}_{r}}[f(x)]-\\mathbb{E}_{x\\sim\\mathbb{P}_{w_g}}[f(x)]\n\\right).\n\\end{equation}\nWe approximate these functions $f$ with a neural net $C_{w_c}$ parametrized by weights $w_c$. \nTo enforce the Lipschitz constraint we follow \\cite{gulrajani2017improved} and introduce a penalty over the norm of the gradients. It is known that a differentiable function is 1-Lipchitz if and only if it has gradients with norm at most 1 everywhere. \nThe objective function, to be maximized w.r.t. $w_c$, becomes:\n\\begin{equation}\\label{eq:wassdist_gp}\n\\small \n\\mathcal{L}({w_c}, w_g) := \\mathbb{E}_{x\\sim\\mathbb{P}_{r}}[C_{w_c}(x)]-\\mathbb{E}_{x\\sim\\mathbb{P}_{w_g}}[C_{w_c}(x)]-\\lambda \\mathbb{E}_{\\hat{x}\\sim\\mathbb{P}_{\\hat{x}}}\n( \\lVert \\nabla_{\\hat{x}}C_{w_c} (\\hat{x})  \\lVert_2-1 )^2]  ,\n\\end{equation}\nwhere $\\lambda$ is the penalty coefficient and $\\mathbb{P}_{\\hat{x}}$ is defined by sampling uniformly along straight lines between pairs of points sampled from $\\mathbb{P}_r$ and $\\mathbb{P}_{w_g}$. This is actually a softer constraint that however performs well in practice~\\cite{gulrajani2017improved}.\nThe $C_{w_c}$ network is referred to as \\textit{critic} and it outputs different scores for real and fake samples, its objective function (Eq.~\\eqref{eq:wassdist_gp}) provide an estimate of the Wasserstein distance among the two distributions. On the other hand, the distribution $\\mathbb{P}_{w_g}$ is parametrized by $w_g$; we seek the parameters that make it as close as possible to $\\mathbb{P}_r$. To achieve this, we consider a random variable $Z$ with a fixed simple distribution $\\mathbb{P}_Z$ and pass it through a parametric function, the \\textit{generator}, $G_{w_g} : Z \\rightarrow S$ that\ngenerates samples following the distribution $\\mathbb{P}_{w_g}$.\nTherefore, the WGAN architecture consists of two deep neural nets, a generator that proposes a distribution and a critic that estimate the distance between the proposed and the real (unknown) distribution. Using WGAN brings several important advantages compared to traditional GAN: it avoids the mode collapse problem, which makes WGAN more suitable for capturing stochastic dynamics, it drastically reduces the problem of vanishing gradients and it also have an objective function that correlates with the quality of generated samples, making the results easier to interpret. \n\n\\paragraph{Conditional GAN.} Conditional Generative Adversarial Nets (cGAN)~\\cite{mirza2014conditional} are a type of GANs that involves the conditional generation of examples, i.e., the generator produces examples of a required type, e.g. examples that belong to a certain class, and thus they introduce control over the desired generated output. In our application, we want the generation of stochastic trajectories to be conditioned on some model parameters and on the initial state of the system.\n\nFurthermore, dealing with inputs that are trajectories, i.e. sequences of fixed length, requires the use of convolutional neural networks (CNNs)~\\cite{goodfellow2016deep} for both the generator and the critic. \nThe architecture used in this work is thus a conditional Wasserstein Convolutional GAN with gradient penalty, it is going to be referred to as cWCGAN-GP.\n\n\n\\section{Experimental Results}\\label{sec:experiments}\n\nIn this section we validate our GAN-based model abstraction procedure on the following case studies. More details  are provided in Appendix~\\ref{sec:casestudies}.\n\\begin{itemize}\n    \\item \\textbf{SIR Model (Absorbing state).} \nThe SIR epidemiological model describes the spread, in a population, of an infectious disease that grants immunity to those who recover from it. The population is divided in three mutually exclusive groups: susceptible (S), infected (I) and recovered (R). The possible reactions, given by the interaction of individuals are infection and recovery. An important feature is the presence of an absorbing states.\n    \\item \\textbf{Ergodic SIRS Model.} \n    A SIR model in which the population is not perfectly isolated, meaning there is always a chance of getting infected from some external individuals, and in which immunity is only temporary. As a consequence, this model has no absorbing state.\n    \\item \\textbf{Genetic Toggle Switch Model (Bistability).} \nThe toggle switch is a well-known bistable biological circuit consisting of two genes, $G_1$ and $G_2$, that mutually repress each other in the production of proteins $P_1$ and $P_2$ respectively. The system displays two stable equilibria.\n\n    \\item \\textbf{Oscillator Model.} \nThe\ncircuit consists of three species A, B and C and three cyclic reactions: A converts B to itself, B converts C to itself, and C converts A to itself.\nThe concentrations of the three species oscillates in time.\n\n\\item \\textbf{MAPK Model.} The mitogen-activated protein kinase cascade models the amplification of an output signal ($\\it MAKP\\_PP$) thorough a multi-level cascade with negative feedback which is ultra-sensitive to an input stimulus ($V_1$).\nThe output signal shows either stable or oscillating behaviour, depending on the input signal.\n\n\\end{itemize}\n\n\nIn order to evaluate the performance of our abstraction procedure we consider two important measures: the accuracy of the abstract model, evaluated for each species at each time step of the time grid, and the computational gain compared to SSA simulation time. \n\n\\paragraph{Experimental Settings.} \nThe workflow can be divided in steps: (1) define a CRN model, (2) generate the synthetic datasets via SSA simulation, (3) learn the abstract model by training the cWCGAN-GP and, finally, (4) evaluate such abstraction. All the steps have been implemented in Python. \nIn particular, CRN models are defined in the \\texttt{.psc} format, CRN trajectories are simulated using Stochpy~\\cite{maarleveld2013stochpy} (stochastic modeling in Python) and PyTorch~\\cite{paszke2017automatic} is used to craft the desired architecture for the cWCGAN-GP and to evaluate the latter on the test data. All the experiments were performed on a \nIntel Xeon Gold 6140\nwith 24 cores and a 128GB RAM.\nThe source code for all the experiments can be found at the following link: \\url{https://github.com/francescacairoli/WGAN_ModelAbstraction}.\n \n\n\\paragraph{Datasets.}\nFor each case study with fixed parameters, the training set consists of $20$K different SSA trajectories. In particular, $N_{train} =2$K and $k_{train}= 10$. \nThe test set, instead, consists of $25$ new initial settings \nand from each of these we simulate $2$K trajectories, so to obtain an empirical approximation of the distribution targeted by model abstraction.\nWhen a parameter is allowed to vary, the training set consists of $50$K SSA trajectories ($N_{train} =1$K and $k_{train}= 50$). \nWe manually choose $H$ and $\\Delta t$ so that the system is close to steady state at time $H\\cdot \\Delta t$, without spending there too many steps. The time interval should be small enough to capture the full transient behavior of the system. For systems with no steady state, such as the oscillating models, we choose $H$ and $\\Delta t$ so to observe a full period of oscillation. The chosen values are the following: SIR: $\\Delta t = 0.5$, $H = 16$; e-SIRS: $\\Delta t = 0.1$, $H = 32$; Toggle Switch: $\\Delta t = 0.1$, $H = 32$; Oscillator: $\\Delta t = 1$, $H = 32$; MAPK: $\\Delta t = 60$, $H = 32$.\n\n\\paragraph{Data Preparation.} \nData have been scaled to the interval $[-1,1]$ to enhance the performance of the two CNNs and to avoid sensitivity to different scales in species counts. During the evaluation phase, the trajectories have been scaled back. Hence, results and errors are shown in the original scale.\n\n\\vspace{-0.2cm}\n\n\\subsection{cWCGAN-GP architecture}\n\nThe same architecture and the same set of hyper-parameters works well for all the analyzed case studies, showing great stability and usability of the proposed solution.\nThe Wasserstein formulation of GANs, with gradient penalty\n, strongly contributes to such stability. Traditional GANs have been tested as well, but they do not have such strength.\nThe details of the archictecture follows the best practice suggestions provided in~\\cite{gulrajani2017improved}. \nThe critic network has two hidden one-dimensional convolutional layers, with $n+m$ channels, each containing $64$ filters of size $4$ and stride $2$. We use a leaky-ReLU activation function with slope $0.2$, we do layer normalization and at each layer we introduce a dropout with probability $0.2$. An additional dense layer, with linear activation function, is used to connect the single output node, that contains the critic value. In order to enforce the Lipschitz constraint on the critic\u2019s model we add a gradient penalty term, as described in Section~\\ref{sec:gan_intro}.\nOn the other hand, the generator network takes as input the noise and the initial settings and it embeds the inputs in a larger space with $N_{ch}$ channels ($512$ in our experiments) through a dense layer. Four one-dimensional convolutional transpose layers are then inserted, containing respectively $128$, $256$, $256$ and $128$ filters of size $4$ with stride $2$. Here we do batch normalization and use a leaky-ReLU activation function with slope $0.2$.\nFinally, a traditional convolutional layer is introduced to reduce the number of output channels to $n$. \nThe Adam algorithm~\\cite{bengio2015rmsprop} is used to optimize loss function of both the critic and the generator. The learning rate is set to $0.0001$ and $\\beta= \\{0.5,0.9\\}$. The above settings are shared by all the case studies, the only exception is the more complex MAPK model for which a deeper cWCGAN-GP architecture is selected: a critic with five layers, each containing $256$ filters of size $4$ and stride $2$, and a generator with five layers, containing respectively $128$, $256$, $512$, $256$ and $128$ filters of size $4$ with stride $2$.\n\nTraining times depend on the dimension of the dataset, on the size of mini-batches, on the number of species, and on the architecture of the cWCGAN-GP. The latter has been kept constant for all the case studies. Batches of $256$ samples have been used and the number of epochs varies from $200$ to $500$ depending on the complexity of the model. Moreover, each training iteration of the generator correspond to $5$ iterations of the critic, to balance the power of the two player. The average time required for each training epoch is around one minute. \nTherefore, training the cWCGAN-GP model for $500$ epochs takes around $8$ hours leveraging the GPU.\n\n\\vspace{-0.2cm}\n\n\\subsection{Results}\n\n\\paragraph{Computational gain.}\nThe time needed to generate abstract trajectories does not depend on the complexity of the original system. Moreover, as the cWCGAN-GP architecture is shared by all the case studies, the computational time required to generate abstract trajectories is the same for all the case studies. In particular, considering a noise variable of size $480$, it takes around $1.75$ milliseconds (ms) to simulate a single trajectory. However, when generating batches of at least 200 trajectories the overhead reduces and the time to generate a single trajectory stabilizes around 0.8 ms. The same does not hold for the SSA trajectories, whose computational costs depends on the complexity of the model and on the chosen reaction rates. In the case studies considered the time required to simulate a single trajectory varies from $0.04$ to $0.22$ seconds, but it easily increases for more complex models or for smaller reaction rates, whereas the cost of abstract simulation stays constant.\nDetails about the computational gain for each model are presented in Table~\\ref{table:comp_times}. Computations are performed exclusively on a single CPU processor, to perform a fair comparison. However, the evaluation of cWCGAN-GP can be further sped up using GPUs, especially for large batches of trajectories, but this would have introduced a bias in their favour. It is important to stress how GPU parallelization is extremely straightforward in PyTorch and how the time to generate a single trajectory decrease to 1.9 $\\times 10^{-5}$ seconds when generating a batch of at least $2K$ trajectories (see last line of Table~\\ref{table:comp_times}).\n\nThe training phase introduces a fixed overhead that affects the overall computational gain. For instance, the training phase of the MAPK model takes around 8 hours, which is equivalent to the time needed to generate 140K SSA trajectories. It follows that, together with the trajectories needed to generate the training set, the cost of the training procedure is paid off when we simulate at least 200K trajectories. In a typical biological multi-scale scenario in which we seek to simulate the evolution in time of a tissue containing millions of cells, having additional internal pathways, the number of trajectories needed for the training phase becomes negligible and the training time is soon paid off.\n\n\\input{table}\n\n\\paragraph{Measures of performance.}\nResults are presented as follows. For each model, we present a small batch of trajectories, both real and abstract. From the plots of such trajectories we can appreciate if the abstract trajectories are similar to real ones and if they capture the most important macroscopic behaviors. We also show the histograms of empirical distributions at time $t_H$ for each species to quantify the behavior over all the $2$K trajectories present in the test set (see Fig.~\\ref{fig:sir_trajectories}-~\\ref{fig:mapk_trajectories}). Additional plots are shown in Appendix~\\ref{sec:plots} (Fig.~\\ref{fig:sir_trajectories_extra}-~\\ref{fig:clock_trajectories_extra}). \n\n\n\n\\begin{figure}[ht]\n    \\centering\n    \\subfigure[eSIRS]{   \\includegraphics[scale=0.235]{imgs/eSIRS/eSIRS_avg_wass_distance_1000epochs_32steps.png}}\n        \\hfill\n    \\subfigure[eSIRS-1P]{  \n    \\includegraphics[scale=0.235]{imgs/eSIRS_1P/Scaled_avg_wass_distance_1epochs_32steps.png}}\n        \\hfill\n    \\subfigure[SIR]{  \n    \\includegraphics[scale=0.235]{imgs/SIR/Scaled_avg_wass_distance_500epochs_16steps.png}}\n    \n    \\vspace{-0.25cm}\n    \n    \\subfigure[Oscillator]{  \n    \\includegraphics[scale=0.235]{imgs/Oscillator/Oscillator_avg_wass_distance_1000epochs_32steps.png}}\n        \\hfill\n    \\subfigure[Toggle Switch]{  \n    \\includegraphics[scale=0.235]{imgs/TS/ToggleSwitch_avg_wass_distance_2000epochs_32steps.png}}\n        \\hfill\n    \\subfigure[MAPK]{  \n    \\includegraphics[scale=0.235]{imgs/MAPK/MAPK_avg_wass_distance_1500epochs_32steps_scaling.png}}\n\n\\vspace{-1\\baselineskip}\n\n\\caption{Plots of the error over time for each model and each species. Errors are computed using the Wasserstein distance over the entire test set. Generated trajectories have been keep scaled to the interval $[-1,1]$ so that the scale of the system does not affect the scale of the error measure.\\vspace{-0.5cm}}\\label{fig:wass_errors}\n\\end{figure}\n\n\\paragraph{Measuring error propagation.}\nThe reconstruction accuracy of the proposed abstraction procedure is performed on test sets consisting of $25$ different initial settings. For each of these points $2$K SSA trajectories represent the empirical approximation of the true distribution over $S^H$. \nFrom each of these initial settings we also simulate $2$K abstract trajectories. Given a species $i\\in\\{1,\\dots n\\}$ and a time step $j\\in\\{1,\\dots H\\}$, we have the real one-dimensional distribution $\\eta_{i,j}$ and the generated abstract distribution $\\hat{\\eta}_{i,j}$, where $\\eta_{i,j}$ denotes the counts of species $i$ at time $t_j$ in a trajectory $\\eta_{[1,H]}$. In order to quantify the reconstruction error, we compute five quantities: the Wasserstein distance among the two one-dimensional distributions, the absolute and relative difference among the two means and the absolute and relative difference among the two variances. By doing so, we are capable of seeing whether the error propagates in time and whether some species are harder to reconstruct than others.\nThe error plots for the Wasserstein distance\nare shown in Figure~\\ref{fig:wass_errors}. Plots of means and variances distances are provided in Appendix~\\ref{sec:plots} (Fig.~\\ref{fig:avg_means_errors}-~\\ref{fig:avg_vars_rel_errors}). In addition, for two-dimensional models, i.e. eSIRS, Toggle Switch and MAPK, we show the landscapes of these five measures of the reconstruction error at three different time steps: step $t_1$, step $t_{H/2}$ and step $t_H$ (Fig.~\\ref{fig:eSIRS_distance_landscapes}-\\ref{fig:mapk_distance_landscapes} in Appendix~\\ref{sec:plots}).  We observe that, in all the models, each species seems to contribute equally to the global error and, in general, the error stays constant w.r.t. time, i.e., it does not propagate. This was a major concern in previous methods, based on the abstraction of transition kernels. \nIn fact, in order to simulate a trajectory of length $H$ the abstract kernel has to be applied iteratively $H$ times. As a consequence, this results in a propagation of the error introduced in the approximation of the transition kernel.\n\n\n\\begin{figure}[ht]\n    \\centering\n    \n    \\includegraphics[scale=0.25]{imgs/SIR/SIR_Trajectories14.png}\n    \\includegraphics[scale=0.25]{imgs/SIR/SIR_hist_comparison_last_timestep_14.png}\n    \n    \\vspace{-1\\baselineskip}\n    \n    \\caption{SIR\n    model: \\textbf{(left)} comparison of trajectories generated with a cWCGAN-GP (orange) and the trajectories generated with the SSA algorithm (blue); \\textbf{(right)} comparison of the real and generated histogram at the last timestep. Performance on a randomly chosen test point represented by three trajectories: the top one (species S), the central one (species I) and the bottom one (species R).\\vspace{-0.5cm}}\n    \\label{fig:sir_trajectories}\n\\end{figure}\n\n\n\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[scale=0.25]{imgs/eSIRS_1P/eSIRS_1P_Trajectories0.png}\n    \\includegraphics[scale=0.25]{imgs/eSIRS_1P/eSIRS_1P_hist_comparison_last_timestep_0.png}\n    \\includegraphics[scale=0.25]{imgs/eSIRS_1P/eSIRS_1P_Trajectories1.png}\n    \\includegraphics[scale=0.25]{imgs/eSIRS_1P/eSIRS_1P_hist_comparison_last_timestep_1.png}\n    \n    \\vspace{-1\\baselineskip}\n    \n    \\caption{e-SIRS model with one varying parameter: \\textbf{(left)} comparison of trajectories generated with a cWCGAN-GP (orange) and the trajectories generated with the SSA algorithm (blue); \\textbf{(right)} comparison of the real and generated histogram at the last timestep. \n    \\label{fig:esir_trajectories_one_par}\n   \n    \n\n   \n    \\includegraphics[scale=0.25]{imgs/TS/ToggleSwitch_Rescaled_Trajectories0_tight.png}\n    \\includegraphics[scale = 0.25]{imgs/TS/ToggleSwitch_rescaled_hist_comparison_-1th_timestep_0_tight.png}\n    \n    \\vspace{-1\\baselineskip}\n    \n    \\caption{Toggle Switch model: \\textbf{(left)} comparison of trajectories generated with a cWCGAN-GP (orange) and the trajectories generated with the SSA algorithm (blue); \\textbf{(right)} comparison of the real and generated histogram at the last timestep. Performance for a randomly chosen test point represented by a pair of trajectories: the top one (species P1) and the bottom one (species P2).\\vspace{-0.5cm}}\n    \\label{fig:ts_trajectories}\n\\end{figure}\n\n\\begin{figure}[ht]\n   \\centering\n   \\includegraphics[scale=0.25]{imgs/Oscillator/Oscillator_Rescaled_Trajectories2.png}\n   \\includegraphics[scale=0.25]{imgs/Oscillator/Oscillator_rescaled_hist_comparison_-1th_timestep_2.png}\n   \n   \\vspace{-1\\baselineskip}\n   \n    \\caption{Oscilator model: \\textbf{(left)} comparison of trajectories generated with a cWCGAN-GP (orange) and the trajectories generated with the SSA algorithm (blue);\\textbf{(right)} comparison of the real and generated histogram at the last timestep. Performance on a randomly chosen test point represented by three trajectories: the top one (species A), the central one (species B) and the bottom one (species C). \\vspace{-0.5cm}\n   }\\label{fig:clock_trajectories}\n\\end{figure}\n\n\\begin{figure}[ht]\n   \\centering\n   \\includegraphics[scale=0.19]{imgs/MAPK/MAPK_Rescaled_Trajectories8.png}\n   \\includegraphics[scale=0.19]{imgs/MAPK/MAPK_Rescaled_Trajectories15.png}\n   \\includegraphics[scale=0.19]{imgs/MAPK/MAPK_Rescaled_Trajectories22.png}\n   \\includegraphics[scale=0.19]{imgs/MAPK/MAPK_rescaled_hist_comparison_-1th_timestep_8.png}\n   \\includegraphics[scale=0.19]{imgs/MAPK/MAPK_rescaled_hist_comparison_-1th_timestep_15.png}\n   \\includegraphics[scale=0.19]{imgs/MAPK/MAPK_rescaled_hist_comparison_-1th_timestep_22.png}\n   \n   \\vspace{-1\\baselineskip}\n   \n    \\caption{MAPK model: \\textbf{(top)} comparison of trajectories generated with a cWCGAN-GP (orange) and the trajectories generated with the SSA algorithm (blue);\\textbf{(bottom)} comparison of the real and generated histogram at the last timestep. Performance on three, randomly chosen, test points. Each point is represented by the ouput species MAPK\\_PP. \\vspace{-0.5cm}\n   }\\label{fig:mapk_trajectories}\n\\end{figure}\n\n\\textbf{SIR.} The results for the SIR model are presented in Fig.~\\ref{fig:sir_trajectories} and Fig.~\\ref{fig:sir_trajectories_extra} (Appendix~\\ref{sec:plots}), which shows the performance on two, randomly chosen, test points. Each point is represented by three trajectories, the top one is for species S, the central one is for species I and the bottom one is for species R. The population size, given by $S+I+R$, is variable. The abstraction was trained on a dataset with fixed parameters, $\\theta = \\{3,1\\}$. Likewise, in the test set only the initial states are allowed to vary.\nWe observe that our abstraction method is able to capture the absorbing nature of SIR trajectories. It is indeed very important that once state $I=0$ or state $R=N$ are reached, the system should not escape from it. Abstract trajectories satisfy such property without requiring the imposition of any additional constraint.\nThe empirical distributions, real and generated, at time $t_H$ are almost indistinguishable.\n\n\\textbf{e-SIRS.} The e-SIRS model represents our baseline. We train two abstractions: in the first case the model is trained on a dataset with fixed parameters, $\\theta=\\{2.36, 1.67,  0.9, 0.64\\}$, and in the second case we let parameter $\\theta_1$ vary as well. Results are very accurate in both scenarios\nIn the fixed-parameters case, Fig.~\\ref{fig:esir_trajectories} (Appendix~\\ref{sec:plots}), the results are shown for two, randomly chosen, initial states. In the second case, Fig.~\\ref{fig:esir_trajectories_one_par}, the results are shown on two, randomly chosen, pairs $(s_0,\\theta_1)$. Each point is represented by a pair of trajectories, the top one is for species S and the bottom one is for species I. We performed a further analysis on the generalization capabilities of the abstraction learned on the dataset with one varying parameter, using larger test sets and computing mean and standard deviation of the distribution of Wasserstein distances over such sets. The mean stays around $0.04$ with a tight standard deviation ranges from $0.01$ to $0.05$, showing little impact of the chosen conditional setting (see Fig.~\\ref{fig:esirs_1p_analysis} in Appendix~\\ref{sec:plots}).\n\n\\textbf{Toggle Switch.} The results for the Toggle Switch model, on two, randomly chosen, test points, are shown in Fig.~\\ref{fig:ts_trajectories}) and Fig.~\\ref{fig:ts_trajectories_extra} (Appendix~\\ref{sec:plots}). The abstraction was trained on a dataset with fixed symmetric parameters ($kp_i=1,kb_i=1,ku_i=1, kd_i=0.01$ for $i = 1,2$). Likewise, in the test set only the initial states are allowed to vary. In this model, we tried to abstract only trajectories of the proteins $P1$ and $P2$, which are typically the observable species, ignoring the state of the genes. By doing so, we reduce the dimensionality of the problem but we also lose some information about the full state of the system. Nonetheless, the cWCGAN-GP abstraction is capable of capturing the bistable behaviour of such trajectories. In Fig.~\\ref{fig:ts_trajectories}, each point is represented by two trajectories, the top one is for species $P1$, whereas the bottom one is for species $P2$. \n\n\n\\textbf{Oscillator.} The results for the Oscillator model, on two, randomly chosen, test points, are shown in Fig.~\\ref{fig:clock_trajectories} and Fig.~\\ref{fig:clock_trajectories_extra} (Appendix~\\ref{sec:plots}). The abstraction was trained on a dataset with fixed parameter ($\\theta = 1$). Likewise, in the test set only the initial states are allowed to vary. Each point is represented by three trajectories, the top one is for species $A$, the central one is for species $B$ and the bottom one is for species $C$. The abstract trajectories well capture the oscillating behaviour of the system. \n\n\\textbf{MAPK.} The results for the MAPK model, on three, randomly chosen, test points, are shown in Fig.~\\ref{fig:mapk_trajectories}. The abstraction was trained on a dataset considering only a varying $V_1$ parameter and the dynamics of species $MAPK\\_PP$. \nThis case study represents a complex scenario in which the abstract distribution should capture the marginalization over the other seven unobserved variables. Moreover, the emergent behaviour of the only observed variable, $MAPK\\_PP$, is strongly influenced by the input parameter $V_1$ and further  amplified by the multi-scale nature of the cascade: for some values of $V_1$ the system oscillates, whereas for others it stabilizes around an equilibrium. Results show that our abstraction technique is flexible enough to capture such sensitivity.\n\n\\vspace{-0.2cm}\n\n\\subsection{Discussion}\nPrevious approaches to model abstraction, see Related work in Section~\\ref{sec:introduction}, focus on approximating the transition kernel, meaning the distribution of possible next states after a time $\\Delta t$, rather than learning the distribution of full trajectories of length $H$. The main reason for such choice is the limited scalability of the tool used for learning the abstraction. In fact, learning a distribution over $S^H\\subseteq\\mathbb{N}^{H\\times n}$ with a Mixture Density Network is unfeasible even for small $H$. Moreover, in learning to approximate the transition kernel one must split the SSA trajectories of the dataset in pairs of subsequent states. By doing so, a lot of information about the temporal correlation among states is lost. Having a tool strong and stable enough to learn distributions over $S^H$ allows us to preserve this information and make abstraction possible even for systems with a complex dynamics, which the abstraction of the transition kernel was failing to capture. For instance, we are now able to abstract the transient behaviour of multi-stable or oscillating systems. When attempting to abstract the transition kernel, either via MDN or via c-GAN, for such complex systems, we did not succeed in learning\nmeaningful solutions.\nA collateral advantage in generating full trajectories, rather than single subsequent states, is that it introduces an additional computational speed-up in the time required to generate a large pool of trajectories of length $H$. For instance, if a cWGAN is used to approximate the transition kernel, it takes around $31$ seconds to simulate the $50$K trajectories of length $32$ present in the test set. Our trajectory-based method takes only $3.4$ seconds to generate the same number of trajectories. Furthermore, our cWCGAN-GP was trained with relative small datasets, which leaves room for further improvements where needed.\nAn additional strength of our method is that one can train the abstract model only on species that are observable, reducing the complexity of the CRN model while preserving an accurate reconstruction for the species of interest. Once again, this was not possible with transition kernels and it may be extremely useful in real world applications.\n\nIn general, the cWCGAN-GP approximation does not provide any statistical guarantee about the reconstruction error. In addition, the set of observations used to learn the abstraction is rather small, typically $10$ samples for each initial setting. Therefore, it is not surprising that the real and the abstract distributions are not indistinguishable from a statistical point of view, as shown in Appendix~\\ref{sec:stat_test}.\nHowever, the abstract model is actually capable of capturing, from the little amount of information provided, the emergent features of the behaviour of the original system, such as multimodality or oscillations. \nIn this regard, formal languages can be used to formalize and check such qualitative properties. In particular, we can check whether the satisfaction probability (of non rare events) is similar in real and abstract trajectories. Examples are shown in Appendix~\\ref{sec:satisf}. Furthermore, such quantification of qualitative properties can be used to measure how good the reconstruction is. As future work, we intend to use it as query strategy for an active learning approach, so that the obtained abstract model is driven in the desired direction.\n\n\n\n\\section{Introduction}\\label{sec:introduction}\n\nA wide range of complex systems can be modeled as a network of chemical reactions. Stochastic simulation is typically the only feasible analysis approach that scales in a computationally tractable manner with the increase in system size, as it avoids the explicit construction of the state space. The well known Gillespie Stochastic Simulation Algorithm~\\cite{gillespie1977exact} is widely used for simulating models, as it  samples from the exact distribution over trajectories. This algorithm is  effective to simulate systems of moderate complexity, but it does not scale well to systems with many species and reactions, large populations, or internal stiffness. In these scenarios, a more effective choice is to rely on approximate simulation algorithms such as tau-leaping~\\cite{cao2006efficient} and hybrid simulation~\\cite{pahle2009biochemical}. \nNonetheless, when the  number of simulations required is extremely large and possibly costly, e.g. when one needs to simulate a large population of heterogeneous cells in a multi-scale model of a tissue or to simulate many heterogeneous individuals in an population ecology scenario,  all these methods become extremely computationally demanding, even for HPC facilities. \n\nA viable approach to address such problem is model abstraction, which aims at reducing the underlying complexity of the model, and thus reduce its simulation cost. However, building effective model abstractions is difficult, requiring a lot of ingenuity and man power. Here we advocate the strategy of learning an abstraction from simulation data. Our strategy is to frame model abstraction as a supervised learning problem, and learn an abstract probabilistic model using state of the art deep learning.\nThe probabilistic model should then be able to generate approximate trajectories efficiently and in constant time, i.e., independent on the complexity of the original system, thus sensibly reducing the simulation cost.\n\\paragraph{Related work.}\nThe idea of using machine learning as a model abstraction tool to approximate and simplify the dynamics of a Markov Population Process has received some attention in recent years. \nIn~\\cite{bortolussi2018deep} the authors use a Mixture Density Network (MDN)~\\cite{bishop2006pattern} to approximate the transition kernel of the stochastic process. In~\\cite{petrov2020automated} the authors extend the previous approach by introducing an automated search of the MDN architecture that better fit the data. In~\\cite{bortolussi2019bayesian} the authors present a Bayesian model abstraction technique, based on Dirichlet Processes, that allows  the quantification of the reconstruction uncertainty. In all cases, what is learned is an approximate transition kernel,  i.e., the probabilistic distribution of a single simulation step. \n\nIn this paper we address a more general and more complex problem. Instead of learning an approximate transition kernel, we learn the distribution of an entire trajectory of fixed length. This latter problem is not solvable with any of the previously adopted approaches, and its major goal is to keep abstraction error under control. In fact, training the abstract model on a full trajectory, rather than on pairs of subsequent states, allows the abstract model to retain and capture more information about the dynamics of the Markov process.\n\n\\paragraph{Contributions.}\nOur approach leverages Generative Adversarial Nets (GAN), which are one of the most strong and flexible techniques to learn probabilistic models. In fact, the GAN-based model abstraction technique is capable of learning a conditional distribution over the trajectory space, keeping into account the correlation, both spatial and temporal, among all the different species and conditioning both on initial states and model parameters. All the previous approaches focus on learning the distribution of the state of the system after a time $\\Delta t$, the so called \\textit{transition kernel}. However, such approaches perform poorly when the time interval is small and the dynamics is transient, showing a clear propagation of the error as the approximate kernel is applied iteratively to form a trajectory. \nFurthermore, producing a full trajectory reduces even more the computational cost of simulating a large pool of trajectories for different initial settings.\n\n\n\\paragraph{Paper structure.}\nThe paper is organized as follows: in Section~\\ref{sec:background} the relevant background notions are introduced, in Section~\\ref{sec:abstraction} we describe in detail the abstraction procedure, Section~\\ref{sec:experiments} presents the case studies and the experimental evaluation.\nConclusions are drawn in Section~\\ref{sec:conclusions}.\n\n\\section{Conclusions}\\label{sec:conclusions}\nIn the paper we presented a technique to abstract the simulation process of stochastic trajectories for various CRNs. The WGAN-based abstraction improves considerably the computational efficiency, which is no more related to the complexity of the underlying CRN. \nThis would be extremely helpful in all those applications in which a large number of simulations is required, i.e., applications whose solution is unfeasible via SSA simulation. It would enable the simulation of multi-scale models for very large populations, it would speed-up statistical model checking \\cite{younes2006statistical} and it can be used in particular cases of parameter estimation, for example when only few parameters have to be estimated multiple times.\nIn conclusion, the c-WCGAN-based solution to model abstraction perform well in scenarios that are very complex and challenging, requiring relatively little data and very little fine-tuning.\n\nAs future work, we plan to study how our abstraction technique works on real data. In this regard, we do not aim at capturing the underlying dynamical system, but we would rather be able to reproduce the trajectories observed in real applications. \nA great strength of our method, compared to state of the art solutions, is that it is able to generate trajectories only for a subset of the species present in the system domain, ignoring the information that is not observable, even during the training phase.\nAnother interesting extension is to adapt our technique to sample bridging trajectories, where both the initial and the terminal states are fixed. Typically, the simulation of such trajectories requires expensive Monte Carlo simulations, which makes clear the benefits of resorting to model abstraction.\n\n{\\footnotesize\t \\noindent\\textbf{Acknowledgements}\nThis work has been partially supported by \nthe Italian PRIN project ``SEDUCE'' n.\\ 2017TWRCNB.}\n\n\n\\bibliographystyle{splncs04}\n\n", "meta": {"timestamp": "2021-06-25T02:17:54", "yymm": "2106", "arxiv_id": "2106.12981", "language": "en", "url": "https://arxiv.org/abs/2106.12981"}}
{"text": "\\section{Introduction}\n\\label{sec:introduction}\nTwitter is a popular online social media platform which was released in 2006. Individuals can sign up for a Twitter account to view and publish content of their interests. As reported by Statista\\footnote{\\url{https://www.statista.com/}}, the number of daily active Twitter users in the United States is over 35 million in the second quarter of 2020\\footnote{\\url{https://www.statista.com/statistics/970911/monetizable-daily-active-twitter-users-in-the-united-states/}}. Twitter has become not only an essential social platform in people's daily life but also an information publishing venue. The open nature and widespread popularity of Twitter have made itself an ideal target of exploitation from automated programs, also known as bots. These bot accounts are often operated to achieve malicious goals. Bots have been actively involved in many important events, including the elections in the United States and Europe~\\cite{10.1145/3308560.3316486, DBLP:journals/corr/Ferrara17aa}. Bots are also responsible for spreading fake news and propagating extreme ideology~\\cite{berger2015isis}. These malicious bots try to hide their automated nature by imitating the behaviors of normal users. Across the whole Twittersphere, it is reported that bot accounts for 9\\% to 15\\% of total active users~\\cite{yardi2010detecting}. Since bots jeopardize user experience in Twitter and may even induce undesirable social effects, many research efforts have been devoted to Twitter bot detection. \n\nThe first work to detect automated accounts in social media dates back to 2010~\\cite{yardi2010detecting}. Early studies conducted feature engineering and adopted traditional classification algorithms. Three categories of features were considered: (1) user property features~\\cite{d2015real}; (2) features derived from tweets~\\cite{miller2014twitter}; and (3) features extracted from neighborhood information~\\cite{yang2013empirical}. Later, researchers began to propose neural network based bot detection frameworks. Wei \\textit{et al.} ~\\cite{wei2019twitter} adopted long short-term memory to extract semantics information from tweets. Kudugunta \\textit{et al.}~\\cite{kudugunta2018deep} proposed a method that combined feature engineering and neural network models. Heuristic methods for bot detection were also put forward recently. Minnich \\textit{et al.}~\\cite{minnich2017botwalk} proposed a bot detection method based on anomaly detection. Cresci \\textit{et al.}~\\cite{cresci2016dna} encoded tweets into a string to find out the difference between human and bots in tweeting behaviors.\n\nDespite early successes, ever-shifting social media brought two new challenges to the task of bot detection: generalization and adaptation. The challenge of generalization in social media bot detection demands bot detectors to simultaneously identify bots that attack in many different ways and exploit diversified features on Twitter. Cresci \\textit{et al.}~\\cite{cresci2017paradigm} points out that Twitter bots attack in different ways such as retweet frauds, malicious hashtag promotion and URL spamming. They also imitate the tweeting behaviour of different types of genuine users, fill out profile items differently and follow each other to boost their follower count. Since Twitter bots are indeed becoming more diversified, a robust Twitter bot detector should therefore address the challenge of generalization to induce real-world impact. However, previous bot detection methods fail to generalize since they only leverage limited user information and are trained on datasets with few types of bots.\n\nApart from that, the challenge of adaptation in bot detection demands bot detectors to maintain desirable performance in different times and catch up with rapid bot evolution. Cresci \\textit{et al.}~\\cite{10.1145/3409116}'s investigation shows that bots in the past used to be simple and easily identified, possessing too little profile and friend information to be genuine. However, more recently evolved bots have large numbers of friends and followers, use stolen profile pictures and intersperse malicious tweets with neutral ones. These newly evolved bots often evade existing detection measures, thus a robust bot detector should address the challenge of adaptation to put an end to the arms race between bot evolution and bot detection research. However, previous bot detection measures rely heavily on feature engineering and are not designed to adapt to emerging trends in bot evolution.\n\n\n\nIn light of the two challenges of Twitter bot detection, we propose a novel framework SATAR (\\textbf{S}elf-supervised \\textbf{A}pproach to \\textbf{T}witter \\textbf{A}ccount \\textbf{R}epresentation learning). SATAR adopts self-supervised learning to obtain user representation and identify bots on social media. Specifically, SATAR jointly encodes tweet, property and neighborhood information of users without feature engineering to promote bot detection generalization. SATAR follows a pre-training and fine-tuning learning schema to adapt to different generations of bots. Our main contributions are summarized as follows:\n\\begin{itemize} [topsep=4pt, leftmargin=*]\n    \\item We propose a novel framework SATAR to conduct generalizable and adaptable Twitter bot detection. SATAR is an end-to-end framework that jointly uses semantic, property and neighborhood information of users without feature engineering.\n    \\item To the best of our knowledge, this paper is the first work to introduce self-supervised representation learning to improve the performance of bot detection.\n   \n   \n    \\item We conduct extensive experiments on three real-world datasets to evaluate SATAR and competitive baselines. SATAR outperforms baselines on all three datasets and is proved to generalize and adapt through further exploration.\n\\end{itemize}\n\n\\noindent In the following, we first review related work in Section ~\\ref{sec:relatedwork} and define the task of Twitter bot detection in Section ~\\ref{sec:problemdefinition}. Next, we propose SATAR in Section ~\\ref{sec:SATAR}, following with extensive experiments in Section ~\\ref{sec:experiments}. Finally, we conclude the whole paper in Section ~\\ref{sec:conclusion}.\n\n\\begin{figure*}[h]\n  \\centering\n  \\includegraphics[width=.95\\linewidth]{70.png}\n  \\caption{Overview of our proposed self-supervised approach to Twitter account representation learning framework SATAR.}\n  \\Description{SATAR architecture in a nutshell}\n  \\label{fig:SATAR}\n\\end{figure*}\n\n\\section{Related Work}\n\\label{sec:relatedwork}\nIn this section, we briefly review the related literature on self-supervised learning and Twitter bot detection.\n\n\\subsection{Self-Supervised Learning}\n\nIn order to use unsupervised dataset in a supervised manner, self-supervised learning frames a special learning task, predicting a subset of entities' information using the rest.\nAs a promising learning paradigm, self-supervised learning has drawn massive attention for its fantastic data efficiency and generalization ability, with many state-of-the-art models following this paradigm \\cite{liu2020self}. Doersch \\textit{et al.}~\\cite{doersch2017multi} combined several self-supervised tasks to jointly train a network. \nZhai \\textit{et al.}~\\cite{zhai2019s4l} proposed that semi-supervised learning can benefit from self-supervised learning. \n\nSelf-supervised learning has been used in different domains, such as \nnatural language processing ~\\cite{devlin2018bert, zhang2019hibert}, \ncomputer vision ~\\cite{oord2016conditional, larsson2016learning} and graph analysis ~\\cite{grover2016node2vec, kipf2016variational}.\nIn natural language processing, self-supervised tasks are designed based on following words ~\\cite{radford2018improving} or the whole sentence ~\\cite{mikolov2013distributed}. Masked language models are also adopted to  better attend to the content in general ~\\cite{devlin2018bert}. In computer vision, adjacent pixels \\cite{oord2016conditional, van2016pixel} and the full image \\cite{dinh2014nice, dinh2016density} are used for pretext tasks similarly. In graph analysis, self-supervised tasks are designed based on edge attributes ~\\cite{dai2018adversarial, tang2015line} or node attributes ~\\cite{ding2018semi}.\n\n\\subsection{Twitter Bot Detection}\nTraditional bot detection methods mainly focused on extracting basic features from user information. Among them, Gao \\textit{et al.}~\\cite{gao2012towards} used text shingling and incremental clustering to merge spam messages into campaigns for real-time classification. \nLee \\textit{et al.} \\cite{lee2013warningbird} proposed to use the redirection of URLs in tweets and Thomas \\textit{et al.} \\cite{thomas2011design} focused on classification of mentioned websites . \nOther features are also adopted such as information on the user profile ~\\cite{lee2011seven}, social networks ~\\cite{minnich2017botwalk} and timeline of accounts ~\\cite{cresci2016dna}. Yang \\textit{et al.}~\\cite{yang2013empirical} designed several new features to counter the evolution of modern Twitter bots. Cresci \\textit{et al.}~\\cite{cresci2018reaction} proposed that confrontation between bot detectors and bot operators is a never-ending arms race. It is also argued that we should refrain from methods that rely on posterior observations.\n\nNeural networks are also adopted to detect Twitter bots because of their strong learning capability. \nWei \\textit{et al.}~\\cite{wei2019twitter} employed recurrent neural networks to efficiently capture features across tweets. Kudugunta \\textit{et al.}~\\cite{kudugunta2018deep} divided user features into account-level features, such as follower count, and tweet-level features, such as the number of hashtags. Both kinds of features and semantic information are used to set up an LSTM-based bot detection framework. Stanton \\textit{et al.}~\\cite{stanton2019gans} utilized generative adversarial network for spam detection to avoid annotation costs and inaccuracies. \nAlhosseini \\textit{et al.}~\\cite{ali2019detect} proposed a model based on graph convolutional networks for spam bot detection to leverage both node features and neighborhood information.\n\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Problem Definition}\n\\label{sec:problemdefinition}\nLet $U$ be a Twitter user, consisting of three aspects of user information. Let $T = \\{t_i\\}_{i=1}^{M}$ be a user's semantic information of $M$ tweets. Each tweet $t_i = \\{w_1^i, \\cdot \\cdot \\cdot, w_{Q_i}^i\\}$ contains $Q_i$ words. Let $P = \\{p_i\\}_{i=1}^{R}$ be a user's property information with a total of $R$ properties. Each property $p_i$ could be numerical such as follower count or categorical such as whether the user is verified. Let $N = \\{N^f, N^t\\}$, where $N^f = \\{N_1^f, \\cdot \\cdot \\cdot, N_u^f\\}$ are $u$ followings of the user and $N^t = \\{N_1^t, \\cdot \\cdot \\cdot, N_v^t\\}$ are $v$ followers. Similar to previous research~\\cite{yang2020scalable, kudugunta2018deep}, we treat Twitter bot detection as a binary classification problem, where each user could either be human ($y = 0$) or bot ($y = 1$). Formally, we can define the Twitter bot detection task as follows:\n\n\\hspace{2pt}\n\n\\begin{tcolorbox}[\n    standard jigsaw,\n    opacityback=0,\n    boxrule=0.5p\n]\n\\textbf{Problem: Twitter Bot Detection} Given a Twitter user $U$ and its information $T$, $P$ and $N$, learn a bot detection function $f:f(U(T,P,N)) \\rightarrow \\hat{y}$, such that $\\hat{y}$ approximates ground truth $y$ to maximize prediction accuracy.\n\\end{tcolorbox}\n\n\\section{SATAR methodology}\n\\label{sec:SATAR}\nIn this section, we present the details of the proposed Twitter user representation learning framework named as SATAR (\\textbf{S}elf-supervised \\textbf{A}pproach to \\textbf{T}witter \\textbf{A}ccount \\textbf{R}epresentation learning).\n\nIn Section~\\ref{subsec:SATARover}, we provide an overview of the proposed framework SATAR. In Section ~\\ref{subsec:SATARTSSN} - ~\\ref{subsec:SATARCIA}, we formally define the architecture of SATAR and details regarding its four major components. In Section ~\\ref{subsec:SATARSSLO}, we provide details about the self-supervised learning schema and present the overall SATAR training algorithm.\n\n\\subsection{Overview}\n\\label{subsec:SATARover}\nFigure ~\\ref{fig:SATAR} illustrates the proposed framework SATAR. It consists of four major components: (1) a tweet-semantic sub-network, (2) a profile-property sub-network, (3) a following-follower sub-network and (4) a Co-Influence aggregator.\n\nSpecifically, we use the Twitter API\\footnote{\\url{https://developer.twitter.com/en/products/twitter-api/early-access}} to obtain relevant data regarding a user's semantic, property and neighborhood information. The tweet-semantic sub-network encodes a Twitter user's textual information into $r_s$ with hierarchical RNNs of different depth accompanied by the attention mechanism. The profile-property sub-network encodes a Twitter user's profile properties into $r_p$ with property data encoding and fully connected layers. The following-follower sub-network encodes a Twitter user's neighborhood relationships into $r_n$ with neighborhood information extractor and fully connected layers. Finally, a non-linear Co-Influence aggregator takes the correlation between three aforementioned components into account, generating a representation vector that fully embodies the social status of a specific Twitter user. A softmax layer is then applied for user classification and enables model learning.\n\\vspace{-5pt}\n\n\\subsection{Tweet-Semantic Sub-Network}\n\\label{subsec:SATARTSSN}\nMost of the previous works about Twitter bot detection have utilized users' tweet content information. Firstly, hand-picked keywords and feature engineering are pervasive in bot detection endeavors. These approaches extracted information that is conceived as helpful to bot detection, such as URL count ~\\cite{AHMED20131120}, hashtag count~\\cite{yang2013empirical} and the frequency of spam words~\\cite{8508495}. Perceived as effective to begin with, these approaches are generally abandoned due to the inevitable bias introduced in the feature engineering process. With the advent of deep learning, techniques in natural language processing are adopted to capture the semantic information in a specific user's tweets, which shows promising results for bot detection. These efforts treat each tweet as a distinct entity, which is deemed independent from other tweets in evaluating a Twitter user. However, two characteristics of tweeting behaviors would weaken such an assumption of independence. Firstly, Twitter currently has a 280-character limit for tweets, which forces longer texts to become a thread while tweets in a thread often has a coherent meaning. Secondly, a specific user's tweets represent a sequential flow of the user's engagements on social media, but the temporal dependence of different tweets are not considered by existing works.\n\nIn this paper, we exploit user semantic information at two different levels, tweet-level and word-level, to capture the tweet content of users. Specifically, words in a user's tweets could be fitted into two hierarchical structures. For tweet-level characterization, as defined in Section~\\ref{sec:problemdefinition}, $w_i^j$ denotes the $i$-th word in the $j$-th tweet of the user timeline, and $t_j$ represents the $j$-th tweet of a specific user. We also concatenate temporally adjacent tweets: $\\{w_1, \\cdot \\cdot \\cdot, w_K\\} = \\{w_1^1,  \\cdot \\cdot \\cdot, w_{Q_1}^1, w_1^2, \\cdot \\cdot \\cdot, w^M_{Q_M}\\}$, where the total word count $K = \\sum_{i=1}^M Q_i$. Thus for word-level characterization, $w_k$ denotes the $k$-th word in the user's tweet history with temporally adjacent tweets concatenated to form a sequence. It is noteworthy that the underlying words are identical between tweet-level and word-level, but their annotations differ according to the user's tweeting behaviors. To jointly leverage user tweet information on these two different levels, we propose tweet-level and word-level encoders of hierarchical RNNs to model tweet text sequences respectively and derive an overall semantic representation for Twitter users.\n\n\n\n\n\n\n\n\n\\noindent \\textbf{Tweet-Level Encoder.}\nThe tweet-level encoder follows a bottom-up approach. For the $j$-th tweet of a specific user, we first embed words in it with an embedding layer:\n\\vspace{-2pt}\n\\begin{equation}\n  \\label{sbegin}\n  x_i^j = emb(w_i^j), 1 \\leqslant i \\leqslant Q_j, 1 \\leqslant j \\leqslant M,\n\\end{equation}\n\\vspace{-2pt}\n\\noindent where $Q_j$ is the length of the $j$-th tweet, and we use Word2Vec~\\cite{mikolov2013distributed} as the embedding layer $emb(\\cdot)$. To encode the tweet, a bidirectional RNN processes the tweet in a forward pass and a backward pass. For the forward pass, a sequence of forward hidden states is generated for the $j$-th tweet:\n\\begin{equation}\n    \\overrightarrow{h}^t_j = \\bigg[\\overrightarrow{h}^t_{j,1}, \\overrightarrow{h}^t_{j,2}, \\cdot \\cdot \\cdot, \\overrightarrow{h}^t_{j,Q_j}\\bigg],\n\\end{equation}\n\n\\noindent where the hidden representation for each step is generated by\n\\vspace{-2pt}\n\\begin{equation}\n    \\overrightarrow{h}^t_{j,i} = RNN\\bigg(\\overrightarrow{h}^t_{j,i-1}, x_i^j\\bigg).\n\\end{equation}\n\\vspace{-2pt}\n\nHere we use LSTM~\\cite{lstm} as $RNN(\\cdot)$, which is widely adopted to model long-term dependencies in a sequence. For the backward pass, a sequence of backward hidden states is generated similarly:\n\n\\begin{equation}\n    \\overleftarrow{h}^t_j = \\bigg[\\overleftarrow{h}^t_{j,1}, \\overleftarrow{h}^t_{j,2}, \\cdot \\cdot \\cdot, \\overleftarrow{h}^t_{j,Q_j}\\bigg].\n\\end{equation}\n\nWe concatenate the forward and backward results to form a sequence of word representations in the $j$-th tweet:\n\\begin{equation}\n    h^t_j = \\bigg[ h^t_{j,1}, h^t_{j,2},\\cdot \\cdot \\cdot, h^t_{j,Q_j} \\bigg],\n\\end{equation}\n\n\\noindent where $h^t_{j,i}=\\bigg[ \\overrightarrow{h}^t_{j,i}; \\overleftarrow{h}^t_{j,i} \\bigg]$. Since words in a tweet vary in their contribution to the tweet's overall semantic meaning, the attention mechanism is adopted to aggregate word hidden representations into a tweet vector. Specifically,\n\\begin{equation}\n    \\alpha^t_{j,i} = \\frac{exp(u^t_{j,i} \\cdot v_l^t)}{\\sum_{i'}exp(u^t_{j,i'} \\cdot v_l^t)},\n\\end{equation}\n\n\\noindent where $u^t_{j,i} = tanh(W_l^t h^t_{j,i}+b_l^t)$ transforms vectors for each word and $v_l^t$, $W_l^t$ and $b_l^t$ are learnable parameters. $\\alpha^t_{j,i}$ represents the weight of the $i$-th word in the $j$-th tweet. Finally, the representation of the $j$-th tweet can be obtained as follows:\n\\vspace{-2pt}\n\n\\begin{equation}\n    v^t_j = \\sum_i \\alpha^t_{j,i}h^t_{j,i}.\n\\end{equation}\n\nAfter deriving a vector for each tweet, the tweet-level encoder applies RNN similarly to tweet representations $\\{v^t_j\\}_{j=1}^M$, generating a forward and a backward sequence. We concatenate the forward and backward results to form a sequence of tweet representations:\n\\vspace{-2pt}\n\n\n\n\n\\begin{equation}\n    h^t = \\bigg[ h^t_1, h^t_2, \\cdot \\cdot \\cdot, h^t_M \\bigg],\n\\end{equation}\n\n\\noindent where $h^t_i = \\bigg[\\overrightarrow{h}^t_i;\\overleftarrow{h}^t_i\\bigg]$. An attention layer is applied to model the influence each tweet has on the overall semantics of the user:\n\\begin{equation}\n    \\alpha^t_i = \\frac{exp(u^t_i \\cdot v^t_h)}{\\sum_{i'}exp(u^t_{i'} \\cdot v^t_h)},\n\\end{equation}\n\n\\noindent where $u^t_i = tanh(W^t_h h^t_j + b^t_h)$ transforms vectors for each tweet and $v^t_h$, $W^t_h$ and $b^t_h$ are learnable parameters. $\\alpha^t_i$ represents the weight of the $i$-th tweet. Finally, the representation of a user's tweet semantics from a tweet-oriented perspective can be obtained as follows:\n\\vspace{-2pt}\n\n\\begin{equation}\n    r^t_s = \\sum_i \\alpha^t_i h^t_i.\n\\end{equation}\n\n\n\\noindent \\textbf{Word-Level Encoder.}\nThe word-level encoder concatenates temporally adjacent tweets into a long sequence of words. For the $i$-th word of the sequence, we first embed it with the embedding layer identical to the tweet-level encoder:\n\\begin{equation}\n    x_i = emb(w_i), 1 \\leqslant i \\leqslant K,\n\\end{equation}\n\n\\noindent where $K$ is the total word count in the temporally concatenated tweets. A bidirectional RNN with attention is adopted to encode the concatenated sequence. For the forward pass, we have: \n\\begin{equation}\n    \\overrightarrow{h}^w = \\bigg[ \\overrightarrow{h}^w_1,\\overrightarrow{h}^w_2,\\cdot \\cdot \\cdot, \\overrightarrow{h}^w_K \\bigg],\n\\end{equation}\n\n\\noindent where $\\overrightarrow{h}^w_i = RNN(\\overrightarrow{h}^w_{i-1}, x_i)$ and LSTM is adopted for $RNN(\\cdot)$ regarding its particular length. For the backward pass, we have:\n\\begin{equation}\n    \\overleftarrow{h}^w = \\bigg[ \\overleftarrow{h}^w_1,\\overleftarrow{h}^w_2,\\cdot \\cdot \\cdot, \\overleftarrow{h}^w_K \\bigg],\n\\end{equation}\n\n\\noindent where $\\overleftarrow{h}^w_i = RNN(\\overleftarrow{h}^w_{i+1}, x_i)$. Then we concatenate the forward and backward results to form a sequence of word representations in the user's tweet history:\n\\begin{equation}\n    h^w = \\bigg[h^w_1,h^w_2,\\cdot \\cdot \\cdot, h^w_K \\bigg],\n\\end{equation}\n\n\\noindent where $h^w_i = \\bigg[\\overrightarrow{h}^w_i; \\overleftarrow{h}^w_i \\bigg]$. Then the attention mechanism is applied:\n\\begin{equation}\n    \\alpha^w_i = \\frac{exp(u^w_i \\cdot v^w)}{\\sum_{i'}exp(u^w_{i'} \\cdot v^w)},\n\\end{equation}\n\\noindent where $u^w_i = tanh(W^w h^w_i + b^w)$, $v^w$, $W^w$ and $b^w$ are learnable parameters, $\\alpha^w_i$ represents the weight of the $i$-th word in the concatenated sequence. Finally, the representation of a user's tweet semantics from a word-oriented perspective is as follows:\n\\begin{equation}\n    r_s^w = \\sum_i \\alpha^w_i h^w_i.\n\\end{equation}\n\n\\vspace{-2pt}\n\\noindent \\textbf{Overall Semantic Representation.}\nThe tweet-semantic sub-network produces an overall representation $r_s$ based on the two encoders:\n\\begin{equation}\n    \\label{send}\n    r_s = concatenation(r^t_s; r^w_s).\n\\end{equation}\n\n\n\\begin{algorithm}[!t]\n    \\caption{SATAR Learning Algorithm}\n    \\label{alg:SATAR}\n    \\SetAlgoLined\n    \\KwIn{Twitter user dataset $TU$, each user $u \\in TU$ has tweets $T$, properties $P$ and neighbors $N$}\n    \\KwOut{SATAR-optimized parameters $\\theta$}\n    Initialize $\\theta$; \\\\\n    \\For{each user $u \\in TU$}\n    {\n    Initialize $r_n(u)$; \\\\\n    $u.y \\leftarrow$ self-supervised label assignment according to user $u$'s follower count; \\\\\n    }\n    \\While{$\\theta$ has not converged}\n    {\n    \\For{each user $u \\in TU$}\n    {\n    $r_s(u) \\leftarrow$ Equation (\\ref{sbegin} - \\ref{send}) with $u.T$; \\\\\n    $r_p(u) \\leftarrow$ Equation (\\ref{p}) with $u.P$; \\\\\n    $r(u) \\leftarrow$ Equation (\\ref{cobegin} - \\ref{coend}) with $r_s(u)$, $r_p(u)$ and $r_n(u)$; \\\\\n    $L_u \\leftarrow$ Equation (\\ref{lossbegin} - \\ref{equ:lossend}) with $r(u)$ and $u.y$; \\\\\n    }\n    $\\theta \\leftarrow$ BackPropagate($L_u$); \\\\\n    \\For{each user $u \\in TU$}\n    {\n    $r_n(u) \\leftarrow$ Equation (\\ref{nbegin} - \\ref{equ:nend}) with $u.N$;\n    }\n    }\n\\end{algorithm}\n\n\\vspace{-2pt}\n\\subsection{Profile-Property Sub-Network}\n\\label{subsec:SATARPPSN}\n\nTo avoid the undesirable bias incorporated in feature engineering, the profile-property sub-network utilizes profile properties that could be directly retrieved from the Twitter API. Different encoding strategies are adopted for different types of property data:\n\\begin{itemize}[leftmargin=*]\n    \\item There are 15 true-or-false property items in total. We use 1 for true and 0 for false. e.g. \u201cprofile uses background image\u201d.\n    \\item There are 5 numerical property items in total. We apply z-score normalization to numerical properties over the whole dataset. e.g. \u201cfavorites count\u201d.\n    \\item There is one special property item: \u201clocation\u201d. We divide locations geographically and apply one-hot encoding.\n\\end{itemize}\n\nIt is noteworthy that the follower count of a specific user would not be included in the property vector, which would be part of the self-supervised learning schema presented in Section ~\\ref{subsec:SATARSSLO}.\n\nThe encoded property items are concatenated to form a raw property vector $u_p$, which is then transformed to produce the Twitter user's property representation $r_p$:\n\\begin{equation}\n    \\label{p}\n    r_p = ReLU(FC_p(u_p)),\n\\end{equation}\n\n\\noindent where $FC_p(\\cdot)$ is a fully connected layer and $ReLU(\\cdot)$ is a nonlinearity adopted as the activation function.\n\n\\subsection{Following-Follower Sub-Network}\n\\label{subsec:SATARFFSN}\nFor user followings, according to Twitter mechanism, their tweets will appear in the timeline and the following behaviors often demonstrate interest in their tweet content. Thus we propose $u_n^{f}$ to model the following relationships:\n\\begin{equation}\n    \\label{nbegin}\n    u_n^{f} = \\frac{1}{\\sum_{u\\in N^f}TF(u)}\\sum_{u\\in N^f} TF(u) r_s(u),\n\\end{equation}\n\n\\noindent where $N^f$ denotes the following set of a Twitter user, $TF(u)$ denotes the tweet frequency of user $u$ and $r_s(u)$ is the semantic representation of user $u$ generated by the tweet-semantic sub-network. Tweet frequency $TF$ is approximated by a user's total tweet count divided by account active time, which is the time period between a user's registration and its last update.  Note that $\\frac{TF(u)}{\\sum_{u'\\in N^f} TF(u')}$ represents the proportion that user $u$ appears in one's timeline, thus $u_n^f$ serves as a weighted sum of followings' semantics information according to their relative tweeting frequency.\n\nFor followers, as the average quality of followers of an account defines its social status and the quality could be evaluated by its properties, we propose to model the follower relationships as follows:\n\\begin{equation}\n    u_n^t = \\frac{1}{|N^t|}\\sum_{u\\in N^t} r_p(u),\n\\end{equation}\n\n\\noindent where $N^t$ denotes the follower set of a Twitter user, $|\\cdot|$ denotes the cardinality of a set and $r_p(u)$ is the property representation of user $u$ generated by the profile-property sub-network.\n\nThe following-follower sub-network then produces a raw hidden vector for neighborhood information $u_n = concatenation(u_n^f; u_n^t)$. The intermediate vector is then transformed to produce the Twitter user's neighborhood representation $r_n$:\n\\begin{equation}\n    \\label{equ:nend}\n    r_n = ReLU(FC_n(u_n)),\n\\end{equation}\n\n\\noindent where $FC_n(\\cdot)$ is a fully connected layer and $ReLU(\\cdot)$ is the adopted activation function.\n\n\\subsection{Co-Influence Aggregator}\n\\label{subsec:SATARCIA}\nSo far, we have obtained the representation vectors regarding three and all three aspects of a Twitter user, namely $r_s$, $r_p$ and $r_n$ for tweet semantics, user property and follow relationships. A good bot detector should be comprehensive and robust to tamper. In other words, independently considering each aspect of user information would inevitably jeopardize the robustness of the bot detector. Co-attention has been a successful mechanism at handling correlation between two sequences, but it is not designed for mutual influence between multiple representation vectors. Thus we propose a Co-Influence aggregator to take the mutual correlation between tweet semantics, user property and follow relationships into consideration.\n\n    \n    \n\nFirstly, the affinity index between a pair of aspects is derived:\n\\begin{equation}\n\\label{cobegin}\n\\begin{aligned}\n    F_{sp} = tanh(r_s^T W_{sp} r_p),\\\\\n    F_{pn} = tanh(r_p^T W_{pn} r_n),\\\\\n    F_{ns} = tanh(r_n^T W_{ns} r_s),\n\\end{aligned}\n\\end{equation}\n\n\\noindent where $W_{sp}$, $W_{pn}$ and $W_{ns}$ are learnable parameters of the aggregator. A hidden representation for each aspect which incorporates relevant information from the other two aspects are derived:\n\\begin{equation}\n\\begin{aligned}\n    h^s = tanh(W_sr_s + F_{sp}(W_pr_p) + F_{ns}(W_nr_n)),\\\\\n    h^p = tanh(W_pr_p + F_{sp}(W_sr_s) + F_{pn}(W_nr_n)),\\\\\n    h^n = tanh(W_nr_n + F_{ns}(W_sr_s) + F_{pn}(W_pr_p)),\n\\end{aligned}\n\\end{equation}\n\n\\noindent where $W_s$, $W_p$ and $W_n$ are learnable parameters of the aggregator. Finally, the proposed framework SATAR produces the Twitter user representation $r$ as follows:\n\\begin{equation}\n    \\label{coend}\n    r =  tanh(W_V\\cdot concatenation(h^s;h^p;h^n)),\n\\end{equation}\n\n\\noindent where $W_V$ is a learnable parameter of the aggregator.\n\n\\begin{table}\n\\setlength{\\tabcolsep}{1pt}\n\\setlength{\\abovecaptionskip}{1pt}\n\\caption{Overview of three adopted bot detection datasets.}\n\\label{tab:dataset}\n\\begin{tabular}{c c c c c c c c}\n\\toprule\n\nDataset & \\tabincell{c}{User \\\\ Count} & \\tabincell{c}{Human \\\\ Count} & \\tabincell{c}{Bot \\\\ Count} & \\tabincell{c}{S \\\\ Info} & \\tabincell{c}{P \\\\ Info} & \\tabincell{c}{N \\\\ Info} & \\tabincell{c}{Release\\\\ Year} \\\\ \\midrule\n\n\\ \\ TwiBot-20\\ \\ & \\ \\ 229,573 \\ \\ & \\ \\ 5,237 \\ \\ & \\ \\ 6,589 \\ \\ &  \\ \\ \\checkmark \\ \\  & \\ \\  \\checkmark \\ \\  & \\ \\ \\checkmark \\ \\  & \\ \\ 2020 \\ \\  \\\\\n\nCresci-17 & 9,813 & 2,764 & 7,049 & \\checkmark & \\checkmark & & 2017 \\\\\n\nPAN-19 & 11,378 & 5,765 & 5,613 & \\checkmark & & & 2019 \\\\\n \n\\bottomrule\n\\end{tabular}\n\\vspace{-10pt}\n\\end{table}\n\n\n\\begin{table*}\n \\caption{Components of Twitter user information used by each bot detection method.}\n \\label{tab:SPN}\n \\begin{tabular}{c c c c c c c c c c c} \n \\toprule\n  &\n  \\tabincell{c}{Lee \\textit{et}\\\\ \\textit{al.}~\\cite{lee2011seven}} &\n  \\tabincell{c}{Yang\\\\ \\textit{et al.}~\\cite{yang2020scalable}} &\n  \\tabincell{c}{Kudugunta\\\\ \\textit{et al.}~\\cite{kudugunta2018deep}} &\n  \\tabincell{c}{Wei \\textit{et}\\\\ \\textit{al.}~\\cite{wei2019twitter}} &\n  \\tabincell{c}{Miller\\\\ \\textit{et al.}~\\cite{miller2014twitter}} &\n  \\tabincell{c}{Cresci\\\\ \\textit{et al.}~\\cite{cresci2016dna}} &\n  \\tabincell{c}{Botometer\\\\ ~\\cite{davis2016botornot}} &\n  \\tabincell{c}{Alhosseini\\\\ \\textit{et al.}~\\cite{ali2019detect}} &\n  $\\rm SATAR_{FC}$ &\n  $\\rm SATAR_{FT}$ \\\\\n  \\midrule\n   \n  $\\bf Semantic$ &\n  \\checkmark & \n  & \n  \\checkmark & \n  \\checkmark & \n  \\checkmark & \n  \\checkmark & \n  \\checkmark & \n  &\n  \\checkmark &\n  \\checkmark \\\\\n \n  $\\bf Property$ &\n  \\checkmark & \n  \\checkmark & \n  \\checkmark & \n  & \n  \\checkmark & \n  & \n  \\checkmark & \n  \\checkmark &\n  \\checkmark &\n  \\checkmark \\\\\n   \n  $\\bf Neighbor$ &\n  & \n  & \n  & \n  & \n  & \n  & \n  \\checkmark & \n  \\checkmark &\n  \\checkmark &\n  \\checkmark \\\\\n\n \n \\bottomrule\n\\end{tabular}\n\\end{table*}\n \n\\begin{table*}\n \\caption{Performance comparison for bot detection methods. \u201c/\u201d denotes insufficient user information to support the baseline.}\n \\label{tab:TwiBotMetric}\n \\begin{tabular}{c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c}\n \\toprule\n    \\multicolumn{2}{c}{} &\n    \\multicolumn{3}{c}{\\tabincell{c}{Lee \\textit{et}\\\\ \\textit{al.}~\\cite{lee2011seven}}} &\n    \\multicolumn{3}{c}{\\tabincell{c}{Yang\\\\ \\textit{et al.}~\\cite{yang2020scalable}}} &\n    \\multicolumn{3}{c}{\\tabincell{c}{Kudugunta\\\\ \\textit{et al.}~\\cite{kudugunta2018deep}}} &\n    \\multicolumn{3}{c}{\\tabincell{c}{Wei \\textit{et} \\\\\\textit{al.}~\\cite{wei2019twitter}}} &\n    \\multicolumn{3}{c}{\\tabincell{c}{Miller\\\\ \\textit{et al.}~\\cite{miller2014twitter}}} &\n    \\multicolumn{3}{c}{\\tabincell{c}{Cresci\\\\ \\textit{et al.}~\\cite{cresci2016dna}}} &\n    \\multicolumn{3}{c}{\\tabincell{c}{\\tabincell{c}{Botometer\\\\ ~\\cite{davis2016botornot}}}} &\n    \\multicolumn{3}{c}{\\tabincell{c}{Alhosseini\\\\ \\textit{et al.}~\\cite{ali2019detect}}} &\n    \\multicolumn{3}{c}{$\\rm SATAR_{FC}$} & \\multicolumn{3}{c}{$\\rm SATAR_{FT}$} \\\\\n   \n\n   \n     \n   \n \\midrule\n \n\n \n \n   \n\n \n\n \n \n \n \n \\multirow{3}{*}{\\textbf{TwiBot-20}} &\n Acc & \n \\multicolumn{3}{c}{0.7456} &\n \\multicolumn{3}{c}{0.8191} &\n \\multicolumn{3}{c}{0.8174} &\n \\multicolumn{3}{c}{0.7126} &\n \\multicolumn{3}{c}{0.4801} &\n \\multicolumn{3}{c}{0.4793} &\n \\multicolumn{3}{c}{0.5584} & \n \\multicolumn{3}{c}{0.6813} & \n \\multicolumn{3}{c}{0.7838} & \n \\multicolumn{3}{c}{\\bf 0.8412} \\\\\n \n \n  \n \n &\n F1&\n \\multicolumn{3}{c}{0.7823} &\n \\multicolumn{3}{c}{0.8546} &\n \\multicolumn{3}{c}{0.7517} &\n \\multicolumn{3}{c}{0.7533} &\n \\multicolumn{3}{c}{0.6266} &\n \\multicolumn{3}{c}{0.1072} &\n \\multicolumn{3}{c}{0.4892} &\n \\multicolumn{3}{c}{0.7318} &\n \\multicolumn{3}{c}{0.8084} &\n \\multicolumn{3}{c}{\\bf 0.8642} \\\\\n\n &\n MCC &\n \\multicolumn{3}{c}{0.4879} &\n \\multicolumn{3}{c}{0.6643} &\n \\multicolumn{3}{c}{0.6710} &\n \\multicolumn{3}{c}{0.4193} & \n \\multicolumn{3}{c}{-0.1372} & \n \\multicolumn{3}{c}{0.0839} & \n \\multicolumn{3}{c}{0.1558} & \n \\multicolumn{3}{c}{0.3543} & \n \\multicolumn{3}{c}{0.5637} & \n \\multicolumn{3}{c}{\\bf 0.6863} \\\\\n \n\n\n \n\n \n \\midrule\n \\multirow{3}{*}{\\textbf{Cresci-17}} & \n Acc & \n \\multicolumn{3}{c}{0.9750} & \n \\multicolumn{3}{c}{0.9847} & \n \\multicolumn{3}{c}{0.9799} & \n \\multicolumn{3}{c}{0.9670} & \n \\multicolumn{3}{c}{0.5204} & \n \\multicolumn{3}{c}{0.4029} & \n \\multicolumn{3}{c}{0.9597} & \n \\multicolumn{3}{c}{/} & \n \\multicolumn{3}{c}{0.9622} & \n \\multicolumn{3}{c}{\\bf 0.9871} \\\\\n \n \n  \n \n &\n F1&\n \\multicolumn{3}{c}{0.9826} &\n \\multicolumn{3}{c}{0.9893} &\n \\multicolumn{3}{c}{0.9641} & \n \\multicolumn{3}{c}{0.9768} & \n \\multicolumn{3}{c}{0.4737} & \n \\multicolumn{3}{c}{0.2923} & \n \\multicolumn{3}{c}{0.9731} & \n \\multicolumn{3}{c}{/} & \n \\multicolumn{3}{c}{0.9737} & \n \\multicolumn{3}{c}{\\bf 0.9910} \\\\\n\n &\n MCC &\n \\multicolumn{3}{c}{0.9387} &\n \\multicolumn{3}{c}{0.9625} &\n \\multicolumn{3}{c}{0.9501} &\n \\multicolumn{3}{c}{0.9200} &\n \\multicolumn{3}{c}{0.1573} &\n \\multicolumn{3}{c}{0.2255} &\n \\multicolumn{3}{c}{0.8926} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{0.9069} &\n \\multicolumn{3}{c}{\\bf 0.9685} \\\\\n\n\n \n \n \\midrule \n \\multirow{3}{*}{\\textbf{PAN-19}} &\n Acc &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{0.9464} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{0.8797} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{0.8728} &\n \\multicolumn{3}{c}{\\bf 0.9509} \\\\\n \n \n  \n \n &\n F1&\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{0.9448} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{0.8701} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{0.8729} &\n \\multicolumn{3}{c}{\\bf 0.9510} \\\\\n\n &\n MCC &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{0.8948} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{0.7685} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{/} &\n \\multicolumn{3}{c}{0.7456} &\n \\multicolumn{3}{c}{\\bf 0.9018} \\\\\n\n \n \\bottomrule\n\\end{tabular}\n\\end{table*}\n\n\n\\subsection{Self-Supervised Learning and Optimization}\n\\label{subsec:SATARSSLO}\nTwitter user representation learning attempts to model a specific user with a distributed representation. We adopt \\textbf{follower count} as the self-supervised signal for SATAR training. Specifically, a user's follower count is separated into several categories based on its numerical scale and the overall follower count distribution. We train the representation learning framework SATAR to classify each user into such categories, obtaining user representation in the process. We believe that \\textbf{follower count} would be an ideal self-supervised training signal due to the following reasons:\n\n\\begin{itemize}[leftmargin=*]\n    \\item Self-supervised training with follower count is task-agnostic. Whether it is bot detection, content recommendation or online campaign modeling, follower count relates to all tasks on social media without being specific to any of them.\n    \\item Follower count is most representative of a Twitter user. There is no better choice to describe a Twitter user more efficiently and accurately, especially when follower count also involves the evaluation of other users.\n    \\item Follower count is more robust to large-scale tamper. Although it is possible to purchase fake followers, according to Cresci \\textit{et al.}~\\cite{Cresci2015FameFS}'s investigation, an increase of 1,000 followers often costs from 13 to 19 U.S. dollars. As a result, it is costly to significantly alter the magnitude of a user's follower count, let alone launch a campaign with many active bots.\n\\end{itemize}\n\nSpecifically, assuming that a user could be categorized into $D$ classes based on its follower count, a softmax layer is applied to the representation of the user $r$:\n\\vspace{-2pt}\n\\begin{equation}\n    \\label{lossbegin}\n    \\hat{y} = softmax(W_fr + b_f),\n\\end{equation}\n\n\\noindent where $\\hat{y} = [\\hat{y_1}, \\hat{y_2}, \\cdot \\cdot \\cdot, \\hat{y_D}]$ is the predicted probability vector for each class, $W_f$ and $b_f$ are learnable parameters. $y = [y_1,y_2,\\cdot \\cdot \\cdot, y_D]$ denotes the self-supervised ground-truth for such classification in one-hot encoding. We minimize the cross-entropy loss function as follows:\n\\begin{equation}\n    \\label{equ:lossend}\n    L(\\theta) = -\\sum_{1 \\leqslant i \\leqslant D} y_i log(\\hat{y_i}),\n\\end{equation}\n\n\\noindent where $\\theta$ denotes the parameters in the proposed framework SATAR.\n\n\n\n\nAlgorithm \\ref{alg:SATAR} presents the overall training schema of our proposed Twitter account representation learning framework SATAR.\n\n\\section{Experiments} \n\\label{sec:experiments}\nIn this section, we conduct extensive experiments with in-depth analysis on three real-world bot detection datasets.\n\n\\subsection{Experiment Settings}\n\\label{subsec:expsetting}\nIn this section, we provide information about datasets, bot detection baselines and evaluation metrics adopted in the experiments. \n\n\\noindent \\textbf{Datasets.} We make use of three datasets, {\\verb|TwiBot-20|}, {\\verb|cresci-17|} and {\\verb|PAN-19|}. \nAs Twitter bots bear different purposes and evolve rapidly, these high quality datasets are adopted to provide a comprehensive evaluation and verify the generalizability and adaptability of baselines and our proposed method.\n\n\\begin{itemize} [leftmargin=*]\n\n\\item We publicized a bot detection dataset {\\verb|TwiBot-20|}\\footnote{\\url{https://github.com/GabrielHam/TwiBot-20}}. It is a comprehensive sample of the current Twittersphere to evaluate whether bot detection methods can generalize in real-world scenarios. Users in {\\verb|TwiBot-20|} could be generally split into four interest domains: politics, business, entertainment and sports. As of user information, {\\verb|TwiBot-20|} contains semantic, property and neighborhood information of Twitter users.\n\n\n\\item {\\verb|cresci-17|} ~\\cite{cresci2017paradigm} is a public dataset with 4 components: genuine accounts, social spambots, traditional spambots and fake followers. We merge the four parts and utilize {\\verb|cresci-17|} as a whole. {\\verb|cresci-17|} contains semantic and property information.\n\n\\item {\\verb|PAN-19|} \\footnote{\\url{https://zenodo.org/record/3692340}} is a dataset of a Bots and Gender Profiling shared task in the PAN workshop at CLEF 2019. It is used for bots and gender profiling and only contains user semantic information.\n\\end{itemize}\n\\vspace{-3pt}\n\nA summary of these three datasets is presented in Table~\\ref{tab:dataset}. We randomly conduct a 7:2:1 partition for three datasets as training, validation and test set. Such a partition is shared across all experiments in Section ~\\ref{subsec:expBDP}, Section ~\\ref{subsec:GeneralizeStudy} and Section ~\\ref{subsec:AdaptStudy}. We choose these three benchmarks out of numerous bot detection datasets due to their larger size, collection time span and superior annotation quality.\n\n\\noindent \\textbf{Baseline Methods.} We compare SATAR with the following bot detection methods as baselines:\n\n\n\n \\begin{figure}[!t]\n     \\centering \n     \\setlength{\\belowcaptionskip}{-0.4cm}\n     \\subfigure[train on politics domain]{\\label{fig:subfig:a}\n     \\includegraphics[width=0.47\\linewidth]{domain_1.png}}\n     \\hspace{0.01\\linewidth}\n     \\subfigure[train on business domain]{\\label{fig:subfig:b}\n     \\includegraphics[width=0.47\\linewidth]{domain_2.png}}\n     \\vfill\n     \\subfigure[train on entertainment domain]{\\label{fig:subfig:c}\n     \\includegraphics[width=0.47\\linewidth]{domain_3.png}}\n     \\hspace{0.01\\linewidth}\n     \\subfigure[train on sports domain]{\\label{fig:subfig:d}\n     \\includegraphics[width=0.47\\linewidth]{domain_4.png}}\n     \\caption{Train SATAR and two competitive baselines on one domain of TwiBot-20 and test on the other three domains.}\n     \\label{fig:Domain}\n \\end{figure}\n \\vspace{-2pt}\n\n\\begin{itemize} [leftmargin=*]\n\n\\item Lee \\textit{et al.}~\\cite{lee2011seven}: Lee \\textit{et al.} use random forest classifier with several Twitter user features. e.g. the longevity of the account.\n\n\\item Yang \\textit{et al.}~\\cite{yang2020scalable}: Yang \\textit{et al.} use random forest with minimal account metadata and 12 derived features.\n\n\\item Kudugunta \\textit{et al.}~\\cite{kudugunta2018deep}: Kudugunta \\textit{et al.} propose an architecture that uses both tweet content and the metadata. \n\n\\item Wei \\textit{et al.}~\\cite{wei2019twitter}: Wei \\textit{et al.} use word embeddings and a three-layer BiLSTM to encode tweets. A fully connected softmax layer is adopted for binary classification.\n\n\\item Miller \\textit{et al.}~\\cite{miller2014twitter}: Miller \\textit{et al.} extract 107 features from a user's tweet and property information. Bot users are conceived as abnormal outliers and modified stream clustering algorithm is adopted to identify Twitter bots.\n\n\\item Cresci \\textit{et al.}~\\cite{cresci2016dna}: Cresci \\textit{et al.} utilize strings to represent the sequence of a user's online actions. Each action type can be encoded with a character. By identifying the group of accounts that share the longest common substring, a set of bot accounts are obtained.\n\n\\item Botometer~\\cite{davis2016botornot}: Botometer \nis a publicly available service that leverages more than one thousand features to classify an account.\n\n\\item Alhosseini \\textit{et al.}~\\cite{ali2019detect}: Alhosseini \\textit{et al.} utilize graph convolutional network to detect Twitter bots. It uses following information and user features to learn representations and classify Twitter users.\n\\end{itemize}\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width = \\linewidth]{ablation.png}\n    \\caption{Ablation study that removes the semantic, property and neighborhood sub-networks from SATAR respectively.}\n    \\label{fig:ablation}\n\\end{figure}\n\nFor the following SATAR-based bot detection methods, the self-supervised representation learning step adopts the Pareto Principle\\footnote{\\url{https://en.wikipedia.org/wiki/Pareto\\_principle}} as a self-supervised classification task, where the framework learns to predict whether a Twitter user's follower count is among the top $20\\%$ or the bottom $80\\%$. It is an instance of the self-supervised representation learning strategy in Section ~\\ref{subsec:SATARSSLO}.\n\n\\begin{itemize}[leftmargin=*]\n\\item $\\rm SATAR_{FC}$: The proposed representation learning framework SATAR is firstly trained with self-supervised user classification tasks based on their follower count, then the final softmax layer is reinitialized and trained on the task of bot detection.\n\n\\item $\\rm SATAR_{FT}$: The proposed representation learning framework SATAR is firstly trained using self-supervised users, then the final softmax layer is reinitialized and fine-tuning is performed on the whole framework using the training set of bot detection.\n\\end{itemize}\n\n\n\n\\noindent \\textbf{Evaluation Metrics.}\nWe adopt Accuracy, F1-score and MCC~\\cite{matthews1975comparison} as evaluation metrics of different bot detection methods. Accuracy is a straightforward indicator of classifier correctness, while F1-score and MCC are more balanced evaluation metrics.\n\n \\begin{figure*}\n    \\centering\n    \\setlength{\\belowcaptionskip}{-0.3cm}\n    \\includegraphics[width = .95\\linewidth]{time5.png}\n    \\caption{SATAR's prediction of specific users in TwiBot-20. Scattered points demonstrate SATAR's prediction for specific users and the line indicates SATAR's overall accuracy of capturing bots registered in a 3-month time span.}\n    \\label{fig:time}\n\\end{figure*}\n\n\n\\begin{figure}\n    \\centering\n    \\setlength{\\belowcaptionskip}{-0.5cm}\n    \\includegraphics[width = \\linewidth]{super.png}\n    \\caption{Ablation study removing the self-supervised pre-training step from SATAR and train on the three datasets.}\n    \\label{fig:super}\n   \n\\end{figure}\n \n\\subsection{Bot Detection Performance}\n\\label{subsec:expBDP}\nTable \\ref{tab:SPN} identifies the user information that each compared method uses. Table ~\\ref{tab:TwiBotMetric} reports bot detection performance of different methods on three datasets. Table ~\\ref{tab:TwiBotMetric} demonstrates that:\n\n\n\n\n\n\n\n\\begin{itemize} [leftmargin=*]\n \n \\item $\\rm SATAR$ based methods achieve competitive performance compared with other baselines, which demonstrates that SATAR is generally effective in Twitter bot detection. $\\rm SATAR_{FT}$ outperforms $\\rm SATAR_{FC}$, which demonstrates the efficacy of the pre-training and fine-tuning approach.\n \n \\item $\\rm SATAR_{FT}$ generalizes to real-world scenarios because it outperforms the state-of-the-art methods on the comprehensive and representative dataset {\\verb|TwiBot-20|}, which imitates the real-world Twittersphere. Meanwhile, $\\rm SATAR_{FT}$ adapts to evolving generations of bots because it achieves the best performance on all three datasets with varying collection time from 2017 to 2020. Section ~\\ref{subsec:GeneralizeStudy} and Section ~\\ref{subsec:AdaptStudy} will provide further analysis to demonstrate that SATAR successfully addresses the challenges of generalization and adaptation, while critical components and design choices of SATAR are the reasons behind its success. \n\n \\item For methods mainly based on LSTM, we see that Kudugunta \\textit{et al.} ~\\cite{kudugunta2018deep} outperforms Wei \\textit{et al.} ~\\cite{wei2019twitter}. It indicates that Kudugunta \\textit{et al.} ~\\cite{kudugunta2018deep} can better capture bots by incorporating property items. $\\rm SATAR_{FT}$ leverages even more user information than Kudugunta \\textit{et al.} ~\\cite{kudugunta2018deep} and achieves better performance, which suggests that bot detection methods should incorporate more aspects of user information.\n \n\\item Feature-engineering based methods, such as Yang \\textit{et al.} ~\\cite{yang2020scalable}, perform well on {\\verb|cresci-17|} but inferior to $\\rm SATAR_{FT}$ on {\\verb|TwiBot-20|}. This shows that traditional bot detection methods that emphasize feature engineering fail to adapt to new generations of bots.\n\n \n \\item Both Alhosseini \\textit{et al.} ~\\cite{ali2019detect} and $\\rm SATAR$ use neighborhood information. $\\rm SATAR$ based methods outperform Alhosseini \\textit{et al.} ~\\cite{ali2019detect}, which shows that $\\rm SATAR$ better utilizes user neighbors that put Twitter users into their social context.\n\n\\end{itemize}\n \n\n\n\\subsection{SATAR Generalization Study}\n\\label{subsec:GeneralizeStudy}\n\nThe challenge of generalization in social media bot detection demands bot detectors to simultaneously identify bots that attack in many different ways and exploit diversified user information. To prove that SATAR generalizes, we examine SATAR and competitive baselines' performance on {\\verb|TwiBot-20|}. As demonstrated in Table ~\\ref{tab:TwiBotMetric}, SATAR outperforms all baselines on {\\verb|TwiBot-20|}. Given the fact that {\\verb|TwiBot-20|} contains diversified bots and human which imitates the real-world Twittersphere, SATAR is demonstrated to best generalize in real-world scenarios. \n\nTo further prove SATAR's generalizability, we train SATAR and two competitive baselines, Alhosseini \\textit{et al.} ~\\cite{ali2019detect} and Yang \\textit{et al.} ~\\cite{yang2020scalable}, on one of the four user domains of dataset {\\verb|TwiBot-20|} and test on the others. The results are presented in Figure ~\\ref{fig:Domain}. It is illustrated that SATAR could better capture other types of bots even when not explicitly trained on them, which further establishes the claim that SATAR successfully generalizes to diversified bots that co-exist on social media.\n\n\nSATAR is designed to generalize by jointly leveraging all three aspects of user information, namely semantic, property and neighborhood information. To figure out whether our proposal of using as much user information as possible has lead to the generalizability of SATAR, we conduct ablation study that removes one aspect of user information at a time. The results are demonstrated in Figure ~\\ref{fig:ablation}.\n\n\n\nResults in Figure ~\\ref{fig:ablation} show that removing any aspect of information from SATAR would result in a considerable loss in bot detection performance, limiting SATAR's ability to generalize to different types of bots in {\\verb|TwiBot-20|}. It indicates that SATAR's strategy of leveraging more aspects of user data is crucial in addressing the challenge of generalization.\n\n\n\n\\subsection{SATAR Adaptation Study}\n\\label{subsec:AdaptStudy}\n\n \\begin{figure*}[h]\n     \\centering\n     \\includegraphics[width = 0.95\\textwidth]{representation.png}\n     \\caption{2D t-SNE plot of the user representation vectors of SATAR, Alhosseini \\textit{et al.} ~\\cite{ali2019detect} and Yang \\textit{et al.} ~\\cite{yang2020scalable}.}\n     \\label{fig:representation}\n \\end{figure*}\n\n\n\nThe challenge of adaptation in bot detection demands bot detectors to maintain desirable performance in different times and catch up with rapid bot evolution. To prove that SATAR adapts, we examine SATAR and competitive baselines' performance on three datasets, since they are released in 2017, 2019 and 2020 respectively and could well characterize the bot evolution. Results in Table ~\\ref{tab:TwiBotMetric} demonstrate that SATAR reaches state-of-the-art performance on all three datasets, which indicates that SATAR is more successful at adapting to the bot evolution than existing baselines.\n\nTo further prove SATAR's ability to adapt, we examine SATAR's prediction of users in dataset {\\verb|TwiBot-20|}'s validation set and test set. We present SATAR's prediction results of specific users and SATAR's accuracy in any 3-month time span of user registration time in Figure ~\\ref{fig:time}. It is illustrated that SATAR maintains a steady detection accuracy for users created from 2007 to 2020, which further establishes the claim that SATAR successfully adapts to the everlasting bot evolution.\n\nSATAR is designed to adapt by pre-training on mass self-supervised users and fine-tuning on specific bot detection scenarios. To figure out whether this pre-training and fine-tuning schema has enabled SATAR to adapt to newly evolved bots, we conduct ablation study to remove the self-supervised pre-training step. SATAR's performance on different datasets are illustrated in Figure ~\\ref{fig:super}.\n\nFigure ~\\ref{fig:super} shows that SATAR's performance increases with the adoption of the self-supervised pre-training step, and such trend is especially salient on the dataset PAN-19 with less user information. It indicates that SATAR's ability to adapt indeed comes from the innovative strategy to use follower count as a self-supervised signal for user representation pre-training.\n\\vspace{-5pt}\n\n\\subsection{Representation Learning Study}\n\\label{subsec:expRLS}\nSATAR improves representation learning for Twitter users. Extrinsic evaluation has proven that SATAR representations are of desirable quality. We further conduct intrinsic evaluation by comparing SATAR representations with Alhosseini \\textit{et al.} ~\\cite{ali2019detect} and Yang \\textit{et al.} ~\\cite{yang2020scalable}, which also provide user representations. We cluster representations using $k$-means with $k = 2$, and calculate the homogeneity score, which is the extent to which clusters contain a single class. Higher homogeneity score indicates that users with the same label are more likely to be close to each other.\n\nFigure ~\\ref{fig:representation} visualizes representations of users in a subgraph of {\\verb|TwiBot-20|}. Figure ~\\ref{fig:representation}(a) is the t-SNE plot of SATAR representations, which shows moderate collocation for groups of bot and human users, while Figure ~\\ref{fig:representation}(b) and (c) show little collocation within either group. Quantitatively, SATAR achieves the highest homogeneity score, which indicates that SATAR produces user representations of higher quality.\n\n\n\\subsection{Case Study}\n\\vspace{-2pt}\nTo further understand how SATAR identifies bots, we study a specific case of several bots. We use the affinity index values in Equation (\\ref{cobegin}) to quantitatively analyze SATAR's decision making. Figure ~\\ref{fig:case} shows the detailed information of the sampled users:\n\n\\begin{itemize} [leftmargin=*]\n    \\item SATAR identifies user B and E through their repeated or similar tweets that signal automation. For example, user B has affinity values of $F_{sp} = -0.9989$, $F_{pn} = 0.0017$ and $F_{ns} = 0.6376$. Absolute values of $F_{sp}$ and $F_{ns}$ are significantly greater than $F_{pn}$, which demonstrates that semantic information is the dominant factor for SATAR's decision in this case.\n    \\item SATAR identifies user C and D through their properties. Abnormal characteristics such as too many followings and default background image are detected by SATAR. User D has larger absolute values for $F_{sp}$ and $F_{pn}$ than $F_{ns}$, which shows that property information is critical in SATAR's judgement.\n    \\item SATAR captures the anomaly that user A has bot users B, C, D and E as neighbors, which is unlikely for genuine users. User A has larger absolute values for $F_{ns}$ and $F_{pn}$ than $F_{sp}$, which also bears out the claim that user A's abnormal neighborhood has led to SATAR's decision.\n\\end{itemize}\n\n\n\nThe case study in Figure ~\\ref{fig:case} demonstrates that SATAR identifies bot users by jointly evaluating their semantic, property and neighborhood information. Affinity values of our proposed Co-Influence aggregator provides explanation to SATAR's decisions.\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.46\\textwidth]{case_study5.png}\n  \\setlength{\\belowcaptionskip}{-0.3cm}\n  \\caption{A sample bot cluster to explain SATAR's decision.}\n  \\label{fig:case}\n\n\\end{figure}\n\n\\section{Conclusion and Future Work}\n\\label{sec:conclusion}\nSocial media bot detection is attracting growing attention. We proposed SATAR, a self-supervised approach to Twitter account representation learning and applied it to the task of bot detection. SATAR aims to tackle the challenges of generalizing in real-world scenarios and adapting to bot evolution, where previous efforts failed. We conducted extensive experiments to demonstrate the efficacy of SATAR-based bot detection in comparison to competitive baselines. Further exploration proved that SATAR also succeeded in generalizing on the real Twittersphere and adapting to different generations of Twitter bots. In the future, we plan to apply the SATAR representation learning framework to other tasks in the social media domain such as fake news detection and content recommendation.\n\n\n\n   \n \n   \n\n \n\n   \n\n     \n \n \n   \n \n \n \n \n  \n \n\n \n\n\n \n\n \n \n \n  \n \n\n\n\n \n \n \n \n  \n \n\n\n \n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n \n \n\n\n \n \n \n \n\n \n\n   \n \n \n \n\n \n \n \n\n\n \n\n   \n \n \n\n\n \n \n \n\n\n \n\n\n\n\n\n \n \n \n \n \n \n \n\n \n \n \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n", "meta": {"timestamp": "2021-06-25T02:21:27", "yymm": "2106", "arxiv_id": "2106.13089", "language": "en", "url": "https://arxiv.org/abs/2106.13089"}}
{"text": "\\section{Introduction}\nQuantum spin liquids (QSLs) are novel states of magnetic systems characterized by the absence of long range spin order down to zero temperature\\cite{Balents2010,RevModPhys.89.025003,Savary_2016}. Amongst various QSLs, the Kitaev QSL based on honeycomb lattice is of especial importance. Different from spin liquids arising from geometrically frustrated spin arrangements, the bond-dependent Kitaev spin interactions frustrate the spin configuration. The Kitaev model hosts exactly solvable QSL ground state and fractionalized excitations described by Majorana fermions\\cite{KITAEV20062}. Experimentally, the Kitaev model is expected to be realized in honeycomb Mott insulators with spin-orbit coupling\\cite{PhysRevLett.102.017205}. Much effort has been made to experimentally explore materials dominated by the bond dependent Kitaev interaction, firstly in transition metal 5d-electron iridates and then in 4d-electron RuCl$_{3}$. However, there are significant non-Kitaev interactions in those real materials, such as the Heisenberg type exchange interaction and off-diagonal exchange interactions. The non-Kitaev interactons hinder the formation of pure Kitaev quantum spin liquid and push the system to be ordered at low temperature\\cite{PhysRevLett.112.077204,PhysRevB.94.064435}. To approach the Kitaev QSL, one promising route is to suppress the long range magnetic order by applying magnetic field. The field induced quantum spin liquid candidate states are widely studied in these Kitaev materials\\cite{Ruiz2017,PhysRevLett.110.097204,PhysRevLett.117.277202,Banerjee2016,Banerjee1055}.\n\nRecently, the $3d^{7}$ Co-based honeycomb materials with high spin state S=3/2 and effective orbital angular momentum L=1 have been proposed as new candidates of Kitaev QSLs and have attracted wide attention \\cite{PhysRevB.97.014407,PhysRevB.97.014408,PhysRevLett.125.047201,PhysRevB.97.134409,REGNAULT1977660,10.1016/j.heliyon.2018.e00507,REGNAULT1979194,VICIU20071060,PhysRevMaterials.3.074405,PhysRevB.102.224429,PhysRevB.101.085120,PhysRevB.102.054414,PhysRevB.102.224411,PhysRevB.94.214416,PhysRevB.103.L180404,doi:10.1063/1.5029090,2012.00940v2,Motome_2020,2106.11982v1}. In transition metal 4d- and 5d-electron systems, the spatially extended d-electron wave function leads to nonnegligible longer range coupling, which is detrimental to Kitaev QSL. Comparing with 4d- and 5d-electron systems, the 3d-electrons have more localized wave functions, thus recede the longer range coupling. Besides, the Heisenberg interactions in 3d systems are easier to be minimized by tuning the external parameters. Accordingly, the $3d^{7}$ systems may be more appropriate to realize the Kitaev QSL. BaCo$_{2}$(AsO$_{4}$)$_{2}$ is among the suggested candidates with $3d^{7}$ Co honeycomb lattice. It has similar properties as the well-established Kitaev QSL candidate RuCl$_{3}$. The magnetic susceptibility shows strong anisotropy, indicating the anisotropic exchange interactions in the spin Hamiltonian. Especially, a small magnetic field applied in the honeycomb plane can significantly change the magnetic system and completely suppress the long range order. The critical field $\\sim$0.5 T is much weaker compare to that of 7 T in RuCl$_{3}$, indicating very small Heisenberg interaction\\cite{Zhongeaay6953}. Those studies suggest that this system is an excellent candidate for realization of a field-induced QSL state. To characterize the magnetic excitations at low energies, we perform terahertz (THz) time domain spectroscopy measurement on BaCo$_{2}$(AsO$_{4}$)$_{2}$ crystals under external magnetic field. We monitored the evolution of the spin wave under magnetic field and characterized the emergent magnetic excitations in the field induced paramagnetic state.\n\n\\section{Sample and experiments}\n\nThe single crystals of BaCo$_{2}$(AsO$_{4}$)$_{2}$ with a typical size of $3\\times3\\times0.3 mm^{3} $ were grown by the flux method \\cite{Zhongeaay6953}. The compound crystallizes in the trigonal centrosymmetric space group R-3. The honeycomb structure is made of edge sharing CoO$_6$ tetrahedra and stacked along the c-axis with an ABC periodicity. Below 5.4 K, the system transforms to an antiferromagnetic ordered state. The magnetic order is rather complex with spiral spin chains \\cite{10.1007/978-94-009-1860-3}. By applying in-plane magnetic field, the system exhibit two phase transitions near 0.2 T and 0.5 T, respectively. The antiferromagnetic long range order is suppressed at the second transition, while the first transition is more complicated \\cite{Zhongeaay6953}.\n\nTime domain THz transmission spectra were measured by using a home built spectroscopy system equipped with helium cryostat and Oxford spectramagnet \\cite{PhysRevB.98.094414}. As shown in Figure~\\ref{Fig:1} (a), the wave vector of the incident THz beam is perpendicular to the crystallographic ab-plane. The polarization of the magnetic field component of THz wave can be tuned from a-axis to b-axis. By rotating the superconducting magnet, the external magnetic field can be applied either parallel or perpendicular to the ab-plane. The time domain signals of the sample and reference (empty aperture) were detected via free space electro-optics sampling in ZnTe crystal. Fourier transformation of the time domain spectra provides the frequency dependent complex transmission spectra containing both magnitude and phase information, from which the real and imaginary parts of optical constants can be extracted (see Supplementary Material)\\cite{sup}.\n\n\n\n\\section{Evolution of the magnon mode with temperature and magnetic field in the antiferromagnetic order region}\n\n\\begin{figure}[htbp]\n\t\\centering\n\n\t\\includegraphics[width=8cm]{graph1.eps}\\\\\n\t\\caption{(a) Schematic of the terahertz transmission spectra measurement.   (b) Real part of the optical conductivity at selected temperatures without external field.\n\t}\\label{Fig:1}\n\\end{figure}\n\nFigure~\\ref{Fig:1} (b) shows the temperature dependence of the real part of the optical conductivity $\\sigma$ at low temperatures. Below $T_N$, a narrow peak emerges abruptly on the flat background at 0.35 THz. Then, its intensity gradually increases and reaches a maximum at $\\sim$3 K. We label this peak as mode A. The frequency of the mode is consistent with previous neutron scattering experiments, being identified as the magnon at the $\\Gamma$ point of the magnetically ordered state \\cite{10.1007/978-94-009-1860-3,10.1016/j.heliyon.2018.e00507}. In time domain THz measurement, ordered spins are excited by THz wave through Zeeman torque $dM/dt = \\gamma M \\times H_{THz}$, where $\\gamma$ denotes the gyromagnetic constant, $M$ the magnetic momentum and $H_{THz}$ the magnetic field component of the THz wave. To effectively drive the spin procession, \\emph{i.e.} excite magnon, $H_{THz}$ should be perpendicular to the magnetic moment. Therefore, measuring the polarization dependence of the magnon can help identify the spin orientation. When we rotate the polarization of magnetic field component of the THz wave from a- to b-axis without applying external magnetic field, the magnon mode is visible in all directions. The result is consistent with the antiferromagnetic order with presence of spiral spin chains \\cite{10.1007/978-94-009-1860-3}.\n\n\n\n\n\n\\begin{figure}[htbp]\n\t\\centering\n\n\t\\includegraphics[width=9cm]{graph2.eps}\\\\\n\t\\caption{ Evolution of the optical conductivity under magnetic field at 2 K. (a) Spectra for H along a axis and $H_{THz}$ along b axis.  (b) Spectra for H along a axis and $H_{THz}$ along a axis. (c) Temperature dependence of the spectra for H (0.4 T) along a axis and $H_{THz} \\parallel H$. (d) Spectra for H along c axis.\n\t }\\label{Fig:2}\n\\end{figure}\n\n\nWe now present the evolution of THz spectra under applied external magnetic field. Figure~\\ref{Fig:2} (a) shows the frequency-dependent conductivity measured at 2 K in the configuration of the external magnetic field $H$ being applied along the a-axis and the magnetic field of THz wave perpendicular to $H$. Upon increasing the magnetic field, the intensity of the magnon mode A initially decreases slightly, but keeps the mode frequency unchanged. At around $H_{c1}$ = 0.2 T, the excitation mode suddenly shifts to 0.39 THz, and its intensity is strongly enhanced, suggesting a magnetic field induced phase transition above 0.2 T. We denote this sharp peak as mode B for the field above 0.2 T. As the magnetic field increases to 0.5 T, the narrow peak excitations are fully suppressed. The results are consistent with previous reports that a magnetic field induced transition from antiferromagnetic phase I to phase II occurs near 0.2 T and a complete suppression of antiferromagnetic order \\cite{10.1007/978-94-009-1860-3,Zhongeaay6953}.\n\nThe spectra with $H_{THz}\\parallel H$ are quite different. As shown in Fig.~\\ref{Fig:2} (b), in the antiferromagnetic phase II above 0.2 T, the excitation mode at 0.39 THz is completely absent. Instead, a slight enhancement in conductivity spectra above 0.7-0.8 THz seems to be visible. Figure~\\ref{Fig:2} (c) shows the temperature dependent spectra measured at 0.4 T in the antiferromagnetic phase II region. We do not observe any excitation peak emerging below T$_N$. The low energy spectral weight drops below T$_N$ and reduced spectral weight is shifted to the higher energy above 0.7-0.8 THz. Based on those results, we identify that the first phase transition near 0.2 T is a spin reorientation transition. The moments in spiral spin chains are tuned by the magnetic field. The antiferromagnetic phase II is consistent with a collinear antiferromagnetic state, consistent with previous report\\cite{Zhongeaay6953}. The disappearance of magnon mode in Fig.~\\ref{Fig:2} (c) can be attributed to the polarization selection rule of $H_{THz}$ parallel to the magnetic moments. For a comparison, the magnetic state is stable when the external magnetic field is applied along the c-axis, as shown in Fig.~\\ref{Fig:2} (d). Those results are consistent with static measurement \\cite{10.1007/978-94-009-1860-3,Zhongeaay6953}.\n\n\n\n\\begin{figure}[htbp]\n\t\\centering\n\n\t\\includegraphics[width=9cm]{graph3.eps}\\\\\n\t\\caption{  (a) Real part of the optical conductivity at different magnetic field for H along a axis and $H_{THz}$ along b axis.  (b) Temperature evolution of the excitation at 2 T for $H_{THz} \\perp H$. (c)(d) Contour plot of $\\sigma$ as a function of field and frequency for $H_{THz} \\perp H$ and $H_{THz} \\parallel H$. (e) Temperature dependence of the spectra at 2 T for $H_{THz} \\parallel H$. The spectrum at 15 K are subtracted as a reference. (f) Field dependence of the excitation modes.\t}\\label{Fig:3}\n\\end{figure}\n\n\n\\section{Magnetic excitations in the field induced paramagnetic state}\n\nAcross the second critical field $H_{c2}$ at around 0.5 T, the sample transforms to a paramagnetic phase. Identifying the nature of the excitations in this phase is essential to judge whether or not it is a quantum spin liquid state. The measured spectra of this phase are shown in Fig.~\\ref{Fig:3}. The field evolution of the spectra with\n$H_{THz}\\perp H$ are shown in Fig.~\\ref{Fig:3} (a). All the well defined modes are fully suppressed at 0.5 T. Upon further increasing the in-plane magnetic field slightly, a new narrow peak emerged immediately at $\\sim$0.6 T, denoted as mode C. This peak has the largest intensity in our measurement. Its frequency increases with the magnetic field. Obviously, mode C is the magnon in field-polarized paramagnetic phase, being essentially the same as the ferromagnetic order. Surprisingly, we find an anomalous behavior for this mode as shown in Fig.~\\ref{Fig:3} (a). At 2 K, the intensity of the mode drops dramatically when the magnetic field is above 1 T. To track the characteristics of the mode, we measured the temperature dependence of the spectra at each field. The typical behavior of the mode at relatively high field, \\emph{e.g.} 2 T, are shown in Fig.~\\ref{Fig:3} (b). The temperature evolution of the mode intensity shows an unusual strong resonance character. Below 1 T, the resonance temperature is around 2 K, the mode decreases monotonically as temperature increases. With increasing magnetic field, the resonance temperature also increases. At 2 T, the mode intensity gets maximum at around 10 K, while the intensity at 2 K becomes much smaller. It is reasonable to attribute the resonance behavior of the mode to the competing interactions among the exchange interaction, magnetic field and thermal excitations.\n\nFigure~\\ref{Fig:3} (c) displays the intensity plot of the mode frequency as a function of external magnetic field in the configuration of $H_{THz}\\perp H$ at 2 K, in which we can identify clearly three magnon modes upon increasing the magnetic field. Mode A and B are magnons in antiferromagentic phase I and II, respectively, while mode C is the magnon in field-polarized paramagnetic phase.\n\nSimilarly, we plot the intensity map of the conductivity spectra for $H_{THz} \\parallel H$ in Fig.~\\ref{Fig:3} (d). Note that the intensity of the excitation spectrum is much weaker than that observed for $H_{THz} \\perp H$. In order to clearly identify the magnetic excitations, we substrate the spectrum at 15 K as a reference. The typical spectra at a representative field, \\emph{e.g.} 2 T, is shown in Fig.~\\ref{Fig:3} (e). We emphasize that the mode C observed with $H_{THz} \\parallel H$ is completely absent here, indicating that this mode also follows the polarization selection rules. On the contrary, another broad mode feature at higher energy, labelled as mode C', is observed with a dip at lower energy side and a broad peak at higher energy side. The temperature evolution is monotonous for mode C' in all applied magnetic field above 0.6 T. The characteristic behaviors imply an opening of energy gap below the mode C' for $H_{THz} \\parallel H$.\n\n\nTo further identify the relation of magnetic excitations between $H_{THz} \\perp H$ and $H_{THz} \\parallel H$, we plot the peak positions of mode C and C' in Fig.~\\ref{Fig:3} (f). We noticed that both have a sublinear field dependence, and mode C' has approximately the energy twice of that of mode C. The doubled frequency of mode C is also plotted as the circles in Fig.~\\ref{Fig:3} (f), showing a good match with mode C'. In field-polarized paramagnetic phase, the magnetic moment in each spin chain is along the external field direction. With $H_{THz} \\perp H$, the THz wave coherently drives the spin procession and excites the single magnon. While for $H_{THz} \\parallel H$, the THz wave can not excite the single magnon, but excite the two magnons which show an energy gap below the mode and a continuum structure, as we shall explain below.\n\nTo understand the physics begetting the novel spin excitation spectrum of BaCo$_{2}$(AsO$_{4}$)$_{2}$, we consider a simplified model on honeycomb lattice and show that its excitation spectrum qualitatively reproduces the observed mode C and C' in the high field phase.\nThe model Hamiltonian is\n\\begin{eqnarray}\n\\hat{H}=-J\\sum_{<ij>} \\boldsymbol{S}_i\\cdot \\boldsymbol{S}_j-g_z\\mu_B H\\sum_{i}S_i^{z},\n\\end{eqnarray}\nwhere $J$ is the nearest-neighbor ferromagnetic Heisenberg exchange coupling, $g_z$ is the gyromagnetic ratio, $\\mu_B$ is the Bohr magneton, $H$ is the applied external magnetic field. Without loss of generality we define the $z$ direction along the applied field $H$, and the $x$ direction along the $H_{THz}$ direction of THz wave that is perpendicular to applied field $H$. The measured THz spectra is then essentially the dynamical spin structure factors $\\chi^{xx}(\\boldsymbol{q},\\omega)$ and $\\chi^{zz}(\\boldsymbol{q},\\omega)$ for the $H_{THz}\\perp H$ and $H_{THz}\\parallel H$ cases respectively.\nThe wave vector ${\\boldsymbol{q}}$ of the THz wave is much smaller than the Brillouin zone size and is treated as ${\\boldsymbol{q}}=0$ hereafter.\nWe compute the magnon dispersion and dynamical spin structure factors for this model by the standard linear spin wave theory, the calculation details can be found in Supplementary Materials \\cite{sup}.\nThis model has a polarized ground state and two branches (because of the two-site unit cell of honeycomb lattice) of gapped magnons with dispersion\n\\begin{eqnarray}\n\\varepsilon_{\\boldsymbol{q}}^{\\pm}=g_z\\mu_B H+\\frac{3J}{2}\\left ( 1\\pm \\left\\vert \\frac{1}{3}\\sum_{\\delta_{1,2,3}}e^{i {\\boldsymbol{q}}\\cdot {\\boldsymbol{\\delta}}} \\right\\vert \\right ),\n\\end{eqnarray}\nwhere $\\boldsymbol{\\delta}_{1,2,3}$ are the three bond vectors connecting nearest-neighbor sites on the honeycomb lattice.\nThe magnon dispersions $\\varepsilon_{\\boldsymbol{q}}^{\\pm}$ depend on external field $H$ linearly.\nThe calculated $\\chi^{xx}({\\boldsymbol{q}}=0,\\omega)$ is\n\\begin{eqnarray}\n\\chi^{xx}({\\boldsymbol{q}}=0,\\omega)\\propto \\delta(\\omega -\\varepsilon_{\\boldsymbol{q}}^{-}),\n\\end{eqnarray}\nand shows a sharp peak at single magnon energy\n$\\varepsilon_{\\boldsymbol{q}=0}^{-}=g_z\\mu_B H$.\nThe calculated $\\chi^{zz}({\\boldsymbol{q}}=0,\\omega)$  is\n\\begin{eqnarray}\n\\chi^{zz}({\\boldsymbol{q}}=0,\\omega)\\propto \\sum_k [\\delta(\\omega -\\varepsilon_{\\boldsymbol{k}}^{+}-\\varepsilon_{-{\\boldsymbol{k}}}^{+})+\\delta(\\omega -\\varepsilon_{\\boldsymbol{k}}^{-}-\\varepsilon_{-{\\boldsymbol{k}}}^{-})],\n\\end{eqnarray}\nand shows broad two-magnon continuum.\nFig.~\\ref{Fig:4} shows an example of the calculated dynamical spin structure factors.\nThe low energy edge of the continuum in $\\chi^{zz}({\\boldsymbol{q}}=0,\\omega)$ is twice of the minimal single magnon energy $\\omega=2\\varepsilon_{{\\boldsymbol{k}}=0}^-=2g_z\\mu_B H$.\nThe spin model for BaCo$_{2}$(AsO$_{4}$)$_{2}$ will certainly be much more complicated than the Heisenberg model considered here\\cite{10.1007/978-94-009-1860-3}. But its dynamical spin structure factors in the high field polarized phase will have the same qualitative behaviors, namely that $\\chi^{xx}$ shows a single magnon peak\nand $\\chi^{zz}$ shows a two magnon continuum whose low energy edge is twice of the minimal single magnon energy.\n\n\\begin{figure}[htbp]\n\t\\centering\n\n\t\\includegraphics[width=7.5cm]{graph4.eps}\\\\\n\t\\caption{The calculated dynamical spin structure factors $\\chi^{xx}({\\boldsymbol{q}}=0,\\omega)$ (orange line, for $H_{THz}\\perp H$ case) showing a single magnon peak, and $\\chi^{zz}({\\boldsymbol{q}}=0,\\omega)$ (blue line, for $H_{THz}\\parallel H$ case) showing two-magnon continuum whose low energy edge is twice of the single magnon energy. Details about this calculation can be found in Supplementary Materials \\cite{sup}.}\\label{Fig:4}\n\\end{figure}\n\nBased on above discussions, we summarize the assignment of magnetic excitations displayed in Fig. \\ref{Fig:3} (c) and (d). The mode A at 0.35 THz below 0.2 T is the magnon excitation in antiferromagnetic phase I. It is observed for both $H_{THz} \\perp H$ and $H_{THz} \\parallel H$ due to the presence of spiral spin chains. The mode B at 0.39 THz above 0.2 T but below 0.5 T is the magnon excitation in antiferromagnetic phase II. It is observed only for $H_{THz} \\perp H$. There is a very weak feature near 0.8 THz for $H_{THz} \\parallel H$, denoted as B', which is likely originated from the two-magnon excitations in the field-induced collinear antiferromagnetic phase II. The mode C and C' observed for $H_{THz} \\perp H$ and $H_{THz} \\parallel H$ respectively are single magnon and two-magnon excitations in field-polarized paramagnetic phase. The reason that the single magnon excitation is absent for $H_{THz} \\parallel H$ in antiferromagnetic phase II and field-polarized paramagnetic state is because the magnetic component of THz wave is parallel to the moment orientation in both phases.\n\n\n\n\\section{Further discussions on the field induced paramagnetic state}\n\nOur study indicates that BaCo$_{2}$(AsO$_{4}$)$_{2}$ offers an ideal system to investigate the magnetic excitations in\n$3d^{7}$ Co-based honeycomb lattice systems. The external magnetic field required to suppress the antiferromagnetic order is much smaller than that in any other known magnetic honeycomb compounds, enabling a full and careful characterization of magnetic excitations by time domain THz spectroscopy technique.\n\nIt is also interesting to note that the present measurement result on BaCo$_{2}$(AsO$_{4}$)$_{2}$ shares similarity to the well-studied RuCl$_3$ compound under high magnetic field. As mentioned above, both the mode C and mode C' in the field induced paramagnetic state have a sublinear field dependence, with the slopes gradually decreasing with the field increasing. These modes seem to have the similar characters with the excitations observed in the field induced disorder state of RuCl$_3$ \\cite{PhysRevLett.119.227202,PhysRevB.96.241107,PhysRevLett.125.037202,PhysRevB.101.140410,Wulferding2020}.\n\nAlthough the compound BaCo$_{2}$(AsO$_{4}$)$_{2}$ was suggested to be a possible new Kitaev QSL candidate when the magnetic order was suppressed by the magnetic field of 0.5 T \\cite{Zhongeaay6953}, our study revealed sharp single magnon and two magnon excitations even the in-plane magnetic field is as low as 0.6 T, which suggests against formation of Kitaev QSL state. The sharp magnon excitation is absent only in a region when the magnetic field is extremely close to 0.5 T.\n\n\n\n\\section{Conclusion}\nIn conclusion, we have presented THz spectroscopy study on Co-based honeycomb BaCo$_{2}$(AsO$_{4}$)$_{2}$ under in-plane magnetic field up to 4 T. The fact that an extremely small magnetic field can suppress the long range antiferromagnetic phase makes it an ideal system to investigate the magnetic excitations in the field induced states by THz spectroscopy measurement. Our field and polarization dependent measurement reveals two first order transitions. The first transition at 0.2 T is from spiral order to collinear antiferromagnetic order. The second transition at 0.5 T is the suppression of the antiferromagnetic order. We observed different magnon excitations in different regions of applied magnetic field. In particular, after the long range magnetic order was suppressed by a weak field $H_{c2}$, the system was driven immediately to a field-polarized paramagnetic phase being similar to a ferromagnetic state. The spectra beyond $H_{c2}$ are dominated by single magnon and two magnon excitations. However, no signature of a quantum spin liquid state is observed. We also compared the excitation spectra of BaCo$_{2}$(AsO$_{4}$)$_{2}$ with that of widely studied 4d-electron Kitaev candidate RuCl$_{3}$ and addressed their similarities and differences in magnetic excitations.\n\n\n\\begin{center}\n\\small{\\textbf{ACKNOWLEDGMENTS}}\n\\end{center}\n\nThis work was supported by National Natural Science Foundation of China (No. 11888101), the National Key Research and Development Program of China (No. 2017YFA0302904). The crystals were grown in the laboratory of R.J. Cava at Princeton University.\n\n\n\\bibliographystyle{apsrev4-1}\n\n\n", "meta": {"timestamp": "2021-06-25T02:21:47", "yymm": "2106", "arxiv_id": "2106.13100", "language": "en", "url": "https://arxiv.org/abs/2106.13100"}}
{"text": "\\section{Introduction} \n\\label{sec:introduction}\n\nThe problem of dealing with missing  or incomplete information in machine learning and computer vision arises naturally in many applications. Recent strategies make use of generative models to complete missing or corrupted data. Advances in computer vision using deep generative models have found applications in image/video processing, such as denoising \\cite{jain2009natural}, restoration \\cite{xu2014deep}, super-resolution \\cite{dong2016image}, or inpainting \\cite{xie2012image,newson2014video}. \n\nWe focus on image and video inpainting tasks, that might benefit from novel methods such as Generative Adversarial Networks (GANs) \\cite{pathakCVPR16context} or Residual connections \\cite{he2016deep,mao2016image}. Solutions to the inpainting problem may be useful in a wide variety of computer vision tasks. In this study, we focus on three important applications of image and video inpainting: human pose estimation, video de-captioning and fingerprint recognition. Regarding the former task, it is challenging to perform human pose recognition in images containing occlusions. It  still  is a present problem in most realistic scenarios. Since tracking human pose is a prerequisite for human behavior analysis in many applications, replacing occluded parts may help the whole processing chain. Regarding the second task, in the context of news media, video entertainment and broadcasting programs from various languages (such as news, series or documentaries), there are frequently text captions or embedded commercials or subtitles. These reduce visual attention and occlude parts of frames, potentially decreasing the performance of automatic understanding systems. Finally,\nthe problem of  validation of fingerprints in images with occlusions which has applications mostly in forensics and security systems. Despite recent advances in machine learning, it is still challenging to aim at fast (real time) and accurate automatic removal of occlusions (text, objects or stain) in images and video sequences. \n\nIn this chapter we study recent advances in each given task. Specifically, we conduct the \\emph{ChaLearn Looking at People Inpainting Challenge} that comprised three tracks, one  for each task. Such event had a satellite event collocated with ECCV2018. Herein we explain the challenge setups, provided datasets, baselines and challenge results for each task. The main contributions of this challenge are: the organization of an academic competition that attracted a number of participants to deal with the inpainting problem; the release of three novel datasets that will foster research in visual inpainting; an evaluation protocol that will remain open so that new methodologies can be compared with those proposed in the challenge. The data can be downloaded from:\n\\begin{itemize}\n\\item Still images of humans: \\\\https://chalearnlap.cvc.uab.cat/dataset/30/description/\n\\item Video decaptioning: \\\\https://chalearnlap.cvc.uab.cat/dataset/31/description/\n\\item Fingerprint reconstruction and denoising: \\\\https://chalearnlap.cvc.uab.cat/dataset/32/description/\n\\end{itemize}\n\nThe remainder of the chapter is organized as follows. In Section~\\ref{sec:track1} we explain the  inpainting of  human images track where images  are occluded with random blocks. In Section~\\ref{sec:track2} we elaborate on the problem of inpainting of video clips subtitles with different size and color. Next, we describe the inpainting of noisy fingerprints track in Section~\\ref{sec:track3}, where  noise is added artificially with the aid  of different patterns. Finally, we discuss conclusions and future directions of research in Section~\\ref{sec:conclusion}.\n\n\n\\section{Inpainting still images of humans} \\label{sec:track1}\n\nHuman subjects are important targets in many images and photographs. In this sense, having an image with clear details and without noise is critical for some applications in uncontrolled environments. For instance, it is quite likely to remove an object that occludes part of the body and replace it with a realistic background. This has an instant application in media production or security cameras. It can also be used as an intermediate task to help other applications like human pose or gesture estimation.\n\nThe human pose has many degrees of freedom, giving the human apperance very high variability. This coupled with cluttered backgrounds, different viewpoints and differences in foreground/background illumination, pose great challenge to the task of inpainting parts of the human body in still images.\nThe lack of temporal context in highly masked images adds an extra difficulty for these approaches. Therefore, an algorithm must be aware of global context while generating consistent and realistic local patches for masked areas.\n\nIn this track we tackle inpainting still images of humans with different level of missing areas. As a standard approach in the inpainting domain, we artificially mask out several blocks around body joints with random position and size. Given an RGB image and its mask in training time, a participant's method must be able to accurately reconstruct the masked region in test time. Evaluation is done based on standard similarity metrics. Given inpainting as an intermediate task, we also evaluate accuracy of each reconstructed image in human pose. That is, a pretrained method is used to estimate body pose based on reconstructed image and an error is computed based on estimated 2D joints.\n\n\\subsection{Data}\nFor this dataset we collected images from multiple sources that can be seen in Table~\\ref{tab:sources}. They have been selected because they provide a high variability of content while being representative enough to allow the resulting methods to generalize. We randomly split samples into train/val/test sets by 70/15/15 \\% ratio, respectively.\n\\begin{table}[h]\n\t\t\\centering\t\t\n                \\small\n\t\t\\caption{Image source overview, note that the number of images used is not the same as the original size of the dataset, this is due to extracting multiple human crops from the original image and filtering out some not suitable for the planned task.}\\label{tab:sources} \n\t\t\\resizebox{\\linewidth}{!}{\\begin{tabular}{|l|l|l|}\\toprule\n\t\t\t\\textbf{Name} & \\textbf{\\#Images Used} & \\textbf{Cropped} \\\\\n\t\t\t\\midrule\n\t\t\tMPII Human Pose Dataset \\cite{andriluka14cvpr} & 26571 & Yes \\\\\n\t\t\tLeeds Sports Pose Dataset \\cite{Johnson10}& 2000 & No\\\\\n\t\t\tSynchronic Activities Stickmen V \\cite{eichner2012human} & 1128 & Yes\\\\\n\t\t\tShort BBC Pose \\cite{charles2013domain}& 996 & No\\\\\n\t\t\tFrames Labelled In Cinema \\cite{modec13}& 10381 & Yes\\\\ \n\t\t\t\\bottomrule\n\t\t\\end{tabular}}\n\\end{table} \n\\vspace{0.5cm}\n\\paragraph{Preprocessing}\n To make the joint annotations consistent we made sure that all the selected datasets used the same criteria when annotating each body part. It is important to note that while not every image has all joints annotated, the annotations of all the joints of the same type are consistent. In Figure~\\ref{fig:joints} one can see which joint positions we consider valid. In Table~\\ref{tab:jointComparision} we can see which joints we extracted from each dataset. The only ones that appear through all the datasets are \\textbf{wrists}, \\textbf{elbow} and \\textbf{shoulder}.\n \n\\begin{figure}[h]\n\\centering\n\t\\includegraphics[width=0.35\\textwidth]{imgs/joints.png}\n\t\\caption{Position of the selected joints.}\n\t\\label{fig:joints}\n\\end{figure}\n\n\n\\begin{table*}[h!]\n\t\t\\centering\t\t\n\t\t\\caption{Check-marks indicate that either the joint appears in some images of the dataset, or it can be extracted from the original annotations. Joints refer to 1-\\textbf{Ankles}, 2-\\textbf{Knees}, 3-\\textbf{Hips}, 4-\\textbf{Pelvis}, 5- \\textbf{Thorax}, 6-\\textbf{Upper Neck}, 7-\\textbf{Head Top}, 8-\\textbf{Wrists},  9-\\textbf{Elbows}, 10-\\textbf{Shoulders}, 11-\\textbf{Eyes}, 12-\\textbf{Nose}}\n\t\t\\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}\\toprule\n\t\t\t\\textbf{Name} & \\textbf{\\#Joints} & \\textbf{1} & \\textbf{2} & \\textbf{3}& \\textbf{4} & \\textbf{5} & \\textbf{6} & \\textbf{7}& \\textbf{8}& \\textbf{9} & \\textbf{10} & \\textbf{11} & \\textbf{12}\n\t\t\t\\\\\n\t\t\t\\midrule\n\t\t\tMPII & 19 & \\checkmark & \\checkmark & \\checkmark & \\checkmark &\\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark& & \\\\\n\t\t\tLSP & 8 & \\checkmark & \\checkmark & \\checkmark& & &\\checkmark & \\checkmark& \\checkmark & \\checkmark & \\checkmark & & \\\\\n\t\t\tSynch & 6 & & & & & \\checkmark & \\checkmark& \\checkmark&\\checkmark &\\checkmark & \\checkmark& &\\\\\n\t\t\tBBC & 4 & & & & & & &\\checkmark &\\checkmark & \\checkmark& \\checkmark& &\\\\\n\t\t\tFLIC & 6 & & & \\checkmark& & & & & \\checkmark& \\checkmark&\\checkmark &\\checkmark &\\checkmark\\\\ \n\t\t\t\\bottomrule\n\t\t\\end{tabular}\\label{tab:jointComparision}\n\\end{table*}\n\n\n\n\\noindent\nSome adjustments had to be done to ensure that the annotations were consistent:\n\t\n\\begin{itemize}\n\t\\item We removed the information about right and left of the joints since some of the datasets were using the camera as reference and others the human subject.\n\t\\item In the Synchronic dataset we did not have a thorax joint, but, from what we could see, it contained a vertical line through the upper body and the middle of that line was pretty similar to the thorax position, so we decided to use those instead.\n\\end{itemize}\n\n\\paragraph{Masking} \nFor each image we generated a different mask to hide parts of the image. Algorithms will be evaluated on how well they can restore the parts of the image occluded by this mask. To avoid having fixed bounds we set the following procedure: masks consist of $N$ blocks, where $0<N<11$ for each image, each block being a square of size $s$ ranging from $$\\frac{min(w,h)}{20}<s<\\frac{min(w,h)}{3}$$ where $w$ and $h$ are the width and height of the image, respectively. The blocks cover joints in part and are randomly positioned around the center of the joint. The blocks do not overlap and have a margin of $100px$ from the image's edge. At most $70\\%$ of the image is masked. \n\n\\subsection{Baselines}\nThe baselines decided for track 1 are adaptations of ~\\cite{pathakCVPR16context, Yang_2017_CVPR, yeh2017semantic} that all involve deep convolutional architectures. Below are descriptions of the considered baselines:\n\\begin{itemize}\n\t\\item {\\bf Context encoders}.~\\cite{pathakCVPR16context}. This method uses an autoencoder model along with a generative adversarial model (GAN). It was trained by a conjunction of a reconstruction loss (normalized masked L2) and an adversarial loss provided by the discriminator. \n\t\\item {\\bf Multi-scale neural patches}~\\cite{Yang_2017_CVPR}. This baseline is focused on obtaining high resolution results. To do so it uses a model based on two networks, a content network tasked with creating an initial approximation and a texture network which is tasked with adding high frequency details by constraining the texture of the generated image according to the texture of the non masked area. Context Encoders are used as the global content prediction network which serves as initialization for the multi-scale algorithm.\n    \\item {\\bf Semantic inpainting}~\\cite{yeh2017semantic}. This model learns to generate new samples from the dataset, which means it learns the manifold where the dataset exists. Then they infer a reconstruction by finding the point on this manifold closest to the encoded masked image.\\\\\n\\end{itemize}\nWe trained these baselines on the same train set used for the competition. Some qualitative results are shown in Table \\ref{tab:samples_t1}. As one can see, the baselines perform quite well on small parts but visual quality must be improved when inferring bigger missing regions.\n\n\\begin{table*}[htb]\n  \\begin{center}\n  \\begin{tabular}{cc@{\\hspace{2em}}ccc@{\\hspace{2em}}c}\n  Image & Masked & \\cite{pathakCVPR16context} & \\cite{yeh2017semantic} & \\cite{Yang_2017_CVPR} & Pose Estimation\\\\\n\\includegraphics[width=0.1\\textwidth]{samples/t1_0_0.png} & \n  \\includegraphics[width=0.1\\textwidth]{samples/t1_0_1.png} & \\includegraphics[width=0.1\\textwidth]{samples/t1_0_2.png} & \\includegraphics[width=0.1\\textwidth]{samples/t1_0_3.png} &\n  \\includegraphics[width=0.1\\textwidth]{samples/t1_0_4.png} &\n  \\includegraphics[width=0.1\\textwidth]{samples/t1_0_5.png}\\\\\n   DSSIM: & - & 0.306 & 0.351 & 0.312 & -\\\\\n  \\includegraphics[width=0.1\\textwidth]{samples/t1_1_0.png} & \n  \\includegraphics[width=0.1\\textwidth]{samples/t1_1_1.png} & \\includegraphics[width=0.1\\textwidth]{samples/t1_1_2.png} & \\includegraphics[width=0.1\\textwidth]{samples/t1_1_3.png} &\n  \\includegraphics[width=0.1\\textwidth]{samples/t1_1_4.png} &\n  \\includegraphics[width=0.1\\textwidth]{samples/t1_1_5.png}\\\\ \n  DSSIM: & - & 0.135 & 0.231 & 0.172 & -\\\\ \n\n  \\includegraphics[width=0.1\\textwidth]{samples/t1_2_0.png} & \n  \\includegraphics[width=0.1\\textwidth]{samples/t1_2_1.png} & \\includegraphics[width=0.1\\textwidth]{samples/t1_2_2.png} & \\includegraphics[width=0.1\\textwidth]{samples/t1_2_3.png} &\n  \\includegraphics[width=0.1\\textwidth]{samples/t1_2_4.png} &\n  \\includegraphics[width=0.1\\textwidth]{samples/t1_2_5.png} \\\\\n DSSIM: & - & 0.353 & 0.477 & 0.353 & -\\\\\n  \\end{tabular}\n  \\caption{Qualitative and quantitative samples of chosen baselines for track 1 (resized). DSSIM to the original has been added below the reconstructions. Pose estimation (last column) done over \\cite{Yang_2017_CVPR} reconstruction. }\\label{tab:samples_t1}\n  \\end{center}\n\\end{table*}\n\n\\subsection{Competition results}\nTo evaluate results, we used traditional image similarity metrics such as mean squared error (MSE) and structural dissimilarity (DSSIM)~\\cite{wang2004image}. Additionally, we proposed a new highly domain specific metric, Weakly Normalized Joint Distance (henceforth WNJD), which evaluates the performance of a state of the art pose estimation technique on the image before and after the reconstruction. It can be calculated for each image with:\n$$WNJD = \\frac{\\sum_{i=0}^{N}{\\left|OJoint_i - PJoint_i\\right|}}{N\\left|\\left|(w,h)\\right|\\right|},$$\nwhere $OJoint$ and $PJoint$ are vectors containing the original and predicted joints, respectively. $N$ is the number of labeled joints and $w,h$ are the width and height of the image. Finally, due to the emphasis on the human pose domain of the problem the submissions were evaluated using a mean rank of multiple metrics. \n\nWe used \\cite{newell2016stacked} as pose estimation algorithm, which presents an architecture called hourglass which is stated to be able to capture information from all possible scales. Each hourglass module is an autoencoder with residual connections from encoder convolutional layers to corresponding decoder ones. A number of hourglass modules are stacked sequentially, a loss is applied on the output of each stack and the whole model is trained jointly. Finally, the joints are extracted from output joint heatmaps. In the competition, this information was withheld from the participants to avoid unfair advantages.\n\nThe final results on the competition test set can be seen in Table \\ref{tab:testT1Res}. We also show some qualitative images of participants methods in Figure~\\ref{tab:qualitative_t1}.\n\n\\begin{table}\n\\centering\n\\caption{Test set results for track 1.}\n\\label{tab:testT1Res}\n\\resizebox{\\linewidth}{!}{\\begin{tabular}{@{}llllll@{}}\n\\toprule\n\\textbf{\\#} & \\textbf{User} & \\textbf{Rank} & \\textbf{MSE} & \\textbf{DSSIM} & \\textbf{WNJD} \\\\ \\midrule\n1           & UNLU         & 1.3          & 0.0158 (1)   & 0.2088 (2)   & 0.1489 (1)    \\\\\n2           & Inception       & 1.7           & 0.0158 (2)   & 0.2048 (1)   & 0.1495 (2)    \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\\begin{table*}[!htbp]\n  \\begin{center}\n  \\begin{tabular}{ccc}\n  Ground truth & UNLU & Inception\\\\\n\\includegraphics[width=0.24\\textwidth]{T1/gt__1_.png} & \\includegraphics[width=0.24\\textwidth]{T1/unlu__1_.png} & \\includegraphics[width=0.24\\textwidth]{T1/anubha__1_.png} \\\\\n\\includegraphics[width=0.24\\textwidth]{T1/gt__3_.png} & \\includegraphics[width=0.24\\textwidth]{T1/unlu__3_.png} & \\includegraphics[width=0.24\\textwidth]{T1/anubha__3_.png} \\\\\n\\includegraphics[width=0.24\\textwidth]{T1/gt__5_.png} & \\includegraphics[width=0.24\\textwidth]{T1/unlu__5_.png} & \\includegraphics[width=0.24\\textwidth]{T1/anubha__5_.png} \\\\\n\\includegraphics[width=0.24\\textwidth]{T1/gt__7_.png} & \\includegraphics[width=0.24\\textwidth]{T1/unlu__7_.png} & \\includegraphics[width=0.24\\textwidth]{T1/anubha__7_.png} \\\\\n\\includegraphics[width=0.24\\textwidth]{T1/gt__8_.png} & \\includegraphics[width=0.24\\textwidth]{T1/unlu__8_.png} & \\includegraphics[width=0.24\\textwidth]{T1/anubha__8_.png} \\\\\n  \\end{tabular}\n  \\caption{Qualitative samples for track 1.}\\label{tab:qualitative_t1}\n  \\end{center}\n\\end{table*}\n\n\n\\subsection{Discussion}\n\nFrom the results of the competition we can see that most of the approaches, from participants or baselines, involved the use of a GANs. The main differences with the solutions were on the Loss or mixture of Losses used to train the GAN and the architecture. These differences can be seen in Table~\\ref{tab:T1methSumm}.\n\n\\begin{table*}\n\\centering\n\\caption{Main approach differences between participant approaches.}\n\\begin{threeparttable}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}lllllcccc@{}}\n\\toprule\n\\multirow{2}{*}{\\# } & \\multirow{2}{*}{Work Name} & \\multicolumn{1}{c}{\\multirow{2}{*}{Input Size}} & \\multicolumn{1}{c}{\\multirow{2}{*}{Pretraining}}                                                          & \\multicolumn{3}{c}{Generator}                                                                                                                    & \\multicolumn{1}{c}{\\multirow{2}{*}{Discriminators}} \\\\ \\cmidrule(lr){5-7}\n                                                       & \\multicolumn{1}{c}{}                           & \\multicolumn{1}{c}{}                            & \\multicolumn{1}{c}{}                                                                                      & \\multicolumn{1}{c}{S\\footnotemark[1]} & \\multicolumn{1}{c}{D\\footnotemark[2]} & \\multicolumn{1}{c}{Loss}                                                                      & \\multicolumn{1}{c}{}                                \n\\\\ \\midrule\n                                                       \n\n1 & \\begin{tabular}[c]{@{}l@{}} People Inpainting with \\\\Generative Adversarial \\\\ Networks \\end{tabular}\n& 256x256\n& \\begin{tabular}[c]{@{}l@{}}Generative Image Inpainting \\\\with Contextual Attention\\\\ (Places2)\\end{tabular}\n& N & Y\n& \\begin{tabular}[c]{@{}l@{}}Generalized \\\\ Loss-Sensitive GAN \\end{tabular}\n& Global + Local \\\\\n\\\\\n2 &\\begin{tabular}[c]{@{}l@{}} Generative Image \\\\Inpainting for Person\\\\Pose Generation \\end{tabular}\n& 128x128\n& \\begin{tabular}{@{}l@{}}None\\\\ (VGG used to\\\\ calculate loss)\\end{tabular}                               & Y & Y\n& \\begin{tabular}[c]{@{}l@{}}Mixture:\\\\ {[}L1 \\& Adversarial \\\\ \\& Perceptual(VGG){]}\\end{tabular} \n& Global\\\\\n\\bottomrule\n\\end{tabular}}\n\\begin{tablenotes}\n      \\footnotesize\n      \\item[1] Skip-connections\n      \\item[2] Dilated Convolutions\n\\end{tablenotes}\n\\end{threeparttable}\n\\label{tab:T1methSumm}\n\\end{table*}\n\nAnother interesting point to note is that while both approaches transferred knowledge from networks trained in other domains, they used different methods to do so. One of them used pretraining and the other one incorporated the knowledge into the loss function, by using VGG to evaluate the quality of the reconstruction.\\\\\nWe believe that these results indicate that GANs have consolidated their place as a strong competitor for inpainting problems, and that new methods live and die by their ability to correctly evaluate the quality of the reconstruction. We can see the field moving from using the original image as the only solution and instead evaluating each solution on their own merits, independently of how much they resemble the original. This is probably one of the main contributing factors behind the predominance of GANs. We can also see that there is no generalized standard to evaluate these possible solutions which causes most of the methods to focus heavily on loss engineering.\\\\\n\nIn this competition though we did not want to only evaluate the image reconstruction quality. We also wanted to check how capable an inpainting model was to store information from an specific domain such as human poses. To that end we can see a more fine grained breakdown of the mean WNJD for each joint type on Table~\\ref{tab:T1joint}\n\n\\begin{table*}[]\n\\centering\n\\caption{Mean WNJD for all predictions of each \\textbf{predicted} joint type in the test set. Lower is better. \\textbf{RK} - Right Knee, \\textbf{RH} - Right Hip, \\textbf{RW} - Right Wrist,\\textbf{RE} - Right Elbow, \\textbf{RS} - Right Shoulder, \\textbf{RA} - Right Ankle, \\textbf{P} - Pelvis, \\textbf{LE} - Left Elbow, \\textbf{LS} - Left Shoulder, \\textbf{LK} - Left Knee,\\textbf{LH} - Left Hip, \\textbf{LW}  Left Wrist, \\textbf{T} - Torso, \\textbf{LA} - Left Ankle}\n\\begin{tabular}{@{}llllllllllllllll@{}}\n\\toprule\n\\textbf{\\#} & \\textbf{User} & \\textbf{RK} & \\textbf{RH} & \\textbf{RW} & \\textbf{RE} & \\textbf{RS} & \\textbf{RA} & \\textbf{P} & \\textbf{LE} & \\textbf{LS} & \\textbf{LK} & \\textbf{LH} & \\textbf{LW} & \\textbf{T} & \\textbf{LA} \\\\ \\midrule\n1 & UNLU &  \\textbf{0.07} &  \\textbf{0.16} &  \\textbf{0.24} &  \\textbf{0.13} &  \\textbf{0.12} &  \\textbf{0.16} & \\textbf{0.06} &  \\textbf{0.11} &  \\textbf{0.10} &  \\textbf{0.06} &  \\textbf{0.09} &  \\textbf{0.19} & \\textbf{0.08} &  \\textbf{0.10} \\\\\n2 & Inception & 0.51 & 0.46 & 0.40 & 0.38 & 0.39 & 0.57 & 0.32 & 0.28 & 0.26 & 0.34 & 0.31 & 0.31 & 0.47 &  0.35  \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:T1joint}\n\\end{table*}\nAs we can see there was a definitive difference between the quality of the human pose reconstruction between the approaches, as one model outperformed the other in every type of joint reconstructed. It is notable that no model added any kind of handcrafted feature or used pretrained networks specific to the human domain. All the human domain knowledge was learned from this dataset.\n\n\\section{Video decaptioning} \\label{sec:track2}\nVideo data is nowadays collected everywhere, as well as shared on various internet platforms in an extraordinary exponential growing., This offers a wide diversity of contents to final users, e.g. more than 400 hours of new video are uploaded to Youtube every single minute\\cite{statista}. Live streaming from social media platforms also offer a huge quantity of video to final users, that for some poor network reasons might be degraded to low resolution. In various situations, available video data might benefit from recent advances in computer vision to enhance its quality and improving the user experience. \\\\\n\nExisting similar tasks, such as automatic video denoising or video coloration allow for restoring ancient archives that constitute an effective way of preserving such rich source of cultural knowledge. Video inpainting has multiple forms because of the spatio-temporal nature of such signal: in the temporal axis, inpainting might consists in inferring entire frames within a given video sequence, leading to artificial slow motion effect \\cite{slomo-nvidia}. For both spatial and temporal axis, video inpainting replaces missing or occluding parts in video sequences with semantically relevant pixels. In particular, removing occluding parts in video sequences might lead to better automatic object detection and indexing systems, especially for autonomous vehicle vision systems. From the user point of view, text overlays in news or media programs reduce the visual attention, making hard to focus on the original video message. \\\\\n\nThis section introduces the ChaLearn LaP Inpainting Challenge Track on video decaptioning. The video decaptioning task might be viewed as a simplified but challenging and realistic scenario of the video inpainting task. We define the video decaptioning task in a supervised setting, where the goal is to generate a decaptioned version of a captioned video sequence. Addressing this task must resolve the complex task of replacing overlaid captions with original set of semantically coherent pixels in the video sequence. We propose the first large dataset of paired training data that contains both degraded and groundtruth data from natural, wide and diverse video sequences. We hope that such large video dataset can push deep learning based model to resolve this challenging task.\n\n\\subsection{Data}\nWe collected a large brand new dataset for the video inpainting track of the ChaLearn LaP Inpainting Challenge. The dataset is composed of pairs of the form (X, Y), where X is a 5 seconds video clip (containing embedded text) and Y the corresponding target video clip (without embedded text, see Figure~\\ref{sample}). The participants in this track must develop a method that receives as input the video with captions and provide as output the decaptioned video. \n\n\\paragraph{Sources}\nWe collected about 150 hours of diverse videos of TV movies of various genres with associated generated subtitles available from YouTube. The various video contents are intended to be as diverse as possible, making the dataset almost generic for subtitle removal task. Data sources were checked to fulfill the required copyrights to be reused during this challenge for research purposes.\n\n\\paragraph{Preprocessing}\nRaw videos were  processed in order to obtain a set of well formed short video sequences: we removed letter-boxing area (top/down black bars where subtitles could be printed), normalized frame rate to 25 frames per second and rescaled frames to width=256 keeping aspect ratio, then cropped the central region with size $256\\times256$. In order to create the set X of corrupted videos, we hard printed subtitles by applying several variations in fonts and text style (size, color, italic/bold, background box transparent/opaque). Finally, we split the original and corrupted videos in 5 seconds non-overlapped video clips. \nWe kept 70\\% of the clips that contain the most subtitles that represents about 90,000 video clips with 125 frames of 256x256 pixels. Finally, we split the resulting video clips into training (60\\% of clips), validation (20\\% ) and, test (20\\%) sets. \n\nIn Figure~\\ref{sample} a few examples are shown. The figure illustrates the diversity of video content, style and resolution, as well as the vaeried font transformations and subtitle overlays. We also notice a few examples containing text or logo in both corrupted and ground truth clips.\n\\begin{figure*}[htbp] \n\\begin{center}\n\\includegraphics[scale=0.4]{imgs/X770112.png}\n\\includegraphics[scale=0.4]{imgs/X5301563.png}\n\\includegraphics[scale=0.4]{imgs/X10003120.png}\n\\includegraphics[scale=0.4]{imgs/X2000092.png}\\\\\n\\vspace{0.1cm}\n\\includegraphics[scale=0.4]{imgs/Y770112.png}\n\\includegraphics[scale=0.4]{imgs/Y5301563.png}\n\\includegraphics[scale=0.4]{imgs/Y10003120.png}\n\\includegraphics[scale=0.4]{imgs/Y2000092.png}\n\\caption{Few dataset samples. On top, corrupted X frames. At bottom, groundtruth Y frames}\n\\label{sample}\n\\end{center}\n\\end{figure*}\n\n\\subsection{\\bf Novelty of the dataset}\n\nThis dataset is the largest and more diverse video dataset especially designed for the inpainting task. The nature and the diversity of video content make it both, challenging and of practical relevance, \nLarge variations of text overlays offers various kind of inpainting contributions, from denoising methods (for samples where overlay is small, font is well contrasted and background transparent), to large missing region (where background is opaque and overlay is large). Dealing with such variations and video contents lead to a difficult and challenging dataset. \\\\\n\n\\noindent\nMissing parts (i.e., captioned area) in the video dataset have various sizes, since overlays size vary with the font size as well as length of subtitles. Figure~\\ref{fig:blockhists} shows the distribution of missing parts ratio, highlighting the various difficulty of video samples, up to 50\\% of frames to replace. Variations in number of pixels makes the proposed task challenging but realistic.\n\\begin{figure}[h]\n\\begin{center}\n\t\\includegraphics[width=0.9\\linewidth]{imgs/maskData.png}\n\t\\caption{Distribution of different missing parts on the video dataset.}\n\t\\label{fig:blockhists}\n\\end{center}\n\\end{figure}\n\n\\noindent\nWe provide metadata with this dataset, consisting of every video sources, timestamped subtitles text, overlay size and background style for each video clip. Such metadata allows for analyzing systems performance according to overlays variations.\n\n\\subsection{Baselines}\n\nThe video track's main objective can be achieved by multiple auxiliary tasks, i.e.: detect regions that contain text, video segmentation, video/patch generation, etc. We implemented baselines from recent state of the art methods from the image inpainting domain. We adapted models from ~\\cite{pathakCVPR16context, Yang_2017_CVPR, yeh2017semantic}, which  involve deep convolutional encoder-decoder architectures with reconstruction loss. We do not use temporal information, neither adversarial loss, giving room for lots of possible improvements during the challenge.\n\n\\begin{itemize}\t\n    \\item {\\bf Baseline 1} considers the whole frames as global context to infer missing parts, such as the complete spatial structure  to generate complete frames. We use a deep convolutional autoencoder, with a VGG-like encoder consisting of 3 blocks of 2 convolutional layers, batch normalization and relu non-linearities, with respectively 64, 128 and 256 filters of size 3x3 pixels. The decoder is composed of 4 blocks of convolutional, upsampling, batch normalization and relu activation. This model is heavy in terms of parameters and memory usage, since it assumes inputs and outputs of size 256x256x3.\n\t\\item {\\bf Baseline 2} processes frames by patches of 32x32 pixels, allowing to reduce the number of parameters. A patch auto-encoder (similar but lighter than first baseline) is jointly learned with a patch classifier that detects the presence of text. At inference, a frame is first split into non-overlapped patches. Then if the classifier informs about the presence of text, the patch is passed through the decoder. Patches classified as non-text are simply copied to the generated output frame. This approach is faster to train and efficient to remove text with no background overlays. However, since it ignores the context, it cannot perform well on patches containing only missing values.\n\\end{itemize} \n\n\\subsection{Competition results}\n\nTables~\\ref{validationTable2} and~\\ref{testTable2} show the results of this track obtained in both validation and test phases of the Challenge, respectively. We report three evaluated metrics: MSE, peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). \nDuring the first phase, 35 different participants registered and downloaded the starting kit. Among them, 18 did at least one submission (not always valid), and 7 participants kept active. In total, we received 50 submissions, Table~\\ref{validationTable2} and ~\\ref{testTable2} list scores reached by active participants. \\\\\n\n\\begin{table}[]\n\\centering\n\\caption{Validation Results of track 2.}\n\\label{validationTable2}\n\\resizebox{\\linewidth}{!}{\\begin{tabular}{@{}llllll@{}}\n\\toprule\n\\textbf{\\#} & \\textbf{User} & \\textbf{Rank} & \\textbf{MSE} & \\textbf{PSNR} & \\textbf{SSIM} \\\\ \\midrule\n1           & \\textbf{hcilab}    \t& 3.3           & 0.0013 (1)   & 32.6001 (1)   & 0.0439 (8)    \\\\\n2           & \\textbf{arnavkj95}    & 3.6           & 0.0014 (2)   & 31.9629 (2)   & 0.0512 (7)    \\\\\n3           & \\textbf{dhkim}        & 4.0           & 0.0015 (3)   & 31.0613 (3)   & 0.0516 (6)    \\\\\n4           & anubhap93  \t\t\t& 4.3           & 0.0016 (6)   & 30.5311 (4)   & 0.0610 (5)    \\\\\n5           & mcahny01     \t\t    & 4.6           & 0.0018 (5)   & 29.9306 (6)   & 0.0751 (3)    \\\\\n6           & ucs    \t\t\t \t& 4.6           & 0.0021 (4)   & 28.6957 (7)   & 0.0935 (1)    \\\\\n7           & \\textbf{baseline2}\t& 5.3           & 0.0023 (7)   & 30.0993 (5)   & 0.0621 (4)    \\\\\n8           & yangchris11    \t\t& 6.0           & 0.0045 (8)   & 27.0632 (8)   & 0.0876 (2)    \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\nAt the test phase, some participants merged their submissions, constituting a new team, while some did not submit any factsheets or codes as required to officially claim prizes. Table~\\ref{testTable2} list every participants submission (in italic we show discarded participants).\n\\begin{table}[]\n\\centering\n\\caption{Test set Results of track 2.}\n\\label{testTable2}\n\\resizebox{\\linewidth}{!}{\\begin{tabular}{@{}llllll@{}}\n\\toprule\n\\textbf{\\#} & \\textbf{User} & \\textbf{Rank} & \\textbf{MSE} & \\textbf{PSNR} & \\textbf{SSIM} \\\\ \\midrule\n1           & \\textbf{SanghyunWoo}    \t& 2.6           & 0.0011 (1)   & 33.3527 (1)   & 0.0404 (6)    \\\\\n2           & \\textbf{hcilab}  \t\t    & 3.0           & 0.0012 (3)   & 33.0228 (2)   & 0.0424 (4)    \\\\\n3           & \\textbf{\\textit{ucs}}    \t& 3.3           & 0.0011 (2)   & 33.0052 (3)   & 0.0410 (5)    \\\\\n4           & \\textbf{anubhap93}  \t\t& 3.6           & 0.0012 (4)   & 32.0021 (5)   & 0.0499 (2)    \\\\\n5           & arnavkj95     \t\t    & 4.0           & 0.0012 (5)   & 32.1713 (4)   & 0.0482 (3)    \\\\\n6           & \\textbf{baseline2}\t\t& 4.3           & 0.0022 (6)   & 30.1856 (6)   & 0.0613 (1)    \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\\subsection{Discussion}\n\nMost of active participants did succeed in improving baseline methods. Next we briefly discuss on differences of proposed methods and their impact on inpainting performance.\\\\\n\n\\begin{itemize}\t\n\\item{\\textbf{SanghyunWoo}} used a U-Net based architecture combined with 3D and 2D gated convolutions to predict the middle frame of N given frames. Gated convolution allows for learning an attention mechanism in intermediate feature maps, while temporal dependence aims at taking advantage of the video structure. Training is performed by optimizing a loss that combines L1 reconstruction term, SSIM perceptual score, and Gradient L1 loss. \\\\\n\\item{\\textbf{hcilab}} trained a U-Net model in a multi-stage procedure, where output of the previous stage is given as input of next stage, in a recurrent manner. The use of skip connections from U-Net in addition to an iterative scheme increase significantly the inpainting performance, compared to initial baseline2 performance. \\\\\n\\item{\\textbf{anubhap93}} used an almost standard U-Net architecture with dilated convolutions. Their model was trained by combining MSE, SSIM and GAN losses. Dilated convolutions seem to significantly improve the generation quality. \\\\\n\\item{\\textbf{arnavkj95}} used a standard U-Net architecture, trained with MSE loss. \\\\\n\\end{itemize}\n\nWe notice that only the winning team did take account of temporal context in video sequences to perform decaptioning. However, it is not clear whether they reached top performance due to this characteristic, since they are also the only team using gated convolutions. GANs were not intensively considered during this challenge, only one team combined Adversarial loss with MSE and SSIM losses, with a sensibly low impact, probably due to the difficulty and unstable nature of such training. The U-Net model has been widely considered as an effective encoder-decoder architecture. Most participants extended this model to improve inpainting performance. \\\\\n\n\\begin{figure*}[htbp]\n\\begin{center}\n\t\\includegraphics[width=0.7\\textwidth]{imgs/track2qualitative2.png}\n\t\\caption{Qualitative comparison from the middle frame of three test samples.}\n\t\\label{fig:track2_quali}\n\\end{center}\n\\end{figure*}\n\n\nFigure~\\ref{fig:track2_quali} shows a qualitative comparison of participants outputs for three different examples that belong to three levels of difficulty with respect to the size of missing part to inpaint: \\textit{easy} samples mostly contain subtitles with small font, transparent background and few text; \\textit{medium} samples have bigger font, semi-transparent background, more text ; \\textit{difficult} samples contain a big part to replace (up to 50\\% of frame) corresponding to subtitles with black-opaque background. We note that  all methods perform well for easy sample. From the medium sample, we remark small texture variations in the curtain reconstruction from participants outputs, where the baseline output fails to reconstruct well the right texture. The U-Net architecture seems here to bring a significant positive impact, since only our baseline method did not use any skip connections within the encoder-decoder architecture. Hard samples are extreme case of inpainting problem where a large part must be replaced. All the methods fail in getting acceptable reconstruction and generate blurry pixels. However, watching carefully, it appears that all the methods (except baseline) succeed in reconstructing the global structure of missing parts (face, hairs, neck, jacket). More carefully, we can observe that 'hcilab' seemed to generate details slightly better than others, especially considering the shape of face, where 'arnavkj95' missed more details than others. The blurry effect in this hard case might mostly come from the MSE loss used by most participants. In this case, GANs objective should help in generating sharper and more accurate regions.\n\n\n\n\\section{Fingerprint reconstruction and denoising} \\label{sec:track3}\n\nBiometrics play an increasingly important role in security. They ensure privacy and identity verification, as evidenced by the increasing prevalence of fingerprint sensors on mobile devices. Fingerprint retrieval keeps also being an important law enforcement tool used in forensics. However, much remains to be done to improve the accuracy of verification, both in terms of false negatives (in part due to poor image quality when fingers are wet or dirty) and false positives (due to the ease of forgery). This track involves reducing noise (denoising) and/or replacing missing parts (inpainting) due to various kinds of alterations or sensor failures in fingerprint images. This is viewed as a preprocessing step to ease verification carried out either by humans or existing third party software. To protect privacy and have full control over the experimental design, we synthesized a (very) large dataset of realistic artificial fingerprints for the purpose of training learning machines.\n\nThus, the objective of participants was to develop algorithms that can inpaint and denoise fingerprint images that contain artifacts like noise, scratches, etc. Such procedures can be applied to improve the performance of subsequent operations like fingerprint verification. Developed algorithms were evaluated based on reconstruction performance. That is, participants were required to reconstruct the degraded fingerprint images using their developed algorithms and submit the reconstructed fingerprint images. After the submission, the reconstructed  images were compared against the corresponding ground-truth fingerprint images in the pixel space to determine the quality of the reconstructions.\n\n\\subsection{Data}\n\nWe generated images of fingerprints by first degrading synthetic fingerprints with a distortion model (blur, brightness, contrast, elastic transformation, occlusion, scratch, resolution, rotation), then overlaying the fingerprints on top of various backgrounds. The resulting images were typical of what law enforcement agents have to deal with. For training participants got pairs of original and distorted images. Their goal was to recover original images from distorted image on a test dataset. \n\nTraining set was constructed in the following two steps:  i) 75,600 $275\\times400$ pixel ground-truth fingerprint images without any noise or scratches, but with random transformations (at most five pixels translation and +/-10 degrees rotation) were generated by using the software Anguli: Synthetic Fingerprint Generator. ii) 75,600 $275\\times400$ pixel degraded fingerprint images were generated by applying random artifacts (blur, brightness, contrast, elastic transformation, occlusion, scratch, resolution, rotation) and backgrounds to the ground-truth fingerprint images. In total, it contains 151,200 fingerprint images (75,600 fingerprints, and two impressions - one ground-truth and one degraded - per fingerprint).\n\nValidation and test sets were constructed similarly to the training set and were used to evaluate the reconstruction performance. In total, each set contains 16,800 fingerprint images (8,400 fingerprints and two impressions - one ground-truth and one degraded - per fingerprint). Since this set was used for the purpose of evaluating the reconstruction performance, only the degraded and not the ground-truth fingerprint images were provided to participants.\n\n\\subsection{Baselines}\n\nA deep learning based baseline was used for reconstructing/enhancing the degraded fingerprint images with a standard deep convolutional neural network (CNN). To this end, a CNN was trained on the synthetic training set by minimizing the MSE loss function with Adam. The CNN comprises four convolutional layers, five residual blocks, two deconvolutional layers and another convolutional layer. Each of the five residual blocks comprises two convolutional layers. All of the layers except for the last layer are followed by batch normalization and rectified linear units. The last layer is followed by batch normalization and hyperbolic tangent units. The model is implemented in Chainer. \n\n\\subsection{Competition results}\n\nSimilar to the other tracks, the results of this track were evaluated based on three different metrics. The metrics were mean squared error (MSE), peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). Table \\ref{validationTable3} and \\ref{testTable3} show the final results of the validation phase and the test phase. Figure \\ref{fig:track_3_samples} shows qualitative results of a number of submissions. In total, there were 11 submissions in the validation phase and five submissions in the test phase. In the test phase, only four out of a potential 10 submissions outperformed the baseline, whereas in the validation phase, the baseline was outperformed by all four submissions. Interestingly, the only submissions in the test phase were those that were ranked above the baseline in the validation phase.\n\n\\begin{figure*}[h]\n\\begin{center}\n\t\\includegraphics[width=1\\textwidth]{imgs/track_3_samples.png}\n\t\\caption{Sample results of three Track 3 submissions.}\n\t\\label{fig:track_3_samples}\n\\end{center}\n\\end{figure*}\n\n\\begin{table}[]\n\\centering\n\\caption{Validation Results of track 3.}\n\\label{validationTable3}\n\\resizebox{\\linewidth}{!}{\\begin{tabular}{@{}llllll@{}}\n\\toprule\n\\textbf{\\#} & \\textbf{User} & \\textbf{Rank} & \\textbf{MSE} & \\textbf{PSNR} & \\textbf{SSIM} \\\\ \\midrule\n1           & \\textbf{CVxTz}         & 1.0           & 0.0191 (1)   & 17.6640 (1)   & 0.8426 (1)    \\\\\n2           & \\textbf{rgsl888}       & 2.3           & 0.0239 (2)   & 16.8363 (2)   & 0.8069 (3)    \\\\\n3           & \\textbf{hcilab}        & 3.3           & 0.0241 (3)   & 16.6062 (3)   & 0.8031 (4)    \\\\\n4           & sukeshadigav  & 4.0           & 0.0276 (6)   & 16.4162 (4)   & 0.8234 (2)    \\\\\n5           & baseline      & 5.0           & 0.0252 (5)   & 16.4098 (5)   & 0.7954 (5)    \\\\\n6           & finlouarn     & 5.3           & 0.0251 (4)   & 16.3992 (6)   & 0.7904 (6)    \\\\\n7           & Xiaojing      & 7.0           & 0.0381 (7)   & 14.6347 (7)   & 0.6990 (7)    \\\\\n8           & BriceRauby    & 8.0           & 0.0398 (8)   & 14.1740 (8)   & 0.6954 (8)    \\\\\n9           & go            & 9.0           & 0.0414 (9)   & 14.1710 (9)   & 0.6709 (9)    \\\\\n10          & yashkotadia   & 10.0          & 0.0564 (10)  & 12.7785 (10)  & 0.6417 (10)   \\\\\n11          & yg            & 11.0          & 0.7282 (11)  & 1.3781 (11)   & 0.0001 (11)   \\\\ \\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\\begin{table}[]\n\\centering\n\\caption{Test results of track 3.}\n\\label{testTable3}\n\\resizebox{\\linewidth}{!}{\\begin{tabular}{@{}llllll@{}}\n\\toprule\n\\textbf{\\#} & \\textbf{User}         & \\textbf{Rank} & \\textbf{MSE} & \\textbf{PSNR} & \\textbf{SSIM} \\\\ \\midrule\n1           & \\textbf{CVxTz}        & 1.0           & 0.0189 (1)   & 17.6968 (1)   & 0.8427 (1)    \\\\\n2           & \\textbf{rgsl888}      & 2.3           & 0.0231 (2)   & 16.9688 (2)   & 0.8093 (3)    \\\\\n3           & \\textbf{hcilab}       & 3.3           & 0.0238 (3)   & 16.6465 (3)   & 0.8033 (4)    \\\\\n4           & \\textbf{sukeshadigav} & 3.3           & 0.0268 (4)   & 16.5534 (4)   & 0.8261 (2)    \\\\\n5           & baseline           & 5.0           & 0.0241 (5)   & 16.4160 (5)    & 0.8234 (5)    \\\\ \\bottomrule\n\\end{tabular}}\n\\end{table}\n\n\\begin{table*}[]\n\\centering\n\\caption{Summary of the winning methods.}\n\\label{conclusionTable3}\n\\begin{tabular}{@{}lllll@{}}\n\\toprule\n\\textbf{\\#} & \\textbf{User} & \\textbf{Model} & \\textbf{Preprocessing} & \\textbf{Training} \\\\ \\midrule\n\\textbf{1} & \\textbf{CVxTz} & U-Net & \\begin{tabular}[c]{@{}l@{}}Normalization,\\\\ rescaling,\\\\ resizing\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Vanilla backprop with MSE,\\\\ data augmentation\\\\ (affine, contrast, saturation)\\end{tabular} \\\\\n\\textbf{2} & \\textbf{rgsl888} & \\begin{tabular}[c]{@{}l@{}}U-Net,\\\\ dilated conv\\end{tabular} & - & Vanilla backprop with MSE \\\\\n\\textbf{3} & \\textbf{hcilab} & Baseline & - & Iterative backprop with MSE \\\\\n\\textbf{4} & \\textbf{sukeshadigav} & M-Net & \\begin{tabular}[c]{@{}l@{}}Normalization,\\\\ padding,\\\\ resizing\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Vanilla backprop with MSE,\\\\ SSIM\\end{tabular} \\\\\n5 & baseline & Residual encoder & Resizing & Vanilla backprop with MSE \\\\ \\bottomrule\n\\end{tabular}\n\\end{table*}\n\n\\subsection{Discussion}\n\nThere were a number of similarities and differences between the winning methods and the baseline. A summary thereof is presented in Table \\ref{conclusionTable3}.\n\nThree out of four of the winning methods used a U-net like architecture and the remaining method used the same architecture as the baseline. Given that the most striking difference between the baseline and such U-net like architectures are skip connections and bottleneck layers, these results demonstrate the importance of these architectural components in the task of inpainting and denoising for fingerprint verification.\n\nNext to the architectural differences, there were other important factors that differentiated the higher ranking methods from the rest. In particular, it seems that the extensive use of data augmentation by the first placed team and the use of dilated convolutions by the second placed team were the main factors that separated them from the rest of the teams.\n\nAnother interesting observation was that almost all teams exclusively used the MSE loss function, which is notorious for causing artefacts in image generation tasks, suggesting that the noise ceiling on this dataset was not reached as the methods were held back by suboptimal loss functions.\n\nTaken together, the results are expected to be improved with more realistic forward models, adversarial/feature loss functions and perhaps variational methods. It would also be interesting to see how the current methods or such improved methods compare with physical models and transfer to real data.\n\n\\section{Conclusion and future directions of research} \\label{sec:conclusion}\nWe have described the design and results of a novel challenge focusing on inpainting in images and video. Three tracks were proposed, each associated to a realistic scenario. For each track we prepared new datasets and baselines, exposing the complexity of the tasks and showing the feasibility of solving them. Results obtained by participants were presented and analyzed. In all tracks, participants succeeded at improving the performance of baseline methods. Also, it is impressive the qualitative performance of proposed solutions. \n \nWe can outline the following conclusions and corresponding future research directions as derived from the challenge:\n\\begin{itemize}\n\\item \\textbf{Methodological similarity among solutions.} Results obtained by participants together with the undeniable similarity among solutions, all of them based on deep learning, confirm the establishment of specific architectures of deep learning (e.g., U-NET like architectures) for visual inpainting. In fact, the difference among solutions and their performance in some cases was only due to hyperparameter or subtle changes in the overall architecture of the models. This finding seems to suggest that a promising line of future research is that of the \\emph{automatic construction of deep learning models} (AutoDL)~\\cite{AutoDL}. That is, methods that automatically can determine the architecture and hyperparameters of a model given a specific task. This is in fact a growing area not only in terms of academic research but of industrial importance. \n\n\\item \\textbf{Feasibility of the approached tasks and potential impact.} The quantitative, but mainly the qualitative performance of solutions evidences the feasibility of the approached task: for the three proposed tracks, top ranked solutions represent satisfactory solutions that could be put in practice in short time. In this sense, the challenge was successful in providing academically challenging problems, but also in organizing tracks around problems of practical relevance. What is more, solutions are based on what can be considered domain public knowledge. This will surely motivate other researchers and practitioners to approach similar tasks.\n\n\\item \\textbf{Quality and quantity on newly released datasets.} As previously mentioned, a major contribution of the organization of the inpainting challenge lies in the resources being generated. We provided novel datasets, associated to realistic scenarios with a large potential of application. The released datasets are now public and comprise the largest resources to date in their corresponding application. We foresee these datasets will be decisive in the progress of the field in the forthcoming years. Likewise, we think that a promising line of research has to do with the generation of resources for visual inpainting, including novel ways of labeling huge amounts of data. \n\n\\item \\textbf{Evaluation metric / loss function.} Although we have used SSIM as structural similarity metric, the qualitative results show such a metric is not sufficient to evaluate realistic inpainting and perceptions. Training a network with SSIM or VGG-based perceptual loss can not guarantee a realistic inpainting to the unseen test data. This can be an interesting and important research topic in the future.\n\n\\end{itemize}\n\n\n\\section*{Acknowledgements}\n\nThe sponsors of ChaLearn Looking at People inpainting and denoising events are Google, ChaLearn, Amazon, and Disney Research. This work has been partially supported by the Spanish project TIN2016-74946-P (MINECO/FEDER, UE) and CERCA Programme / Generalitat de Catalunya. This work was also partially funded by the French national research agency (grant number ANR16-CE23-0006). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPU used for this research. This work is partially supported by ICREA under the ICREA Academia programme. We thank all challenge participants for their excellent contributions.\n\n\n\n\n\n", "meta": {"timestamp": "2021-06-25T02:20:53", "yymm": "2106", "arxiv_id": "2106.13071", "language": "en", "url": "https://arxiv.org/abs/2106.13071"}}
{"text": "\\section{Introduction}\n\nDespite advances in the fields of optics and nano-optics, the detection of fluorescent molecules remains a big challenge. In general, low excitation cross-sections at room temperature, low quantum yields of molecules and their dipolar emission patterns are only some of the problems that we have to face to detect low fluorescence signals.\nTo overcome these issues a lot of different techniques have been developed over the years, ranging from simple ones such as the use of high numerical aperture (NA) mirrors and objectives~\\cite{drechsler2001confocal,enderlein2000theoretical} to the complicated fabrication techniques for  creating nanostructures that can change the optical properties of the emitters and facilitate their detection, such as  optical microcavities~\\cite{kaupp2016purcell,huckabay2011whispering}, photonic nano-wires~\\cite{claudon2010highly,babinec2010diamond} and nano-antennas~\\cite{kinkhabwala2009large, langguth2017nano}. \nFor example, in the vicinity of plasmonic nanoparticles illuminated by laser light,  the excitation intensity can increase by orders of magnitude~\\cite{russell2012large, sugimoto2018hybridized}. In addition, plasmonic particles spectrally matched with fluorescent molecules can also increase the emission rate as well as the quantum efficiency, making them  significantly brighter~\\cite{bidault2016picosecond, flauraud2017plane, acuna2012fluorescence,lu2020quantum}.  It is also well known that some of these structures operate like antennas to change the radiation pattern of emitters and make them  radiate into smaller angles \\cite{aouani2011bright,akselrod2014probing, xia2021enhanced, kuhn2008modification}.\nFor example, a single quantum dot coupled to a Yagi-Uda  nano-antenna shows strongly polarized and highly directional luminescence~\\cite{curto2010unidirectional}. However, the production of these nano-objects demands complicated fabrication techniques and precise positioning of the emitters in the antenna's hotspot.\n\n\n\nRecently, a high collection efficiency up to 99\\% from a single molecule was demonstrated with dielectric \\cite{lee2011planar}  and metallo-dielectric \\cite{chu2014experimental} planar antennas. Since a planar antenna has a broadband resonance, the position of the emitter is not crucial and the fabrication of planar structures is relatively simple and straightforward. However, these antennas direct the radiation towards the microscope objective at wide angles, which requires a high-NA  objective for efficient collection. \n\nA planar Yagi-Uda antenna, which consists of two thin metallic films, a reflector and a director, not only beams the radiation pattern into smaller angles, but also enhances the emission rate of a molecule \\cite{checcucci2017beaming}. In this system, the distance from the emitter to the reflector is not a sensitive parameter and it can vary between $\\lambda / 6 n$ to $\\lambda / 4 n$ where, $n$ is the refractive index of the medium between them. This type of antenna can also enhance the collection efficiency for light sources embedded in high-refracting index materials~\\cite{galal2017highly,galal2019highly}.\n\nSo far, planar Yagi-Uda antennas have been designed to operate in a fixed configuration and the emitters are embedded in a solid matrix. \nHowever, tunable microcavities or antennas allow more operational freedom and they can change the optical properties of emitters in a controlled manner~\\cite{miguel2013cavity, chu2014experimental,kelkar2015sensing,wang2019turning, casabone2018cavity}, hence offering additional possibilities and paving the way towards potential applications.\nIn this work, we introduce and investigate a scanning implementation of a planar Yagi-Uda antenna, which could even be exploited for fiber collection~\\cite{soltani2019planar}. Our main goal is to examine how the antenna operates as a function of the reflector-director distance and tune for instance the collection efficiency, the quantum yield and the radiation pattern of a nanoscale light source. \nThis approach can be applied to different kinds of nanoscale light sources, such as molecules, quantum dots, beads, and it can be operated in different environments, as in water or at cryogenic temperatures. Therefore, it represents a novel platform for enhanced spectroscopy and sensing.\n\n\n\n\n\\section{Layout of the problem and simulation results}\n\nAlthough previous works simulate the radiation pattern and the maximum collection efficiency for the optimal distance between the emitter and the antenna elements~\\cite{checcucci2017beaming,galal2017highly}, we extend this approach and simulate the parameters for continuous reflector-director distances even larger than  2\\,$\\lambda$ (up to 1.5 $\\mu$m). Moreover, in a realistic simulation of fluorescence detection, one should also consider the excitation enhancement and the quantum efficiency of the molecule ($\\eta_0$). Therefore, we examine the  collection efficiency of a dipole in a scanning planar antenna excited by a plane monochromatic electromagnetic wave.\n \n\n\n\nWe take into consideration only horizontal projections of the dipole moments of emitters, since the vertical ones are strongly coupled to the surface plasmon polariton modes of the metal films and they do not contribute to the emission in the far field~\\cite{checcucci2017beaming}. Thus, for emitters with random orientation, the coupling efficiency would scale by a factor of 2/3~\\cite{soltani2019planar}. \nIn our calculations, the gold layer, which represents the director is 10~nm thick and the reflector is a gold mirror movable along the $z$-axis. As already demonstrated, the optimal distance between the dipole and the director should be approximately between $\\lambda/6n$ and $\\lambda/4n$~\\cite{checcucci2017beaming}.\nFor this reason, we need a spacer to provide such a distance in a realistic simulation. The ideal spacer should be optically transparent and the emitters should not be quenched in its vicinity~\\cite{rodriguez2016self}.\nTherefore, the dipole is located 20~nm above a 75~nm thick SiO$_2$ layer used to separate the dipole from the director (Fig.~\\ref{theory_emission}a). This distance is chosen to mimic the 20~nm radius of fluorescent beads on the SiO$_2$ layer, which we model as a single dipole.\nThe parameters are calculated  in the range of distances from 45 nm to 1500 nm between a semi-infinite gold mirror (reflector) and the SiO$_2$ layer.  To simulate the realistic structure we also consider that the director layer is on the top of a 2~nm titanium (Ti) film, which is supported by a semi-infinite borosilicate glass (Fig.~\\ref{theory_emission}a).\n\nWe calculate the normalized total decay rate $\\Gamma_\\mathrm{tot}/ \\Gamma_0$ and the normalized radiative decay rate to the far field $\\Gamma_\\mathrm{rad} / \\Gamma_0$ by respectively considering the power dissipated and radiated by a Hertzian dipole emitting at 680 nm in the antenna configuration with respect to free space~\\cite{taminiau2008enhanced,kaminski2007finite}. \nHere, $\\Gamma_0$ represents the radiative decay rate of the dipole in vacuum. Hence, $\\Gamma_\\mathrm{tot} / \\Gamma_0$ corresponds to the Purcell factor for an emitter with an intrinsic quantum yield $\\eta_0 = 1$ and it reaches a value of about 2.8 at the first maximum (Fig.~\\ref{theory_emission}b). A normalization with respect to a dipole near a glass coverslip would reduce the Purcell factor by a factor of about 1.25 (average refractive index between SiO$_2$ and air), since the radiative decay rate is directly proportional to the refractive index. \nWe also consider the dipole excitation at a wavelength of 636 nm, which for simplicity is assumed to be a plane-wave. To estimate the enhancement of the excitation rate we calculate the electric-field intensity $|E|^2$ at the dipole position. The plane-wave reflectivity $R$ is also calculated to relate the excitation enhancement with this measurable quantity. We numerically solve  the electromagnetic problem (computation of $|E|^2$ and $R$) using FDTD Solutions (Lumerical)~\\cite{lumerical} and semi-analytically (computation of the decay rates and of the radiation patterns)~\\cite{neyts1998simulation}.\n\n\\begin{figure}\n  \\includegraphics[width=12cm]{theory_emission}\n  \\centering\n  \\caption{\\textbf{Hertzian dipole in a scanning planar antenna}. (a) Simulation layout of a dipole excited  with a 636 nm monocromatic light and its emission is at 680 nm in the antenna configuration. The glass coverslip and the gold wire (reflector) are semi-infinite. The values of the fixed parameters are given in the figure. The distance between the SiO$_2$ layer and the gold reflector (air gap) varies from 45 nm to 1500 nm.  The reflectivity $R$ and the excitation intensity $|E|^2$  are recorded with a frequency-domain monitor. (b) Plane-wave intensity at the dipole position, $|E|^2$, reflectance $R$ and collection efficiency $P_\\mathrm{coll}$ plotted as a function of the air gap. \n  Modification of the decay rate $\\Gamma$ (assuming the quantum yield of 0.7) as well as $\\Gamma_\\mathrm{tot}$ and $\\Gamma_\\mathrm{rad}$ also plotted as a function of the air gap. The far-field radiation patterns, corresponding to the  maximum and minimum of $P_\\mathrm{coll}$, are highlighted with a circle and a triangle and they are represented in (c) and (d), respectively. The FWHM of the intensity profile in (c) is 45$^{\\circ}$.}\n \n  \\label{theory_emission}\n\\end{figure}\n\nFigure~\\ref{theory_emission}b plots $|E|^2$ (dashed blue curve), normalized to its maximum value, and $R$ (dotted blue curve), normalized to the incident power, as a function of the air gap in the planar antenna. Since we use a plane wave, both quantities oscillate with a periodicity of $\\lambda/2$, where $\\lambda=636$ nm, and with a constant maximum height, like in a Fabry-Perot etalon. Notice that the maxima and minima of $|E|^2$ and $R$ do not coincide, because the planar antenna is a kind of asymmetric cavity and $|E|^2$ is calculated at the fixed dipole position, which is not at the maximum of the cavity modes.\n\nSince we consider a detection with a high-NA objective, the collected power $P_\\mathrm{coll}$ can be assumed proportional to the product of $|E|^2$ with $\\Gamma_\\mathrm{rad}$, as we will demonstrate later, and to be nearly independent of the radiation pattern. In Fig.~\\ref{theory_emission}b, $P_\\mathrm{coll}$ (solid blue curve) is plotted for different values of the air gap between the reflector and the SiO$_2$ layer.\n$P_\\mathrm{coll}$ oscillates with $|E|^2$ and $\\Gamma_\\mathrm{rad}$ and therefore its behaviour is determined by both excitation and radiation enhancement and by the radiation pattern. \n\nWhen $R$ has a minimum, $P_\\mathrm{coll}$ is not maximal. It is also remarkable to note that even when $|E|^2$ is maximal, $P_\\mathrm{coll}$ is not maximal for the first peak. This indicates the different contributions of $|E|^2$ and  $\\Gamma_\\mathrm{rad}$ to the emission. Nonetheless, when the distance increases $\\Gamma_\\mathrm{rad}$ has a smaller contribution and $P_\\mathrm{coll}$ oscillates with $|E|^2$.\n\nTo understand the role of the decay rates, Fig.~\\ref{theory_emission}b plots $\\Gamma_\\mathrm{rad}$ (dashed red curve), $\\Gamma_\\mathrm{tot}$ (dotted red curve) and the inverse excited-state lifetime $\\Gamma$ (solid red curve), calculated by $(1-\\eta_0) \\Gamma_0/ \\eta_0 + \\Gamma_\\mathrm{tot}$ for an intrinsic quantum yield $\\eta_0=0.7$~\\cite{chance1978molecular}, which is close to that of our fluorescent beads in the experiment.\n\nSince the beads have an intrinsic quantum yield of 0.7, the modification of the excited state decay rate is given by  $ \\eta_0  \\Gamma / \\Gamma_0$. This value oscillates around 2 as shown in Fig.~\\ref{theory_emission}b because the normalization is with respect to a dipole in free space.\n\n\nBecause part of the emitted light is absorbed by the metallic elements of the antenna, it is relevant to pay attention to the antenna efficiency $\\eta_\\mathrm{a}=\\Gamma_\\mathrm{rad}/\\Gamma_\\mathrm{tot}$, which represents the fraction of photons reaching the far field.\nThis efficiency is about 0.6 and it is almost independent of the distance between the reflector and the director (not shown). Moreover, $\\Gamma_\\mathrm{rad}$ is the quantity that contributes to  $P_\\mathrm{coll}$ as we show here. \nIn fact, below saturation the fluorescence signal $S$ is proportional to the product of intensity, quantum yield and decay rate, i.e.  $S=|E|^2\\eta\\Gamma$, where $\\eta=\\eta_0/[(1-\\eta_0)\\Gamma_0/\\Gamma_\\mathrm{rad}+\\eta_0/\\eta_\\mathrm{a}]$ and $\\Gamma_0$ is the radiative decay rate without antenna. Since $\\Gamma=(1-\\eta_0) \\Gamma_0 / \\eta_0 + \\Gamma_\\mathrm{tot}$, it is easy to show that $S=|E|^2\\Gamma_\\mathrm{rad}$.\nNow, considering the fact that the high-NA objective does not significantly modify the collection efficiency, we can assume $P_\\mathrm{coll} \\simeq S$.\n\nThe far field emission patterns of the dipole in the first local maximum and minimum of $P_\\mathrm{coll}$ are shown in Fig.~\\ref{theory_emission}c and d, respectively. In Fig.~\\ref{theory_emission}c, the radiated intensity is peaked at the center and its full width at half maximum (FWHM) is 45$^{\\circ}$. On the other hand, in Fig.~\\ref{theory_emission}d, the radiated intensity at small angles drops almost to zero. \n \n\n\n\n\\section{Experimental methods}\n\n\\subsection{Antenna fabrication and setup configuration}\nTo verify our theoretical predictions, we design a planar antenna according to the parameters used for the calculations in the previous section.\nFor light sources, we choose bright fluorescent beads with a 40 nm diameter (ThermoFisher, 0.04 $\\mu$m, dark red fluorescent 660/680), highly concentrated fluorescent molecules embedded in polystyrene (approximately 350 molecules), which are less prone to bleaching. \nSince our goal is to address individual beads, we dilute them in pure water (MilliQ 18.2 M$\\Omega \\cdot$cm resistivity) and subsequently spin-cast them on a substrate in order to obtain spatially separated beads (at least few $\\mu$m mutual distance). \n\nA planar antenna consists of two main parts, a director and a reflector. To produce the director, a glass coverslip (PLANO, L42342 St\\\"arke \\#1) is firstly coated with 2 nm titanium (Ti) \\cite{werner2009optical} which plays the role of an adhesion layer between the glass coverslip and the 10~nm gold film \\cite{olmon2012optical} (director). Furthermore, we evaporated 75 nm SiO$_2$ layer as a spacer on top of Au film (similar to the structure presented in Fig.~\\ref{theory_emission}a). All the material deposition processes are preformed by e-beam evaporation (Edwards E306A) with an evaporation rate of 0.1 nm/s.  As shown in Fig.~\\ref{etching1}a, the 10~nm gold on layer on Ti does not form a uniform film, but ultrathin islands.\nIn these kind of structures, polarized light can excite localized surface plasmons~\\cite{sun2012temperature}.\n \n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=12cm]{etching1}\n\\caption{\\textbf{Fabrication of gold director and wire reflector}. (a) Scanning electron microscopy (SEM) image of a 10~nm gold layer on glass coverslip. Instead of flat films,  ultrathin islands are formed. The surface roughness of the film (director) is 10.2 nm peak to peak, which was measured by atomic force microscope (data not shown).\n(b) Schematic representation of a tilted gold wire before and after the thinning process. The distance from the substrate to the center of the wire varies significantly depending on the wire's diameter. The insets are magnified images of the central part the gold wire tip and the underneath substrate. SEM image of a gold wire (c) before and (d) after electrochemical etching. (e) SEM image of the wire tip after FIB cutting. (f) The interference pattern formed by a tilted wire and the gold director. Since only two maxima are visible (white arrows), the estimated angle between  the wire and the substrate is around $2.6^{\\circ}$. In (g) and (h) this angle is close to zero and thus these images represent consecutive dark and bright fringes due to 316~nm wire displacement.}\n\\label{etching1}\n\\end{figure}  \n  \nThe numerical calculations indicate that the air gap between the SiO$_2$ layer and the gold mirror should be 160 nm to receive the maximal directional emission from a Hertzian dipole (Fig.~\\ref{theory_emission}b). However, we note that a small tilt angle between reflector and substrate (Fig.~\\ref{etching1}b), can make the center gap larger than the optimal distance. One way to mitigate this issue is to reduce the diameter of the gold mirror, e.g. by etching the tip of the wire. From a simple geometrical consideration we can easily demonstrate that even for 3$^{\\circ}$ tilt angle between the wire and the substrate, the distance between the centre of the gold wire and the substrate is 183~nm for a 7~$\\mu$m wire and 5.23~$\\mu$m for a 200~$\\mu$m wire diameter.  \n\nIn order to obtain gold tips with a 5-10 $\\mu$m diameter, we follow the procedure of Ref.~\\cite{ren2004preparation}. Namely, we first vertically submerge a 200~$\\mu$m thick gold wire (Fig.~\\ref{etching1}c) up to 1 cm into an electrolyte solution (37 \\% hydrochloric acid (HCl)). By applying a 3-Volt potential difference between the anode, the gold wire, and a platinum cathode, the gold wire is etched down to a cone shaped micro-tip (Fig.~\\ref{etching1}d). Several parameters such as the time of applying the voltage, the length of the wire inside the HCl and the HCl concentration, play a role in the fine etching procedure. However, since we only need a sharp tip with a radius of a few micrometers these parameters are not crucial. \nAfter the etching process in order to obtain a flat mirror, the tip of the wire is subsequently cut by Focused Ion Beam (FIB) milling (FEI Helios Nanolab 600) as shown in Fig.~\\ref{etching1}e. The face diameter of the top is determined by the position of the FIB cut. To achieve a flat tip and to avoid edge rounding at the tail of the ion beam, FIB milling is performed at 30 kV and with the smallest aperture possible while maintaining reasonable cutting times. Depending on the diameter of the wire tip, the used apertures typically resulted in a beam current of 0.92~nA to 2.8~nA. As FIB milling is well-suited for high resolution electron microscopy sample preparation, the local roughness of the tip is expected to be within a few nm.  \n\n\nThe contact point and the tilt angle can be measured by monitoring interference fringes as shown in Fig.~\\ref{etching1}f-h. For a 7-$\\mu$m gold tip and 636 nm coherent light, the minimal angle to obtain only two bright fringes is 2.6$^{\\circ}$ (Fig.~\\ref{etching1}f). In the experiment this angle can be adjusted by a manual tilting stage (Newport M-561-TILT) below this value to overcome the central gap distance (Fig.~\\ref{etching1}g,h).\n\n \n \n  \n\n\\subsection{Optical setup}\nThe optical setup is based on an epi-fluorescence inverted microscope (Supporting Information Fig.~S1a). A pulsed laser with a wavelength of 636 nm and a repetition rate of 40 MHz (Picoquant, LDH-P-C-640B) is coupled to a commercial inverted  microscope (Zeiss, Axio Observer 3) and the laser light is focused with an oil objective (Zeiss, Plan-Apochromat 63x/1.4) onto the sample. A motorized stage (ASI, PZ-2000FT) controls the position of the glass substrate with the spin-casted beads, in the $xyz$-directions in order to address a single bead.\nThe reflector is aligned above the focal point of the objective with a home-built module on the top part of the microscope. A piezo controller (Newport, NPM140SG) moves the wire in the $z$-direction in order to set the distance between the reflector and the bead, positioned in the focal spot of the laser. The piezo-controller movement is  monitored by a closed loop piezo stack amplifier (Newport NPC120SG).\n\nThe fluorescence signal from a single bead is collected by the microscope objective and simultaneously sent to the CMOS camera (Andor, Zyla 4.2 Plus) and the single-photon avalanche photodiode (SPAD) (Excelitas, SPCM-AQRH-TR), with a 95:5 Transmitance:Reflectence ratio defined by a beam splitter (BS). A Time-Correlated Single-Photon Counting (TCSPC) device (PicoQuant, Pico Harp 300) measurs the photon count rate. The time resolution in this experiment is less than 250~ps, which is the time  resolution of SPADs. The radiation pattern was obtained by Back-Focal Plane (BFP) imaging with the CMOS camera. \n\nThe setup contains also a laser clean-up filter (CF 637/7) with central wavelength of 637 nm and 7 nm bandwidth, a flippable wide-field lens (WFL) used for monitoring the beads in the alignment process, a dichroic mirror (DM) with a 650-nm cut-off wavelength, a long-pass (LP) and a band-pass (BP) filter (LP 650, BP 680/42, with central wavelength of 680 nm and 42 nm bandwidth), whose position can change based on the experiment (Fig.~S1b). Flip mirrors (FM1 and FM2) can send the light to the CMOS camera, the spectrometer (Ocean Optics QE pro) or to another single-photon avalanche photodiode (SPAD1). A\nback-focal plane lens (BFL) is used to image the radiation pattern. The inset of Fig.~S1a sketches the 3D model of gold wire in the antenna configuration. The distance of the wire from glass coverslip is monitored by a tilted objective mounted on the home-built stage  such that the wire tip is clearly visible while approaching the substrate surface. \nFor precise alignment, the tilting of the gold wire is observed by the laser interference pattern also collects by the microscope objective and corrects by a manual tilt stage as described in Fig.~\\ref{etching1}f-h. Moreover, the contact point between the reflector and the substrate is determined by interference fringes. Namely, once the wire and the substrate are in contact, further attempt to approach does  not result in any visible interference pattern change. We take this position as a starting point of the wire retraction.\n \nThe experiment is performed at room temperature and the vibration of the antenna system is estimated by interference patterns to around 40~nm. Therefore, in all steps we measure the position several times and average the result. This further helps us to decrease the vertical position error to $\\pm$5~nm at each step.\n\n\n\n\n\n\\section{Experimental results and discussion}\n\nHere, we investigates the emission pattern, the excited-state decay rate $\\Gamma$ and the collected power $P_\\mathrm{coll}$ for a single bead in the planar antenna configuration, as a function of the distance between the SiO$_2$ layer and the reflector (air gap), as shown in Fig.~\\ref{Beads_antenna1}. The reflector position is adjusted using the piezo controller in a step-wise fashion (10 nm step size), from the contact point with the SiO$_2$ in 150 steps (Fig~\\ref{Beads_antenna1}a). At each step the position of the wire and the measured quantities are recorded.\nThe vertical dashed line in Fig.~\\ref{Beads_antenna1}b indicates the contact point. \n\nIn our experiment, the excitation power at the back entrance of the microscope is 3.5 $\\mu$W. Since the 10~nm gold director has only 30\\% transmittance, the power passing through the director is around 1 $\\mu$W.\nHowever, this is sufficient to detect the fluorescent beads and align them in the focus point of the objective using the  $xyz$-stage.  \n\n\n\\begin{figure}[h]\n  \\includegraphics[width=12cm]{Beads_antenna1}\n  \\centering\n  \\caption{\\textbf{A fluorescent bead in the scanning antenna configuration}. (a) Schematic representation of the home-built scanning planar antenna setup. The gold wire and the sample can move by piezo controller (along Z-axis) and motorized stage (in XYZ), respectively. (b) Collected fluorescence intensity (counts) and decay rates $ns^{-1}$ measured as a function of the distance between the gold wire and the SiO$_2$ layer. The background intensity is subtracted. The vertical black dashed line indicates the position, where the gold wire and the SiO$_2$ are in contact. The collection intensity of a bead on the glass coverslip is shown with the blue dotted line and the range of this intensity for different beads is showed by the gray shaded area. (c,d) Radiation pattern for two selected positions marked by a circle and a triangle. (e) Radiation pattern of a bead when the reflector is far away. Each BFP image is normalized to its maximum values.}\n  \\label{Beads_antenna1}\n\\end{figure}\n\n\nTo determine fluorescence lifetimes at each step of the reflector position, we construct fluorescence lifetime decay histograms based on start-stop events of the TCSPC. Using the deconvolution method, we were able to separate fluorescence from background and to reach the single exponential fluorescence decay rate ($\\Gamma$) (see Supporting Information for details). \nThe integration of the fluorescence decay histogram determines the collected power  ($P_\\mathrm{coll}$) as a function of the distance between the reflector and the SiO$_2$ spacer. (Supporting Information Fig.~S2). The variation of $P_\\mathrm{coll}$ and $\\Gamma$ at different reflector positions are plotted in Fig.~\\ref{Beads_antenna1}b  with blue and red curves, respectively.\n\n\n\nThe decay rate $\\Gamma$ in the scanning Yagi-Uda antenna exhibits a periodic behavior with values between 0.48 and 0.27 ns$^{-1}$, i.e. lifetime $\\tau$ is between 2.08 and 3.70 ns, and it has a phase shift with respect to $P_\\mathrm{coll}$ (Fig.~\\ref{Beads_antenna1}b). \nThis is due to the fact that the decay rate increases at the cavity resonance for the emission wavelength ($\\lambda \\simeq 680$ nm), while the detected intensity follows the laser wavelength (excitation enhancement at $\\lambda = 636$ nm) and the emission wavelength (enhancement of the radiative decay rate).\nThe fluctuation of the decay rate, especially at low $P_\\mathrm{coll}$ is mainly due to the low intensity of the fluorescence decay from a single bead as well as the lower background caused by  the ultrathin gold islands (director). Although this background is deconvolved from the fluorescence signal of the bead, the signal to noise ratio still remains very low at the low emission intensities (Fig.~S2).  \n\nA smoothed $\\Gamma$ (red dashed curve in  Fig.~\\ref{Beads_antenna1}b) was obtained by averaging  the decay rate at each point with its 6 neighbours. Moreover, by calculating the ratio between the first maximum and minimum of the smooth curve (1.6) and comparing it to  $\\Gamma$ in Fig.~\\ref{theory_emission}b, one can estimate the quantum efficiency of the beads, which is  around $\\eta_0 = 0.7$~\\cite{chance1978molecular,buchler2005measuring}. The modification of the excited-state lifetime due to the Purcell factor of about 2, which is in a good agreement with the simulation results presented in Fig.~\\ref{theory_emission}a.\n\nThe beads exhibit an emission spectrum peaked at 680~nm with a FWHM of about 38~nm and the excited-state decay rate is about $0.29 \\pm 0.05$ ns$^{-1}$ on the glass coverslip (not shown). The collection intensity of a single bead on the glass coverslip (without any additional layer) is shown by the blue dotted line in Fig~\\ref{Beads_antenna1}b for reference.\nAlthough the bleaching of the fluorescent molecules inside the bead can potentially reduce its emission, this effect is neglected in our experiments owing to the long bleaching time of the beads (blue dotted line in Fig.~\\ref{Beads_antenna1}b). \n\nThe main reason for the reduced $P_\\mathrm{coll}$ in the antenna configuration (Fig.~\\ref{Beads_antenna1}b) must be attributed to the smaller excitation intensity due to the standing wave pattern formed by the laser. One should take into account that the antenna acts also as a cavity and gives rise to the intensity enhancement (blue dashed line in Fig.~\\ref{theory_emission}b ). \n\nFurthermore, at larger distances, the cavity resonance has a dominant effect and the signal exhibited a series of maxima and minima that follow the excitation wavelength.\nSince fluorescent beads contain a different number of fluorescent molecules, the emitted power of the individual beads is not the same. It is therefore difficult to compare the antenna configuration with the glass coverslip in an quantitative manner. However, a comparison of the $P_\\mathrm{coll}$ in the antenna with $P_\\mathrm{coll}$ for several beads on the glass coverslip, shown by a gray shaded area (Fig.~\\ref{Beads_antenna1}b), demonstrates that overall the antenna enhances the fluorescence signal by more than a factor of 3. The dotted curve represents one measurement of $P_\\mathrm{coll}$ of a bead on the glass coverslip.\n\nThe maximum of $P_\\mathrm{coll}$ is at 0.17 $\\mu$m distance and the back-focal plane (BFP) image  at this position (Fig.~\\ref{Beads_antenna1}c) indicates a highly directional emission.\nThe FWHM of the emission pattern at the first maximum of $P_\\mathrm{coll}$ shrinks to roughly 45$^\\circ$. Moreover, the emission pattern at the first minimum of $P_\\mathrm{coll}$ is a ring with 70$^\\circ$ angle (Fig.~\\ref{Beads_antenna1}d), which is also in good agreement with the simulation results shown in  Fig.~\\ref{theory_emission}c,d. Since the NA of the objective is high, we can compare the measured intensity with the calculated $P_\\mathrm{coll}$ shown in Fig.~\\ref{theory_emission}b. The radiation pattern of the bead when the reflector is far away is shown in Fig.~\\ref{Beads_antenna1}e and the results are consistent with earlier studies~\\cite{zhu2017out,brunstein2018decoding,dasgupta2021direct}.\n\nInspired by the planar metallo-dielectric antennas ~\\cite{chu2014experimental,devilez2010compact} we design a semi-antenna, which is a simplified version of the planar Yagi-Uda antenna (without SiO$_2$, gold director and Ti layers). In Supporting Information (Fig.~S4) we provide the radiation pattern for the single bead influenced by this antenna configuration.  Comparing the radiation patterns for the  planar Yagi-Uda antenna presented above (Fig.~\\ref{Beads_antenna1}c) with the semi-antenna, we clearly demonstrate that even a 10~nm thick gold layer (director), with all its imperfections, can dramatically change the radiation pattern (see for comparison Fig.~S4c,d and Fig.~\\ref{Beads_antenna1}c,d).\n\n\nTo determine the relationship between $R$ and $P_\\mathrm{coll}$, the collected light is split into two channels. We slightly modified the previous setup and, instead of a CMOS camera, we employed SPAD1 for photon detection (Supporting Information Fig.~S1b). In this case, 95\\% of the light was passing through a long-pass and a band-pass filter and detected by the SPAD1. Conversely, after being attenuated only by a neutral density (ND) filter (OD=2), the other 5\\% of light (laser reflection) was detected by a second photo-detector (SPAD2).\n\nFigure~\\ref{beads_antenna_fast1} plots the intensity detected by SPAD1 and SPAD2 as a function of distance between the director and the SiO$_2$ layer. We note that the detected intensity from the beads is much higher than $P_\\mathrm{coll}$ in Fig.~\\ref{Beads_antenna1}b, because in the previous configuration only 5\\% of the light was collected with SPAD2, but in this configuration 95\\%  was detected by SPAD1.\n\n\n\nIn Fig.~\\ref{beads_antenna_fast1} the detected intensity (blue curve) is the convolution of the  fluorescence of the bead ($P_\\mathrm{coll}$) and the background emission from gold islands. This will cause a slight shift (roughly 40~nm) in the position of $P_\\mathrm{coll}$ maxima and artificially create a larger distance between the maximum of fluorescence and reflection intensity as compared to theory (Fig.~\\ref{theory_emission}). \n\nThe spacing between two minima of the laser reflection (red curve) is roughly 330 nm, which should correspond to the cavity resonance at the excitation light wavelength. This is slightly larger than $\\lambda/2$, for $\\lambda=636$ nm. The 10~nm difference can be related to the piezo calibration and the system vibrations.\nThe reflection intensity oscillations gradually decrease with the air gap. This phenomenon can be simply explained by the fact that while the laser light remains focused on the bead, the reflector is not able to refocus the incident light back into the laser mode. In our theoretical considerations the amplitude of the reflection (R) is constant, since the excitation is a plane wave (Fig.~\\ref{theory_emission}b).  \n\n\\begin{figure}\n  \\includegraphics[width=8cm]{fastmove_maxima}\n  \\centering\n  \\caption{\\textbf{Fluorescence signal and laser reflection in the planar antenna configuration.} The fluorescence signal consisting of the bead and the director emission (blue curve) as well as the reflected laser power (red curve) are shown as a function of distance between gold wire and SiO$_2$ layer (air gap).  The reflection is associated with  the excitation rate, which modulates the fluorescence signal. }\n\\label{beads_antenna_fast1}\n\\end{figure}\n\nThe distance between the first two peaks in $P_\\mathrm{coll}$ (blue curve in Fig.~\\ref{beads_antenna_fast1}) is different than the next ones. This is probably due to the near-field of the emission at $\\lambda = 680$ nm, in combination with the intensity enhancement. Moreover, since the antenna effect has a higher impact at sub-wavelength distances ($\\lambda /6 n$ to $ \\lambda /4 n$), the first maximum in $P_\\mathrm{coll}$ is 2.4 times larger than the second one. \nOn the other hand, the first minimum in $R$ is 1.8 times deeper than the second one. By dividing these numbers we get 1.33, which is equal to the first and second peak difference of $P_\\mathrm{coll}$ in theory (Fig.~\\ref{theory_emission}b).\n\nAs the distance increases, $P_\\mathrm{coll}$  roughly follows $1-R$, as expected from the predominant contribution of the intensity enhancement. However, in agreement with the simulation results (Fig.~\\ref{theory_emission}b), the maxima of $P_\\mathrm{coll}$ are slightly ahead of the minima of $R$, because they are related to $|E|^2$. \n\n\\section{Conclusions}\n\nWe have proposed and investigated, both theoretically and experimentally, a scanning planar Yagi-Uda antenna, to better understand its influence on the optical properties of nanoscale light sources. Simulations and measurements of the excitation enhancement, the beaming effect, and the Purcell enhancement, give us  insight into the underlying processes and help us maximize fluorescence collection efficiencies over a large parameter range.\nThese physical quantities are simply addressed by scanning the distance between the antenna elements, the reflector, and the director. Moreover, the experimental findings are consistent and in good agreement with the semi-analytical model.\nThe results clearly demonstrate that at around 160~nm antenna air gap the  Purcell factor is small, but not negligible (around 2 at the first maximum), and the FWHM of the main radiation lobe at this position is 45$^{\\circ}$. All together, the fluorescence signal is 3 fold higher than that on a regular glass coverslip. A comparable improvement can also be found for larger distances.\n\nCompared to other scanning cavity approaches~\\cite{colombe2007strong, toninelli2010scanning,kelkar2015sensing}, our method is broadband and less sensitive to the fine position control of the emitters with respect to the antenna elements. Therefore, it is particularly advantageous for detecting fluorescence at ambient temperatures, where the signal is spectrally broad and it allows standard immobilization techniques of the emitters, such as spin-casting or chemical functionalization of the substrate surface. We already envision that we can easily extend this technique to the detection of emitters in a liquid environment. \n\nSince planar antennas strongly influence the emission directionality, our findings suggest that this approach can be further extended to low-NA optics, particularly to low-NA objectives with long working distances. Avoiding expensive and sometimes impractical high-NA objectives would facilitate the experiments and extend the range of possible applications. Eventually, the tip of an optical fiber coated with a thin gold film (director) could completely replace the objective and as such optical fibers would be employed both for the excitation of the emitters and fluorescence collection~\\cite{soltani2019planar}. \nHence, a scanning planar Yagi-Uda antenna holds promise for a variety of applications in fluorescence-based biosensors~\\cite{wolfbeis2005materials}, single-photon sources~\\cite{benedikter2017cavity}, enhanced spectroscopy~\\cite{gagliardi2014cavity} and scanning microscopy~\\cite{shotton1989confocal}.\n\n\\subsection*{Funding}\nUniversity of Siegen; the German Research Foundation (DFG) (INST 221/118-1 FUGG); and the Bundesministerium f\\\"{u}r Bildung und Forschung (13N14746).\n\n\\subsection*{Acknowledgments}\nThe authors would like to thank C. Toninelli,  P. Lombardi and G. Shafiee for helpful discussions. Part of this work was performed at the Micro- and Nanoanalytics Facility (MNaF) of the University of Siegen.\n\n\\subsection*{Disclosures}\nThe authors declare no conflicts of interest.\n\n\\subsection*{Supplemental document}\nSee Supplement 1 for supporting content. It includes\nexperimental setup,  decay rates and deconvolution of the collected signal, collection efficiency with a reflector (semi-antenna) and directionality of planar antenna at larger distances.\n\n\n", "meta": {"timestamp": "2021-06-25T02:19:43", "yymm": "2106", "arxiv_id": "2106.13036", "language": "en", "url": "https://arxiv.org/abs/2106.13036"}}
{"text": "\\section{Introduction}\\label{sec:introduction}\nThe main purpose of the paper is the analysis of the decaying and scattering properties of the solution to the defocusing nonlinear fourth-order Hartree-Fock-Choquard equations (HFC4) in dimension $d \\geq 3 $:\n\n\\begin{equation}\\label{eq:HFC4}\n\\begin{cases}\ni\\partial_t u_j + \\Delta^2 u_j -\\sigma_1 \\Delta u_j  +\\sigma_2 V(x)  u_j +\n\\sum_{k=1}^N\\mathcal F(u_j , u_k)=0,  \\\\ \n(u_j(0,\\cdot))_{j=1}^N= (u_{j,0})_{j=1}^N \\in H^2({\\mathbb R}^d)^N,\n\\end{cases}\n\\end{equation}\nwith $\\sigma_1,\\sigma_2=0,1$. We shall assume that\n$V(x)$ is a nonnegative Schwartz  radial function such that $x \\cdot \\nabla V \\leq 0$ and\n\\begin{equation}\\label{eq.hopot}\n\\|V\\|_{L^{\\frac{d}{4}}}+\\sup _{y \\in \\mathbb{R}^{d}} \\int_{\\mathbb{R}^{d}} \\frac{V(x)}{|x-y|^{d-4}} d x \\leq +\\infty.\n\\end{equation}\nThe nonlinear term is given in the following general form\n\\begin{align}\\label{eq.nonlinHF}\n\\mathcal F(u_j, u_k)\n\\\\\n=b_{jk} \\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)}|x|^{-\\rho_{1}} | u_j|^{p-2}  u_j\n\\nonumber\\\\\n+b\\round {\\squad{|x|^{-(d-\\gamma_2)}*|x|^{-\\rho_{2}}| u_k|^2  } |x|^{-\\rho_{2}}u_j-\\squad{|x|^{-(d-\\gamma_2)}* |x|^{-\\rho_{2}}\\overline u_k u_j  } |x|^{-\\rho_{2}} u_k}.\n\\nonumber\n\\end{align}\nHere, for all $j,k=1,\\dots,N$, $u_j=u_j(t,x):{\\mathbb R}\\times{\\mathbb R}^d\\to\\C$, $(u_j)_{j=1}^N=(u_1,\\dots, u_N)$ and  $b, b_{jk}\\geq0$\n are coupling constants. We require that the nonlinearity parameters $p$, $\\gamma_{\\kappa}$ and $\\rho_{\\kappa}$, $\\kappa=1,2$, are $L^2$-$H^1$-intercritical, that is, when they satisfy the following conditions \n\\begin{align}\n\\label{eq:bs}\n\\max (0, d-8)<\\gamma_{\\kappa}<d,& \\ \\\n 2\\leq p<p^*(d), \\ \\  p_1^*(d)=\n\\begin{cases}\n\\infty \\,  & \\text{if} \\ \\  d=3,4, \\\\\n\\frac {d+\\gamma_{1}+\\rho_{1}}{d-4}  \\,    &\\text{if} \\ \\ d\\geq5,\n\\end{cases}\n\\\\\n&p_1> p_{1*}(d), \\ \\  p_{\\kappa*}(d)=\\frac{d+ \\gamma_{\\kappa}+4+\\rho_{\\kappa}}d,\n\\label{eq:bsII}\n\\\\\n&0\\leq  \\rho_{\\kappa}<\\min\\{4(1+\\gamma_{k}/d),d-8-\\gamma_{\\kappa}\\} .\n\\label{eq:bsIII}\n\\end{align}\nWe recall two important quantities linked to \\eqref{eq:HFC4}. The mass\n\\begin{align}\\label{eq.mass}\nM(u_{j})(t)=\\int_{{\\mathbb R}^d}|u_{j}(t)|^2\\,dx\n\\end{align}\nand the energy  \n\\begin{equation}\\label{eq:Ener}\n\\begin{split}\n  E(u_{1},\\dots,u_{N}) =  \\sum_{j=1}^N\\int_{{\\mathbb R}^d}  \\abs{\\nabla{u_j}}^2+  \\sum_{j=1}^N\\int_{{\\mathbb R}^d}  \\abs{\\Delta{u_j}}^2\n  \\\\\n + \\frac{1}{2p}\\sum_{j,k=1 }^Nb_{jk} \\int_{{\\mathbb R}^d}\\squad{|x|^{-(d-\\gamma_1)}\\ast\n(|x|^{-\\rho_{1}}|u_k|^{p})}|x|^{-\\rho_{1}}|u_j|^{p} dx\n\\\\\n+\\frac {b}2\\sum_{j,k=1 }^N \\int_{{\\mathbb R}^d}\\squad{|x|^{-(d-\\gamma_2)}*( |x|^{-\\rho_{2}}| u_k|^2  )} |x|^{-\\rho_{2}}|u_j|^2\\,dx\n\\\\\n-\\frac {b}2\\sum_{j,k=1 }^N \\int_{{\\mathbb R}^d} \\squad{|x|^{-(d-\\gamma_2)}*( |x|^{-\\rho_{2}}\\overline u_k u_j ) } |x|^{-\\rho_{2}}u_k \\overline u_j\\, dx.\n\\end{split}\n   \\end{equation}\nThe Hartree-Fock system of $N$-particles is of fundamental interest from a physical point of view and plays an important role in several models of the mathematical physics. In fact, in the quantum mechanics it portrays the mean-field limit of large systems of bosons, \\emph{i.e.} the so-called Bose-Einstein condensates, in view of the self-interactions of the charged particles. This scenario is considered, for example, in \\cite{ES}, \\cite{Len}, \\cite{LR} and references therein. The Hartree-Fock equation was applied in \\cite{Fo} in order to depict systems of charged fermions as well as an exchange term resulting from Pauli's principle and in \\cite{Lip} to describe the fermions as an approximation of the equation, disregarding the effect of their fermionic nature. Furthermore, the Hartree-Fock-Choquard system, extensively treated in \\cite{TarVenk}, ensured various other applications: in \\cite{FrLe} for developing models of white dwarfs, for sketching an electron trapped in its own hole, as exhibited in \\cite{ChS}, \\cite{CSV} and in \\cite{Pen}, for representing self-gravitating matter together with quantum entanglement and quantum information effects. On the other hand, the fourth-order Schr\\\"odinger equations were introduced in \\cite{FIP} to portray dispersion in the propagation of intense laser beams in a medium with Kerr nonlinearity and successively employed in the theory of motion of a vortex filament in an incompressible fluid \\cite{Kar}, \\cite{KaS}, \\cite{Se03} (see also \\cite{HJ05} and \\cite{HJ07}). It is natural, as a next step, to generalize the model to a higher order one, in our framework the HFC4 system \\eqref{eq:HFC4}, given in a rather general way, which discusses various equations of motions. In addition, the second issue is to extend the scope of HFC4 to the short range-potentials. The relevant papers \\cite{BJPSS} and \\cite{BSS2} (see also the references inside) suggest also that it is necessary to bound the solutions in some $L^{2}$-based spaces, so to show that the evolution initial mixed states remains close to a (mixed) quasi-free state, evolving according to Hartree-Fock type equations, such as HFC4 systems. Influenced by this and by  \\cite{CM}, \\cite{CLM}, our main target is the analysis of the decay of the solutions to  \\eqref{eq:HFC4} in the energy space, which means the decay of $L^r$-norms of the solutions, provided that $2<r\\leq2d/(d-4)$, for $d\\geq 5$, $2<r<\\infty$ for $d=3,4$ and large-data in $H^2({\\mathbb R}^d)^N$. Namely, by carrying on the ideas initiated in \\cite{Ta} and \\cite{TarVenk} for systems of Schr\\\"odinger equations with local and non-local nonlinearities, we set up the Morawetz action, its tensor counterpart and get new bilinear Morawetz inequalities for \\eqref{eq:HFC4}. A localized version of such inequalities and a contradiction argument imply the decay of $L^r$-norms of $(u_j(t,x))_{j=1}^N$, also when the HFC4 equation is lacking of translation invariance. The decay of the solution is a strong property of the HFC4 and quickly bears to the asymptotic completeness if one uses an appropriate reformulation of the theory in \\cite{Ca}. We move now on the breakthroughs available in our paper. We underline that our results are new in the whole literature, not only for the HFC4 equations but also for the fourth-order Hartree-Fock equation ($b_{jk}=0$, for all $j,k=1, \\dots, N$ in \\eqref{eq:HFC4}).  \nFor what concerns the fourth-order Schr\\\"odinger-Choquard equation\n\\begin{equation}\\label{eq:GenSchoq}\n  \\begin{cases}\ni \\partial_t u + (\\Delta^{2} -\\sigma_{1}\\Delta+\\sigma_{2}V)u =\\lambda ({|x|^{-(d-\\gamma_1)}}*\n|x|^{-\\rho_{1}}|u|^{p})|x|^{-\\rho_{1}}|u|^{p - 2} u,\\\\\nu(0,x)=u(x),\n  \\end{cases}\n\\end{equation}\nwith $\\lambda<0$, that is \\eqref{eq:HFC4}  when $N=1$, $\\sigma_{1}=0$ and $\\sigma_{2}=0$, we improve to the non-radial setting the various results available in \\cite{Saa1}, \\cite{Saa2}, including also $\\sigma_{1}=1$, $d\\geq 3$ and $\\sigma_{2}>0$. Finally, we also enhance to the non-radial framework the scattering results in \\cite{FWY}, because, as underlined in \\cite{TarVenk}, our technique enables us to treat local and non-local terms in an unified way (see \\cite{CT}). \n\\\\\n\\\\\n Now, we state the first main result of this paper. Namely,\n\n\\begin{theorem}\\label{thm:desl}\n   Let be $2\\leq p<p^*(d)$ and assume \\eqref{eq:bs}, \\eqref{eq:bsIII} being satisfied. Then for any $(u_{j,0})_{j=1}^N \\in H^2({\\mathbb R}^d)^N,$ the unique global solution to \\eqref{eq:HFC4}\n  $(u_j(t,x))_{j=1}^N \\in\\mathcal C({\\mathbb R}, H^2({\\mathbb R}^d))^N$, enjoys:\n   \\begin{align}\\label{eq:desol0}\n\\lim_{t\\rightarrow \\pm \\infty} \\|u_j(t, \\cdot)\\|_{L^r({\\mathbb R}^d)}=0,\n\\end{align}\nfor all $j=1,\\dots,N$, with $2<r<2d/(d-4),$ if $d\\geq 5$ and $2<r<+\\infty$ if $d=3, 4$, in the following cases\n    \\begin{itemize}\n    \\item[1)] if $d\\geq 3$, $(b_{jj},\\sigma_1)\\neq(0,0),$ for all $j=1, \\dots, N$, $(\\rho_{1},\\rho_{2})=(0,0)$ and $\\sigma_2=0$;\n    \\item[2)] if $d\\geq 5$, for  $(\\rho_{1},\\rho_{2}, \\sigma_2)\\neq(0,0,0)$.\n    \\end{itemize}\n   \\end{theorem}\n   \n The direct consequence of this theorem is the scattering in the energy space. \n \n \n\\begin{theorem}\\label{thm:scattHFC4}\n   Let be $2\\leq p<p^*(d)$, $p_{2*}(d)\\leq 2$ and assume \\eqref{eq:bs},  \\eqref{eq:bsII}, \\eqref{eq:bsIII} being satisfied. Then for any $(u_{j,0})_{j=1}^N \\in H^2({\\mathbb R}^d)^N,$ there exist $(u_0^{\\pm})_{j=1}^N \\in H^2({\\mathbb R}^d)^N$ such that the solution to \\eqref{eq:HFC4}\n  $(u_j(t,x))_{j=1}^N \\in\\mathcal C({\\mathbb R}, H^2({\\mathbb R}^d))^N$, fulfills\n    \\begin{equation}\\label{eq:scattering0}\n      \\lim_{t\\to\\pm\\infty}\\left\\|u_j(t,\\cdot)-e^{it(\\Delta^2_x-\\sigma_{1} \\Delta_x+\\sigma_{2}V)}u_{j,0}^{\\pm}(\\cdot)\\right\\|_{H^2({\\mathbb R}^d)}=0,\n    \\end{equation}\nfor any $j=1, \\dots, N$, in the following cases\n    \\begin{itemize}\n    \\item[1)] if $d\\geq 3$, $(b_{jj},\\sigma_1)\\neq(0,0),$ for all $j=1, \\dots, N$, $p>2$, $(b, \\rho_{1},\\rho_{2})=(0, 0,0)$ and $\\sigma_2=0$;\n    \\item[2)]  if $d\\geq 5$, for $p=2$ or $(\\rho_{1},\\rho_{2}, \\sigma_{2})\\neq(0, 0,0)$.\n    \\end{itemize}\n   \\end{theorem}\n\n\n  \n\\begin{remark}[Decay]\\label{decay}\nThe decay of the solutions to \\eqref{eq:HFC4} in $L^r$ spaces requires less restriction on the ranges of the parameters involved in the equation, which is a consequence of the fact that we just need satisfied that $(u_j(t,x))_{j=1}^N \\in C({\\mathbb R}, H^2({\\mathbb R}^d))^N$, assured by the Proposition \\ref{CoLa} below. As well, all the covered different cases can be summarized as follow.\n\\begin{itemize}\n\\item[] \\emph{Case $1)$ Theorem \\ref{thm:desl}}. It embraces  the fourth-order Hartree-Fock equation (with $\\sigma_{1}=1$, $N\\geq2$, $b_{jk}=0$), the fourth-order Schr\\\"odinger-Choquard system ($b_{jj}\\neq 0$, $b=0$, $N\\geq 1$), the more general fourth-order HFC4, this means that the full interaction between the two terms in \\eqref{eq.nonlinHF} is permitted. \n\\item[] \\emph{Case $2)$ Theorem \\ref{thm:desl}}. It comprehends the fourth-order Hartree-Fock  equation (with $\\sigma_{1}\\geq 0)$, the fourth-order  inhomogeneous Schr\\\"odinger-Choquard system ($N\\geq 1$) and  the more general fourth-order inhomogeneous HFC4 equation ($N\\geq2$), both in the unperturbed and perturbed by a potential regime as well as if\n$( \\rho_{1},\\rho_{2})\\neq(0,0)$. Here the space dimension is $d\\geq 5$ because \\eqref{eq:HFC4} is not translation invariant.\n\\end{itemize}\n\\end{remark}\n\n\\begin{remark}[Scattering]\\label{scatter}\nThe scattering of the solutions to \\eqref{eq:HFC4} in $H^{2}({\\mathbb R}^{d})$ needs more summability given by  the Strichartz norms. This reflect to have more constrains on the parameters associated to the equation. Thus, the cases involved are the following.\n\\begin{itemize}\n\\item[] \\emph{Case $1)$ Theorem \\ref{thm:scattHFC4}}. It investigates the fourth-order Schr\\\"odinger-Choquard system only. \n\\item[] \\emph{Case $2)$ Theorem \\ref{thm:scattHFC4}}. It embraces all the other models itemized in the previous Remark \\ref{decay}.\n\\end{itemize}\n\\end{remark}\n\nSurveying the literature, which is not so wide according to our knowledge, we are unaware of alike results, with the exception of the aforementioned \\cite{Saa1} and \\cite{Saa2}. The author prove, in the former, scattering for the solution to \\eqref{eq:HFC4}, with $\\rho_{1}=0,$ by using the radial concentration-compactenss method of \\cite{Guo}, while in the latter establishes the radial decay of the solution to \\eqref{eq:HFC4}, with $\\rho_{1}>0,$ and the corresponding scattering. Additionally, we look to the interaction Morawetz estimates, which were first displayed for the Hartree-fock equations in the recent works \\cite{CGMZ} and \\cite{TarVenk}. \\\\\nThe paper is organized as follows. After the preliminary Section \\ref{Prl}, we afford both the standard and tensor  Morawetz inequalities and their localized analogues in Section \\ref{TMoraw}. Sections \\ref{DecSolu} and \\ref{NFC4scat} are dedicated to the proofs of the Theorems \\ref{thm:desl} (decay) and \\ref{thm:scattHFC4} (scattering), respectively. The final Section  \\ref{appendix} is the Appendix, where an equivalence result between bilinear and tensor Morawetz identities is acquired.\n\n\n\n\n \n\n\n\n\n\\section{Preliminaries}\\label{Prl}\n\\subsection{Notations.} We denote by $L_x^r$ the Lebesgue space $L^r({\\mathbb R}^{d})$, with $r\\geq 1$ and respectively by $W^{s,r}_x$ and  $H^s_x$ the  inhomogeneous Sobolev spaces $W^{s,r}({\\mathbb R}^{d})=(1-\\Delta)^{-\\frac s2}L^r({\\mathbb R}^{d})$ and $H^2({\\mathbb R}^{d})=(1-\\Delta)^{-\\frac s2}L^2({\\mathbb R}^{d})$, with $s\\geq 1$ (for more details see \\cite{Ad}). We indicate by $B^{d}_{x}(r)$, the $d$-dimensional ball centered at $x\\in{\\mathbb R}^{d}$. Moreover, we set $A^N=A \\times \\dots \\times A$, $N$-times, for any general set $A$ and integer $N\\geq 1$.\nWe also utilize the symbol $\\mathcal D_x$ (resp. $\\mathcal D_y$) to unfold the dependence w.r.t. $x$ (resp. $y$) variable of a general differential operator $\\mathscr D$. We adopt in the sequel the following notation: for any two positive real numbers $a, b,$ we write $a\\lesssim b$  (resp. $a\\gtrsim b$) to denote $a\\leq C b$ (resp. $Ca\\geq b$), with $C>0.$\n\\subsection{Inequalities.}\nOne can recall from \\cite{FWY}, \\cite{Pa1}, \\cite{PaSh}, \\cite{PaX}, the following \n\n\n\n\\begin{definition}\\label{Sadm}\nAn exponent pair $(q, r)$ is biharmonic-admissible, in short $(q, r) \\in \\mathcal {B}$, if $2\\leq q,r\\leq \\infty,$  $(q, r, n ) \\neq (2,\\infty, 4),$ and\n\\begin{align}\\label{StrBharmP}\n\\frac 4q +\\frac d {r}=\\frac d2.\n\\end{align}\nWe say  that $(\\tilde q', \\tilde r')\\in  \\mathcal {B}'$  if it is the H\\\"older conjugate of the pair $(q, r)\\in \\mathcal {B}.$\n\\end{definition} \n\n\\begin{proposition}\\label{Stri}\nLet be two biharmonic-admissible pairs $(q,r)$, $(\\widetilde q,\\widetilde r)$. Then we have, for $s=0,1$, that the following estimates\n\\begin{align}\n\\label{eq:StriBiharm}\n \\| \\Delta_{x}^s e^{ it(\\Delta^2_x-\\sigma_{1} \\Delta_x+\\sigma_{2}V)} f\\|_{L^q_t L^r_x} + \\left \\|\\Delta_{x}^s\\int_0^t e^{i (t-\\tau) (\\Delta^2_x-\\sigma_{1} \\Delta_x+\\sigma_{2}V)} F(\\tau)\nd\\tau\\right\\|_{L^q_t L^r_x}&\\\\\\nonumber\n\\leq C\\big (\\|\\Delta_{x}^s f\\|_{ L^2_x}+\\|\\Delta_{x}^s F\\|_{L^{\\widetilde q'}_t\nL^{\\widetilde r'}_x}\\big ),&\n \\end{align}\n is fulfilled in the following cases:\n \\begin{itemize}\n \\item[a)] for $d\\geq 3$, $\\sigma_1 \\geq 0$ and $\\sigma_2=0$;\n \\item[b)] for $d\\geq 5$, $\\sigma_{1}=0$ and $\\sigma_2\\neq 0.$\n \\end{itemize}\n We retreive also, for $d\\geq 5$, the estimates\n  \\begin{align}\\label{eq.casStrex}\n  \\|\\Delta_{x}  e^{ it(\\Delta^2_x+\\sigma_{2} V)} f\\|_{L_{t}^{q}L_{x}^{r}} + \\left \\| \\Delta_{x}\\int_0^t e^{ i (t-\\tau) (\\Delta^2_x+\\sigma_{2} V)} F(\\tau)\nd\\tau\\right\\|_{L^q_t L^r_x}&\\\\\\nonumber\\leq C\\left(\\left\\|\\Delta_{x} f\\right\\|_{L^{2}}+\\|\\nabla_{x} F\\|_{L_{t}^{2}L_{x}^{\\frac{2 d}{d+2}}}\\right)\n\\end{align}\nand\n\\begin{align}\\label{eq.casStrexDu}\n\\left\\|(\\Delta^2_x+ V)^{\\frac{1}{2}} \\int_{\\mathbb{R}} e^{-i \\tau (\\Delta^2_x+ V)} F(\\tau) d s\\right\\|_{L_{x}^{2}} \\leq C\\|\\nabla_{x} F\\|_{L_{t}^{2}L_{x}^{\\frac{2 d}{d+2}}}.\n\\end{align}\n \\end{proposition}\n It is also useful the following (see \\cite{FWY})\n\\begin{proposition}\\label{NormEquivF}\nLet be $V$ as in \\eqref{eq.hopot}. For any $1<r<\\infty$ and $0<s^{*}<2$, one gets\n \\begin{align}\\label{eq.SobEquiv}\n\\left\\|(\\Delta^2_x)^{\\frac{s^{*}}{2}} f\\right\\|_{L^{r}_{x}}\\lesssim \\left\\|(\\Delta^2_x+ V)^{\\frac{s^{*}}{2}} f\\right\\|_{L^{r}_{x}} \\lesssim\\left\\|(\\Delta^2_x)^{\\frac{s^{*}}{2}} f\\right\\|_{L^{r}_{x}}.\n\\end{align}\n \\end{proposition} \n \n \n\\subsection{Well-posedness in energy space.} We summarize some of the well-posedness results for \\eqref{eq:HFC4}, already appeared in \\cite{CLM} \\cite{Saa1}, \\cite{Saa2}, which can be obtained by standard energy method (see Theorem 3.3.9 and Remark 3.3.12 in \\cite{Ca}) and remain valid for the HFC4 systems. This is done in the following\n\n\n\n\\begin{proposition}\\label{CoLa}\nAssume  \\eqref{eq.nonlinHF} is such as in Theorem \\eqref{thm:desl}. Then for $u_{j,0}\\in H^2_x$, with $j=1,\\dots,N$, there exists a unique global solution $(u_j)_{j=1}^N \\in  C({\\mathbb R},H^2_x) ^{N}$ to \\eqref{eq:HFC4}, moreover \n\\begin{align}\n  &M(u_j)(t)=\\norma{u_j(0)}_{L^2_x} \n   \\label{eq:mcon}\n  \\end{align}\n and \n  \\begin{align}\n  &E(u_1(t),\\dots,u_N(t))=E(u_1(0),\\dots,u_N(0))\n  \\label{eq:econs},\n\\end{align}\nwith $E(u_1(t),\\dots,u_N(t))$ as in \\eqref{eq:Ener}.\n\\end{proposition}\n\n\n \n\n\\section{Morawetz and Tensor Morawetz identities}\\label{TMoraw}\nThe main aim of this section is to provide the Morawetz-type identities and inequalities. From now on we hide the $t$-variable for easiness, spreading it out only when it is required. We have\n\\begin{proposition}[Morawetz]\\label{lem:mor}\nLet $(u_j(t,x))_{j=1}^N \\in C({\\mathbb R}, H^2({\\mathbb R}^d))^N$ be a global solution to the system \\eqref{eq:HFC4},\n let $a=a(x): {\\mathbb R}^d \\rightarrow {\\mathbb R}$ be a sufficiently regular function and introduce the $j$-action given, for any $j=1,\\dots, N$, by\n\\begin{equation}\\label{eq.singMor}\nM_j(t)\n=2 \\Im\\int_{{\\mathbb R}^d} \\overline{u_j}(x)\\nabla u_j(x) \\cdot\\nabla a(x)\\,dx.\n\\end{equation}\nThe following identity holds:\n\\begin{align}\\label{eq:mor2}\n \\sum _{j=1}^N\\dot M_j(t)\n=  \\sum _{j=1}^N\\int_{{\\mathbb R}^d} (-\\Delta^3a(x)+\\sigma_1 \\Delta^2a(x))|u_j(x)|^2+2\\Delta^2a(x)|\\nabla u_j(x)|^2\\,dx&\n\\\\\n+\n4  \\sum _{j=1}^N\\Re\\int_{{\\mathbb R}^d}\\nabla u_j(x)D^2(\\Delta-\\sigma_1)a(x)\\cdot\\nabla \\overline u_j(x)\\,dx\n\\nonumber\n\\\\\n-8 \\sum _{j=1}^N \\Re\\int_{{\\mathbb R}^d}D^2 u_j(x)D^2a(x)D^2 \\overline u_j(x)\\,dx+\\sigma_2 \\sum _{j=1}^N\\int_{{\\mathbb R}^d}\\nabla a(x) \\cdot \\nabla V(x)|u_j(x)|^2\\,dx\n\\nonumber\n\\\\\n-\\frac{2(p-2)}{p}\\sum _{j,k=1}^Nb_{jk}\n\\int_{{\\mathbb R}^d} \\Delta a(x)\\squad{|x|^{-\\rho_{1}}\\round{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}|u_k|^p)}} | u_j(x)|^{p}\\,dx\n\\nonumber\\\\\n+\n\\frac 4 p\\sum_{j,k=1}^N b_{jk}\\int_{{\\mathbb R}^d}\\nabla a(x)\\cdot \\nabla \\squad{|x|^{-\\rho_{1}}\\round{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)}} | u_j(x)|^{p}\\,dx\n\\nonumber\\\\\n -2b\\sum_{j,k=1}^N\\int_{{\\mathbb R}^d}\\Delta a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_2}| u_k|^2) }|x|^{-\\rho_2}|u_j(x)|^2\\,dx\n\\nonumber \\\\\n  +2b\\sum_{j,k=1}^N\\int_{{\\mathbb R}^d}\\Delta a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_2}\\overline u_k u_j ) }|x|^{-\\rho_2} u_k(x)  \\overline u_j(x)\\,dx\n \\nonumber\\\\\n+2b\\sum_{j,k=1}^N\\int_{{\\mathbb R}^d}\\nabla a(x)\\cdot\\nabla\\squad{|x|^{-\\rho_{2}}\\round{|x|^{-(d-\\gamma_2)}*(-|x|^{\\rho_{2}}| u_k|^2 )}} |u_j(x)|^2\\, dx\n\\nonumber\n \\\\\n -2b\\sum_{j,k=1}^N\\int_{{\\mathbb R}^d}\\nabla a(x)\\cdot \\nabla \\squad{|x|^{-\\rho_{2}}\\round{|x|^{-(d-\\gamma_2)}*({|x|^{-\\rho_{2}}\\overline u_k u_j  }}} u_k(x)  \\overline u_j(x) \\, dx,\n\\nonumber\n\\end{align}\nwith $\\sigma_{1}, \\sigma_{2}=0,1$, $D^2\\phi\\in\\mathcal M_{d\\times d}({\\mathbb R}^d)$ is the hessian matrix of $\\phi$, $\\Delta^2a=\\Delta(\\Delta\\phi)$\nand $\\Delta^3a=\\Delta(\\Delta^2a)$.\n\\end{proposition}\n\\begin{proof} We can assume without loosing generality that the solution of  \\eqref{eq:HFC4} is smooth and decaying, switching to the general case by using in a final density argument in the space $ C({\\mathbb R},H^2({\\mathbb R}^d)^N).$ Then, an integration by parts implies\n\n  \\begin{align}\\label{eq.smor1}\n \\sum _{j=1}^N \\partial_t M_j(t)  =-2 \\sum _{j=1}^N\\Im  \\int_{{\\mathbb R}^d} \\partial_t u_j(x) [\\Delta a(x)\\overline u_j(x)+2\\nabla a(x)\\cdot \\nabla \\overline u_j(x)]\\,dx& \\\\\n    =2 \\sum _{j=1}^N\\Re \\int_{{\\mathbb R}^d} i \\partial_t u_j(x) [\\Delta a(x)\\overline u_j(x)+2\\nabla a(x)\\cdot \\nabla \\overline u_j(x)]\\,dx& \\nonumber\n    \\\\ \n    =2 \\sum _{j=1}^N\\Re \\int_{{\\mathbb R}^d} [-\\Delta^2 u_j(x) +\\sigma_1\\Delta u_j(x)\n    ][(\\Delta a(x)\\overline u_j(x)+2\\nabla a(x)\\cdot \\nabla \\overline u_j(x)]\\,dx&\n \\label{eq.smor1a}\n \\\\\n -2\\sigma_2 \\sum _{j=1}^N\\Re \\int_{{\\mathbb R}^d} V(x)u_j(x)\n    [(\\Delta a(x)\\overline u_j(x)+2\\nabla a(x)\\cdot \\nabla \\overline u_j(x)]\\,dx&\n   \\nonumber\n    \\\\\n   -2\\sum_{j,k=1}^Nb_{jk}\\int_{{\\mathbb R}^d} \\Delta a(x)\\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_1}| u_j|^p)}|x|^{-\\rho_1} | u_k(x)|^{p}\\,dx\n \\label{eq.smor1b}\n\\\\\n-4\\sum_{j,k=1}^Nb_{jk}\\Re\\int_{{\\mathbb R}^d} \\nabla a(x)\\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_1} |u_k|^p)}|x|^{-\\rho_1} | u_j(x)|^{p-2}u_j(x)\\nabla \\overline u_j(x)\\,dx\n \\label{eq.smor1c}\n    \\\\\n -2b\\sum_{j,k=1}^N\\int_{{\\mathbb R}^d}\\Delta a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_2}| u_k|^2) }|x|^{-\\rho_2}|u_j(x)|^2\\,dx\n \\label{eq.smor1d}\n \\\\\n  +2b\\sum_{j,k=1}^N\\int_{{\\mathbb R}^d}\\Delta a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_2}\\overline u_k u_j)  }|x|^{-\\rho_2} u_k(x)  \\overline u_j(x)\\,dx\n \\nonumber\\\\\n -4b\\sum_{j,k=1}^N\\Re\\int_{{\\mathbb R}^d}\\nabla a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_2}| u_k|^2) }|x|^{-\\rho_2}u_j(x)\\nabla\\overline u_j(x)\\, dx\n \\label{eq.smor1e}\n \\\\\n +4b\\sum_{j,k=1}^N\\Re\\int_{{\\mathbb R}^d}\\nabla a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_2}\\overline u_k u_j)  }|x|^{-\\rho_2} u_k(x) \\nabla \\overline u_j(x) \\, dx.\n\\nonumber\n\\end{align}\n  By proceeding as in \\cite{Ta} in combination with a further integrations by parts of the term involving $V(x)$, we get \n    \\begin{align}\\label{eq.smor2}\n \\eqref{eq.smor1a}=  \\sum _{j=1}^N \\int_{{\\mathbb R}^d} (-\\Delta^3a(x)+\\sigma_1 \\Delta^2a(x))|u_j(x)|^2+2\\Delta^2a(x)|\\nabla u_j(x)|^2\\,dx&\n\\\\\n+\n4 \\sum _{j=1}^N \\int_{{\\mathbb R}^d}\\nabla u_j(x)D^2(\\Delta-\\sigma_1)a(x)\\cdot\\nabla \\overline u_j(x)\\,dx\n-8 \\sum _{j=1}^N\\int_{{\\mathbb R}^d}D^2 u_j(x)D^2a(x)D^2 \\overline u_j(x)\\,dx\n\\nonumber\n\\\\\n+\\int_{{\\mathbb R}^d}\\nabla a(x) \\cdot \\nabla V(x)|u_j(x)|^2\\,dx\n    \\nonumber\n  \\end{align}\n  By arguing as in \\cite{TarVenk} we have \n  \\begin{align}\\label{eq.smor3}\n \\eqref{eq.smor1b}+\\eqref{eq.smor1c}\n \\\\\n = -2\\sum_{j,k=1}^Nb_{jk}\\int_{{\\mathbb R}^d} \\Delta a(x)\\squad{|x|^{-(d-\\gamma)}*(|x|^{-\\rho_1}| u_j|^p)} |x|^{-\\rho_1}| u_j(x)|^{p}\\,dx\n\\nonumber\\\\\n-4\\sum_{j,k=1}^Nb_{jk}\\int_{{\\mathbb R}^d} \\nabla a(x)\\squad{|x|^{-(d-\\gamma)}*(|x|^{-\\rho_2}| u_k|^p)} |x|^{-\\rho_1}| u_j(x)|^{p-2}\\frac 12\\nabla |u_j(x)|^2\\,dx\n    \\nonumber\n    \\\\\n    =-\\frac{2(p-2)}{p}\\sum _{k=1}^Nb_{jk}\n\\int_{{\\mathbb R}^d} \\Delta a(x)\\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_1}|u_k|^p)} |x|^{-\\rho_1}| u_j(x)|^{p}\\,dx \n\\nonumber\n    \\\\\n    +\\frac 4p\\sum_{j,k=1}^Nb_{jk}\\int_{{\\mathbb R}^d} \\nabla a(x)\\nabla\\squad{|x|^{-\\rho_1}\\round{ |x|^{-(d-\\gamma)}*(|x|^{-\\rho_1}| u_k|^p)}} | u_j(x)|^{p}\\,dx\n    \\nonumber\n    \\end{align}\nand\n   \\begin{align}\\label{eq.smor4}\n\\eqref{eq.smor1e}\n \\\\\n-4b\\sum_{j,k=1}^N\\Re\\int_{{\\mathbb R}^d}\\nabla a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_2}| u_k|^2 )}|x|^{-\\rho_2}u_j(x)\\nabla\\overline u_j(x)\\, dx\n\\nonumber\n \\\\\n +4b\\sum_{j,k=1}^N\\Re\\int_{{\\mathbb R}^d}\\nabla a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_2}\\overline u_k u_j)  }|x|^{-\\rho_2} u_k(x) \\nabla \\overline u_j(x) \\, dx\n\\nonumber\n\\\\\n=-2b\\sum_{j,k=1}^N\\Re\\int_{{\\mathbb R}^d}\\nabla a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_2}| u_k|^2 )}|x|^{-\\rho_2}\\nabla |u_j(x)|^2\n\\nonumber\n \\\\\n +2b\\sum_{j,k=1}^N2\\Re\\int_{{\\mathbb R}^d}|x|^{-\\rho_2}|y|^{-\\rho_2}\\nabla a(x)\\frac{\\overline u_k(y)   u_k(x) \\nabla u_j(y)  \\overline u_j(x) }{|x-y|^{d-\\gamma_2}}   \\, dxdy=\n\\nonumber\n\\\\\n=-\\eqref{eq.smor1d}+2b\\sum_{j,k=1}^N\\int_{{\\mathbb R}^d}\\nabla a(x)\\cdot \\nabla \\squad{|x|^{-\\rho_2}\\round{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_2}| u_k|^2 )}} |u_j(x)|^2\\, dx\n\\nonumber\n \\\\\n -2b\\sum_{j,k=1}^N\\int_{{\\mathbb R}^d}\\nabla a(x)\\cdot \\nabla \\squad{|x|^{-\\rho_2}\\round{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_2}\\overline u_k u_j)} } |x|^{-\\rho_2} u_k(x)  \\overline u_j(x) \\, dx.\n\\nonumber\n \\end{align}\nFinally, by taking into account \\eqref{eq.smor2}, \\eqref{eq.smor3}, \\eqref{eq.smor4}, the identity \\eqref{eq.smor1} gives rise to \\eqref{eq:mor2}.\n  \\end{proof}\n\n\n\n\n\n\n\n\nBy an application of the above lemma, we can now go over to the proof of the tensor Morawetz identities. More precisely, we have\n\n\n\\begin{lemma}[Tensor Morawetz]\\label{lem:tenmor}\nAssume $d\\geq 3$ and let $(u_j(t,x))_{j=1}^N \\in C({\\mathbb R}, H^2({\\mathbb R}^d))^N$ be a global solution to system \\eqref{eq:HFC4}. Further, let us denote by $z_{j,\\ell}(x,y)=u_j(x)u_{\\ell}(y)$, $a(x,y)=|x-y|$ and set the $j,\\ell$-tensor action\n\\begin{equation}\\label{eq:tenmor1}\n  \\mathcal M_{j,\\ell}(t) =2\\Im\\int_{{\\mathbb R}^{2d}}\\overline{z_{j,\\ell}}(x,y)(\\nabla_{x},\\nabla_{y})z_{j,\\ell}(x,y)  \\cdot(\\nabla_{x},\\nabla_{y})a(x,y)\\,dxdy.\n\\end{equation}\nThen the following inequality holds:\n  \\begin{align}\\label{eq:tenmor2}\n  \\sum_{j,\\ell=1}^N\\mathcal{ \\dot{M}}_{j,\\ell}(t)\n  \\\\\n  \\lesssim 2\\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\n  \\round{\\Xi(x,y)+\\sigma_{1} \\Delta^2_xa(x,y)| u_j(x)|^2|u_{\\ell}(y)|^2} \\,dxdy\n \\nonumber\n \\\\\n- \\sum_{j,\\ell, k}^N \\widetilde b_{jk}\\int_{{\\mathbb R}^{2d}}\\Delta_x a(x,y) \\squad{|x|^{-(d-\\gamma_1)}* (|x|^{-\\rho_{1}}| u_k(x)|^p)}|x|^{-\\rho_{1}}|u_j(x)|^{p}|u_{\\ell}(y)|^2\\,dxdy\n\\nonumber\n \\\\\n+ \\mathcal R(t),\n\\nonumber\n\\end{align}\nwith $ \\widetilde b_{jk}=4b_{jk}(p-2)/p,$\n\\begin{equation}\\label{eq.leadtherm}\n\\Xi(x,y)=\n\\begin{cases}\n \\Delta^2_xa(x,y) \\nabla_x| u_j(x)|^2\\nabla_y|u_{\\ell}(y)|^2 ,& \\text{if} \\ \\ \\ d\\geq3,\\cr\n\\\\\n- \\Delta^3_xa(x,y) | u_j(x)|^2|u_{\\ell}(y)|^2, & \\text{if} \\ \\ \\ d\\geq5\n\\end{cases}\n\\end{equation}\nand \n  \\begin{align}\\label{eq:tenmor3}\n \\mathcal R(t)\n  \\\\\n =2\\sigma_2\\sum_{j,\\ell=1}^N \\int_{{\\mathbb R}^{2d}}\\nabla_{x} a(x,y) \\cdot \\nabla V(x)| u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy&\n \\nonumber\n \\\\\n+ \\sum_{j, k, \\ell}^Nb_{jk}\\int_{{\\mathbb R}^{2d}}\\nabla_x a(x,y) \\cdot \\nabla_x |x|^{-\\rho_{1}} \\squad{|x|^{-(d-\\gamma_1)}* (|x|^{-\\rho_{1}}| u_k(x)|^p)}|u_j(x)|^{p}|u_{\\ell}(y)|^2\\,dxdy\n\\nonumber\n\\nonumber\\\\\n+2b\\sum_{j,k, \\ell=1}^N\\int_{{\\mathbb R}^{2d}}\\nabla a(x,y)\\cdot\\nabla |x|^{-\\rho_{2}}\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_{2}}| u_k|^2 )} |u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy\\nonumber\n \\\\\n -2b\\sum_{j,k, \\ell=1}^N\\int_{{\\mathbb R}^{2d}}\\nabla a(x,y)\\cdot \\nabla |x|^{-\\rho_{2}}\\squad{|x|^{-(d-\\gamma_2)}*({|x|^{-\\rho_{2}}\\overline u_k u_j  )}}u_k(x)  \\overline u_j(x) |u_{\\ell}(y)|^2\\,dxdy.\n\\nonumber\n\\end{align}\n\\end{lemma}\n\n\\begin{proof}\nAs before, we prove the identities for a smooth, decaying solution to \\eqref{eq:HFC4}.\n First, we observe that,\n \\begin{align}\\label{eq.tenid}\n  i \\partial_t z_{j,\\ell}(x,y)+ (\\Delta_{x,y}^2-\\sigma\\Delta_{x,y})z_{j,\\ell} (x,y)\n=- (V(x)+V(y))u_{j}(x) u_{\\ell} (y) \\\\\n-\\sum_{k=1}^N\\mathcal F(u_j(x) , u_k(x))u_{\\ell}(y)-\\sum_{k=1}^N\\mathcal F(u_{\\ell}(y) , u_k(y))u_{j}(x),\n  \\nonumber\n \\end{align}\nwith $\\Delta^s_{x,y}=\\Delta^s_{x}+\\Delta^s_{y}$, for $s=1,2$. Then, we differentiate the tensor action w.r.t. time variable, achieving\n  \\begin{align}\\label{eq.TenMor1}\n  \\mathcal {\\dot M}_{j,\\ell}(t)\\\\\n  =2\\Re\\int_{{\\mathbb R}^{2d}} i \\partial_t z_{j,\\ell}(x,y) [\\Delta_{x,y} a(x,y)\\overline z_{j,\\ell}(x,y)+2(\\nabla_{x},\\nabla_{y}) a(x,y)\\cdot (\\nabla_{x},\\nabla_{y}) \\overline z_{j,\\ell}(x,y)]\\,dx&\n    \\nonumber\\\\ \n    =2\\Re\\int_{{\\mathbb R}^{2d}} [\\Delta_{x,y}^2 z_{j,\\ell}(x,y) -\\sigma_1\\Delta_{x,y}  z_{j,\\ell}(x,y)]\n    [(\\Delta_{x,y} a(x,y)\\overline z_{j,\\ell}(x,y)]\\,dxdy& \n    \\nonumber\\\\\n   + 4\\Re\\int_{{\\mathbb R}^{2d}}  [\\Delta_{x,y}^2 z_{j,\\ell}(x,y) -\\sigma_1\\Delta_{x,y}  z_{j,\\ell}(x,y)]\n     [(\\nabla_{x},\\nabla_{y}) a(x,y)\\cdot (\\nabla_{x},\\nabla_{y}) \\overline z_{j,\\ell}(x,y)]\\,dxdy& \n    \\nonumber\\\\\n    +\\mathcal N^p_{j,\\ell}(t)+\\mathcal N_{j,\\ell}(t)+ \\mathcal N^V_{j,\\ell}(t),\n     \\nonumber\n    \\end{align}\n   where, by the identity \\eqref{eq.tenid} and after exploiting the\n  symmetry of $a(x,y)$ in combination with the Fubini's Theorem, we have\n     \\begin{align}\\label{eq.imoranl}\n\\sum_{j, \\ell=1}^N \\mathcal   N^p_{j,\\ell}(t)\n \\\\\n=  -4\\sum_{j, k, \\ell=1}^Nb_{k}\\int_{{\\mathbb R}^{2d}}\\left( \\Delta_x a(x)\\squad{|x|^{-(d-\\gamma_{1})}*(|x|^{-\\rho_{1}}| u_j|^p)} |x|^{-\\rho_{1}}| u_k(x)|^{p}|u_{\\ell}(y)|^2\\right . \n\\nonumber\\\\\n\\left . -2\\Re\\nabla_x a(x)\\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)} \\cdot |x|^{-\\rho_{1}}| u_j(x)|^{p-2}u_j(x)\\nabla_x \\overline u_j(x)|u_{\\ell}(y)|^2\\,dxdy\\right),\n\\nonumber\n   \\end{align}\n      \\begin{align}\\label{eq.imorbnl}\n\\sum_{j, \\ell=1}^N \\mathcal   N_{j,\\ell}(t)\n \\\\\n =   -4b\\sum_{j, k, \\ell=1}^N\\Re\\int_{{\\mathbb R}^{2d}}\\Delta_x a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_{2}}| u_k|^2 )}|x|^{-\\rho_{2}}|u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy\n\\nonumber \\\\\n  +4b\\sum_{j, k, \\ell=1}^N\\Re\\int_{{\\mathbb R}^{2d}}\\Delta_x a(x)\\squad{|x|^{-(d-\\gamma_2)}* (|x|^{-\\rho_{2}}\\overline u_k u_j)  }|x|^{-\\rho_{2}} u_k(x)  \\overline u_j(x)|u_{\\ell}(y)|^2\\,dxdy\n  \\nonumber\n\\\\\n -8b\\sum_{j, k, \\ell=1}^N\\Re\\int_{{\\mathbb R}^{2d}}\\nabla_x a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_{2}}| u_k|^2) }|x|^{-\\rho_{2}}u_j(x)\\nabla\\overline u_j(x)|u_{\\ell}(y)|^2\\,dxdy\n\\nonumber\n \\\\\n +8b\\sum_{j, k, \\ell=1}^N\\Re\\int_{{\\mathbb R}^{2d}}\\nabla_x a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_{2}}\\overline u_k u_j)  }|x|^{-\\rho_{2}} u_k(x) \\nabla \\overline u_j(x) |u_{\\ell}(y)|^2\\,dxdy\n\\nonumber   \n\\end{align}\nand\n      \\begin{align}\n\\sum_ {j,\\ell=1}\\mathcal   N^V_{j,\\ell}(t)\n \\\\\n =-\\sigma_2\\sum_{j, \\ell=1}\\Re \\int_{{\\mathbb R}^{2d}}V(x)u_{j}(x) \n    [(\\Delta_{x} a(x,y)\\overline u_{j}(x)+2\\nabla_{x} a(x,y)\\cdot \\nabla_{x}\\overline u_{j}(x)]|u_{\\ell}(y)|^2\\,dxdy.& \n    \\nonumber\n \\end{align}\n\\emph{Bilaplacian's terms.} We start by handling the terms in \\eqref{eq.tenid} involving $\\Delta^2_{x,y}$. We apply  \\eqref{eq:mor2} (see \\cite{MWZ} and \\cite{Ta}), then the Fubini's Theorem allows us to write\n  \\begin{align}\\label{eq:2iniz}\nI_1(t)=  2\\sum_{j,\\ell=1}^N\\Re \\int_{{\\mathbb R}^{2d}} \\Delta_{x,y}^2 z_{j,\\ell}(x,y) \n    [(\\Delta_{x,y} a(x,y)\\overline z_{j,\\ell}(x,y)]\\,dxdy& \\\\\n    + 4 \\sum_{j,\\ell=1}^N\\Re \\int_{{\\mathbb R}^{2d}} \\Delta_{x,y}^2 z_{j,\\ell}(x,y) \n    [(\\nabla_{x},\\nabla_{y}) a(x,y)\\cdot (\\nabla_{x},\\nabla_{y}) \\overline z_{j,\\ell}(x,y)]\\,dxdy& \n    \\nonumber\\\\\n=\n  2\\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\n \\Delta^2_xa(x,y)\\round{\\nabla_x|u_j(x)|^2\\nabla_y|u_{\\ell}(y)|^2+2|\\nabla_x u_j(x)|^2|u_{\\ell}(y)|^2}\\,dxdy&\n   \\nonumber\n  \\\\\n   \\,8 \\sum_{j,\\ell=1}^N\\Re\\int_{{\\mathbb R}^{2d}}\n\\nabla_xu_{j}(x)D^2_x\\Delta_xa(x,y)\\nabla_x\\overline u_{j}(x) |u_{\\ell}(y)|^2\\,dxdy &\n \\nonumber\\\\\n -16 \\sum_{j,\\ell=1}^N\\Re\\int_{{\\mathbb R}^{2d}}D_x^2 u_{j}(x)D_x^2a(x,y)D_x^2 \\overline u_{j}(x) |u_{\\ell}(y)|^2\\,dxdy,&\n\\nonumber\n   \\end{align}\n  where in the third line of the above \\eqref{eq:2iniz} we applied the identity \n  \\begin{equation}\\label{eq:eqval}\n    \\begin{split}\n    -\\int_{{\\mathbb R}^{2d}} \\Delta_x^3a(x,y) | u_j(x)|^{2}|u_{\\ell}(y)|^2\\,dxdy=\n    \\\\\n    \\int_{{\\mathbb R}^{2d}}\\nabla_x | u_j(x)|^{2}\\cdot\\nabla_y|u_{\\ell}(y)|^2\\Delta_x^2a(x,y)\\,dxdy.\n    \\end{split}\n  \\end{equation}\n  Let us exploit  the fact that $a(x,y)=|x-y|$. We achieve \n  \\begin{equation}\\label{eq.delta1}\n\\Delta_x|x-y|=\n\\begin{cases}\n\\frac{d-1}{|x-y|}    &\\text{if} \\ \\ \\ d\\geq2,\\cr\n\\\\\nc_{1}\\delta_{x=y} &\\text{if} \\ \\ \\ d=1,\n\\end{cases}\n\\end{equation}\n\\begin{equation}\\label{eq.delta2}\n\\Delta^2_x|x-y|=\n\\begin{cases}\n-\\frac{(d-1)(d-3)}{|x-y|^3}   &\\text{if} \\ \\ \\ d\\geq4,\\cr\n\\\\\n-c_{2}\\delta_{x=y} &\\text{if} \\ \\ \\ d=3\n\\end{cases}\n\\end{equation}\nand\n\\begin{equation}\\label{eq.delta3}\n\\Delta^3_x|x-y|=\n\\begin{cases}\n\\frac{(d-3)(d-5)}{|x-y|^5}  &\\text{if} \\ \\ \\ d\\geq6,\\cr\n\\\\\nc_{3}\\delta_{x=y}, &\\text{if} \\ \\ \\ d=5,\n\\end{cases}\n\\end{equation}\nfor some $c_{1}, c_{2},c_{3}>0$. Furthermore, if one sets $\\nabla_{v}^{\\bot}f=\\nabla f- (v\\cdot \\nabla f)v/|v|^2$, for $v\\in{\\mathbb R}^{d}$, it fulfills\n\\begin{equation}\\label{eq.quad1}\nD^2_x u_j(x)D^2_x|x-y|D^2_x \\overline u_j(x)\\geq \n\\frac{(d-1)}{|x-y|^3}|\\nabla u(x)-\\nabla_{x-y}^{\\bot}u(x)|^2,\n\\end{equation}\nfor $d\\geq2$ (we refer to \\cite{LS}) and  \n\\begin{equation}\\label{eq.quad2}\n\\nabla_x u(x)D_x^2\\Delta^2_x|x-y|\\nabla_x \\overline u(x)\n=-\\frac{(d-1)}{|x-y|^3}(|\\nabla_{x-y}^{\\bot}u(x)|^2- 2|\\nabla u(x)-\\nabla_{x-y}^{\\bot}u(x)|^2),\n\\end{equation}\n(see \\cite{MWZ} for more details). We have also, by the Fourier transform and Plancherel's identity, that\n\\begin{align}\\label{eq.equiv2}\n\\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\n \\Delta^2_xa(x,y)\\nabla_x|u_j(x)|^2\\nabla_y|u_{\\ell}(y)|^2\\,dxdy&\\\\\n =-\\round{\\Delta_x\\sum_{j,\\ell=1}^N|u_j(x)|^2 ,(-\\Delta_x)^{\\frac{1-d}2} \\Delta_x \\sum_{j,\\ell=1}^N|u_{\\ell}(x)|^2}\\leq 0,\n  \\nonumber\n\\end{align}\nfor any $d\\geq 1$. Finally, By gathering \\eqref{eq.quad1}, \\eqref{eq.quad2}, \\eqref{eq.equiv2} and \\eqref{eq:2iniz} we attain $I_1(t)\\leq 0.$\\\\\n\\emph{Laplacian's terms.} We will consider the terms involving $\\Delta_{x,y}$. The approach of \\cite{CGT}, \\cite{Ta} and the Fubini's Theorem, bring to\n  \\begin{align}\\label{eq.laplt}\n I_2(t)= -2\\sum_{j,\\ell=1}^N\\sigma_1\\Re \\int_{{\\mathbb R}^{2d}} \\Delta_{x,y} z_{j,\\ell}(x,y) \n    [(\\Delta_{x,y} a(x,y)\\overline z_{j,\\ell}(x,y)]\\,dxdy& \\\\\n    - 4\\sum_{j,\\ell=1}^N\\sigma_1\\Re \\int_{{\\mathbb R}^{2d}} \\Delta_{x,y} z_{j,\\ell}(x,y) \n    [(\\nabla_{x},\\nabla_{y}) a(x,y)\\cdot (\\nabla_{x},\\nabla_{y}) \\overline z_{j,\\ell}(x,y)]\\,dxdy& \n    \\nonumber\\\\\n=\n  2\\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\n \\Delta^2_xa(x,y) | u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy&\n\\nonumber\\\\\n    - 4\\sigma_1\\sum_{j,\\ell=1}^N \\Re \\int_{{\\mathbb R}^{2d}}  \\nabla_{x}u_{j}(x)D^2_{xy}a(x,y)\\nabla_{x}\\overline u_{j}(x)|u_{\\ell}(y)|^2\\,dxdy&\n    \\label{eq.laplta}\\\\\n -4\\sigma_1\\sum_{j,\\ell=1}^N\\Re\\int_{{\\mathbb R}^{2d}} \\nabla_{y}u_{\\ell}(y)D^2_{xy}a(x,y)\\nabla_{y}\\overline u_{\\ell}(y)|u_{\\ell}(x)|^2\\,dxdy&\n \\label{eq.lapltb}\\\\\n  +8\\sigma_1\\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\n  \\Im(\\overline u_{j}(x)\\nabla_{x}u_{j}(x))D^2_{xy}a(x,y)\n  \\Im(\\overline u_{\\ell}(y)\\nabla_{y}u_{\\ell}(y))\\,dxdy.&\n  \\label{eq.lapltc}\n   \\end{align}\nOne can verifies that (see Lemma \\ref{eq.MorEqiv}, we refer also to \\cite{TarVenk}) \n  \\begin{align}\\label{eq.lapl2}\n  \\eqref{eq.laplta}+\\eqref{eq.lapltb}+\\eqref{eq.lapltc}\n  \\\\=-4\\sigma_1\\int_{{\\mathbb R}^{2d}}\n \\left(H_{jk} D^2_{x}\\phi(|x-y|)\\overline{H}_{j\\ell}+G_{j\\ell} D^2_{x}\\phi(|x-y|)\\overline{G}_{jk}\\right)\\,dxdy&,  \n \\nonumber\n\\end{align}\nwhere\n   \\begin{align*}\n    H_{j\\ell}\n    &\n    :=u_{j}(t,x)\\nabla_{y}\\overline{u_{j}(t,y)}+\\nabla_{x}u_{\\ell}(t,x)\\overline{u_{\\ell}(t,y)},\n    \\\\\n    G_{j\\ell\n    &\n    :=u_{j}(t,x)\\nabla_{y}u_{\\ell}(t,y)-\\nabla_{x}u_{j}(t,x)u_{\\ell}(t,y).\n  \\end{align*}\n  Thus, since $a(x,y)$ is a convex function satisfying \\eqref{eq.delta2}, one gets the chain of inequalities\n  \\begin{equation}\\label{eq.laplt3}\n  I_{2}(t)\\leq  2\\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\n \\Delta^2_xa(x,y) | u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy \\leq 0.\n \\end{equation}\n\\emph{Nonlinear terms.} We will start by treating the term $\\mathcal   N^p_{j,\\ell}(t)$. We have in fact\n\\begin{align}  \\label{eq:nnlna}\n N^p_{j,\\ell}(t)\n\\\\\n=\\sum_{k=1}^N\\left (-\\widetilde b_{jk}\\int_{{\\mathbb R}^{2d}}\\Delta_xa(x,y) \\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k(x)|^p)}|x|^{-\\rho_{1}}|u_j(x)|^{p}|u_{\\ell}(y)|^2\\,dxdy\\right.&\n \\nonumber\\\\\n\\left.+\\frac {8b_{jk}}p\\int_{{\\mathbb R}^{2d}} \\nabla_xa(x,y) \\cdot \\nabla_x \\round{|x|^{-\\rho_{1}}\\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)}}|u_j(x)|^{p}|u_{\\ell}(y)|^2\\,dxdy\\right),&\n \\nonumber\n\\end{align}\nwhere $ \\widetilde b_{jk}$ is as in \\eqref{eq:tenmor2}.\nWe notice also that, by means of\n\\begin{equation}\\label{eq.dMor}\n\\nabla_xa(x,y)=\\frac{x-y}{|x-y|},\n\\end{equation}\nwe can handle the last term in \\eqref{eq:nnlna} as\n\\begin{align}  \\label{eq:nonlin2}\n\\sum_{j,k,\\ell=1}^N\\left(b_{jk}^*\\int_{{\\mathbb R}^{2d}}\n \\frac{(x-y)\\cdot(x-z)|u_j(x)|^{p}| u_k(z)|^{p}}{|x-y||x-z|^{d-\\gamma_1+2}}|x|^{-\\rho_{1}}|y|^{-\\rho_{1}} |u_{\\ell}(y)|^2 \\,dxdydz\\right.\n \\\\\n\\left.+\\frac {8b_{jk}}p\\int_{{\\mathbb R}^{2d}}\n \\nabla_xa(x,y) \\cdot \\nabla_x |x|^{-\\rho_{1}}\\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)}|u_j(x)|^{p}|u_{\\ell}(y)|^2\\,dxdy\\right)\n \\nonumber\\\\\n=\\sum_{\\substack{j,k, \\ell=1}}^N\\left(\\frac 12 b_{jk}^*\\int_{{\\mathbb R}^{2d}}\n \\frac{|x|^{-\\rho_{1}}|y|^{-\\rho_{1}}}{|x-z|^{d-\\gamma_1+2}}|u_j(x)|^{p}| u_k(z)|^{p}K_{\\ell}(x,z)\\,dxdz\\right.\n \\nonumber\\\\\n \\left.+\\frac {8b_{jk}}p\\int_{{\\mathbb R}^{2d}}\n \\nabla_xa(x,y) \\cdot \\nabla_x |x|^{-\\rho_{1}}\\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)}|u_j(x)|^{p}|u_{\\ell}(y)|^2\\,dxdy\\right),\n \\nonumber\n\\end{align}\nwith $b_{jk}^*=-8b_{jk}(d-\\gamma_1)/p$ and where\n\\begin{equation}\\label{eq.Kern}\n K_{\\ell}(x,z)=(x-z)\\cdot \\int_{{\\mathbb R}^d}|u_{\\ell}(y)|^2\\round{\\frac{x-y}{|x-y|}-\\frac{z-y}{|z-y|}}\\, dy.\n\\end{equation}\nThen, the inequality \n\\begin{align}\\label{eq.ineq}\n(x-z) \\cdot \\round{\\frac{x-y}{|x-y|}-\\frac{z-y}{|z-y|}}\n\\\\\n=\\round{|x-y||z-y|-(x-y)\\cdot(z-y)}\\round{\\frac{|x-y|+|z-y|}{|x-y||z-y|}}\\geq 0,\n\\nonumber\n\\end{align}\nenhances to\n\\begin{align}\\label{eq.inf}\n\\inf_{(x,y)\\in{\\mathbb R}^{2d} }K_{\\ell}(x,z)\\geq 0.\n\\end{align}\nBy gathering the above \\eqref{eq.inf} with \\eqref{eq:nonlin2} we obtain then, for any $t\\in{\\mathbb R}$,\n\\begin{align}  \\label{eq:nnlna11}\n\\sum_{j,\\ell=1}^N\\mathcal N^p_{j,\\ell}(t)\n\\\\\n\\leq -\\sum_{j,k,\\ell=1}^N\\left(\\widetilde b_{jk}\\int_{{\\mathbb R}^{2d}}\\Delta_xa(x,y) \\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k(x)|^p)}|x|^{-\\rho_{1}}|u_j(x)|^{p}|u_{\\ell}(y)|^2\\right.\n\\nonumber\n\\\\\n+\\left.\\frac {8 b_{jk}}{p} \\nabla_xa(x,y)\\cdot \\nabla_{x} |x|^{-\\rho_{1}} \\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k(x)|^p)}|u_j(x)|^{p}|u_{\\ell}(y)|^2\\,dxdy\\right).\n\\nonumber\n\\end{align}\n We consider now the term \n\\begin{align} \n\\sum_{j,\\ell=1}^N\\mathcal N_{j,\\ell}(t)&\n\\\\\n=-2\\beta(d-\\gamma_2)\\sum_{\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\n \\frac{|x|^{-\\rho_{2}}|y|^{-\\rho_{2}}}{|x-z|^{d-\\gamma_2+2}}\\round{\\varkappa(x,x) \\varkappa(z,z)-|\\varkappa(x,z)|^2 }K_{\\ell}(x,z)\\,dxdy\n \\nonumber\n \\\\\n +2b\\sum_{j,k,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\\nabla_{x} a(x)\\cdot\\nabla_{x}|x|^{-\\rho_{2}}\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_{2}}| u_k|^2 )} |u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy\n\\nonumber\n \\\\\n -2b\\sum_{j,k,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\\nabla_{x} a(x)\\cdot \\nabla_{x} |x|^{-\\rho_{2}}\\squad{|x|^{-(d-\\gamma_2)}*({|x|^{-\\rho_{2}}\\overline u_k u_j  )}} u_k(x)  \\overline u_j(x) |u_{\\ell}(y)|^2\\,dxdy,\n\\nonumber\n\\end{align}\nwith $K(x,z)$ as in \\eqref{eq.Kern} and \n\\begin{align\n\\varkappa(x,z)=\\sum_{j=1}^N u_j(x)\\overline u_j(z), \\, \\, \\,  \\, \\, \\,\\varkappa(x)=\\varkappa(x,x).\n\\end{align}\nThe Cauchy-Schwartz inequality bears to the bound $|\\eta(x,z)|^2\\leq \\varkappa(x)\\varkappa(z)$, for any $x,y\\in{\\mathbb R}^d$. This fact and  \\eqref{eq.inf}, imply \n\\begin{align}\\label{eq.HFNonl}\n\\sum_{j,\\ell=1}^N\\mathcal N_{j,\\ell}(t)&\n \\\\\n \\leq 2b\\sum_{j,k,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\\nabla_{x} a(x)\\cdot\\nabla_{x}|x|^{-\\rho_{2}}\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_{2}}| u_k|^2 )} |u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy\n\\nonumber\n \\\\\n -2b\\sum_{j,k,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\\nabla_{x} a(x)\\cdot \\nabla_{x} |x|^{-\\rho_{2}}\\squad{|x|^{-(d-\\gamma_2)}*({|x|^{-\\rho_{2}}\\overline u_k u_j  )}} u_k(x)  \\overline u_j(x) |u_{\\ell}(y)|^2\\,dxdy.\n\\nonumber\n\\end{align}\n\nFinally we consider $N^V_{j,\\ell}(t)$. An easy calculations yields \n  \\begin{align}\\label{eq.intmorpot}\n\\sum_{j,\\ell=1}^N \\mathcal   N^V_{j,\\ell}(t)\n \\\\\n =2\\sigma_2\\sum_{j,\\ell=1}^N \\int_{{\\mathbb R}^{2d}}\\nabla_{x} a(x) \\cdot \\nabla_{x} V(x)|u_j(x)|^{2}|u_{\\ell}(y)|^2\\,dxdy.& \n    \\nonumber\n \\end{align}\nBy combining now \\eqref{eq:eqval}, \\eqref{eq.laplt3}, \\eqref{eq:nnlna11}, \\eqref{eq.HFNonl} and \\eqref{eq.intmorpot}, we attain the inequality \\eqref{eq:tenmor2}.\n \\end{proof}\n We need also the following \n \n \\begin{corollary} \\label{singmor}\nAssume $d\\geq 5$ and let $V: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ be a function satisfying  and $x\\cdot \\nabla V \\leq 0 .$ If $(u_j(t,x))_{j=1}^N \\in C({\\mathbb R}, H^2({\\mathbb R}^d))^N$ be a global solution to system \\eqref{eq:HFC4}  and $a(x,0)=a(x)$, then it holds that,\n\\begin{align}\\label{eq.singmor}\n\\sum_{j=1}^N \\dot M_j(t)\\leq \\sum_{j=1}^N \\int_{{\\mathbb R}^d} (-\\Delta^3a(x)+\\sigma_1 \\Delta^2a(x))|u_j(x)|^{2}\\,dx &\\\\\n+ 2\\sum_{j=1}^N  \\int_{\\mathbb{R}^{d}} \\nabla a(x)\\cdot \\nabla V(x)|u_j(x)|^{2} d x&\n\\nonumber\n\\\\\n-\\frac{2\\beta_{1}}{(d-1)p}\\sum _{j,k=1}^Nb_{jk}\n\\int_{{\\mathbb R}^d}\\Delta a(x)\\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}|u_k|^p)} |x|^{-\\rho_{1}} | u_j(x)|^{p}\\,dx\n\\nonumber\\\\\n -\\frac {2b\\beta_{2}}{d-1}\\sum_{j,k=1}^N\\int_{{\\mathbb R}^d}\\Delta a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_{2}}| u_k|^2)}|x|^{-\\rho_{2}}|u_j(x)|^2\\,dx\n\\nonumber \\\\\n  +\\frac {2b\\beta_{2}}{d-1}\\sum_{j,k=1}^N\\int_{{\\mathbb R}^d}\\Delta a(x)\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_{2}}\\overline u_k u_j)  } |x|^{-\\rho_{2}}u_k(x)  \\overline u_j(x)\\,dx,\n \\nonumber\n \\end{align}\nwith $\\beta_{1}=(p-2)(d-1)+2\\rho_{1}$ and $\\beta_{2}=d-1+2\\rho_{2}$.\n\\end{corollary}\n\\begin{proof}\nLet us directly apply the Morawetz identity \\eqref{eq:mor2} discarding some negative terms. We observe first that \n\\begin{equation}\\label{}\n \\sum_{j=1}^N \\int_{{\\mathbb R}^d} (-\\Delta^3a(x)+\\sigma_1 \\Delta^2a(x))|u_j(x)|^{2}\\,dx\\leq 0,\n\\end{equation}\nfrom \\eqref{eq.delta2} and \\eqref{eq.delta3}.  Then we get\n\\begin{align*}\n-\\frac{2(p-2)}{p}\\sum _{j,k=1}^Nb_{jk}\n\\int_{{\\mathbb R}^d} \\Delta a(x)\\squad{|x|^{-\\rho_{1}}\\round{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}|u_k|^p)}} | u_j(x)|^{p}\\,dx\n\\\\\n+\n\\frac 4 p\\sum_{j,k=1}^N b_{jk}\\int_{{\\mathbb R}^d}\\nabla a(x)\\cdot \\nabla \\squad{|x|^{-\\rho_{1}}\\round{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)}} | u_j(x)|^{p}\\,dx\\\\\n=-\\sum_{j,k=1}^N\\frac{2(p-2)}{p} b_{jk}\\int_{{\\mathbb R}^d}\\frac{d-1}{|x|}\\ \\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k(x)|^p)}|x|^{-\\rho_{1}}|u_j(x)|^{p}\\,dx&\n \\nonumber\\\\\n-\\sum_{j,k=1}^N\\frac {4b_{jk}}p\\int_{{\\mathbb R}^d} \\frac{\\rho_{1}}{|x|} |x|^{-\\rho_{1}}\\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)}|u_j(x)|^{p}\\,dx&\n \\nonumber\\\\\n +\\frac 14\\sum_{\\substack{j,k=1}}^Nb_{jk}^*\\int_{{\\mathbb R}^{2d}}\n \\frac{|x|^{-\\rho_{1}}|y|^{-\\rho_{1}}}{|x-z|^{d-\\gamma_1+2}}|u_j(x)|^{p}| u_k(z)|^{p}(x-z) \\cdot \\round{\\frac{x}{|x|}-\\frac{z}{|z|}}\\,dxdz\n \\nonumber\\\\\n \\leq \n -\\frac{2\\beta_{1}}{(d-1)p}\\sum _{j,k=1}^Nb_{jk}\n\\int_{{\\mathbb R}^d}\\Delta a(x)\\squad{|x|^{-\\rho_{1}}\\round{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}|u_k|^p)}} | u_j(x)|^{p}\\,dx,\n\\nonumber\n\\end{align*}\nwieh $b_{jk}^{*}$ as in \\eqref{eq:nonlin2} and where the last inequality is obtained by \\eqref{eq.ineq}. Then we can manage in the same way the last four terms in \\eqref{eq:mor2}. In addition the first two terms are non-negative because of \\eqref{eq.quad1} and \\eqref{eq.quad2}, which are still fulfilled with $y=0$. This guarantees the proof of the corollary.\n\\end{proof}\nThe direct consequence of Proposition \\eqref{CoLa}, Lemma \\eqref{lem:tenmor} and Corollary \\eqref{singmor} are the linear and nonlinear Morawetz estimates localized on the space-time slabs ${\\mathbb R}\\times B_{\\tilde x}^d(r)$. We have\n\n\\begin{proposition}\\label{lowcorrest}\n Assume $d\\geq 3$, $p\\geq 2$, $(b_{jj},\\sigma_1)\\neq(0,0),$ for all $j=1, \\dots, N$, and $(\\rho_{1},\\rho_{2}, \\sigma_{2})=(0,0,0)$. Let $(u_j)^N_{j=1}\\in C({\\mathbb R},H^1({\\mathbb R}^d)^N)$ be as in Proposition \\ref{CoLa}. Then, selecting $a(x,y)=|x-y|$, $r>0$ and $\\tilde x\\in{\\mathbb R}^d$, one retreives the following localized estimates. For $d\\geq4$,\n\\begin{align} \\label{eq:MorLoc1}\n \\sum_{j,k,\\ell=1}^N\\widetilde b_{jk}\\int_{{\\mathbb R}}\\sup_{\\tilde x\\in{\\mathbb R}^d}\\int_{B_{\\tilde x}^d(r)^3}|u_j(t,x)|^{p}| |u_\\ell(t,y)|^{2}|u_k(t,z)|^{p}\n\\,dxdydzdt\n  \\\\\n  +\\sigma_{1}  \\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}}\\sup_{\\tilde x\\in{\\mathbb R}^d}\\int_{B_{\\tilde x}^d(r)^2}\n |u_j(t,x)|^{2} |u_\\ell(t,y)|^{2}\n\\,dxdydt\n \\leq C\\sum_{j=1}^N\\|u_{j,0}\\|^4_{H^2_x},\n \\nonumber\n  \\end{align}\n  where $ \\widetilde b_{jk}=4b_{jk}(p-2)/p$. For $d=3$, \n  \\begin{align} \\label{eq:MorLoc2}\n\\sum_{j, k, \\ell=1}^N\\widetilde b_{jk}\\int_{{\\mathbb R}}\\sup_{\\tilde x\\in{\\mathbb R}}\\int_{B_{\\tilde x}^3(r)^2}|u_j(t,x)|^{p}|u_\\ell(t,x)|^{2}|u_k(t,z)|^{p}\n \\,dx dydt \n  \\\\\n  +\\sigma_{1}  \\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}}\\sup_{\\tilde x\\in{\\mathbb R}^3}\\int_{B_{\\tilde x}^3(r)}\n |u_j(t,x)|^{2} |u_\\ell(t,x)|^{2}\\, dxdt\n \\leq C\\sum_{j=1}^N\\|u_{j,0}\\|^4_{H^2_x}.\n \\nonumber\n  \\end{align}\n\\end{proposition}\n\n\\begin{proof}\nWe treat the general frame $d\\geq 3$. The interaction inequality \\eqref{eq:tenmor2} and the fact that $\\mathcal R(t)=0$ bear to\n\\begin{align}\\label{eq.LocCor1}\n  \\sum_{j,\\ell=1}^N\\mathcal{ \\dot{M}}_{j,\\ell}(t)\n  \\\\\n  \\lesssim 2\\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\n  \\Delta^2_xa(x,y)\\round{\\nabla _{x}| u_j(x)|^2\\nabla_{y}|u_{\\ell}(y)|^2+\\sigma_{1} | u_j(x)|^2|u_{\\ell}(y)|^2} \\,dxdy\n \\nonumber\n \\\\\n- \\sum_{j,\\ell, k}^N \\widetilde b_{jk}\\int_{{\\mathbb R}^{3d}}\\Delta_x a(x,y) \\frac{1}{|x-z|^{d-\\gamma_1}}\n|u_j(x)|^{p}|u_{\\ell}(y)|^2| u_k(z)|^p\\,dxdydz,\n\\nonumber\n\\end{align}\nwhere in the second line we used the first in \\eqref{eq.leadtherm}. This implies, after integrating in time over the interval $J=[t_{1}, t_{2}]$ with $t_{1},t_{2}\\in {\\mathbb R}$ and looking at \\eqref{eq.delta1}, \\eqref{eq.delta2},\n\\begin{align}\\label{eq.LocCor2}\n\\sum_{j,\\ell=1}^N\\sup_{t\\in J}|\\mathcal{M}_{j,\\ell}(t)|\n  \\gtrsim -2\\sigma_{1}\\sum_{j,\\ell=1}^N\\int_{J}\\int_{{\\mathbb R}^{2d}}\n  \\Delta^2_xa(x,y) | u_j(x)|^2|u_{\\ell}(y)|^2 \\,dxdydt\n \\\\\n +\\sum_{j,\\ell, k}^N\\widetilde b_{jk}\\int_{J}\\int_{{\\mathbb R}^{3d}}\\Delta_x a(x,y) \\frac{1}{|x-z|^{d-\\gamma_1}}\n|u_j(x)|^{p}|u_{\\ell}(y)|^2| u_k(z)|^p\\,dxdydzdt.\n\\nonumber\n\\end{align}\nBoth the inequalities \\eqref{eq:MorLoc1} and s\\eqref{eq:MorLoc2} follow finally from the bounds\n\\begin{align}\\label{eq.lboundA}\n\\inf_{x,y,z\\in B_{\\tilde x}^d(r)}\\round{\\frac{1}{|x-y|}, \\frac{1}{|z-y|}}=\\inf_{x,y,z\\in B_0^d(r)}\\round{\\frac{1}{|x-y|}, \\frac{1}{|z-y|}}\\gtrsim 1,\\\\\n\\label{eq.lboundB}\n\\sum_{j,\\ell=1}^N\\sup_{t\\in J}|\\mathcal{M}_{j,\\ell}(t)|\\lesssim \\sum_{j, \\ell=1}^{N}\\sup_{t\\in J}\\|\\nabla_{x}u_{j}\\|_{L^{2}_{x}}^{2}\\|u_{\\ell}\\|_{L^{2}_{x}}^{2}\\lesssim \\sum_{j=1}^{N}\\|u_{j,0}\\|_{H^{2}_{x}}^{4} ,\n\\end{align}\nonce one lets $t_{1}\\rightarrow -\\infty$ and $t_{2}\\rightarrow \\infty$.\n \\end{proof}\n We have also\n \n \\begin{proposition}\\label{higcorrest}\n Assume $d\\geq 5$, $p=2$ or $(\\rho_{1},\\rho_{2},\\sigma_2)\\neq(0,0,0)$. Let $(u_j)^N_{j=1}\\in C({\\mathbb R},H^1({\\mathbb R}^d)^N)$ be as in Proposition \\ref{CoLa}. Then, selecting $a(x,y)=|x-y|$, $r>0$ and $\\tilde x\\in{\\mathbb R}^d$, one gets the following localized estimates. For $d\\geq6$ \n\\begin{align} \\label{eq:MorLoc3}\n \\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}}\\sup_{\\tilde x\\in{\\mathbb R}^d}\\int_{B_{\\tilde x}^d(r)^2}\n |u_j(t,x)|^{2} |u_\\ell(t,y)|^{2}\n\\,dxdydt\n \\leq C\\sum_{j=1}^N\\|u_{j,0}\\|^4_{H^2_x},\n  \\end{align}\n  For $d=5$, \n  \\begin{align} \\label{eq:MorLoc4}\n\\sum_{j, \\ell=1}^N\\int_{{\\mathbb R}}\\sup_{\\tilde x\\in{\\mathbb R}}\\int_{B_{\\tilde x}^5(r)}  |u_j(t,x)|^{2} |u_\\ell(t,x)|^{2}\n \\,dxdt \\leq C\\sum_{j=1}^N\\|u_{j,0}\\|^4_{H^2_x}.\n  \\end{align}\n\\end{proposition}\n\\begin{proof}\nBy \\eqref{eq:tenmor2} and removing the non-positive terms appearing in \\eqref{eq.LocCor1}  we can write\n\\begin{align}\\label{eq.LocCor3}\n  \\sum_{j,\\ell=1}^N\\mathcal{ \\dot{M}}_{j,\\ell}(t)\n  \\lesssim\n  2\\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}^{2d}}\n - \\Delta^3_xa(x,y)| u_j(x)|^2|u_{\\ell}(y)|^2 \\,dxdy+\\mathcal R(t),\n\\end{align}\nwhere we applied now the second in \\eqref{eq.leadtherm}. Then\n because of \\eqref{eq.delta1}, \\eqref{eq.delta2}, indeed one attains\n\\begin{align}\\label{eq.LocCor4}\n\\int_{J} |\\mathcal R(t)| \\, dt +\\sum_{j,\\ell=1}^N\\sup_{t\\in J}|\\mathcal{M}_{j,\\ell}(t)|\n\\\\\n  \\gtrsim\\sum_{j,\\ell=1}^N\\int_{J}\\int_{{\\mathbb R}^{2d}}\n  \\Delta^3_xa(x,y) | u_j(x)|^2|u_{\\ell}(y)|^2 \\,dxdydt,\n  \\nonumber\n\\end{align}\nwith $J$ as above. It turns up also, by \\eqref{eq.dMor} and $\\nabla_x a(x,y)\\lesssim 1,$ that\n\\begin{align}\\label{eq.LocCor5}\n\\int_{J} |\\mathcal R(t)| \\, dt \n\\\\\n\\lesssim 2\\sigma_2\\sum_{j,\\ell=1}^N \\int_{{\\mathbb R}^{2d}}  |\\nabla V(x)|| u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy&\n \\nonumber\n \\\\\n+ \\sum_{j, k, \\ell}^Nb_{jk}\\int_{J}\\int_{{\\mathbb R}^{2d}} \\frac 1{|x|}\\squad{|x|^{-(d-\\gamma_1)}* (|x|^{-\\rho_{1}}| u_k(x)|^p)}|x|^{-\\rho_{1}}|u_j(x)|^{p}|u_{\\ell}(y)|^2\\,dxdydt\n\\nonumber\n\\nonumber\\\\\n+2b\\sum_{j,k, \\ell=1}^N\\int_{{\\mathbb R}^{2d}}\\frac 1{|x|}\\left(\\squad{|x|^{-(d-\\gamma_2)}*(|x|^{-\\rho_{2}}| u_k|^2 )} |x|^{-\\rho_{2}} |u_j(x)|^2|u_{\\ell}(y)|^2\\,\\right.\n\\nonumber\n \\\\\n \\left. - \\squad{|x|^{-(d-\\gamma_2)}*({|x|^{-\\rho_{2}}\\overline u_k u_j  )}}|x|^{-\\rho_{2}} u_k(x)  \\overline u_j(x) |u_{\\ell}(y)|^2\\right)\\,dxdydt\n\\nonumber\\\\\n\\lesssim \\sum_{j,\\ell=1}^N\\sup_{t\\in J}|M_{j}(t)|\\|u_{\\ell}\\|_{L^{2}_{x}}^{2}\\lesssim  \\sum_{j=1}^{N}\\|u_{j,0}\\|_{H^{2}_{x}}^{4} ,\n\\end{align}\nwhere the inequalities in the last line are achieved by \\eqref{eq.singmor} and \\eqref{eq.singMor}. The above estimate combined with \\eqref{eq.LocCor4}, \\eqref{eq.delta3} and the bounds \\eqref{eq.lboundA}, \\eqref{eq.lboundB} guarantees \\eqref{eq:MorLoc3} and \\eqref{eq:MorLoc4}. \n \\end{proof}\n\n\n\\section{Decay in energy space: proof of Theorem \\ref{thm:desl}}\\label{DecSolu}\nThe proof of  the main theorem concerning the dacay of the solutions to HFC4 equations in the energy space is given in this section.\n\n\n{\\bf Proof of Theorem \\ref{thm:desl}.}\nWe prove \\eqref{eq:desol0} for a suitable $2<r<2d/(d-4)$ if $d\\geq 5$ (for $2<r<+\\infty$, if $d=3,4$), the result for the general case will follow by the conservation of mass \\eqref{eq:mcon}, the kinetic energy \n\\eqref{eq:econs} and interpolation. We want to prove then \n\\begin{equation}\\label{eq:potenergy2}\n\\lim_{t\\rightarrow \\pm \\infty} \\|u_{j}(t)\\|_{L^{\\frac{2d+4}{d}}_x}=0,\n\\end{equation}\nfor all $j=1,\\dots, N$. Moreover we focus on forward times only, because\nthe backwards can be managed similarly. \nAssume that \\eqref{eq:potenergy2} does not hold (see \\cite{CT} and \\cite{Vis}). Hence we can find a sequence\n$\\{t_n\\}$ with $t_n \\to +\\infty$ and a $\\delta>0$ such that\n\\begin{equation}\\label{eq:escseq}\n \\inf_n \\|u_{j}(t_n, x)\\|_{\\mathcal{L}^{\\frac{2d+8}{d+2}}_x}=\\delta,\n\\end{equation}\nfor some $j\\in\\{1, \\dots, N\\}$. The Gagliardo-Nirenberg inequality (see \\cite{Ad})\n\\begin{equation}\\label{eq:GNloc3}\n\\|u_{j}\\|_{L^{\\frac{2d+4}{d}}_x}^{\\frac{2d+4}{d}}\\lesssim \\left(\\sup_{x\\in {\\mathbb R}^d} \\|u_{j}\\|_{L^2(B^d_{\\widetilde x}(1))}\\right)^{\\frac{4}{d}} \n\\|u_{j}\\|^2_{H^2_x},\n\\end{equation}\nallows us to say that there exists $x_n \\in {\\mathbb R}^d$ and a $\\varepsilon_0>0$ such that\n\\begin{equation}\\label{eq:seqspac}\n\\inf_{n} \\|u_j(t_n, x)\\|_{\\mathcal{L}^2(B_{x_n}^d(1))}=\\varepsilon_0.\n\\end{equation}\nFix a cut-off function $\\phi(x)\\in C^\\infty_0({\\mathbb R}^d)$, so as\n$\\phi(x)=1$ for $B^d_{0}(1)$ and $\\phi(x)=0$ for $x\\notin B^d_{0}(2)$.\nThen by choosing\n$a(x)=\\phi (x-x_n)$ we get from the relation\n\\begin{align*}\n\\partial_{t}|u_{j}|^{2}=-2 \\nabla_{x} \\Im\\left(\\overline{u}_{j}\\nabla_{x}(\\Delta_{x}-\\sigma_{1}) u_{j}\\right)+2 \\nabla_{x}\\Im\\left(\\nabla_{x} \\overline{u}_{j} D^{2}_{x} u_{j}\\right),\n\\end{align*}\nthe following\n\\begin{equation*}\n\\begin{split}\n\\left|\\frac d{dt} \\int_{{\\mathbb R}^d} \\phi (x -x_n)  | u_{j}(t,x)|^2 dx \\right|\\lesssim\\sigma_{1}\\left|\\int_{{\\mathbb R}^d} \\nabla_x\\phi (x -x_n) \\Im(\\nabla_xu_{j}(t,x)\\overline u_{j}(t,x))dx\\right|\\\\\n+\\left|\\int_{{\\mathbb R}^d} \\Delta_x\\phi (x -x_n) \\Im(\\Delta_xu_{j}(t,x)\\overline u_{j}(t,x))dx\\right|+\\\\\n+\\left|\\int_{{\\mathbb R}^d} \\nabla_x\\phi (x -x_n) \\Im(\\Delta_xu_{j}(t,x)\\nabla_x\\overline u_{j}(t,x)) dx\\right|\n\\lesssim \\sup_t \\|u_{j}(t,x)\\|_{H^2_x}^2.\n\\end{split}\n\\end{equation*}\nConsequently, the fundamental theorem of calculus implies \n\n\\begin{equation}\n\\left|\\int_{{\\mathbb R}^d} \\varphi(x -x_n) |u_j(w_{2}, x)|^2 dx - \\int_{{\\mathbb R}^d} \\varphi(x -x_n) |u_j(w_{1}, x)|^2 dx\\right|\\leq \\widetilde C |w_{1}-w_{2}|,\n\\end{equation}\nfor a $ \\widetilde C>0$ independent by $n$. Choosing $w_{1}=t_n$ and $w_{2}=t_{n}+ t^*$, with $ t^*>0$,  we infer\n\\begin{equation}\n\\int_{{\\mathbb R}^d} \\varphi(x -x_n) |u_j(t_{n}+ t^*, x)|^2 dx\\geq  \\int_{{\\mathbb R}^d} \\varphi(x -x_n) |u_j(t_n, x)|^2 dx - \\widetilde Ct^{*},\n\\end{equation}\nwhich reads, since the support property of $\\phi$, \n\n\\begin{equation}\n\\int_{B_{x_n}^d(2)} |u_j(t_{n}+t^{*}, x)|^2 dx\\geq  \\int_{B_{x_n}^d(1)} |u_j(t_n, x)|^2 dx - \\widetilde Ct^{*}.\n\\end{equation}\nBy the previous estimates and \\eqref{eq:seqspac}, we inherit  \n\\begin{equation}\\label{eq:infbound1}\n \\|u_j(t, x)\\|_{\\mathcal{L}^2(B_{x_n}^d(2))}\\geq \\varepsilon_1,\n\\end{equation}\nwith $\\varepsilon_{1}>0$, for all  $t\\in (t_n, t_n+ t^*)$, provided that $ t^*>0$ is suitable small in order also to come by the time intervals disjoint.\nFurthermore, by means of the lower bound \\eqref{eq:infbound1} we have \n\\begin{equation}\\label{eq:infbound2}\n\\inf_{n}\\round{\\inf_{t\\in (t_n, t_n+t^*)} \\sum_{j=1}^N \\|u_j(t)\\|^{2}_{L^{2}_x(B_{x_n}^d(2))}} \\gtrsim \\varepsilon^{2}_1>0.\n\\end{equation}\nLet us now complete the proof by distinguishing several different cases. \n\\subsubsection*{Let be $d\\geq 3$, $p> 2$, $(b_{jj},\\sigma_1)\\neq(0,0),$ for all $j=1, \\dots, N$, $(\\rho_{1},\\rho_{2})=(0,0)$ and $\\sigma_2=0.$\\\\\nCase $d\\geq 4$}  We carry out, by H\\\"older inequality and \\eqref{eq:infbound2},\n\\begin{align*}\n\\sum_{j, k, \\ell=1}^N\\widetilde b_{jk}\\int_{{\\mathbb R}}\\sup_{\\tilde x\\in{\\mathbb R}^d}\\int_{B_{\\tilde x}^d(2)^3}|u_j(t,x)|^{p}|u_\\ell(t,y)|^{2}|u_k(t,z)|^{p}\n \\,dxdydzdt \n   \\nonumber \\\\\n    +\\sigma_{1}  \\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}}\\sup_{\\tilde x\\in{\\mathbb R}^d}\\int_{B_{\\tilde x}^d(r)^2}\n |u_j(t,x)|^{2} |u_\\ell(t,y)|^{2}  \\,dxdydzdt \n\\nonumber \\\\\n   \\geq \\inf_{j} \\widetilde b_{jj} \\int_{{\\mathbb R}} \\round{\\int_{B_{x_n}^d(2)}|u_j(t,x)|^{2}\n \\,dx}^{\\frac{2p+2}2}dt +  \\sigma_{1} \\int_{{\\mathbb R}} \\round{\\int_{B_{x_n}^d(2)}|u_j(t,x)|^{2}\n \\,dx}^{2}dt \n \t\\nonumber\\\\\n \\gtrsim    \\sum_{n} \\int_{t_n}^{t_n+t^*} (\\varepsilon^{2p+2}_1+\\sigma_{1}\\varepsilon^{4}_1)\\,dt\\gtrsim \\sum_{n} t^*(\\varepsilon^{2p+2}_1+\\sigma_{1}\\varepsilon^{4}_1)=\\infty,&\n\\end{align*}\nwhich is in contradiction with \\eqref{eq:MorLoc1}. \\\\\n\\subsubsection*{Case $d=3$} Arguing in a similar manner as aforesaid, by an application of the H\\\"older inequality, one obtain the inequality\n\\begin{equation} \n\\begin{split}\n\\sum_{j, k, \\ell=1}^N\\widetilde b_{jk}\\int_{{\\mathbb R}}\\sup_{\\tilde x\\in{\\mathbb R}^d}\\int_{(B_{\\tilde x}^3(2))^2}|u_j(t,x)|^{p}|u_\\ell(t,\nx)|^{2}|u_k(t,z)|^{p}\n \\,dxdzdt \n   \\nonumber \\\\\n    +\\sigma_{1}  \\sum_{j,\\ell=1}^N\\int_{{\\mathbb R}}\\sup_{\\tilde x\\in{\\mathbb R}^d}\\int_{B_{\\tilde x}^3(r)}\n |u_j(t,x)|^{2} |u_\\ell(t,x)|^{2}  \\,dxdt \n  \\nonumber\\\\\n  \\gtrsim \\sum_{n} t^*(\\varepsilon^{2p+2}_1+\\sigma_{1}\\varepsilon^{4}_1)=\\infty,\n\\end{split}\n\\end{equation}\nwhich denies \\eqref{eq:MorLoc2}. \n\\subsubsection*{Let be $p=2$ or  $(\\rho_{1},\\rho_{2},\\sigma_{1})\\neq(0,0,0)$.\\\\\nCase $d\\geq 5$} We will focus on $d\\geq 6$ and utilize Proposition \\ref{higcorrest}. Then it is possible to accomplish \n\\begin{equation*} \n\\begin{split}\n \\sum_{j, \\ell=1}^N\\int_{{\\mathbb R}}\\sup_{\\tilde x\\in{\\mathbb R}^d}\\int_{B_{\\tilde x}^d(2)^{2}}|u_j(t,x)|^{2}|u_\\ell(t,y)|^{2}\n \\,dxdydt   \\\\\n  \\gtrsim  \\sum_{j,\\ell=1}^N\\sum_{n} \\int_{t_n}^{t_n+t^*}\\int_{B_{ x_n}^d(2)^{2}}|u_j(t,x)|^{2}|u_\\ell(t,y)|^{2}\n  \\,dxdydt\n  \\nonumber\\\\\n   \\gtrsim   \\sum_{n} \\int_{t_n}^{t_n+t^*} \\varepsilon^{4}_1\\,dt= \\sum_{n} t^*\\varepsilon^{4}_1\\,dt=\\infty,\n\\end{split}\n\\end{equation*}\nwhich is in conflict with \\eqref{eq:MorLoc3}. We skip the case $d=5$ because it can be arranged alike by taking advantage of \\eqref{eq:MorLoc4}. Hence the proof is now complete.\\\\\n\n\n\n \\section{Scattering for the HFC4}\\label{NFC4scat}\nWe want  to prove Theorem \\ref{thm:scattHFC4}. We acquire first the essential space-time summability for the solutions to \\eqref{eq:HFC4}, then we display the scattering. We have the following\n \\begin{lemma}\\label{StriHFC4}\n Assume $(u_j)_{j=1}^N\\in  C({\\mathbb R},H^2_x)^{N}$ as in Theorem \\ref{thm:scattHFC4}. We have\n \\begin{align}\\label{eq:StriHFC4}\n (u_j)_{j=1}^N\\in L^q({\\mathbb R}, W^{2,r}_x)^{N},\n \\end{align}\nfor every pair $(q,r)\\in \\mathcal B$.\n\\end{lemma}\n\\begin{proof}\nWe consider  the integral operator associated to \\eqref{eq:HFC4}, that is\n\\begin{align}\\label{eq:opintHFC4}\n\\sum_{j=1}^{N} u_{j}(t)=e^ {it  \\Delta_x}u_{j,0} +\\sum_{j,k=1}^{N}  \\int_{0}^{t} e^ {i(t-\\tau)(\\Delta^2_x-\\sigma_{1}\\Delta_{x}+\\sigma_{2} V)}  \\mathcal F(\\tau, u_j , u_k) d\\tau,\n\\end{align}\nmoreover, for $t_{0}>0$ we introduce the auxiliary spaces\n\\begin{align}\\label{eq.1st2}\n   \\norm{u}{ X_{(t_{0}, \\infty)}} =\\sup_{(q,r)\\in \\mathcal B} \\left\\{\\norm{u}{L^{q} ((t_{0}, \\infty), L^{r}_x)}\\right\\}, \n   \\\\\n   \\label{eq.1st3}\n     \\norm{u}{\\widetilde X_{(t_{0}, \\infty)}} =\\inf_{(\\tilde q' ,\\tilde r')\\in \\mathcal B^{'}} \\left\\{\\norm{u}{L^{\\tilde q'}((t_{0}, \\infty), L^{\\widetilde r'}_x)}\\right\\}.\n\\end{align}\nLet us start by dealing with $p\\geq 2$, $(b_{jj},\\sigma_1)\\neq(0,0),$ for all $j=1, \\dots, N$ and $(\\rho_{1},\\rho_{2})=(0,0)$. We will \nrestrict to $d\\geq 3$ if $p>2$ and $b=0, \\sigma_{2}=0$, otherwise to $d\\geq 5$.\n\\\\\n\\\\\nIn order to get the property \\eqref{eq:StriHFC4}, we choose now\n$( q_1', r_1')$ so that \n\\begin{equation}\\label{eq:pairHF1}\n ( q_1,r_1):= \\left(\\frac{8p}{dp-d-\\gamma_1},\\frac{2dp}{d+\\gamma_1}\\right).\n\\end{equation}\nIn this way, Strichartz estimates \\eqref{eq:StriBiharm}, H\\\"older and Hardy-Littlewood-Sobolev inequalities bring to \n\\begin{align}\\label{eq.nnl1}\n\\sum_{s=0}^1\\left\\|\\sum_{j,k=1}^N b_{jk}\\Delta_{x}^{s}\\round{ \\squad{|x|^{-(d-\\gamma_1)}*| u_k|^p} | u_{j}|^{p-2}  u_j}\\right\\|_{\\widetilde X_{(t_{0}, \\infty)}}\\\\\n\\lesssim\\sum_{s=0}^1\\left\\|\\sum_{j,k=1}^N b_{jk}\\Delta_{x}^{s}\\round{ \\squad{|x|^{-(d-\\gamma_1)}*|u_k|^p} | u_j|^{p-2}  u_j}\\right\\|_{L^{q_1'}((t_{0}, \\infty), L^{r_1'}_x)}\n\\nonumber\\\\\n\\lesssim \\sum_{s=0}^1\\norm{ \\sum^N_{j,k=1}b_{jk}\\|\\Delta_{x}^{s}u_j\\|_{L^{r_1}_x}\\|u_k\\|_{L_x^{r_1}}^{p} \\|u_j\\|_{L_x^{r_1}}^{p-2}}{L^{q_1'}((t_{0}, \\infty))}\n\\nonumber\\\\\n+\\sum_{s=0}^1\\left\\|\\sum_{j,k=1}^N b_{jk}  \\squad{|x|^{-(d-\\gamma_1)}*\\Delta_{x}^{s}| u_k|^p} | u_j|^{p-2}  u_j\\right\\|_{L^{q_1'}((t_{0}, \\infty), L^{r_1'}_x)}\n\\nonumber\\\\\n\\lesssim \\sum_{s=0}^1\\sum_{j,k=1}^N b_{jk}\\norm{\\|\\Delta_{x}^{s}u_j\\|_{L^{r_1}_x}\\|u_k\\|_{L_x^{r_1}}^{p} \\|u_j\\|_{L_x^{r_1}}^{p-2}+\\|\\Delta_{x}^{s} u_k\\|_{L_x^{r_1}}\\|u_k\\|_{L_x^{r_1}}^{p-1}\\|u_j\\|_{L_x^{r_1}}^{p-1}}{L^{q_1'}((t_{0}, \\infty))}\n\\nonumber\\\\\n\\lesssim\\sum_{s=0}^1 \\sum_{j,k=1}^N\\Big\\|\n    \\|\\Delta_{x}^{s}u_{k}\\|_{L^{r_1}_x}\n   \\|u_{j}\\|_{L_x^{r_1}}^{(2p-2)}\\Big\\|_{L^{q_1'}((t_{0}, \\infty))}.\n\\nonumber\n\\end{align} \nHere we utlized the estimate given in \\cite{Kat}, that is, if $\\nu \\geq 1$, then for $1<r_{1}, r_{2}<\\infty$ and $1<r_{1} \\leqslant \\infty$ \n\\begin{equation}\\label{eq:HiOrDer}\n\\left\\|\\Delta_{x}\\left(|u|^{\\nu} u\\right)\\right\\|_{L_{x}^{r}} \\lesssim\\|u\\|_{L_{x}^{r_{1}}}^{\\nu}\\|\\Delta_{x} u\\|_{L_{x}^{r_{2}}}, \\qquad \\frac{1}{r}=\\frac{\\nu}{r_{1}}+\\frac{1}{r_{2}}.\n\\end{equation}\n We take $\\theta_1=(q_1-q_1')/(2pq_1'-2q'_1)$. Direct calculations unveil that \n$$\n\\frac 1{q_1'}=\\frac{2(p-1)\\theta_1+1}{q_1}, \\quad 1-\\theta_1=\\frac {(2p-1)q'_1-q_1}{(2p-2)q_1'}\n$$\nand  $\\theta_{1}\\in(0,1)$ by virtue of \\eqref{eq:bs}, \\eqref{eq:bsII}. Accordingly, the last term of the inequality \\eqref{eq.nnl1} is not greater than\n \\begin{align}\\label{eq.nnl2}\n \\sum_{s=0}^1\\sum_{j,k=1}^N  \\Big\\|  \\|\\Delta_{x}^{s}u_{k}\\|_{L^{r_1}_x}\n   \\|u_{j}\\|_{L_x^{r_1}}^{(2p-2)(1-\\theta_1)}\\|u_{j}\\|_{L_x^{r_1}}^{(2p-2)\\theta_1}\\Big\\|_{L^{q_1'}((t_{0}, \\infty))}\n   \\\\\n    \\lesssim \n   \\sum_{j,k=1}^N  \\Big\\|  \\|(1-\\Delta_{x})u_{k}\\|_{L^{r_1}_x} \n   \\|u_{j}\\|_{L_x^{r_1}}^{(2p-2)(1-\\theta_1)}\\|(1-\\Delta_{x})u_{j}\\|_{L_x^{r_1}}^{(2p-2)\\theta_1}\\Big\\|_{L^{q_1'}((t_{0}, \\infty))}\n \\nonumber \\\\\n   \\lesssim \n    \\sup_{j=1,\\dots, N} \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r_1}_x)}^{(2p-2)(1-\\theta_1)} \\sum_{j,k=1}^N  \\Big\\|  \\|(1-\\Delta_{x})u_{k}\\|_{L^{r_1}_x} \n   \\|(1-\\Delta_{x})u_{j}\\|_{L_x^{r_1}}^{(2p-2)\\theta_1}\\Big\\|_{L^{q_1'}((t_{0}, \\infty))}\n  \\nonumber \\\\\n    \\lesssim \\sup_{j=1,\\dots, N} \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r_1}_x)}^{(2p-2)(1-\\theta_1)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{L^{q_1}((t_{0}, \\infty), L^{r_1}_x)}}^{(2p-2)\\theta_1+1},\n  \\nonumber\n\\end{align}\nwhere in the second inequality we used the Sobolev embedding and the fact that $\\Delta_{x}/(1-\\Delta_{x})$ is a pseudodifferential operator of order $0$, wihch is $L^{r}_{x}$-bounded, for $1<r<\\infty$. We single out $( q_2', r_2')$ by taking $p=2$ in \\eqref{eq:pairHF1}, that is \n\\begin{equation*}\n ( q_2,r_2):= \\left(\\frac{8}{d-\\gamma_1},\\frac{4d}{d+\\gamma_1}\\right),\n\\end{equation*}\nhence we get, analogously as above, \n\\begin{align}\\label{eq.nnl3}\n\\sum_{s=0}^1 \\sum^N_{j,k=1}b\\left\\| \\Delta_{x}^{s}\\round{ \\squad{|x|^{-(d-\\gamma_2)}*| u_k|^2}    u_j-\\squad{|x|^{-(d-\\gamma_2)}*\\overline u_{k} u_j  } u_k}\\right\\|_{\\widetilde X_{(t_{0}, \\infty)}}\n\\nonumber\\\\\n\\lesssim\\sum_{s=0}^1 \\sum^N_{j,k=1}b\\left\\| \\Delta_{x}^{s}\\round{ \\squad{|x|^{-(d-\\gamma_2)}*| u_k|^2}    u_j-\\squad{|x|^{-(d-\\gamma_2)}*\\overline u_{k} u_j  } u_k}\\right\\|_{L^{q_2'}((t_{0}, \\infty), L^{r_2'}_x)}\n\\nonumber\\\\\n\\lesssim\\sum_{s=0}^1 \\sum^N_{j,k=1}\\Big\\|\n    \\|\\Delta_{x}^{s}u_{k}\\|_{L^{r_2}_x}\n   \\|u_{j}\\|_{L_x^{r_2}}^{2}\\Big\\|_{L^{q_2'}((t_{0}, \\infty))}\n\\nonumber\\\\\n   \\lesssim \n \\sum^N_{j,k=1} \\Big\\|  \n   \\|u_{j}\\|_{L_x^{r_2}}^{2(1-\\theta_2)}\\|(1-\\Delta_{x})u_{k}\\|_{L^{r_2}_x}^{2\\theta_2+1}\\Big\\|_{L^{q_2'}((t_{0}, \\infty))}\n  \\nonumber \\\\\n     \\lesssim \\sup_{j=1,\\dots, N} \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r_2}_x)}^{2(1-\\theta_2)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{L^{q_2}((t_{0}, \\infty), L^{r_2}_x)}}^{2\\theta_2+1},\n\\end{align} \nwhere $\\theta_2=(q_2-q_2')/2q_2'\\in (0,1)$ and such that\n$$\n\\frac 1{q_2'}=\\frac{2\\theta_1+1}{q_2}.\n$$\nAn use of  \\eqref{eq.nnl1}, \\eqref{eq.nnl2} and \\eqref{eq.nnl3}\nleads to\n\\begin{align}\\label{eq.finest1}\n\\sum_{j=1}^N\\|(1-\\Delta_{x})u_{j}\\|_{X_{(t_{0}, \\infty)}}\n\\lesssim \\sum_{j=1}^N\\|u_{j,0}\\|_{H^2_x}\n\\\\\n+ \\sup_{j=1,\\dots, N}\\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r_1}_x)}^{(2p-2)(1-\\theta_1)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{(2p-2)\\theta_1+1}\n \\nonumber \\\\\n  +\\sup_{j=1,\\dots, N} \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r_2}_x)}^{2(1-\\theta_2)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{2\\theta_2+1},\n  \\nonumber\n\\end{align}\nwith \n$$\n\\lim _{t_{0}\\rightarrow +\\infty}\\round{\\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r_1}_x)}+ \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r_2}_x)}}=0,\n$$\nfor any $j=1,\\dots,N$, by Theorem \\ref{thm:desl}. Then, picking up $t_{0}$ sufficiently large we earn\n$(u_j)_{j=1}^N\\in L^{q}((t_{0},+\\infty), W^{2,r}_x)^{N}.$ As well, one infers $(u_j)_{j=1}^N\\in L^{q_1}((-\\infty, -t_{0}), W^{2,r_1}_x)^{N}$ and in conclusion, by continuity we have  $(u_j)_{j=1}^N\\in L^q({\\mathbb R}, W^{2,r}_x)^{N}$.\\\\\n\\\\\nLet us manage $d\\geq 5$, for $p=2$ or $(\\rho_{1},\\rho_{2}, \\sigma_2) \\neq (0,0,0)$.\n\\\\\n\\\\\nWe choose now\n\\begin{equation}\\label{eq:pairHF3}\n ( q_3,r_3):= \\left(\\frac{8p}{dp-d-\\gamma_1+2b},\\frac{2dp}{d+\\gamma_1-2b}\\right)\n\\end{equation}\nand \n\\begin{equation}\\label{eq:pairHF6}\n ( q_4,r_4):= \\left(\\frac{8(2p-1)}{d(2p-1)-(d+4+2 \\gamma_{1}-4 b)},\\frac{2d(2p-1)}{d+4+2\\gamma_{1}-4b}\\right).\n\\end{equation}\nAgain we have similarly as above, for $i=3,4$,\n\\begin{equation}\\label{eq:pairHF4}\n\\frac{d-4}{2d}<\\frac 1{r_{i}}<\\frac 12, \\quad \\frac 1{r_{4}}>\\frac 1d,\n\\end{equation}\nwith the last inequality valid for $d\\geq 5$ and $(2p-1)q_{i}'>q_{i}$ which is equivalent  to\n\\begin{equation}\\label{eq:pairHF5}\n\\frac 1{q_{i}}>\\frac 1{2p},\n\\end{equation}\nalways satisfied by means of \\eqref{eq:bs}, \\eqref{eq:bsII} and \\eqref{eq:bsIII}. Additionally, if we pick up suitable $\\epsilon_{i}>0$ such that $\\epsilon_{i}\\rightarrow 0$, we get by a continuity argument that\n\\begin{align}\\label{eq.index}\n\\frac 1{r^{\\pm}_{i}(\\epsilon_{i})}=\\frac1{r_{i}\\pm \\epsilon_{i}}=\\frac1{r_{i}}\\mp \\frac {\\epsilon_{1}}{r_{i}(r_{i}\\pm \\epsilon_{i}) },\\quad \\frac 1{q^{\\pm}_{i}(\\epsilon_{i})}\n=\\frac 1{q_{i}}\\pm  \\frac {d\\epsilon_{i}}{4r_{i}(r_{i}\\pm\\epsilon_{i}) },\n\\end{align}\nfulfill the same bounds as in \\eqref{eq:pairHF4} and \\eqref{eq:pairHF5}. We pursue by taking $b=0$ and concentrating on the inhomogeneous Choquard-term in the nonlinearity \\eqref{eq.nonlinHF}. Namely, due  to the Strichartz estimates \\eqref{eq:StriBiharm}, \\eqref{eq.casStrex} and structure the space $\\widetilde X_{(t_{0}, \\infty)}$ we can write\n\\begin{align}\\label{eq.nnl1inh}\n\\sum_{j=1}^N\\|(1-\\Delta_{x})(u_{j}-e^{it(\\Delta^{2}_{x}-\\sigma_{1}\\Delta_{x}+\\sigma_{2}V)}u_{j,0})\\|_{X_{(t_{0}, \\infty)}}\\\\\n\\lesssim\n\\sum_{s=0}^1\\left\\|\\sum_{j,k=1}^N b_{jk}\\round{ \\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}\\Delta_{x}^{s}| u_k|^p)} |x|^{-\\rho_{1}} | u_{j}|^{p-2}  u_j}\\right\\|_{\\widetilde X_{(t_{0}, \\infty)}}\n\\nonumber\\\\\n+\n\\sum_{s=0}^1\\left\\|\\sum_{j,k=1}^N b_{jk}\\round{ \\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)} |x|^{-\\rho_{1}} \\Delta_{x}^{s}(| u_{j}|^{p-2}  u_j)}\\right\\|_{\\widetilde X_{(t_{0}, \\infty)}}\n\\nonumber\\\\\n+\\left\\|\\sum_{j,k=1}^N b_{jk}\\round{ \\squad{|x|^{-(d-\\gamma_1)}*(\\nabla_{x}|x|^{-\\rho_{1}}| u_k|^p)} |x|^{-\\rho_{1}} | u_{j}|^{p-2}  u_j}\\right\\|_{L^{2}((t_{0}, \\infty), L_{x}^{\\frac{2d}{d+2}})}\n\\nonumber\\\\\n+\\left\\|\\sum_{j,k=1}^N b_{jk}\\round{ \\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)} \\nabla_{x}|x|^{-\\rho_{1}} | u_{j}|^{p-2}  u_j}\\right\\|_{L^{2}((t_{0}, \\infty), L_{x}^{\\frac{2d}{d+2}})}.\n\\nonumber\n\\end{align} \nAt this point we note that \n\\begin{align}\\label{eq.1st2a}\n  \\norm{u}{\\widetilde X_{(t_{0}, \\infty)}}  \\leq \\norm{u}{ \\widetilde X_{(t_{0}, \\infty)}(\\{|x|\\leq1\\})}+ \\norm{u}{ \\widetilde X_{(t_{0}, \\infty)}\\{(|x|>1\\})},\n\\\\\n  \\norm{u}{L^{2}((t_{0}, \\infty), L_{x}^{\\frac{2d}{d+2}})} \n  \\nonumber\\\\\n   \\leq \\norm{u}{ L^{2}((t_{0}, \\infty), L_{x}^{\\frac{2d}{d+2}}(\\{|x|\\leq1\\}))}+ \\norm{u}{ L^{2}((t_{0}, \\infty), L_{x}^{\\frac{2d}{d+2}}(\\{|x|>1\\}))}\n   \\nonumber\n\\end{align}\nand \n\\begin{align}\\label{eq.weight}\n\\norm{|x|^{-\\rho_{1}}}{L^{a}_{x}(|x|\\leq 1)}<+\\infty\\quad\\text{if}\\quad a\\rho_{1}<d,\n\\\\\n\\norm{|x|^{-\\rho_{1}}}{L^{a}_{x}(|x|> 1)}<+\\infty\\quad\\text{if}\\quad a\\rho_{1}>d.\n\\nonumber\n\\end{align}\n\\end{proof}\nThus we get, by proceeding as in the proof of \\eqref{eq.finest1} and applying \\eqref{eq.index}, \\eqref{eq.1st2a}, \\eqref{eq.weight}, that \nthe second term of \\eqref{eq.nnl1inh} is not greater than\n\\begin{align}\\label{eq.finest2}\n \\sup_{j=1,\\dots, N}\\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{+}_3}_x)}^{(2p-2)(1-\\theta^{+}_3)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{(2p-2)\\theta^{+}_3+1}\n  \\\\\n  +\\sup_{j=1,\\dots, N} \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{-}_3}_x)}^{(2p-2)(1-\\theta^{-}_3)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{(2p-2)\\theta^{-}_3+1}\n  \\nonumber\\\\\n  + \\sup_{j=1,\\dots, N}\\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{+}_4}_x)}^{(2p-2)(1-\\theta^{+}_4)}\\round{\\sum_{k=1}^N \n \\norm{\\frac{\\nabla_{x}}{(1-\\Delta_{x})^{\\frac 12}}(1-\\Delta_{x})^{\\frac 12}u_{k}}{ X_{(t_{0}, \\infty)}}}^{(2p-2)\\theta^{+}_4+1}\n \\nonumber \\\\\n  +\\sup_{j=1,\\dots, N} \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{-}_4}_x)}^{(2p-2)(1-\\theta^{-}_4)}\\round{\\sum_{k=1}^N \n  \\norm{\\frac{\\nabla_{x}}{(1-\\Delta_{x})^{\\frac 12}}(1-\\Delta_{x})^{\\frac 12}u_{k}}{ X_{(t_{0}, \\infty)}}}^{(2p-2)\\theta^{-}_4+1}\n  \\nonumber\\\\\n  \\lesssim\n \\sum_{i=3}^4  \\sup_{j=1,\\dots, N}\\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{+}_i}_x)}^{(2p-2)(1-\\theta^{+}_i)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{(2p-2)\\theta^{+}_i+1}\n \\nonumber \\\\\n  + \\sum_{i=3}^4 \\sup_{j=1,\\dots, N} \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{-}_i}_x)}^{(2p-2)(1-\\theta^{-}_i)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{(2p-2)\\theta^{-}_i+1},\n  \\nonumber\n\\end{align}\nwith $\\theta^{\\pm}_i=(q^{\\pm \\prime}_i-q^{\\pm }_i)/(2pq^{\\prime\\pm}_i-2q^{\\prime\\pm}_i)\\in (0,1)$, where we made an use of the fact that $\\nabla_{x}/(1-\\Delta_{x})^{\\frac12}$ is a pseudo-differential operator of order $0$\\footnote{Notice that one has the operators identity  $$ \\frac {\\nabla_{x}}{(1-\\Delta_{x})^{\\frac12}}=\\frac{\\nabla_{x}}{|\\nabla_{x}|}\\frac {|\\nabla_{x}|}{(1-\\Delta_{x})^{\\frac12}}.$$} together with the natural embedding $W_{x}^{2, r}\\subset W_{x}^{1,r}$. Let us take into account  the Hartree-Fock nonlinear term, that is when $b>0.$ Let us pick up for the purpose \n\\begin{equation}\\label{eq:pairHF7}\n ( q_5,r_5):= \\left(\\frac{16}{d-\\gamma_2+2b},\\frac{4d}{d+\\gamma_2-2b}\\right)\n\\end{equation}\nand \n\\begin{equation}\\label{eq:pairHF8}\n ( q_{6},r_{6}):= \\left(\\frac{24}{2d-(4+2 \\gamma_{2}-4 b)},\\frac{6d}{d+4+2\\gamma_{2}-4b}\\right).\n\\end{equation}\nBy arguing in like manner as above we get the extra terms \n\\begin{align}\\label{eq.finest3}\n \\sum_{i=5}^6  \\sup_{j=1,\\dots, N}\\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{+}_i}_x)}^{2(1-\\theta^{+}_i)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{2\\theta^{+}_i+1}\n  \\\\\n  + \\sum_{i=3}^4 \\sup_{j=1,\\dots, N} \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{-}_i}_x)}^{2(1-\\theta^{-}_i)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{2\\theta^{-}_i+1},\n  \\nonumber\n\\end{align}\nwith $\\theta^{\\pm}_i\\in (0,1)$ defined as above. Coupling \\eqref{eq.nnl1inh}, \\eqref{eq.finest2} and \\eqref{eq.finest3}\nwe arrive at\n\\begin{align}\\label{eq.finest4}\n\\sum_{j=1}^N\\|(1-\\Delta_{x})(u_{j}-e^{it(\\Delta^{2}_{x}-\\sigma_{1}\\Delta_{x}+\\sigma_{2}V)}u_{j,0})\\|_{X_{(t_{0}, \\infty)}}\\\\\n\\lesssim\n\\sum_{i=3}^4  \\sup_{j=1,\\dots, N}\\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{+}_i}_x)}^{(2p-2)(1-\\theta^{+}_i)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{(2p-2)\\theta^{+}_i+1}\n \\nonumber \\\\\n  + \\sum_{i=3}^4 \\sup_{j=1,\\dots, N} \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{-}_i}_x)}^{(2p-2)(1-\\theta^{-}_i)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{(2p-2)\\theta^{-}_i+1}\n  \\nonumber\n  \\\\\n  +\\sum_{i=5}^6  \\sup_{j=1,\\dots, N}\\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{+}_i}_x)}^{2(1-\\theta^{+}_i)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{2\\theta^{+}_i+1}\n\\nonumber  \\\\\n  + \\sum_{i=5}^6 \\sup_{j=1,\\dots, N} \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{-}_i}_x)}^{2(1-\\theta^{-}_i)}\\round{\\sum_{k=1}^N \n  \\|(1-\\Delta_{x})u_{k}\\|_{ X_{(t_{0}, \\infty)}}}^{2\\theta^{-}_i+1},\n  \\nonumber\n\\end{align} \nwhere \n$$\n\\lim _{t_{0}\\rightarrow +\\infty}\\sum_{i=3}^6\\round{\\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{+}_i}_x)}+ \\|u_{j}\\|_{L^{\\infty}((t_{0}, \\infty), L^{r^{-}_i}_x)}}=0,\n$$\nfor any $j=1,\\dots,N$, by Theorem \\ref{thm:desl}. This leads again  to $(u_j)_{j=1}^N\\in L^q({\\mathbb R}, W^{2,r}_x)^{N}$. This ends the proof of the Lemma.\n\n\n\n\n\\begin{proof}[Proof of Theorem \\ref{thm:scattHFC4}]\n We exploit the proof of Theorem \\ref{thm:scattHFC4} covering all the different cases in a unified fashion. We start \n by writing $\\widetilde  u(t)=e^ {-it \\Delta_{x}}u(t)$ and getting then from \\eqref{eq:opintHFC4}\n \n \n \n\\begin{align*}\n\\sum_{j=1}^{N}  \\round{\\widetilde u_{j}(t_2)-\\widetilde  u_{j}(t_1)}=  i\\sum_{j,k=1}^{N} \\int_{t_1}^{t_2} e^ {-i\\tau \\Delta_{x}}  (\\Delta^2_x-\\sigma_{1}\\Delta_{x}+\\sigma_{2} V) \\mathcal F(\\tau, u_j , u_k) d\\tau.\n\\end{align*}\nAn use of the Strichartz estimates \\eqref{eq:StriBiharm}, \\eqref{eq.casStrex}, \\eqref{eq.casStrexDu} along with with \\eqref{eq.SobEquiv} infer\n \\begin{align}\\label{eq:stric1}\n\\sum_{j,k=1}^{N}\\norm{\\int_{t_1}^{t_2} e^ {-i\\tau (\\Delta^2_x-\\sigma_{1}\\Delta_{x}+\\sigma_{2} V)}  \\mathcal F(\\tau, u_j , u_k) d\\tau}{H^{2}_x}\n\\\\\n\\lesssim\\sum_{j,k=1}^{N}\\norm{(\\Delta^2_x+\\sigma_{2} V)\\int_{t_1}^{t_2} e^ {-i\\tau (\\Delta^2_x-\\sigma_{1}\\Delta_{x}+\\sigma_{2} V)}  \\mathcal F(\\tau, u_j , u_k) d\\tau}{L^{2}_x}\n\\nonumber\\\\\n+\\sum_{j,k=1}^{N}\\norm{\\int_{t_1}^{t_2} e^ {-i\\tau (\\Delta^2_x-\\sigma_{1}\\Delta_{x}+\\sigma_{2} V)}  \\mathcal F(\\tau, u_j , u_k) d\\tau}{L^{2}_x}\n\\nonumber\\\\\n\\lesssim\\sum_{s=0}^1\\left\\|\\sum_{j,k=1}^N b_{jk}\\Delta_{x}^{s}\\round{ \\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)} |x|^{-\\rho_{1}} | u_{j}|^{p-2}  u_j}\\right\\|_{\\widetilde X_{(t_{1}, t_{2})}}\n\\nonumber\\\\\n+\\rho_{1}\\sum_{s=0}^1\\left\\|\\sum_{j,k=1}^N b_{jk}\\round{ \\squad{|x|^{-(d-\\gamma_1)}*(\\nabla^{1-s}_{x}|x|^{-\\rho_{1}}| u_k|^p)}\\nabla^{s} |x|^{-\\rho_{1}} | u_{j}|^{p-2}  u_j}\\right\\|_{L^{2}((t_{1}, t_{2}), L_{x}^{\\frac{2d}{d+2}})}\n\\nonumber\\\\\n+b\\sum_{s=0}^1 \\left\\| \\sum^N_{j,k=1}\\Delta_{x}^{s}\\round {\\squad{|x|^{-(d-\\gamma_2)}*|x|^{-\\rho_{2}}| u_k|^2  } |x|^{-\\rho_{2}}u_j}\\right\\|_{\\widetilde X_{(t_{1}, t_{2})}}\n\\nonumber\\\\\n+b\\sum_{s=0}^1 \\left\\|\\sum^N_{j,k=1} \\Delta_{x}^{s}\\round {\\squad{|x|^{-(d-\\gamma_2)}* |x|^{-\\rho_{2}}\\overline u_k u_j  } |x|^{-\\rho_{2}} u_k}\\right\\|_{\\widetilde X_{(t_{1}, t_{2})}}\n\\nonumber\\\\\n+\\rho_{2}b\\sum_{s=0}^1 \\left\\| \\sum^N_{j,k=1}\\round {\\squad{|x|^{-(d-\\gamma_2)}*(\\nabla_{x}^{1-s}|x|^{-\\rho_{2}}| u_k|^2)  } \\nabla_{x}^{s}|x|^{-\\rho_{2}}u_j}\\right\\|_{L^{2}((t_{1}, t_{2}), L_{x}^{\\frac{2d}{d+2}})}\n\\nonumber\\\\\n+\\rho_{2}b\\sum_{s=0}^1 \\left\\|\\sum^N_{j,k=1}\\round {\\squad{|x|^{-(d-\\gamma_2)}* (\\nabla_{x}^{1-s}|x|^{-\\rho_{2}}\\overline u_k u_j  }\\nabla_{x}^{s} |x|^{-\\rho_{2}} u_k}\\right\\|_{L^{2}((t_{1}, t_{2}), L_{x}^{\\frac{2d}{d+2}})}.\n\\nonumber\n\\end{align}\nThen it is sufficient to show that\n \\begin{align*}\n\\lim_{t_1, t_2\\rightarrow \\infty}\\sum_{j=1}^{N}\\|\\widetilde  u_{j}(t_2)-\\widetilde  u_{j}t_1)\\|_{H^{2}_x}=0,\n\\end{align*}\nwhich is guaranteeed by \\eqref{eq:stric1} once\n \\begin{align*}\n\\lim_{t_1, t_2\\rightarrow \\infty}\\sum_{j,k=1}^N b_{jk}\\left\\|\\Delta_{x}^{s}\\round{ \\squad{|x|^{-(d-\\gamma_1)}*(|x|^{-\\rho_{1}}| u_k|^p)} |x|^{-\\rho_{1}} | u_{j}|^{p-2}  u_j}\\right\\|_{\\widetilde X_{(t_{1}, t_{2})}}\n\\\\\n=\\lim_{t_1, t_2\\rightarrow \\infty}\\sum_{j,k=1}^N b_{jk}\\left\\|\\round{ \\squad{|x|^{-(d-\\gamma_1)}*(\\nabla^{1-s}_{x}|x|^{-\\rho_{1}}| u_k|^p)}\\nabla^{s} |x|^{-\\rho_{1}} | u_{j}|^{p-2}  u_j}\\right\\|_{L^{2}((t_{1}, t_{2}), L_{x}^{\\frac{2d}{d+2}})}\n\\nonumber\\\\\n=\\lim_{t_1, t_2\\rightarrow \\infty}\\sum^N_{j,k=1} \\left\\| \\Delta_{x}^{s}\\round {\\squad{|x|^{-(d-\\gamma_2)}*|x|^{-\\rho_{2}}| u_k|^2  } |x|^{-\\rho_{2}}u_j}\\right\\|_{\\widetilde X_{(t_{1}, t_{2})}}\n\\nonumber\\\\\n=\\lim_{t_1, t_2\\rightarrow \\infty}\\sum^N_{j,k=1} \\left\\| \\Delta_{x}^{s}\\round {\\squad{|x|^{-(d-\\gamma_2)}* |x|^{-\\rho_{2}}\\overline u_k u_j  } |x|^{-\\rho_{2}} u_k}\\right\\|_{\\widetilde X_{(t_{1}, t_{2})}}\n\\nonumber\\\\\n=\\lim_{t_1, t_2\\rightarrow \\infty}\\sum^N_{j,k=1} \\left\\| \\round {\\squad{|x|^{-(d-\\gamma_2)}*(\\nabla_{x}^{1-s}|x|^{-\\rho_{2}}| u_k|^2)  } \\nabla_{x}^{s}|x|^{-\\rho_{2}}u_j}\\right\\|_{L^{2}((t_{1}, t_{2}), L_{x}^{\\frac{2d}{d+2}})}\n\\nonumber\\\\\n=\\lim_{t_1, t_2\\rightarrow \\infty}\\sum^N_{j,k=1}\\left\\|\\round {\\squad{|x|^{-(d-\\gamma_2)}* (\\nabla_{x}^{1-s}|x|^{-\\rho_{2}}\\overline u_k u_j  }\\nabla_{x}^{s} |x|^{-\\rho_{2}} u_k}\\right\\|_{L^{2}((t_{1}, t_{2}), L_{x}^{\\frac{2d}{d+2}})}=0,\n\\end{align*}\nfor $s=0,1$ and that can be readily performed following the same lines of the proof of Lemma \\ref{StriHFC4}. \nThen the proof of the theorem is completed.\n\\end{proof}\n\\section{Appendix}\\label{appendix}\nIn this section we shall present an abstract result of independent interest. We will prove the equivalence between the classical \ninteraction Morawetz estimates and the tensor Morawetz estimates, appeared systematically in the papers \\cite{CGT07} and \\cite{CGT}. This result is in fact employed in Section \\ref{TMoraw} (identity \\eqref{eq.laplt}) and could be useful in order to switch from one to another setting making the bilinear Morawetz inequalities a flexible tool.  \n\\begin{lemma}\\label{eq.MorEqiv}\nLet be $z_{j,\\ell}(x,y)$ as in Lemma \\ref{lem:tenmor}. Then one has the following identity\n   \\begin{align}\\label{eq.EquivMorA}\n2\\Re \\int_{{\\mathbb R}^{2d}} \\Delta_{x,y} z_{j,\\ell}(x,y) \n    [(\\Delta_{x,y} a(x,y)\\overline z_{j,\\ell}(x,y)]\\,dxdy& \\\\\n    \\label{eq.EquivMorB}\n    +4\\Re \\int_{{\\mathbb R}^{2d}} \\Delta_{x,y} z_{j,\\ell}(x,y) \n    [(\\nabla_{x},\\nabla_{y}) a(x,y)\\cdot (\\nabla_{x},\\nabla_{y}) \\overline z_{j,\\ell}(x,y)]\\,dxdy& \n    \\\\\n    =-2\\int_{{\\mathbb R}^{2d}}\n \\Delta^2_xa(x,y) | u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy&\n\\nonumber\\\\\n    -4 \\int_{{\\mathbb R}^{2d}}  \\nabla_{x}u_{j}(x)D^2_{xy}a(x,y)\\nabla_{x}\\overline u_{j}(x)|u_{\\ell}(y)|^2\\,dxdy&\n     \\nonumber\\\\\n -4\\int_{{\\mathbb R}^{2d}} \\nabla_{y}u_{\\ell}(y)D^2_{xy}a(x,y)\\nabla_{y}\\overline u_{\\ell}(y)|u_{\\ell}(x)|^2\\,dxdy&\n  \\nonumber\\\\\n=2\\int_{{\\mathbb R}^{2d}}\n \\Delta^2_xa(x,y) | u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy&\n\\nonumber\\\\\n    -4 \\int_{{\\mathbb R}^{2d}}  \\nabla_{x}u_{j}(x)D^2_{xy}a(x,y)\\nabla_{x}\\overline u_{j}(x)|u_{\\ell}(y)|^2\\,dxdy&\n    \\nonumber\\\\\n -4\\int_{{\\mathbb R}^{2d}} \\nabla_{y}u_{\\ell}(y)D^2_{xy}a(x,y)\\nabla_{y}\\overline u_{\\ell}(y)|u_{\\ell}(x)|^2\\,dxdy&\n  \\nonumber\\\\\n=2\\int_{{\\mathbb R}^{2d}}\n \\Delta^2_xa(x,y) | u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy&\n\\nonumber\\\\\n    -4 \\int_{{\\mathbb R}^{2d}}  \\nabla_{x}u_{j}(x)D^2_{xy}a(x,y)\\nabla_{x}\\overline u_{j}(x)|u_{\\ell}(y)|^2\\,dxdy&\n    \\nonumber\\\\\n -4\\int_{{\\mathbb R}^{2d}} \\nabla_{y}u_{\\ell}(y)D^2_{xy}a(x,y)\\nabla_{y}\\overline u_{\\ell}(y)|u_{\\ell}(x)|^2\\,dxdy&\n \\nonumber\\\\\n  -8 \\int_{{\\mathbb R}^{2d}}\n  \\Im(\\overline u_{j}(x)\\nabla_{x}u_{j}(x))D^2_{xy}a(x,y)\n  \\Im(\\overline u_{\\ell}(y)\\nabla_{y}u_{\\ell}(y))\\,dxdy.&\n  \\nonumber\n   \\end{align}\n\n\\end{lemma}\n\\begin{proof}\nA computation of $ \\Delta_{x,y} z_{j,\\ell}(x,y)$ yields\n\n\\begin{align}\\label{eq.MorEquiv4}\n \\eqref{eq.EquivMorA}=2\\Re \\int_{\\mathbb{R}^{2d}} \\Delta_{x,y} a(x,y) \\overline{u}_{j}(x) \\Delta_{x} u_{j}(x)|u_{\\ell}(y)|^{2} \\,dxdy \n\\\\\n+2\\Re \\int_{\\mathbb{R}^{2d}} \\Delta_{x,y} a(x,y) \\overline{u}_{\\ell}(y) \\Delta_{y}u_{\\ell}(y)|u_{j}(x)|^{2} \\,dxdy\n\\nonumber\\\\\n=2\\Re \\int_{\\mathbb{R}^{2d}} \\Delta_{x} a(x,y) \\overline{u}_{\\ell}(y) \\Delta_{y}u_{\\ell}(y)|u_{j}(x)|^{2} \\,dxdy\n\\nonumber\\\\\n+2\\Re \\int_{\\mathbb{R}^{2d}} \\Delta_{y} a(x,y) \\overline{u}_{\\ell}(y) \\Delta_{y}u_{\\ell}(y)|u_{j}(x)|^{2} \\,dxdy\n\\nonumber\\\\\n+2\\Re \\int_{\\mathbb{R}^{2d}} \\Delta_{x} a(x,y) \\overline{u}_{j}(x) \\Delta_{x} u_{j}(x)|u_{\\ell}(y)|^{2} \\,dxdy\n\\nonumber\\\\\n+2\\Re \\int_{\\mathbb{R}^{2d}} \\Delta_{y} a(x,y) \\overline{u}_{j}(x) \\Delta_{x} u_{j}(x)|u_{\\ell}(y)|^{2} \\,dxdy.\n\\nonumber\\\\\n\\end{align}\nIn addition we get\n\\begin{align}\n\\eqref{eq.EquivMorB}\n\\\\\n=4 \\Re \\int_{\\mathbb{R}^{2d}}\\nabla_{x} a(x,y)\\nabla_{x} \\overline{u}_{j}(x) \\overline{u}_{\\ell}(y)\\left(\\Delta_{x} u_{j}(x) u_{\\ell}(y)+u_{j}(x) \\Delta_{y} u_{\\ell}(y)\\right) \\,dxdy \n\\nonumber\\\\\n+4 \\Re \\int_{\\mathbb{R}^{2d}}\\overline{u}_{j}(x) \\nabla_{y} a(x,y)\\nabla_{y}  \\overline{u}_{\\ell}(y)\\left(\\Delta_{x} u_{j}(x) u_{\\ell}(y)+u_{j}(x) \\Delta_{y} u_{\\ell}(y)\\right) \\,dxdy \n\\nonumber\\\\\n=4 \\Re \\int_{\\mathbb{R}^{2d}}\\nabla_{x} a(x,y)\\nabla_{x} \\overline{u}_{j}(x) \\Delta_{x} u_{j}(x)|u_{\\ell}(y)|^{2}  \\,dxdy \n\\nonumber\\\\\n4 \\Re \\int_{\\mathbb{R}^{2d}} \\nabla_{x} a(x,y)\\nabla_{x} \\overline{u}_{j}(x) \\overline{u}_{\\ell}(y) u_{j}(x) \\Delta_{y} u_{\\ell}(y) \\,dxdy\n\\nonumber\\\\\n4 \\Re \\int_{\\mathbb{R}^{2d}} \\nabla_{y} a(x,y)\\nabla_{y} \\overline{u}_{\\ell}(y) \\overline{u}_{j}(x) u_{\\ell}(y) \\Delta_{x} u_{j}(x) \\,dxdy\n\\nonumber\\\\\n+4 \\Re \\int_{\\mathbb{R}^{2d}}\\nabla_{y} a(x,y)\\nabla_{y} \\overline{u}_{\\ell}(y) \\Delta_{y} u_{\\ell}(y)|u_{j}(x)|^{2}  \\,dxdy \n\\nonumber\\\\\n=A_{1}+A_{2}+A_{3}+A_{4}.\n\\end{align}\nIt is crucial to observe that\n$$\nA_{2}=-2 \\Re \\int_{\\mathbb{R}^{2d}} \\Delta_{x} a(x,y) \\overline{u}_{\\ell}(y) \\Delta_{y} u_{\\ell}(y)|u_{j}(x)|^{2} \\,dxdy\n$$\nand\n$$\nA_{3}=-2 \\Re \\int_{\\mathbb{R}^{2d}} \\Delta_{y} a(x,y) \\overline{u}_{j}(x) \\Delta_{x} u_{j}(x)|u_{\\ell}(y)|^{2} \\,dxdy.\n$$\nThen we achieve that\n\\begin{align}\\label{eq.MorEquiv4A}\n\\eqref{eq.MorEquiv4}+A_{1}+A_{2}+A_{3}+A_{4} \\\\\n =2\\Re \\int_{\\mathbb{R}^{2d}} (\\Delta_{x} a(x,y) \\overline{u}_{j}(x)+2\\nabla_{x} a(x,y)\\nabla_{x} \\overline{u}_{j}(x) ) \\Delta_{x} u_{j}(x)|u_{\\ell}(y)|^{2} \\,dxdy \n\\nonumber\\\\\n+2\\Re \\int_{\\mathbb{R}^{2d}} (\\Delta_{x,y} a(x,y) \\overline{u}_{\\ell}(y)+2\\nabla_{y} a(x,y)\\nabla_{y} \\overline{u}_{\\ell}(y)) \\Delta_{y} u_{\\ell}(y)|u_{j}(x)|^{2}   \\,dxdy\n\\nonumber\\\\\n =\\int_{{\\mathbb R}^{2d}}\n \\Delta^2_xa(x,y) | u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy+\\int_{{\\mathbb R}^{2d}}\n\\Delta^2_ya(x,y) | u_j(x)|^2|u_{\\ell}(y)|^2\\,dxdy&\n\\nonumber\\\\\n    -4 \\int_{{\\mathbb R}^{2d}}  \\nabla_{x}u_{j}(x)D^2_{xy}a(x,y)\\nabla_{x}\\overline u_{j}(x)|u_{\\ell}(y)|^2\\,dxdy&\n \\nonumber\\\\\n -4\\int_{{\\mathbb R}^{2d}} \\nabla_{y}u_{\\ell}(y)D^2_{xy}a(x,y)\\nabla_{y}\\overline u_{\\ell}(y)|u_{\\ell}(x)|^2\\,dxdy,&\n  \\nonumber\n\\end{align}\nwhere the last inequality arises from standard calculations (see \\cite{TarVenk}, for example).\nMoreover, we have\n\\begin{align}\\label{eq.MorEquiv5}\n4 \\Re \\int_{\\mathbb{R}^{2d}} \\nabla_{y} a(x,y)\\nabla_{y} \\overline{u}_{\\ell}(y) \\overline{u}_{j}(x) u_{\\ell}(y) \\Delta_{x} u_{j}(x) \\,dxdy\n\\\\\n=-4 \\Re \\int_{\\mathbb{R}^{2d}} D^2_{xy} a(x,y)u_{\\ell}(y) \\nabla_{y} \\overline{u}_{\\ell}(y) \\overline{u}_{j}(x) \\nabla_{x} u_{j}(x) \\,dxdy\n\\nonumber\\\\\n-4 \\Re \\int_{\\mathbb{R}^{2d}}  \\nabla_{y} a(x,y)u_{\\ell}(y) \\nabla_{y} \\overline{u}_{\\ell}(y)  |\\nabla_{x} u_{j}(x)|^{2} \\,dxdy\n\\nonumber\\\\\n=-4 \\Re \\int_{\\mathbb{R}^{2d}}  \\nabla_{y} a(x,y)u_{\\ell}(y) \\nabla_{y} \\overline{u}_{\\ell}(y)  |\\nabla_{x} u_{j}(x)|^{2} \\,dxdy\n\\nonumber\\\\\n-4  \\int_{\\mathbb{R}^{2d}} D^2_{xy} a(x,y)\\Re(\\overline{u}_{\\ell}(y) \\nabla_{y} u_{\\ell}(y))\\Re( \\overline{u}_{j}(x) \\nabla_{x} u_{j}(x)) \\,dxdy\n\\nonumber\\\\\n-4\\int_{\\mathbb{R}^{2d}} D^2_{xy} a(x,y)\\Im(\\overline{u}_{\\ell}(y) \\nabla_{y} u_{\\ell}(y))\\Im( \\overline{u}_{j}(x) \\nabla_{x} u_{j}(x)), \\,dxdy\n\\nonumber\n\\end{align}\nby applying $\\Im(\\overline{u}_{\\ell}(y) \\nabla_{y} u_{\\ell}(y))=-\\Im(u_{\\ell}(y) \\nabla_{y} \\overline{u}_{\\ell}(y))$ and  \n$\\Re(B_{1}B_{2})=\\Re(B_{1})\\Re(B_{2})-\\Im(B_{1})\\Im(B_{2})$. This last identity enhances also to\n\\begin{align}\\label{eq.MorEquiv6}\n2\\Re \\int_{\\mathbb{R}^{2d}} \\Delta_{y} a(x,y) \\overline{u}_{j}(x) \\Delta_{x} u_{j}(x)|u_{\\ell}(y)|^{2} \\,dxdy\n\\\\\n=2\\Re \\int_{\\mathbb{R}^{2d}} D^2_{xy} a(x,y) \\nabla_{y}|u_{\\ell}(y)|^{2} \\overline{u}_{j}(x)\\nabla_{x} u_{j} (x)\\,dxdy\n\\nonumber\\\\\n+2 \\Re \\int_{\\mathbb{R}^{2d}}  \\nabla_{y} a(x,y) \\nabla_{y}|u_{\\ell}(y)|^{2}  |\\nabla_{x} u_{j}(x)|^{2} \\,dxdy\n\\nonumber\\\\\n=4 \\int_{\\mathbb{R}^{2d}} D^2_{xy} a(x,y)\\Re (\\overline{u}_{\\ell}(y)\\nabla_{y}u_{\\ell}(y)) \\Re( \\overline{u}_{j}(x)\\nabla_{x} u_{j} (x)) \\,dxdy\n\\nonumber\\\\\n+4 \\Re \\int_{\\mathbb{R}^{2d}}  \\nabla_{y} a(x,y) \\Re (\\overline{u}_{\\ell}(y)\\nabla_{y}u_{\\ell}(y))  |\\nabla_{x} u_{j}(x)|^{2} \\,dxdy.\n\\nonumber\n\\end{align}\nFinally we come to\n\\begin{align}\n\\eqref{eq.MorEquiv5}+\\eqref{eq.MorEquiv6}\n\\\\=-4\\int_{\\mathbb{R}^{2d}} D^2_{xy} a(x,y)\\Im(\\overline{u}_{\\ell}(y) \\nabla_{y} u_{\\ell}(y))\\Im( \\overline{u}_{j}(x) \\nabla_{x} u_{j}(x)) \\,dxdy\n\\nonumber\n\\end{align}\nand  by symmetry to\n\\begin{align}\n2\\Re \\int_{\\mathbb{R}^{2d}} \\Delta_{x} a(x,y) \\overline{u}_{\\ell}(y) \\Delta_{y}u_{\\ell}(y)|u_{j}(x)|^{2} \\,dxdy\\\\\n4 \\Re \\int_{\\mathbb{R}^{2d}} \\nabla_{x} a(x,y)\\nabla_{x} \\overline{u}_{j}(x) \\overline{u}_{\\ell}(y) u_{j}(x) \\Delta_{y} u_{\\ell}(y) \\,dxdy\n\\nonumber\n\\\\=-4\\int_{\\mathbb{R}^{2d}} D^2_{xy} a(x,y)\\Im(\\overline{u}_{\\ell}(y) \\nabla_{y} u_{\\ell}(y))\\Im( \\overline{u}_{j}(x) \\nabla_{x} u_{j}(x)) \\,dxdy.\n\\nonumber\n\\end{align}\nIt follows that\n\\begin{align}\\label{eq.MorEquiv7}\n\\eqref{eq.MorEquiv4}+A_{1}+A_{2}+A_{3}+A_{4}\\\\=\\eqref{eq.MorEquiv4A}\n-8\\int_{\\mathbb{R}^{2d}} D^2_{xy} a(x,y)\\Im(\\overline{u}_{\\ell}(y) \\nabla_{y} u_{\\ell}(y))\\Im( \\overline{u}_{j}(x) \\nabla_{x} u_{j}(x)) \\,dxdy.\n\\nonumber\n\\end{align}\nThe proof is now  completed.\n \\end{proof}\n\n\\begin{remark}\nWe presented our argument only for the terms involving the operator $\\Delta_{x}$. Similar calculations can be performed also\nfor the fourth-order operator  $\\Delta^{2}_{x}$, with more complicated steps involved (see \\cite{Ta}). It should be not surprising that the above lemma shows that\n$$\n\\int_{\\mathbb{R}^{2d}} \\Im(\\overline{u}_{\\ell}(y) \\nabla_{y} u_{\\ell}(y))D^2_{xy} a(x,y)\\Im( \\overline{u}_{j}(x) \\nabla_{x} u_{j}(x)) \\,dxdy=0.\n$$\nThis because the tensor Morawetz identities act as classical Morawetz ones and is not taking into account,\nat least in the fundamental steps, of the interactive aspect of the action \\eqref{eq:tenmor1}. In fact if $u_{j}, u_{\\ell}$ be solutions to \\eqref{eq:HFC4} in $d$ spatial dimensions we can define the tensor product $\\left(u_{i} \\otimes u_{\\ell}\\right)(t, x,y)$ for $(x,y)$ in\n$$\n\\mathbb{R}^{2d}=\\left\\{\\left(x, y\\right): x \\in \\mathbb{R}^{d}, y \\in \\mathbb{R}^{d}\\right\\},\n$$\nby the formula\n$$\n\\left(u_{j} \\otimes u_{\\ell}\\right)(t, x,y)=u_{j}\\left(t, x\\right) u_{\\ell}\\left(t, y\\right)\n$$\nand utilizing \\eqref{eq.singMor} along with the equation  \\eqref{eq.tenid}. The interactive inequalities are then a byproduct of this approach. At this point there are several ways one can present these estimates: as bilinear generalization of the classical Morawetz estimates \n(see \\cite{PlVe}, \\cite{TarVenk},  \\cite{TzVi}) or as classical Morawetz estimates applied to tensors of solutions  (see \\cite{Dinh} \\cite{MWZ}, \\cite{Ta}). \n\\end{remark}\n\n", "meta": {"timestamp": "2021-06-30T02:03:35", "yymm": "2106", "arxiv_id": "2106.12983", "language": "en", "url": "https://arxiv.org/abs/2106.12983"}}
{"text": "\\section{Introduction}\r\n\r\nThe theoretical study of emission or absorption spectral properties of hot \r\nplasmas, encountered for instance in stellar physics, inertial-confinement \r\nfusion, or laser-plasma experiments, implies taking into account complex \r\nions, i.e., multi-electron configurations with several open subshells. The \r\nissue of finding the number of states corresponding to a given $(J,M)$ set \r\n--- $J$ being the magnitude of the total angular momentum operator and $M$ \r\nthe eigenvalue (in units of $\\hbar$) of its projection on the $z$-axis --- \r\nin the case of a set of indistinguishable particles was first investigated \r\nby Bethe in 1936 for nuclear systems \\cite{BETHE36}. The problem of the \r\nclassification of atomic energy levels is discussed in many textbooks about \r\nquantum mechanics. The determination of the spectroscopic terms arising in a \r\ngiven electronic configuration was addressed by different methods, the first \r\none being the so-called vector model \\cite{COWAN81}. The properties \r\n(regularities, trends) of such terms were also investigated \\cite{JUDD68}. \r\nThe problem of listing the terms arising in a complex configuration can be \r\nsolved from elementary group theory \\cite{BREIT26,CURL60,KARAYIANIS65\nKATRIEL89,XU06}. Besides, the determination of the number of lines between \r\ntwo configurations is of great interest. Using group-theoretical methods, \r\nKrasnitz obtained a compact formula only in the simple case of configurations \r\nbuilt with non-equivalent electrons \\cite{BAUCHE88}. The statistics of \r\nelectric-dipole (E1) lines was studied by Moszkowski \\cite{MOSZKOWSKI60}, \r\nBancewicz \\cite{BANCEWICZ84}, Bauche and Bauche-Arnoult \\cite{BAUCHE87,\r\nBAUCHE90}, and more recently by Gilleron and Pain \\cite{GILLERON09}. Such a \r\nquantity is important for opacity codes, for instance, in order to decide \r\nwhether a transition array can be described statistically or requires a  \r\ndetailed-line accounting calculation, relying on the diagonalization of the \r\nHamiltonian \\cite{PORCHEROT11}. In the same spirit, the statistics of \r\nelectric quadrupole (E2) lines was also investigated \\cite{PAIN12}. A \r\nparticular case of fluctuation, the odd-even staggering (i.e., the fact that, \r\nin an electronic configuration, the number of odd values of $J$ can differ \r\nfrom the number of even values of $J$), was studied by Bauche and Coss\\'e \r\n\\cite{BAUCHE97} and later revisited using the generating-function technique \r\n\\cite{PAIN13}. \r\n\r\nExcept maybe for the odd-even staggering, the knowledge of the moments or \r\ncumulants can be very useful to build a statistical modeling. This was \r\ncarried out by Bauche et al. \\cite{BAUCHE87} for the distributions of energy \r\nlevels and spectroscopic terms in an electronic configuration or for the \r\ndistribution of absorption or emission lines. For instance, following the \r\npioneering work of Moszkowski \\cite{MOSZKOWSKI62}, the first two moments of \r\nthe line energies weighted by their strengths in a transition array were \r\ncalculated exactly by Bauche et al. \\cite{BAUCHE88}. The work on averages of \r\nproducts of operators by Ginocchio \\cite{GINOCCHIO73} enabled Ku\\v{c}as and \r\nKarazija \\cite{KARAZIJA91b,KUCAS95II} to find an algorithm to generate the \r\nmoment of any order and the impact of higher-order moments (without \r\ncalculating them explicitly) was studied recently by Gilleron and Pain \r\n\\cite{GILLERON08}. Kyni\\.ene et al. investigated the statistical properties \r\nof Auger transitions and obtained a fair approximation for the \r\nnumber of Auger amplitudes \\cite{KYNIENE02}. The authors showed that \r\nstatistical properties of Auger spectra mainly depend on the orbital quantum \r\nnumbers of shells involved in the transitions and that rather large values of \r\nskewness and excess kurtosis indicate a significant deviation of the \r\ndistribution of Auger amplitudes from the normal distribution. Moreover, the \r\ngenerating-function formalism is a powerful tool for tackling the counting \r\nproblems, either for finding analytical expressions, deriving recursion \r\nrelations or performing a statistical analysis. Using such a formalism, we \r\nrecently published explicit and recurrence formulas for the number of \r\nelectronic configurations in an atom \\cite{Pain2020}, together with a \r\nstatistical analysis through the computation of cumulants.\r\n\r\nThe object of this work is to show that similar considerations apply to the \r\ndistribution of the magnetic quantum number in a relativistic configuration. \r\nThe present paper is organized as follows. General formulas for the magnetic \r\nquantum number distribution $P(M)$ are recalled in section \\ref{sec:mag}. The \r\ngenerating function of cumulants of this distribution in a single- or \r\nmultiple-subshell configuration is derived in the same section. In section \r\n\\ref{sec:PM_nth_der_recur}, recurrence relations are deduced from the \r\nexpression of the quantum number distribution as a n-th derivative. The \r\nanalytical expression of the cumulants is obtained in section \r\n\\ref{sec:det_cum} and an additional recurrence relation for their generating \r\nfunction is provided in section \\ref{sec:rec_gen}. An analysis of the \r\ndistribution using Gram-Charlier and Edgeworth series is carried out in \r\nsections \\ref{sec:gram} and \\ref{sec:Edgeworth} respectively, and the paper \r\nends with instructive general considerations about the distribution $P(M)$.\r\n\r\n\r\n\\section{Characterization of the magnetic quantum number distribution: the \r\ncumulant generating function}\\label{sec:mag}\r\n\\subsection{Definitions}\r\nOur main objective is to determine the statistics of the angular quantum \r\nnumber $J$. However, due to the fact that the quantum number $J$ is the \r\neigenvalue of no simple operator, its mathematical study is tedious. \r\nTherefore, it is more appropriate to study the distribution of the magnetic \r\nquantum number $M$, another advantage being that when different subshells are \r\npresent their contributions to $M$ simply add up. The $J$ values can be \r\nobtained from the $M$ values by means of the method of Condon and Shortley \r\n\\cite{Condon1935}, which enables one to express the number $Q(J)$ of levels \r\nwith angular momentum $J$ in a configuration as\r\n\\begin{equation}\\label{rec}\r\nQ(J)=\\sum_{M=J}^{J+1}(-1)^{J-M}P\\left(M\\right)\r\n =P\\left(M=J\\right)-P\\left(M=J+1\\right),\r\n\\end{equation}\r\nwhere $P$ represents the distribution of the angular-momentum projection $M$. \r\nIn this work, we consider the case of relativistic configurations, which \r\nmeans that individual electrons are labeled by their total angular momentum \r\n$j$. Pauli principle is fully accounted for. For a configuration $j_1^{N_1}\r\nj_2^{N_2}j_3^{N_3}\\cdots j_w^{N_w}$, $P(M)$ is determined through the relation\r\n\\begin{equation}\\label{eq:convolM}\r\nP_{N_1,N_2,\\cdots}\\left(M\\right)=\\left(P_{N_1}\\otimes P_{N_2}\\otimes P_{N_3}\r\n \\otimes\\cdots\\otimes P_{N_w}\\right)\\left(M\\right),\r\n\\end{equation}\r\nwhere the distributions are convolved two at a time, which means that\r\n\\begin{equation}\r\n\\left(P_{N_i}\\otimes P_{N_j}\\right)\\left(M\\right)=\\sum_{M'=-\\infty}^{\\infty}\r\n P_{N_i}\\left(M'\\right) P_{N_j}\\left(M-M'\\right).\r\n\\end{equation}\r\nLet us consider a system of $N$ identical fermions in a configuration consisting \r\nof a single orbital of degeneracy $g$, $m_i$ being the angular momentum \r\nprojection of electron state $i$. Two constraints must be satisfied:\r\n\\begin{equation}\r\nN=n_1+\\cdots+n_g=\\sum_{i=1}^gn_i,\r\n\\end{equation}\r\nwhere $n_i$ is the number of electrons in state $i$ and\r\n\\begin{equation}\r\nM=n_1m_1+\\cdots+n_gm_g=\\sum_{i=1}^gn_im_i,\r\n\\end{equation}\r\nwhere $n_i=0$ or 1 $\\forall i$. In the particular case of the relativistic \r\nconfiguration $j^N$, the maximum total angular momentum is\r\n\\begin{equation}\r\nJ_{\\text{max}}=\\sum_{m=j-N+1}^jm=(2j+1-N)N/2.\r\n\\end{equation}\r\nAs stated in statistical treatises \\cite{Stuart1994}, the whole information \r\nabout the distribution $P(M)$ of magnetic quantum number is contained in the \r\nexponential of the cumulant generating function defined as \r\n\\begin{equation}\r\n \\exp(K(t)) = \\left<\\exp(tM)\\right>\r\n = \\sum_M P(M)e^{tM} \\left/ \\sum_M P(M) \\right.\\label{eq:defeKt}\r\n\\end{equation}\r\nwhere $P(M)$ is the number of $N$-electron states such as $m_1+\\cdots m_N=M$. \r\nFrom the Pauli principle this normalization factor is given by the product of \r\nsimple binomial coefficients\r\n\\begin{equation}\r\n\\sum_M P(M)=\\prod_{s=1}^w\\binom{2j_s+1}{N_s}.\\label{eq:norm}\r\n\\end{equation}\r\n\r\n\\subsection{Derivation from a recurrence relation in a single-subshell case}\r\n\r\nAs a first step we consider relativistic configurations containing only one \r\nsubshell symbolically written $j^N$. One may express the population $P(M)$ \r\nas a multiple-sum over each magnetic level population \\cite{GILLERON09}\r\n\\begin{equation}\r\nP(M)=\\sum_{p_1=0}^1\\sum_{p_2=0}^1\\cdots\\sum_{p_g=0}^1\r\n \\delta\\left(M-\\sum_{k=1}^gp_km_k\\right)\\ \\delta\\left(N-\\sum_{k=1}^gp_k\\right)\r\n\\end{equation}\r\nwhere $p_k$ is the $k$-state population and $g=2j+1$ the subshell degeneracy.\r\nThe Kronecker symbol $\\delta_{i,j}$ is written here $\\delta(i-j)$ for the \r\nsake of readability.\r\nEach $p_k$ is either 0 or 1, and the individual magnetic quantum numbers are \r\n\\begin{equation}\\label{eq:valm}\r\nm_1=-j, m_2=-j+1,\\cdots m_g=j.\r\n\\end{equation}\r\nWriting the numerator in (\\ref{eq:defeKt}) as \r\n\\begin{equation}\\label{eq:defs}\r\ns(N,j,t)=\\sum_MP(M)e^{Mt}\r\n\\end{equation}\r\none has\r\n\\begin{equation}\r\ns(N,j,t)=\\sum_M\\sum_{p_1=0}^1\\sum_{p_2=0}^1\\cdots\\sum_{p_g=0}^1 \r\n \\delta\\left(M-\\sum_{k=1}^{g}p_km_k\\right)\\ \r\n \\delta\\left(N-\\sum_{k=1}^{g}p_k\\right)e^{Mt}\r\n\\end{equation}\r\nin which the sum over $M$ may be eliminated\r\n\\begin{equation}\r\ns(N,j,t)=\\sum_{p_1=0}^1\\sum_{p_2=0}^1\\cdots\\sum_{p_g=0}^1 \r\n \\delta\\left(N-\\sum_{k=1}^g p_k\\right) \\exp\\left(\\sum_{k=1}^g p_km_kt\\right).\r\n\\end{equation}\r\nIsolating in this multiple sum the contributions of the $p_g$ index and then \r\nthe $p_1$ index, one gets\r\n\\begin{subequations}\\begin{align}\r\ns(N,j,t)=&\\sum_{p_1=0}^1\\sum_{p_2=0}^1\\cdots\\sum_{p_{g-1}=0}^1 \r\n \\delta\\left(N-\\sum_{k=1}^{g-1}p_k\\right) \\exp\\left(\\sum_{k=1}^{g-1}p_km_kt\\right)\r\n \\nonumber\\\\\r\n& +e^{m_gt}\\sum_{p_1=0}^1\\sum_{p_2=0}^1\\cdots\\sum_{p_{g-1}=0}^1 \r\n  \\delta\\left(N-1-\\sum_{k=1}^{g-1}p_k\\right)\r\n  \\exp\\left(\\sum_{k=1}^{g-1}p_km_kt\\right)\\\\\r\n=&\\sum_{p_2=0}^1\\cdots\\sum_{p_{g-1}=0}^1 \\delta\\left(N-\\sum_{k=2}^{g-1}p_k\\right)\r\n \\exp\\left(\\sum_{k=2}^{g-1}p_km_kt\\right)\\nonumber\\\\\r\n &+e^{m_1t}\\sum_{p_2=0}^1\\cdots\\sum_{p_{g-1}=0}^1 \r\n \\delta\\left(N-1-\\sum_{k=2}^{g-1}p_k\\right) \\exp\\left(\\sum_{k=2}^{g-1}p_km_kt\\right)\r\n \\nonumber\\\\\r\n &+e^{m_gt}\\left[\\sum_{p_2=0}^1\\cdots\\sum_{p_{g-1}=0}^1 \r\n \\delta\\left(N-1-\\sum_{k=2}^{g-1}p_k\\right) \\exp\\left(\\sum_{k=2}^{g-1}p_km_kt\\right)\\right.\r\n \\nonumber\\\\\r\n &\\left.+e^{m_1t}\\sum_{p_2=0}^1\\cdots\\sum_{p_{g-1}=0}^1 \r\n \\delta\\left(N-2-\\sum_{k=1}^{g-1}p_k\\right) \\exp\\left(\\sum_{k=2}^{g-1}p_km_kt\\right)\\right]\r\n\\end{align}\\end{subequations}\r\nwhere we have used the fact that $p_1, p_g$ are equal to 0 or 1. \r\nOne may easily verify that the multiple sum over $p_2\\cdots p_{g-1}$ generates \r\nthe subshell with angular momentum $j-1$ and population $M'=\\sum_{k=2}^{g-1}p_k$. \r\nUsing the definitions (\\ref{eq:valm}), one gets the recurrence property on the \r\ngenerating function \r\n\\begin{equation}\r\ns(N,j,t)=s(N,j-1,t)+2\\cosh(jt)s(N-1,j-1,t)+s(N-2,j-1,t).\\label{eq:recj}\r\n\\end{equation}\r\nThe argument consisting in specifying the populations $p_1$ and $p_g$ has \r\nbeen used by Talmi \\cite{Talmi2005} who obtained a recurrence relation on \r\nthe populations $P(M)$ formally similar to Eq.~(\\ref{eq:recj}).\r\nThe recurrence relation (\\ref{eq:recj}) may be initialized by the $N=1$ value. \r\nOne writes from the definition (\\ref{eq:defs})\r\n\\begin{equation}\r\ns(1,j,t)=\\sum_{m=-j}^{j}e^{mt}=e^{-jt}\\frac{e^{(2j+1)t}-1}{e^t-1}\r\n =\\frac{\\sinh((2j+1)t/2)}{\\sinh(t/2)}.\\label{eq:vals1}\r\n\\end{equation}\r\nAccordingly on may define the initial values for the $j=1/2$ case\r\n\\begin{equation}\\label{eq:valsj1}\r\ns(0,1/2,t)=s(2,1/2,t)=1,\\quad s(1,1/2,t)=2\\cosh(t/2),\r\n\\quad s(N,1/2,t)=0\\text{ if }N>2.\r\n\\end{equation}\r\nIn order to derive the general expression of the sum $s(N,j,t)$ we performed \r\na series of explicit computations for various $N,j$. This work leads us to \r\npropose the result\r\n\\begin{equation}\\label{eq:exps}\r\ns(N,j,t)=\\frac{\\displaystyle\\prod_{p=1}^{N}\\sinh((2j+2-p)t/2)}{\\displaystyle\r\n \\prod_{p=1}^{N}\\sinh(pt/2)}.\r\n\\end{equation}\r\nThis form agrees with the $N=1$ value (\\ref{eq:vals1}), and with the $j=1/2$ \r\nvalue (\\ref{eq:valsj1}). Its general validity is proved here by recurrence. \r\nLet us assume the property is true up to angular momentum $j-1$. To complete \r\nthe proof one must compute with the above analytical form the ratio \r\n\\begin{equation}\r\n\\rho=\\Big(s(N,j-1,t)+2\\cosh(jt)s(N-1,j-1,t)+s(N-2,j-1,t)\\Big)/s(N,j,t)\r\n\\end{equation}\r\nand show that it is equal to 1. The expression (\\ref{eq:exps}) leads to \r\n\\begin{subequations}\\begin{align}\r\n \\rho&=\\frac{\\sinh((2j+1-N)u)\\sinh((2j-N)u)}{\\sinh((2j+1)u)\\sinh(2ju)}\\nonumber\\\\\r\n  &\\qquad+2\\cosh(2ju)\\frac{\\sinh(Nu)\\sinh((2j+1-N)u)}{\\sinh((2j+1)u)\\sinh(2ju)}\r\n  +\\frac{\\sinh(Nu)\\sinh((N-1)u)}{\\sinh((2j+1)u)\\sinh(2ju)}\\\\\r\n  &= \\frac{\\mathscr{N}}{\\sinh((2j+1)u)\\sinh(2ju)}\r\n\\end{align}\\end{subequations}\r\nwith $u=t/2$ and the numerator\r\n\\begin{multline}\r\n\\mathscr{N}=\\sinh((2j+1-N)u)\\sinh((2j-N)u)+2\\cosh(2ju)\\sinh(Nu)\\sinh((2j+1-N)u)\\\\\r\n  \\quad+\\sinh(Nu)\\sinh((N-1)u).\r\n\\end{multline}\r\nUsing some elementary trigonometric formulas one easily verifies that \r\n\\begin{equation}\\mathscr{N}=\\sinh((2j+1)u)\\sinh(2ju)\\end{equation}\r\nso that $\\rho=1$. This completes the proof of (\\ref{eq:exps}) by recurrence. \r\nAn alternate derivation based on term counting is briefly mentioned in \r\nAppendix \\ref{sec:relsNsN-1}. Another useful property on the sum $s$ is \r\n\\begin{equation}\r\ns(N,j,t)=\\frac{\\sinh((2j+2-N)t/2)}{\\sinh(Nt/2)}s(N-1,j,t)\\label{eq:relSNsN-1}\r\n\\end{equation}\r\nfrom which one can conventionally define\r\n\\begin{equation}s(0,j,t)=1\\label{eq:s0}\\end{equation}whatever $j$.\r\n\r\n\\subsection{Case of several subshells}\r\nAs deduced from a well-known property of the Laplace transform, since the \r\ndistribution $P(M)$ is obtained from the convolution of the distributions of \r\nevery subshell (\\ref{eq:convolM}), the Laplace transform for the most general \r\nrelativistic configuration $\\sum_M P(M)e^{Mt}$ will be given by the \r\n\\emph{product} of the individual Laplace transforms. For instance if two \r\nsubshells are involved, the exponential of the cumulant generating function \r\nis given by\r\n\\begin{equation}\r\n  e^{K(t)}=\\sum_M \\sum_{M_1}P_1(M_1)P_2(M-M_1)e^{Mt} \r\n   \\left/ \\sum_M \\sum_{M_1}P_1(M_1)P_2(M-M_1)\\right.\r\n\\end{equation}\r\nand since the sums in the numerator and the denominator are the products \r\nof the individual subshell contributions one easily checks that \r\n\\begin{equation}e^{K(t)}=e^{K_1(t)}e^{K_2(t)}.\\end{equation}\r\nIn other words, using the analytical form (\\ref{eq:exps}) one gets \r\nfor the configuration $j_1^{N_1}...j_w^{N_w}$\r\n\\begin{equation}\r\n e^{K(t)}=\\left.\\prod_{i=1}^{w}\r\n  \\frac{\\displaystyle\\prod_{p_i=1}^{N_i}\\sinh((2j_i+2-p_i)t/2)}{\\displaystyle\r\n \\prod_{p_i=1}^{N}\\sinh(p_it/2)}\\right/\r\n \\prod_{i=1}^{w}\\binom{2j_i+1}{N_i}.\r\n\\end{equation}\r\nAccordingly, the cumulant generating function $K(t)$ will be given by the \r\n\\emph{sum} of each subshell cumulant generating function. \r\n\r\n\r\n\\section{Expression of the quantum number distribution as a n-th derivative; \r\napplication to recurrence relations}\\label{sec:PM_nth_der_recur}\r\nWe consider here the case of a configuration made of a single subshell $j^N$. \r\nThe above expression (\\ref{eq:exps}) for the exponential of the cumulant \r\ngenerating function may be reformulated slightly differently. Defining \r\n$z=e^t$, the product of hyperbolic sines may be rewritten after simple \r\ntransformations as\r\n\\begin{equation}\\label{eq:genfnPz}\r\n\\sum_M P(M)z^M =z^{-J_\\text{max}}\\prod_{p=1}^N\\frac{z^{2j+2-p}-1}{z^p-1}\r\n\\end{equation}\r\nwhere $J_\\text{max}=N(2j+1-N)/2$ is the maximum total angular momentum as \r\ndefined previously. Knowing that\r\n\\begin{equation}n=M+J_\\text{max}\\end{equation}\r\nis an integer varying from 0 to $2J_\\text{max}$, one may express $P(M)$ as \r\na n-th derivative of the function\r\n\\begin{equation}\r\n\\mathscr{F}(j,N;z)=\\sum_{n=0}^{2J_\\text{max}} P(n-J_\\text{max})z^n =\r\n \\prod_{p=1}^N\\frac{z^{2j+2-p}-1}{z^p-1}.\\label{eq:FjNz}\r\n\\end{equation}\r\nThe $\\mathscr{F}(j,N;z)$ function is also known in numerical analysis as the \r\nGaussian binomial coefficient or $q$-binomial coefficient \\cite{ANDREWS1984}. \r\nUsing standard notation, one has\r\n\\begin{equation}\\mathscr{F}(j,N;z)=\\qbin{2j+1}{N}{z}.\\end{equation}\r\nFrom well-known Pascal-like relations on these polynomials, two recurrence \r\nrelations on the $P(M)$ can be deduced, as shown in Appendix \r\n\\ref{sec:rec_Gaussbin}.\r\n\r\nOne may also use the expansion (\\ref{eq:genfnPz}) to get an expression of the \r\n$P(M)$ values as an integral. Namely, with $z=e^{2it}$, this expansion can \r\nbe rewritten as\r\n\\begin{equation}\r\n\\prod_{q=1}^N \\frac{\\sin((2j+2-q)t)}{\\sin(qt)} =  e^{-2iJ_\\text{max}t} \r\n  \\sum_{n=0}^{2J_\\text{max}}P(n-J_\\text{max})e^{2int}\r\n=\\sum_{M}P(M)e^{2iMt}.\\label{eq:fgen_sin}\r\n\\end{equation}\r\nAfter multiplication by $e^{-2iMt}$ and integration over $t$ on the $[-\\pi,\\pi]$ \r\ninterval, one gets, accounting for the parity of the above expression\r\n\\begin{equation}\r\nP(M)=\\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}dt\\:e^{-2iMt}\r\n \\prod_{q=1}^N \\frac{\\sin((2j+2-q)t)}{\\sin(qt)}\r\n  =\\frac{1}{\\pi}\\int_{0}^{\\pi}dt\\:\\cos(2Mt)\r\n   \\prod_{q=1}^N \\frac{\\sin((2j+2-q)t)}{\\sin(qt)}.\r\n\\end{equation}\r\nThe above written integrand exhibits a sharp peak close to $t=0$ and\r\nthis may be used to derive an approximate value of $P(M)$ using the \r\nsaddle-point method.\r\n\r\nIdentifying the expansion (\\ref{eq:genfnPz}) as a Taylor expansion at $z=0$, \r\none gets\r\n\\begin{equation}\\label{eq:PM_derivn}\r\nP(n-J_\\text{max}) = \\frac{1}{n!}\\left.\\frac{d^n}{dz^n}\\left(\\frac{\r\n \\left(z^{2j+1}-1\\right)\\left(z^{2j}-1\\right)\\cdots\\left(z^{2j+2-N}-1\\right)\r\n }{\\left(z^{N}-1\\right)\\left(z^{N-1}-1\\right)\\cdots\\left(z-1\\right)}\r\n\\right) \\right|_{z=0}\r\n\\end{equation}\r\nwhich amounts to evaluate the derivative of a rational fraction. One may \r\ntransform the n-th derivative (\\ref{eq:PM_derivn}) with the Leibniz rule. \r\nHowever while the q-th-derivative at $z=0$ of $z^{2j+2-p}-1$ is elementary \r\nsince equal to $q!\\delta(q-(2j+2-p))$, the q-th derivative of $1/(z^p-1)$ is \r\nnonzero whatever $q$. Therefore the above n-th derivative can be expressed \r\nvia the Leibniz rule as a multiple sum of limited usefulness.\r\n\r\nOf course for given $j$ and $N$ a direct analytical computation is tractable. \r\nFor instance if $j=1/2, N=1$\r\n\\begin{equation}\\mathscr{F}(1/2,1;z)= \\frac{z^2-1}{z-1} = 1+z\\end{equation}\r\nfor which the 0-th and first order derivatives in $z=0$ are 1, so that \r\n$P(1/2)=P(-1/2)=1$. Accordingly if $j=3/2, N=2$ \r\n\\begin{equation}\r\n\\mathscr{F}(3/2,2;z)= \\frac{(z^4-1)(z^3-1)}{(z^2-1)(z-1)} \r\n = (1+z^2)(1+z+z^2)=1+z+2z^2+z^3+z^4\\label{eq:F3h2}\r\n\\end{equation}\r\nand the derivatives from order 0 to 4 provide $P(\\pm2)=1, P(\\pm1)=1, P(0)=2$.\r\nHowever obtaining an analytical formula valid for any $j$ and $N$ from formula \r\n(\\ref{eq:PM_derivn}) is not straightforward.\r\n\r\nMoreover, the identity (\\ref{eq:FjNz}) allows the derivation of a recurrence \r\nproperty on $N$. The relation \r\n\\begin{equation}\r\n \\mathscr{F}(j,N;z)(z^N-1)=\\mathscr{F}(j,N-1;z)(z^{2j+2-N}-1)\r\n\\end{equation}\r\nimplies, after $n$ derivations with respect to $z$ and use of the Leibniz rule \r\n\\begin{equation}\r\n\\sum_{s} \\binom{n}{s}\\mathscr{F}^{(s)}(j,N;z)(z^N-1)^{(n-s)} = \r\n \\sum_{t} \\binom{n}{t}\\mathscr{F}^{(t)}(j,N-1;z)(z^{2j+2-N}-1)^{(n-t)}\r\n\\end{equation}\r\nwhere $f^{(n)}(z)$ is the n-th derivative of $f(z)$ with respect to $z$. \r\nThe above derivatives at $z=0$ are fairly simple. Namely one has\r\n\\begin{multline}\r\n \\sum_{s} \\binom{n}{s} s!P(s-J_\\text{max}(N);j,N)\r\n  \\left(N!\\delta_{N,n-s}-\\delta_{0,n-s}\\right)\\\\\r\n = \\sum_{t} \\binom{n}{t} t!P(t-J_\\text{max}(N-1);j,N-1)\r\n  \\left((2j+2-N)!\\delta_{2j+2-N,n-t}-\\delta_{0,n-t}\\right)\r\n\\end{multline}\r\nwhich provides after basic simplifications the relation between the $P(M;j,N)$ \r\nand the $P(M';j,N-1)$\r\n\\begin{multline}\\label{eq:recPM_overN}\r\nP(n-N-N(2j+1-N)/2;j,N) - P(n-N(2j+1-N)/2;j,N) \\\\\r\n = P(n-2j-2+N-(N-1)(2j+2-N)/2;j,N-1) -  P(n-(N-1)(2j+2-N)/2;j,N-1).\r\n\\end{multline}\r\nWith the definition \r\n\\begin{equation}\\label{eq:Pscr_def}\r\n \\mathscr{P}_{j,N}(n)=P(n-N(2j+1-N)/2;j,N)\r\n\\end{equation}\r\nwith $n$ integer in the range $0\\le n\\le N(2j+1-N)$, one gets the more \r\ncompact formula\r\n\\begin{equation}\\label{eq:recOPM_overN}\r\n\\mathscr{P}_{j,N}(n)=\\mathscr{P}_{j,N-1}(n)\r\n -\\mathscr{P}_{j,N-1}(n-2j-2+N)+\\mathscr{P}_{j,N}(n-N)\r\n\\end{equation}\r\nThis relation proves to be very efficient in determining $P(M;j,N)$ \r\nwhatever $j$ and $N$, since the first distribution is elementary\r\n\\begin{equation}\r\n P(M;j,1)=\\mathscr{P}_{j,1}(M+j)=1\\text{\\quad if }-j\\le M\\le+j.\r\n\\end{equation}\r\nFor the first $n$ values ($0 \\le n <N$), the first member of the recurrence \r\n(\\ref{eq:recPM_overN}) is reduced to the second term \r\n$P(n-N(2j+1-N)/2;j,N)$. The same behavior occurs for each $n$ below $N$. \r\nFor larger $n$, in the difference $P(n-N-N(2j+1-N)/2;j,N) - \r\nP(n-N(2j+1-N)/2;j,N)$ the first $P(M)$ has already been computed, which \r\ndefines the population $P(n-N-N(2j+1-N)/2;j,N)$ since the $P(M;j,N-1)$ are \r\nassumed to be known. \r\n\r\nThe identity (\\ref{eq:FjNz}) may be used by varying $j$ too. Explicitly\r\n\\begin{equation}\r\n\\mathscr{F}(j,N;z)(z^{2j-N}-1)(z^{2j+1-N}-1)=\r\n \\mathscr{F}(j-1,N;z)(z^{2j+1}-1)(z^{2j}-1)\r\n\\end{equation}\r\nfrom which one gets after $n$ derivations in $z=0$\r\n\\begin{multline}\r\n  \\sum_s \\binom{n}{s} s!P(s-J_\\text{max};j,N)\\\\\r\n  \\times\\left[(4j+1-2N)!\\delta_{4j+1-2N,n-s}-(2j+1-N)!\\delta_{2j+1-N,n-s}\r\n  -(2j-N)!\\delta_{2j-N,n-s}+\\delta_{0,n-s}\\right]\\\\\r\n  =\\sum_t \\binom{n}{t} t!P(t-J'_\\text{max};j-1,N)\r\n  \\left((4j+1)!\\delta_{4j+1,n-t}-(2j+1)!\\delta_{2j+1,n-t}\r\n  -(2j)!\\delta_{2j,n-t}+\\delta_{0,n-t}\\right)\r\n\\end{multline}\r\nwith $J_\\text{max}=N(2j+1-N)/2$, $J'_\\text{max}=N(2j-1-N)/2$. After some \r\nbasic simplifications, one gets the relation involving four $P$ for each $j$ \r\nvalue, using the notation (\\ref{eq:Pscr_def})\r\n\\begin{multline} \r\n \\mathscr{P}_{j,N}(n-4j-1+2N) -\\mathscr{P}_{j,N}(n-2j-1+N)\r\n  -\\mathscr{P}_{j,N}(n-2j+N) +\\mathscr{P}_{j,N}(n) \\\\\r\n = \\mathscr{P}_{j-1,N}(n-4j-1) -\\mathscr{P}_{j-1,N}(n-2j-1)\r\n  -\\mathscr{P}_{j-1,N}(n-2j) +\\mathscr{P}_{j-1,N}(n)\r\n\\end{multline}\r\nwhich is less tractable than the recurrence on $N$ (\\ref{eq:recOPM_overN}). \r\nA better option is to allow $j$ to vary by 1/2 instead of 1 and to deal with \r\n$\\mathscr{F}(j,N;z)$ with \\textit{integer} $j$ as intermediate calculation \r\nvalues without physical meaning. From\r\n\\begin{equation}\r\n\\mathscr{F}(j,N;z)(z^{2j+1-N}-1)=\\mathscr{F}(j-1/2,N;z)(z^{2j+1}-1)\r\n\\end{equation}\r\none gets after multiple derivation in $z=0$\r\n\\begin{multline}\r\n \\sum_{s} \\binom{n}{s} s!P(s-N(2j+1-N)/2;j,N)\r\n  \\left((2j+1-N)!\\delta_{2j+1-N,n-s}-\\delta_{0,n-s}\\right)\\\\\r\n = \\sum_{t} \\binom{n}{t} t!P(t-N(2j-N)/2;j-1/2,N)\r\n  \\left((2j+1)!\\delta_{2j+1,n-t}-\\delta_{0,n-t}\\right)\r\n\\end{multline}\r\nfrom which, using the above notation (\\ref{eq:Pscr_def})\r\n\\begin{equation}\\label{eq:recOPM_overj}\r\n \\mathscr{P}_{j,N}(n) = \\mathscr{P}_{j-1/2,N}(n) \r\n -\\mathscr{P}_{j-1/2,N}(n-2j-1) +\\mathscr{P}_{j,N}(n-2j-1+N).\r\n\\end{equation}\r\n\r\nIn practical cases, if one has to compute the distribution $P(M)$ for a very \r\nlarge $j$ and moderate $N$ the recurrence on $N$ (\\ref{eq:recOPM_overN}) will \r\nbe faster. In the opposite situation the recurrence on $j$ \r\n(\\ref{eq:recOPM_overj}) will perform better. \r\n\r\nThese properties are interesting alternatives to the method previously \r\nproposed by Gilleron and Pain \\cite{GILLERON09}. To this respect we may \r\nestimate the number of operations needed to obtain the whole set of \r\n$P(M)$ values in a $j^N$ relativistic subshell. The brute force technique \r\nconsists in evaluating all the \r\n\\begin{equation}\\label{eq:nop_bf}N_\\text{bf}=\\binom{2j+1}{N}\\end{equation}\r\nn-tuple elements and compute the sum $\\sum_{i=1}^N m_i$ for each of them. \r\nThe much better alternative provided by the recurrence method by Gilleron \r\nand Pain \\cite{GILLERON09} amounts to perform roughly\r\n\\begin{equation}\\label{eq:nop_recGP}\r\nN_\\text{GP}=N(2j+1)\\big(N(2j+1-N)+1\\big)\r\n\\end{equation}\r\noperations. As a third option, the recurrence over $N$ (\\ref{eq:recOPM_overN}) \r\nwill be initialized by the $N=0$ value and then applied for every \r\n$-J_\\text{max}(j,\\nu)\\le M\\le J_\\text{max}(j,\\nu)$ for $1\\le \\nu\\le N$ with \r\n$J_\\text{max}(j,\\nu)=\\nu(2j+1-\\nu)/2$. Since the formula expresses $P(M)$ as a\r\nfunction of 3 other $P$s, the number of required operations is \r\n\\begin{equation}\\label{eq:nop_recN}\r\nN_\\text{recN}=3\\sum_{\\nu=1}^N \\left(\\nu(2j+1-\\nu)+1\\right)\r\n = N\\left(3j(N+1)-N^2+4\\right).\r\n\\end{equation}\r\nThis is even an overestimate since in some cases due to selection rules the \r\nrecurrence formula involves less than 3 terms in its second member. Moreover \r\nthe symmetry property $P(-M)=P(M)$ is not used.\r\nAccordingly the recurrence (\\ref{eq:recOPM_overj}) will be used initialized \r\nwith the minimum value $j_0=(N-1)/2$. If $i$ represents twice the iterated \r\nangular momentum, ranging from $N$ to $2j$, the number of operations will be \r\n\\begin{equation}\\label{eq:nop_recj}\r\nN_\\text{recj}=3\\sum_{i=N}^{2j} \\left(N(i+1-N)+1\\right)\r\n = \\frac{3}{2}(2j+1-N)\\left(N(2j+1-N)+N+2\\right).\r\n\\end{equation}\r\nSome examples for the numbers (\\ref{eq:nop_bf},\\ref{eq:nop_recGP}\n\\ref{eq:nop_recN},\\ref{eq:nop_recj}) are given in Table \r\n\\ref{tab:number_opn_recur}, in the case of an half-filled subshell which \r\nleads to the maximum complexity. It may be noted that the recurrence on $j$ \r\n(\\ref{eq:recOPM_overj}), though using ``unphysical'' quantities, is sometimes \r\nmore efficient than the recurrence on $N$.\r\n\\begin{table}[htbp]\r\n\\renewcommand{\\arraystretch}{1.5}\r\n\\begin{tabular}{c *8{>{\\raggedleft\\arraybackslash}m{1.5cm}}\n\\hline\\hline\r\n$j$           & 1/2 & 3/2 & 7/2 & 11/2 & 15/2 & 19/2 & 23/2 & 27/2\\\\\r\n\\hline\r\n$N_\\text{bf}$   & 2 &  6 &  70 & 924 & 12870 & 184756 & 2704156 & 40116600 \\\\\r\n$N_\\text{GP}$   & 4 & 40 & 544 &2664 &  8320 &  20200 &   41760 &    77224 \\\\\r\n$N_\\text{recN}$ & 6 & 27 & 162 & 501 &  1140 &   2175 &    3702 &     5817 \\\\\r\n$N_\\text{recj}$ & 6 & 24 & 132 & 396 &   888 &   1680 &    2844 &     4452 \\\\\r\n\\hline\\hline\r\n\\end{tabular}\r\n\\caption{Number of operations needed to obtain the $P(M)$ distribution \r\nfor the $j^N$ configuration with $N=j+1/2$, using a brute-force technique \r\nor recurrence relations. Numbers are given according to formulas \r\n(\\ref{eq:nop_bf},\\ref{eq:nop_recGP},\\ref{eq:nop_recN},\\ref{eq:nop_recj}).\r\n\\label{tab:number_opn_recur}}\r\n\\end{table}\r\n\r\n\r\n\\section{Determination of the cumulants and moments}\\label{sec:det_cum}\r\n\\subsection{Analytical form of the cumulants}\r\nAccording to the definitions (\\ref{eq:defeKt}) and the normalization \r\n(\\ref{eq:norm}) the cumulant generating function is\r\n\\begin{subequations}\\begin{align}\\label{eq:defKt}\r\nK(t)&=\\log\\left(\\sum_M P(M)e^{Mt}\\left/\\binom{2j+1}{N}\\right.\\right)\\\\\r\n &= \\sum_{p=1}^N \\Big[\\log(\\sinh((2j+2-p)t/2) -\\log(2j+2-p) \r\n  -\\log(\\sinh(pt/2)) +\\log p\\,\\Big].\r\n\\end{align}\\end{subequations}\r\nFrom the expansion\r\n\\begin{equation}\\label{eq:devlogsh}\r\n\\log(\\sinh x) = \\log x \r\n +\\sum_{n=1}^\\infty \\frac{B_{2n}}{2n}\\frac{(2x)^{2n}}{(2n)!}\r\n\\end{equation}\r\nwhere $B_j$ are the Bernoulli numbers \\cite{Abramowitz1972}, one gets the \r\nseries expansion for the cumulant generating function \r\n\\begin{equation}\r\nK(t)=\\sum_{k=1}^\\infty \\frac{B_{2k}}{2k}\\left[\\sum_{p=1}^{N}(2j+2-p)^{2k} \r\n - \\sum_{p=1}^N p^{2k} \\right] \\frac{t^{2k}}{(2k)!}.\\label{eq:exprcumulgen}\r\n\\end{equation}\r\nThis expansion allows us to obtain the cumulants $\\kappa_n$ defined by \r\n\\cite{Stuart1994}\r\n\\begin{equation}\r\nK(t)=\\sum_{n=1}^\\infty \\kappa_n\\frac{t^n}{n!}\r\n\\end{equation}\r\nwhere $\\kappa_1$ is the distribution average, $\\kappa_2$ the variance, \r\n$\\kappa_3$ the asymmetry, $\\kappa_4$ the excess kurtosis, etc. Identifying \r\nthis expansion with the analytical form (\\ref{eq:exprcumulgen}) one directly \r\nobtains the even-order cumulants of the $M$ distribution\r\n\\begin{equation}\r\n\\kappa_{2k}=\\frac{B_{2k}}{2k} \\left[\\sum_{p=1}^{N}(2j+2-p)^{2k} \r\n - \\sum_{p=1}^N p^{2k} \\right]\\label{eq:cumul}\r\n\\end{equation}\r\nwhile of course odd-order cumulants vanish. This expression may be rewritten \r\n\\begin{equation}\r\n\\kappa_{2n}(j^N)=\\frac{B_{2n}}{2n}\r\n \\left[\\sum_{p=0}^{2j+1}p^{2n}-\\sum_{p=0}^{N}p^{2n}-\\sum_{p=0}^{2j+1-N} p^{2n}\\right]\r\n\\end{equation}\r\nwhich makes more obvious the invariance of the cumulant under the \r\ntransformation $N\\rightarrow 2j+1-N$. Using the relation\r\n\\begin{equation}\r\n\\sum_{k=0}^{n-1}k^m=\\frac{B_{m+1}(n)-B_{m+1}(0)}{m+1},\r\n\\end{equation}\r\nwhere $B_n(x)$ is the $n$-th Bernoulli polynomial \\cite{Abramowitz1972}, one gets \r\n\\begin{equation}\r\n\\kappa_{2n}(j^N)=\\frac{B_{2n}}{2n}\r\n \\frac{\\left[B_{2n+1}(2j+2)-B_{2n+1}(N)-B_{2n+1}(2j+2-N)\\right]}{2n+1}.\r\n\\end{equation}\r\n\r\n\\subsection{Explicit expressions for the first cumulants}\\label{sec:expli_cum}\r\nA careful analysis of the formula (\\ref{eq:cumul}) shows that the \r\ncumulant at order $2k$ may be expressed as a polynomial of order $2k$ in $N$. \r\nFurthermore, because of the symmetry $P(M)=P(2j+1-N)$, one knows that changing \r\n$N\\rightarrow 2j+1-N$ the cumulant must be invariant. Therefore this cumulant \r\nmust be a polynomial of order $k$ in the variable $N(2j+1-N)$. One defines\r\n\\begin{equation}\\label{eq:kap2kwithC}\r\n\\kappa_{2k}=\\sum_{p=1}^{k}C(2k,p)\\left[N(2j+1-N)\\right]^p.\r\n\\end{equation}\r\nThe values for $C(2k,p)$ have been computed for $k$ up to 6 with Mathematica \r\nsoftware using the explicit form (\\ref{eq:cumul}). One gets\r\n\\begingroup\r\n\\allowdisplaybreaks\r\n\\begin{subequations}\\begin{align}\r\nC(2,1)&=\\frac{j+1}{6}\\label{eq:variance}\\\\\r\nC(4,1)&=-\\frac{1}{30}(j+1)^2(2j+1)\\\\\r\nC(4,2)&=\\frac{j+1}{60}\\\\\r\nC(6,1)&=\\frac{1}{126}(j+1)^2(2j+1)\\left(8j^2+12j+3\\right)\\\\\r\nC(6,2)&=-\\frac{1}{252}(j+1)(2j+1)(8j+9)\\\\\r\nC(6,3)&=\\frac{j+1}{126}\\\\\r\nC(8,1)&=-\\frac{1}{90}(j+1)^2(2j+1)\\left(24j^4+72j^3+70j^2+24j+3\\right)\\\\\r\nC(8,2)&=\\frac{1}{180}(j+1)(2j+1)\\left(36j^3+96j^2+76j+15\\right)\\\\\r\nC(8,3)&=-\\frac{1}{90}(j+1)(2j+1)(5j+6)\\\\\r\nC(8,4)&=\\frac{j+1}{120}\\\\\r\nC(10,1)&=\\frac{1}{66}(j+1)^2(2j+1)\\left(4j^2+6j+1\\right)\r\n \\left(32j^4+96j^3+92j^2+30j+5\\right)\\\\\r\nC(10,2)&=-\\frac{1}{132}(j+1)(2j+1)\\left(256j^5+1072j^4+1648j^3+1112j^2+312j+35\\right)\\\\\r\nC(10,3)&=\\frac{1}{66}(j+1)(2j+1)\\left(56j^3+156j^2+128j+25\\right)\\\\\r\nC(10,4)&=-\\frac{5}{132}(j+1)(2j+1)(4j+5)\\\\\r\nC(10,5)&=\\frac{j+1}{66}\\\\\r\nC(12,1)&=-\\frac{691}{8190}(j+1)^2(2j+1)\\nonumber\\\\ &\\quad\\times\r\n \\left(256j^8+1536j^7+3712j^6+4608j^5+3160j^4+1272j^3+338j^2+48j+3\\right)\\\\\r\nC(12,2)&=\\frac{691}{16380}(j+1)(2j+1)\\nonumber\\\\ &\\quad\\times\r\n \\left(640j^7+3648j^6+8288j^5+9488j^4+5756j^3+1872j^2+356j+27\\right)\\\\\r\nC(12,3)&=-\\frac{691}{8190}(j+1)(2j+1)\\left(192j^5+832j^4+1316j^3+900j^2+247j+28\\right)\\\\\r\nC(12,4)&=\\frac{691}{32760}(j+1)(2j+1)\\left(224j^3+644j^2+542j+105\\right)\\\\\r\nC(12,5)&=-\\frac{691}{8190}(j+1)(2j+1)(7j+9)\\\\\r\nC(12,6)&=\\frac{691}{16380}(j+1)\r\n\\end{align}\\end{subequations}\r\n\\endgroup\r\n\r\n\\subsection{Computation of the distribution moments}\r\nFrom these expressions one may also derive the even-order moments, i.e., the \r\naverage values inside a relativistic subshell\r\n\\begin{equation}\\label{eq:defmt}\\mu_{2k}=\\sum_M M^{2k}P(M)/\\sum_M P(M).\r\n\\end{equation}\r\nThe relation between moments and cumulants, found in textbooks about \r\nstatistics \\cite{Stuart1994}, may be written as\r\n\\begin{equation}\\label{eq:momts_cumults}\r\n\\mu_n = \\kappa_n + \\sum_{m=1}^{n-1}\\binom{n-1}{m-1}\\kappa_m\\mu_{n-m}.\r\n\\end{equation}\r\nThe expressions for the moments $\\mu_{2k}$ are given in the appendix \r\n\\ref{sec:appmomts} for $k$ up to 6.\r\n\r\n\r\n\\section{Another recurrence relation on the generating function}\\label{sec:rec_gen}\r\nAnother relation between $s_N$ and values for lower $N$ but the same $j$ may \r\nbe obtained considering the explicit sum definition with $m_k$ indices. \r\nDefining\r\n\\begin{equation}S_N(t)=N!s(N,j,t)\\end{equation}\r\none has\r\n\\begin{subequations}\\begin{align}\r\nS_N(t) &= N!\\sum_{m_1<m_2<\\cdots<m_N}e^{(m_1+\\cdots+m_N)t} = \r\n \\sum_{\\stackrel{m_1\\cdots m_N}{\\text{all }\\ne}} e^{(m_1+\\cdots+m_N)t}\\\\\r\n &= \\sum_{\\stackrel{m_1\\cdots m_{N-1}}{\\text{all }\\ne}} \r\n  e^{(m_1+\\cdots+m_{N-1})t}\\sum_{m_N}e^{m_Nt}\r\n  -(N-1)\\sum_{\\stackrel{m_1\\cdots m_{N-1}}{\\text{all }\\ne}}\r\n   e^{(m_1+\\cdots+m_{N-2}+2m_{N-1})t}\\\\\r\n &= S_{N-1}(t)S_1(t) \r\n  -(N-1)\\sum_{\\stackrel{m_1\\cdots m_{N-1}}{\\text{all }\\ne}}\r\n   e^{(m_1+\\cdots+m_{N-2}+2m_{N-1})t}\r\n\\end{align}\r\nand repeating the process for the sum over $m_1\\cdots m_{N-1}$\r\n\\begin{align}\r\nS_N(t) &= S_{N-1}(t)S_1(t) \r\n -(N-1)\\sum_{\\stackrel{m_1\\cdots m_{N-2}}{\\text{all }\\ne}}\r\n  e^{(m_1+\\cdots+m_{N-2})t}S_1(2t) \\nonumber\\\\\r\n &\\quad+(N-1)(N-2)\\sum_{\\stackrel{m_1\\cdots m_{N-2}}{\\text{all }\\ne}}\r\n  e^{(m_1+\\cdots+m_{N-3}+3m_{N-2})t}\\\\\r\n &= S_{N-1}(t)S_1(t)-(N-1)S_{N-2}(t)S_1(2t)+(N-1)(N-2)S_{N-3}(t)S_1(3t)\\nonumber\\\\\r\n  &\\quad -(N-1)(N-2)(N-3)\r\n \\sum_{\\stackrel{m_1\\cdots m_{N-3}}{\\text{all }\\ne}}\r\n e^{(m_1+\\cdots+m_{N-4}+4m_{N-3})t}.\r\n\\end{align}\\end{subequations}\r\nOne verifies that the $k$-th term in the expansion is $(-1)^{k-1}(N-1)\r\n\\cdots(N-k+1)S_{N-k}(t)S_1(kt)$. The recurrence is closed by studying the \r\nlast two-index sum for $k=N-1$. One has\r\n\\begin{equation\n (-1)^{N-2}(N-1)\\cdots2\\sum_{\\stackrel{m_1,m_2}{m_1\\ne m_2}}e^{\\Big(m_1+(N-1)m_2\\Big)t}\r\n  = (-1)^{N-2}\\frac{(N-1)!}{1!}\\left[S_1(t)S_1((N-1)t)-S_1(Nt)\\right]\n \\end{equation}\r\nWe have thus proven the general formula\r\n\\begin{equation}\r\nS_N(t)=S_{N-1}(t)S_1(t)+\\sum_{p=2}^{N-1}(-1)^{p-1}\\frac{(N-1)!}{(N-p)!}S_{N-p}(t)S_1(pt)\r\n +(-1)^{N-1}\\frac{(N-1)!}{0!}S_1(Nt).\\label{eq:devSN}\r\n\\end{equation}\r\nThis equation may be simplified using the initial value (\\ref{eq:s0}) which allows us \r\nto write the above sum as \r\n\\begin{equation}\\label{eq:recSN}\r\n s(N,j,t)=\\frac1N \\sum_{p=1}^{N}(-1)^{p-1}s(N-p,j,t)s(1,j,pt).\r\n\\end{equation}\r\nFrom this expression one obtains a recurrence relation on the distribution moments, \r\nas shown in Appendix \\ref{sec:rec_mom}.\r\n\r\n\r\n\\section{Gram-Charlier series}\\label{sec:gram}\r\n\\subsection{General formulas}\r\nAn interesting property of distributions for which the moments or the cumulants \r\nare known up to a certain order is that they can be approximated by analytical \r\nforms. The magnetic quantum number distribution in any relativistic configuration \r\nmay be approximated by a Gram-Charlier expansion defined as (see Sec. 6.17 in \r\nRef.\\cite{Stuart1994})\r\n\\begin{equation}\\label{eq:Gram-Charlier}\r\nF_\\text{GC}(M) = \\frac{G}{(2\\pi)^{1/2}\\sigma}\r\n \\exp\\left[-\\frac{(M-\\left<M\\right>)^2}{2\\sigma^2}\\right] \\left[1+\\sum_{k\\ge 3}\r\n c_k He_k\\left(\\frac{M-\\left<M\\right>}{\\sigma}\\right)\\right]\r\n\\end{equation}\r\nin which $M$ is allowed to vary continuously, while the mean value $\\left<M\\right>$ \r\nvanishes for symmetry reasons. In the above equation, $He_n$ is the \r\nChebyshev-Hermite polynomial \\cite{Stuart1994} \r\n\\begin{equation}\\label{eq:Hermite_stat}He_k(X) = \r\nk!\\sum_{m=0}^{\\lfloor k/2\\rfloor}\\frac{(-1)^m X^{k-2m}}{2^m m! (k-2m)!}\r\n\\end{equation}\r\nand $\\lfloor x\\rfloor$ is the integer part of $x$. The Gram-Charlier \r\ncoefficients $c_k$ are related to the moments $\\mu_k$ --- which \r\nare here \\textit{centred}, i.e, $\\mu_1=0$ --- through the relation\r\n\\begin{equation}\r\nc_k=\\sum_{j=0}^{\\lfloor k/2\\rfloor}\\frac{(-1)^j\r\n \\mu_{k-2j}/\\sigma^{k-2j}}{2^j j!(k-2j)!}\\label{eq:cGC_vs_momts}\r\n\\end{equation}\r\nand from this definition the coefficients $c_1$ and $c_2$ cancel. It is \r\ninteresting to note that Ginocchio and Yen have used a very similar approach \r\nto model the state density in nuclei \\cite{GINOCCHIO1975}. However in the \r\ncase they considered, the asymmetry term $c_3$ was present and the expansion \r\nwas truncated after the fourth term (excess kurtosis). \r\n\r\nFor a symmetric distribution considered here, all the odd-order terms \r\n$c_{2k+1}$ vanish. The coefficient $G$ in Eq. (\\ref{eq:Gram-Charlier}) \r\nis given by the normalization condition\r\n\\begin{equation}\r\nG = \\int_{-\\infty}^{\\infty}\\!\\!dM\\; F_\\text{GC}(M) =\r\n \\prod_s\\binom{2j_s+1}{N_s},\r\n\\end{equation}\r\nthe average value is 0, and the variance is derived from (\\ref{eq:variance})\r\n\\begin{equation}\r\n\\sigma^2=\\frac16\\sum_s (j_s+1)N_s(2j_s+1-N_s).\r\n\\end{equation}\r\nAs shown in Appendix of our previous paper \\cite{Pain2020}, one may also \r\nexpress the Gram-Charlier coefficients as a function of the cumulants. For \r\ninstance owing to the parity of $P(M)$, one has $c_4=\\kappa_4/(4!\\sigma^4)$, \r\n$c_6=\\kappa_6/(6!\\sigma^6)$, $c_8=\\Big(\\kappa_4/(4!\\sigma^4)\\Big)^2/2+\r\n\\kappa_8/(8!\\sigma^8)$, etc. Since the cumulants $\\kappa_{2k}$ are easily \r\nobtained from their analytical expression for any relativistic configuration\r\nthis might look as the preferred method. However in order to get $c_{2k}$ this \r\nprocedure requires to build the various partitions of the integer $k$, \r\nwhich becomes tedious when $k$ is large. Therefore we have used the relation \r\n(\\ref{eq:cGC_vs_momts}), the moments at any order being given by formula \r\n(\\ref{eq:momts_cumults}).\r\n\r\nThe Gram-Charlier expansion truncated for various $k_\\text{max}$ has been \r\ncomputed and compared to exact values for the $P(M)$ distribution. \r\nThe exact values were obtained exactly from the recursive procedure described \r\nby Gilleron and Pain \\cite{GILLERON09} or from the above recurrence relations \r\non $N$ (\\ref{eq:recOPM_overN}) or $j$ (\\ref{eq:recOPM_overj}). In the following \r\nsubsections, ``GC 1 term'' will refer to the value of this series for \r\n$k_\\text{max}=2$, i.e., the plain Gaussian form, ``GC 2 terms'' is the series \r\ntruncated at $k_\\text{max}=4$, i.e., involving the excess kurtosis, etc.\r\n\r\n\\subsection{Numerical accuracy and convergence considerations}\r\nThe accuracy of the Gram-Charlier series (\\ref{eq:Gram-Charlier}) is evaluated \r\nby truncating the series at some maximum $k$. Let us define\r\n\\begin{equation}\r\nP_\\text{GC}(M;k_\\text{max})=\\frac{G}{(2\\pi)^{1/2}\\sigma}\r\n \\exp\\left(-M^2/2\\sigma^2\\right) \\left(1+\\sum_{k\\le k_\\text{max}}\r\n c_k He_k\\left(M/\\sigma\\right)\\right).\r\n\\end{equation}\r\nWe define the global absolute error as\r\n\\begin{equation}\\label{eq:abserr}\r\n\\Delta_\\text{abs}(k)= \\left[\\sum_{M=-J_\\text{max}}^{J_\\text{max}}\r\n \\Big(P_\\text{GC}(M;k)-P(M)\\Big)^2/(2J_\\text{max}+1)\\right]^{1/2}\r\n\\end{equation}\r\nand the global relative error \r\n\\begin{equation}\\label{eq:relerr}\r\n\\Delta_\\text{rel}(k)= \\left[\\sum_{M=-J_\\text{max}}^{J_\\text{max}}\r\n \\Big(P_\\text{GC}(M;k)/P(M)-1\\Big)^2/(2J_\\text{max}+1)\\right]^{1/2}.\r\n\\end{equation}\r\nWe have computed Gram-Charlier series in a wide range of cases using first a \r\nfully numerical approach with high floating-point accuracy (Fortran with \r\n16-byte real numbers, i.e., about 32-digit accuracy), then using formal \r\ncalculation through Mathematica software working with arbitrary precision \r\n--- the $c_k$ coefficients are indeed rational fractions which can be \r\nmanipulated ``exactly'', the only numerical conversion being done when \r\nthe non-rational exponential and the normalization factors in \r\nEq.~(\\ref{eq:Gram-Charlier}) are computed.\r\nWe observed that these two approaches provide very different results \r\nwhen high order terms are computed. Indeed, while the moments $\\mu_{2k}$ \r\nare all positive, the coefficients (\\ref{eq:cGC_vs_momts}) of this \r\nseries involve a sum with alternating signs. The definition of $c_k$ \r\nas a function of the cumulants \\cite{Pain2020} only involves positive \r\ncoefficients, but the cumulants themselves are of alternate signs. We \r\ncould numerically check that when considering very large $k$ the series \r\n$c_k$ indeed tends to 0 but the Fortran computation provides values larger \r\nby order of magnitudes than the Mathematica computation. This numerical \r\ndivergence may appear for $k$ not greater than 50. For instance in the \r\ncase illustrated by Fig. \\ref{fig:GC_j7N4_distribM_err}, we noticed \r\na very strong divergence of the 16-byte computation for $k\\simeq40$. \r\nAs a consequence, when numerical instabilities were observed, we have \r\nmonitored the computation accuracy by comparing to arbitrary precision \r\nresults. This leads us to realize that some of the ``divergences'' observed \r\nin our previous work \\cite{Pain2020} were of numerical nature. However one \r\nmust keep in mind that due to the strong compensation occurring in the \r\nGram-Charlier coefficient computation any numerical approach will encounter \r\nthis loss of accuracy when high enough orders are reached. General \r\nconsiderations about the Gram-Charlier series convergence will be provided at \r\nthe end of this Section. \r\n\r\n\\subsection{Example of small \\textit{j} and small \\textit{N}}\r\nAs a first example we compare on Fig.~\\ref{fig:PM_GC_j7N4} the exact $P(M)$ \r\ndistribution and its Gram-Charlier approximation in the case $j=7/2$ and \r\n$N=4$, for which $J_\\text{max}=8$. \r\nConfigurations with similar $j$ and $N$ are quite common in plasma \r\nspectroscopy, for instance in the context of source design for \r\nnanolithography \\cite{SHEVELKO1998}. In the $j=7/2$ and $N=4$ case, the \r\ndistribution $P(M)$ exhibits plateaus at $P=1$, 5 and 7, and can hardly be \r\ndescribed by a Gaussian form with great accuracy. Nevertheless except for \r\n$M=\\pm8$ the relative accuracy is about 10\\%. One notes on this figure that \r\nincluding as many as 32 terms in the Gram-Charlier series does not \r\nsignificantly improves the agreement. To get a more quantitative description \r\nwe have plotted in Fig.~\\ref{fig:error_GC_j7N4} various accuracy estimates \r\nfor the Gram-Charlier expansion. The absolute error defined by \r\nEq. (\\ref{eq:abserr}) and the relative error from Eq. (\\ref{eq:relerr}) \r\nare plotted as a function of the half truncation index $k/2$. On this figure \r\nwe have also plotted the errors $P_\\text{GC}(M;k)-P(M)$ for $M=0$ and \r\n$M=J_\\text{max}$. One observes that the relative error (\\ref{eq:relerr}) \r\nstays roughly constant at 20\\% for $k<32$.  \r\nOne notes that absolute error is almost constant for large $k$, while \r\nthe error at $M=\\pm J_\\text{max}$ slowly decreases with $k$. It turns out \r\nthat the residual error comes from the $M$values close to 0. As seen from \r\nthe definition (\\ref{eq:relerr}) the relative error is mostly sensitive to \r\nthe $M$ values where $P(M)$ is small, i.e., $|M|\\simeq J_\\text{max}$, and \r\naccordingly this error slowly drops when $k_\\text{max}$ increases.\r\n\r\n\\begin{figure}[htbp]\r\n\\centering\r\n\\subfigure[$M$ distribution: exact and Gram-Charlier approximation. \r\n The Gram-Charlier expansion includes 1, 2, 3, 4, 8, 16 or 32 terms in \r\n the sum (\\ref{eq:Gram-Charlier}).\r\n]{\\label{fig:PM_GC_j7N4\n\\includegraphics[scale=0.3]{Gram-Charlier_j7n4_distrM_math\n\n\\hspace{0.1cm\n\\subfigure[Evaluation of the Gram-Charlier approximation. \r\nThe average absolute and relative errors are defined in main text. \r\nThe error at $M_0=J_\\text{max}$ or $M_0=0$ is the value of the \r\ndifference $P_\\text{GC}(M_0;k_\\text{max})-P(M_0)$ plotted \r\nas a function of the half truncation index $k_\\text{max}/2$.\r\n]{\\label{fig:error_GC_j7N4\n\\includegraphics[scale=0.3]{Gram-Charlier_j7n4_err_math\n\n\\caption{Exact and Gram-Charlier approximation for the magnetic quantum \r\nnumber $P(M)$ in the relativistic configuration $j=7/2$, $N=4$ (e.g., \r\n$4g_{7/2}^4$). The one-term computation is the Gaussian form, the 2-term form \r\nincludes the $c_4$ contribution, i.e., the excess kurtosis, the 3-term form \r\nincludes $c_4$ and $c_6$, etc. \r\n\\label{fig:GC_j7N4_distribM_err}}\r\n\\end{figure}\r\n\r\n\\begin{figure}[htbp]\r\n \\centering\r\n \\subfigure[$M$ distribution]{\\label{fig:PM_GC_j5n2j37\n\\includegraphics[scale=0.3]{Gram-Charlier_j5n2j37_distrM_math\n}\\goodga\n\\subfigure[Accuracy of Gram-Charlier approximation as a function of the \r\n``half-truncation order'' $k/2$. See main text for the definitions.\r\n]{\\label{fig:error_GC_j5n2j37\n\\includegraphics[scale=0.3]{Gram-Charlier_j5n2j37_err_math\n}\r\n \\caption{Exact and Gram-Charlier approximations for the magnetic quantum \r\nnumber $P(M)$ in the relativistic configuration $j_1=5/2$ $N_1=2, j_2=37/2, \r\nN_2=1$. \r\n\\label{fig:GC_j5N2j37_distribM_err}}\r\n\\end{figure}\r\n\r\nAnother interesting example is provided by $P(M)$ distributions exhibiting \r\na wide plateau around $M=0$, which occur in configurations containing both \r\nhigh and low $j$ values. Configurations involving high-$j$ spectators are \r\ncreated for instance by electron capture into high-lying Rydberg states in \r\ncollisions between multiply charged ions and light target gases \r\n\\cite{HVELPLUND1981}. Let us consider the configuration $j_1=5/2, N_1=2, \r\nj_2=37/2, N_2=1$ which is analogous to the case considered in \r\nRef.~\\cite{GILLERON09}.\r\nThe magnetic quantum number distribution for this case is plotted in \r\nFig.~\\ref{fig:PM_GC_j5n2j37}. One notes that the first orders of the \r\nGram-Charlier expansion provide a poor representation of the wide plateau \r\nextending from $M=-29/2$ to $M=29/2$. The quality of this approximation \r\nslowly improves with $k_\\text{max}$, but obtaining a good agreement with \r\nthe exact $P(M)$ distribution requires large $k_\\text{max}$ values.\r\nThe evolution of the accuracy with the truncation index in the Gram-Charlier \r\nseries is quantitatively analyzed on Fig.~\\ref{fig:GC_j5N2j37_distribM_err}. \r\nIt appears \r\nthat all the $P(M)$ values, including those for $M\\simeq0$ and $M\\simeq\r\n\\pm J_\\text{max}$ are correctly described for a cut-off $k_\\text{max} \r\n/2\\simeq40$. The average absolute error is then $0.28$, the average \r\nrelative error is $0.066$, the error at $M=\\pm J_\\text{max}$ is \r\n$0.18$ and the error at $M=\\pm1/2$ is $-0.035$, which means that the \r\nrelative error $|P_\\text{GC}(M;k)/P(M)-1|$ is below 15\\% for any $M$. \r\nAbove this $k$ value, adding more terms slightly improves the accuracy \r\nin the $M\\simeq0$ region, while the larger $|M|$ values are almost \r\ninsensitive to these high-order terms. Though we did not develop a \r\nrigorous mathematical analysis, it appears that the Gram-Charlier series \r\nprovides an asymptotic-type convergence: for a large range of \r\n$k_\\text{max}$ values, the absolute error levels off at 0.28, and for \r\nvery large truncation index, a divergence is expected.\r\n\r\n\\subsection{Example of large \\textit{j} and large \\textit{N}}\r\nOne may note that several works in plasma physics or EBIT spectroscopy \r\ndeal with ions involving almost half-filled d or f subshells \r\n\\cite{ZIGLER1987,RADTKE2001,Jonauskas2012}. Such subshells also deserve \r\nconsideration in plasma sources for nanolithography \\cite{SHEVELKO1998\nOSullivan2015}.\r\nWe have plotted the exact and Gram-Charlier distributions for $P(M)$ in \r\nthe half-filled subshell $N=j+1/2$ with $j=15/2$ on Fig. \r\n\\ref{fig:PM_GC_j15N8}, for which $J_\\text{max}=32$. \r\nOne observes that the Gram-Charlier approximation performs well on the whole \r\n$M$-range. In more detail the simple 1-term form is accurate everywhere except \r\nclose to the $M=0$ region, and the 2-term form, including variance and \r\nkurtosis, provides a fair approximation whatever $M$. In order to get a more \r\nquantitative picture, we have plotted in Fig. \\ref{fig:error_GC_j15N8} the \r\nvarious evaluations of the error done as a function of the half truncation \r\nindex $k/2$. On this figure the errors at $M=\\pm J_\\text{max}$ or $M=0$ \r\nare indeed the absolute differences $|P_\\text{GC}(M;k_\\text{max})-P(M)|$ \r\nto allow for a logarithmic scale, but it is noticeable that, for both $M$ \r\nvalues, the sign of the differences $P_\\text{GC}(M;k_\\text{max})-P(M)$ is \r\npositive for $k_\\text{max}=2$, and negative for higher $k$. \r\nIt turns out that including terms in the Gram-Charlier expansion \r\nbeyond $k=4$ brings little improvement in the analytical representation of \r\n$P(M)$. It is even surprising that the various plotted errors tend to \r\nsome asymptotic value, namely one notes that, for $M=J_\\text{max}$, one has\r\n$P_\\text{GC}(M;k)-P(M)\\rightarrow -0.4$ for large $k$, and for $M=0$ the \r\n``limit'' is $\\sim -2.5$ with some oscillations. Therefore the ``convergence'' \r\nof the Gram-Charlier series is really poor in this case, the two-term \r\nexpansion including up to the excess kurtosis providing a fair approximation \r\nfor such half-filled subshells. This agrees with the conclusions obtained \r\npreviously by Bauche \\textit{et al} \\cite{BAUCHE87}, though the effect of \r\nhigh-order terms was not quantitatively evaluated in this paper. Once again \r\nthis case study suggests that the Gram-Charlier expansion provides an \r\nasymptotic representation of the magnetic momentum distribution.\r\n\\begin{figure}[htbp]\r\n \\centering\r\n \\subfigure[$M$ distribution. For $k_\\text{max}\\ge4$ the plots are almost \r\nindistinguishable at the drawing accuracy.]\r\n{\\label{fig:PM_GC_j15N8\n\\includegraphics[scale=0.3]{Gram-Charlier_j15n8_distrM_math\n}\\goodga\n\\subfigure[Test of Gram-Charlier approximation as a function of the \r\nhalf-truncation order $k_\\text{max}/2$. The green line with square \r\nsymbols (resp. blue line with triangle symbols) is the absolute value  \r\n$|P_\\text{GC}(M;k_\\text{max})-P(M)|$ for $M=\\pm J_\\text{max})$ (resp. \r\n$M=0$).]{\\label{fig:error_GC_j15N8\n\\includegraphics[scale=0.3]{Gram-Charlier_j15n8_err_math\n}\r\n\\caption{Exact and Gram-Charlier approximation for the magnetic quantum \r\nnumber $P(M)$ in the relativistic configuration $j=15/2$, $N=8$.\r\n\\label{fig:GC_j15N8_distribM_err}}\r\n\\end{figure}\r\n\r\n\\subsection{Example of multiple subshells}\r\nIt is not obvious to find situations where configurations with many open \r\nsubshells contribute significantly to plasma spectra. However it is worth \r\nnoting that the case of several singly-populated subshells is connected to \r\nthe numbering of configurations contained in a superconfiguration analyzed \r\nin Ref.~\\cite{Pain2020} because the cumulants are formally identical. \r\nConsequently, as a last illustration for the analysis of the \r\n$P(M)$ distribution we consider here a configuration \r\nwith 10 subshells $j=1/2$--$19/2$, all containing a single electron. \r\nFor this 10-electron configuration one has $J_\\text{max}=50$, \r\nthe degeneracy is $2^{10}.10!=3.7158912\\times10^9$, and the population \r\n$P(M)$ varies on 8 orders of magnitude. We have plotted in \r\nFig.~\\ref{fig:PM_GC_j1j3-j19} the $P(M)$ distribution computed exactly and the \r\ndifferences between Gram-Charlier expansion truncated at various orders and \r\nthe exact value. It turns out that the approximation with one term differs \r\nfrom the exact value, while Gram-Charlier approximation with at least 2 terms \r\nagrees with the exact value at the drawing accuracy. \r\nA more quantitative picture is provided by Figs \\ref{fig:error_GC_j1j3-j19} \r\nand \\ref{fig:errorM0Jmax_GC_j1j3-j19}. The former is a plot of the absolute \r\nand relative errors. The latter is the plot of the absolute difference \r\nbetween Gram-Charlier and exact values $\\left|P_\\text{GC}(M;k)-P(M)\\right|$ \r\nfor various $k$ values at $M=0$ and $M=J_\\text{max}$. The differences \r\n$\\left|P_\\text{GC}(M;k)-P(M)\\right|$ may also be divided by the exact $P(M)$ \r\nwhich are $P(0)=1.27707302\\times10^8$ and $P(J_\\text{max})=1$ respectively. \r\nTherefore from Fig. \\ref{fig:errorM0Jmax_GC_j1j3-j19} it appears that the \r\nrelative accuracy is much better for $M=0$ than for $M=J_\\text{max}$.\r\n\r\nAs seen on these figures, the description of the distribution $P(M)$ by the \r\nGram-Charlier expansion improves continuously with $k_\\text{max}$. With \r\n16-byte floating point accuracy, we noticed a divergence on the absolute and \r\nrelative errors for $k_\\text{max}\\simeq64$, while such behavior disappears \r\nin the present computations using Mathematica software. One notes that using \r\n$k_\\text{max}=5$, the gain in accuracy versus an approximation including \r\nonly the kurtosis ($k_\\text{max}=5$) is significant, which gives a certain \r\ninterest to the present analysis.\r\n\r\n\\begin{figure}[htbp]\r\n\\centering\r\n\\includegraphics[scale=0.50,angle=0]{Gram-Charlier_j1j3-j17j19_DiffPM_math}\r\n\\caption{Difference between Gram-Charlier approximation at various orders and \r\nexact value for the distribution $P(M)$. The relativistic configuration analyzed \r\nconsists of 10 subshells $j=1/2$--$19/2$, all containing a single electron. \r\nThe inset shows the exact distribution. \\label{fig:PM_GC_j1j3-j19}}\r\n\\end{figure}\r\n\\begin{figure}[htbp]\r\n\\centering\r\n\\subfigure[Average error done in using the Gram-Charlier formula for the \r\ndistribution $P(M)$ according to formulas (\\ref{eq:abserr},\\ref{eq:relerr})]\r\n{\\label{fig:error_GC_j1j3-j19\n\\includegraphics[scale=0.3125]{Gram-Charlier_j1j3-j17j19_err_math\n}\\goodga\n\\subfigure[Absolute error done in using the Gram-Charlier formula for the \r\nvalues $P(J_\\text{max})$ and $P(0)$.]{\\label{fig:errorM0Jmax_GC_j1j3-j19\n\\includegraphics[scale=0.3125]{Gram-Charlier_j1j3-j17j19_errM0Jmax_math\n}\r\n\\caption{Analysis of the Gram-Charlier series convergence for the \r\n$P(M)$ distribution in the relativistic configuration with 10 subshells \r\n$j=1/2$--$19/2$, all containing a single electron. \r\n\\label{fig:GC_analysis_j1j3-j19}}\r\n\\end{figure}\r\n\r\n\\subsection{Convergence of the Gram-Charlier series}\r\n\\begin{figure}[htb]\r\n\\centering\r\n\\subfigure[Configuration $j=7/2,N=2$]{\\label{fig:cGCHe_j7n2\n\\includegraphics[scale=0.325,angle=0]{cGCHe_vs_k_j7n2_math\n}\\\\\r\n\\subfigure[Configuration $j=15/2,N=8$]{\\label{fig:cGCHe_j15n8\n\\includegraphics[scale=0.3125,angle=0]{cGCHe_vs_k_j15n8_math\n}\r\n\\goodga\n\\subfigure[Configuration with 5 subshells, $j_i=i-1/2, N_i=i$ ($i=$1--5)]\n\\label{fig:cGCHe_j1j3n2j5n3j7n4j9n5\n\\includegraphics[scale=0.3125,angle=0]{cGCHe_vs_k_j1j3n2j5n3j7n4j9n5_math\n\n\\caption{Dependence of the generic term in the Gram-Charlier expansion \r\n$c_k He_k(M/\\sigma)$ versus $k$ for three relativistic configurations \r\nand for two $M$ values. In the last two cases, the absolute value of this \r\nterm is plotted in order to allow for logarithmic scale.\\label{fig:cGCHe}}\r\n\\end{figure}\r\nWhile we estimate the question of the mathematical convergence of the \r\nGram-Charlier series to be outside the scope of this work, it is useful to \r\ncheck how the generic term of the sum in Eq. (\\ref{eq:Gram-Charlier}) varies \r\nwith $k$. To this respect, we have plotted in Fig.~\\ref{fig:cGCHe} \r\nthe term $c_k He_k(M/\\sigma)$ or its absolute value versus $k$ for the \r\nvalues $M=J_\\text{max}$ and $M=0$ or $M=\\pm1/2$ for three configurations. \r\nOf course these computations were performed with arbitrary precision \r\nsoftware to avoid inaccuracies when computing large-order coefficients. \r\nIn Fig.~\\ref{fig:cGCHe_j7n2} illustrating a 2-electron configuration case we \r\nnotice that the $c_kHe_k$ term oscillates with $k$ and do not decrease in \r\nabsolute value below 0.1. For the more populated configurations shown in \r\nFig.~\\ref{fig:cGCHe_j15n8} (resp. \\ref{fig:cGCHe_j1j3n2j5n3j7n4j9n5}) the \r\ngeneric term of the series also oscillates and decreases to lower values. \r\nOne notices a plateau in the oscillation amplitudes at $10^{-4}$ for $M=0$ \r\n(resp. $10^{-7}$ for $M=1/2$). However, as far as we could check, we did not \r\nobserve a subsequent decrease of this generic term for greater $k$ values. \r\nThese numerical considerations lead us to estimate that the Gram-Charlier \r\nseries is probably not convergent, though accounting for a large number of \r\nterms may significantly improve the quality of this approximation, with \r\nbetter results for configurations with a large number of electrons. This \r\nbehavior is characteristic of an asymptotic expansion. \r\n\r\n\r\n\\section{Edgeworth series}\\label{sec:Edgeworth}\r\n\\subsection{Definition}\r\nAs mentioned by various authors \\cite{Blinnikov1998,deKock2011} some \r\nstatistical distributions are better represented by Edgeworth series than by \r\nGram-Charlier series. The Edgeworth distribution of the variable $X$ is \r\nnaturally expressed in terms of cumulants, and is written as an expansion \r\nversus powers of the standard deviation\r\n\\begin{subequations}\\begin{equation}\r\n E(X) = G\\frac{\\exp(-x^2/2)}{\\sqrt{2\\pi}\\sigma} \r\n  \\left[ 1+\\sum_{s=1}^\\infty\\sigma^s\\sum_{\\{k_m\\}}He_{s+2r}(x)\r\n  \\prod_{m=1}^s\\frac{1}{k_m!}\\left(\\frac{S_{m+2}}{(m+2)!}\\right)^{k_m} \r\n  \\right]\\label{eq:Edgeworth}\r\n\\end{equation}\r\nwhere we have introduced the reduced variable $x=(X-\\left<X\\right>)/\\sigma$ \r\nand the modified cumulants $S_n=\\kappa_n/\\sigma^{2n-2}$. The set of indices \r\n$\\{k_m\\}$ refer to all $s$-tuples verifying \r\n\\begin{gather} \r\nr=k_1+k_2+\\cdots k_s\\\\\r\nk_1+2k_2+\\cdots+sk_s=s,\r\n\\end{gather}\\end{subequations}\r\ni.e., partitions of the integer $s$. Since the analyzed distribution $P(M)$ \r\nis even, this series involves only even $s$ orders. An inspection of the \r\nabove formulas shows that the $s=1$ contribution in the sum is proportional \r\nto the asymmetry $\\kappa_3$ which cancels for the $P(M)$ distribution. \r\nThe $s=2$ term is proportional to the excess kurtosis $\\kappa_4$ and is \r\nidentical to the first correction in the Gram-Charlier series. More \r\ngenerally one can check that the sum of coefficients factoring a polynomial \r\nof given order $He_{k}(x)$ in the expansion (\\ref{eq:Edgeworth}) is equal \r\nto the coefficient of the same order $c_{k}$ in the Gram-Charlier\r\nseries. This property has been used to check the consistency of the \r\ncoefficients in these expansions. It means that the Edgeworth series is \r\nindeed a rearrangement of the Gram-Charlier series, where each individual \r\nterm is recast in various orders of the Edgeworth series. \r\n\r\n\\subsection{A test of Edgeworth accuracy}\r\n\\begin{figure}[htbp]\r\n\\centering\r\n\\includegraphics[scale=0.50,angle=0\n{GC_Edgeworth_j1j3n2j5n3j7n4j9n5_math}\r\n\\caption{Absolute error and relative done in using the Gram-Charlier \r\nand Edgeworth formula for the $P(M)$ distribution. The analyzed configuration \r\nis $j_1=1/2, N_1=1, j_2=3/2, N_2=2, j_3=5/2, N_3=3, j_4=7/2, N_4=4, j_5=9/2, \r\nN_5=5$.\\label{fig:GC_Edgeworth_j1j3j5j7j9}}\r\n\\end{figure}\r\n\r\nThe relative efficiency of Edgeworth and Gram-Charlier series is illustrated \r\nin Fig. \\ref{fig:GC_Edgeworth_j1j3j5j7j9} for the configuration with 5 \r\nhalf-filled subshells $j_1=1/2, N_1=1, j_2=3/2, N_2=2, j_3=5/2, N_3=3, \r\nj_4=7/2, N_4=4, j_5=9/2, N_5=5$, where we have plotted the errors \r\n(\\ref{eq:abserr}) and (\\ref{eq:relerr}) for both series as functions of the \r\nhalf truncation index $k_\\text{max}/2$. For the lowest $k_\\text{max}$ values, \r\nthe Edgeworth series provides a better approximation by a factor of 10. \r\nFor greater values of this index, the Gram-Charlier expansion tends toward an \r\nacceptable approximation, with a relative error below 0.01. Conversely, the \r\nEdgeworth expansion is clearly divergent for $k_\\text{max}\\gtrsim 40$ or 50. \r\n\r\nThis expansion has been studied numerically using high-precision \r\narithmetic in Fortran and arbitrary precision in Mathematica, which allows us \r\nto conclude that the divergence is not an artifact due to loss of accuracy in \r\nnumerical computations but really a mathematical divergence. This behavior is \r\nsimilar to that observed for Gram-Charlier expansion, with two noticeable \r\ndifferences: the ``best-convergence plateau'' is reached earlier in the present \r\ncase, and the onset of the divergence is also earlier and more pronounced here.\r\n\r\n\r\n\\section{Discussion and conclusion}\\label{sec:conclu}\r\nWe have studied in this paper various aspects of the statistics of the \r\ntotal quantum magnetic number distribution $P(M)$ in the most general \r\nrelativistic configuration, accounting for the fermion character of the \r\nelectrons. We have mentioned that atomic configurations considered  \r\nhere may be of importance in several domains such as plasma spectroscopy, \r\nnanolithography-source design or EBIT facilities.\r\nUp to our knowledge, there exists no compact analytical expression for the \r\nquantum magnetic number distribution, which justifies the present effort. \r\nUsing the cumulant generating function formalism we have derived a \r\nrecurrence relation for this function connecting four adjacent $j$ and $N$ \r\nvalues in the case of a single subshell $j^N$. This relation allowed us to \r\nestablish a compact analytical expression for the cumulant generating \r\nfunction, which is straightforwardly generalized to a relativistic \r\nconfiguration with any subshell number. In the case of a single subshell this \r\ngenerating function allows us to express $P(M)$ as a n-th derivative. This \r\nformal property leads to two recurrence relations on $P(M)$ for adjacent $N$ \r\nor $j$, $j$ being allowed to span integer as well as half-integer values. \r\nSuch recurrences prove to be quite efficient in obtaining the whole $P(M)$ \r\ndistribution for complex configurations. We have been able to express the \r\ncumulants of the $P(M)$ distribution at any order for the most general \r\nrelativistic configuration. This allowed us to build a Gram-Charlier \r\napproximation of the magnetic quantum number distribution at any order. \r\nThe Gram-Charlier analysis performed here has provided a variety of results. \r\nFirst, it has been stressed that the handling of series with several tens \r\nof terms requires the use of arbitrary precision, since a ``divergence'' due \r\nto a loss of numerical accuracy may be observed for a $k_\\text{max}\\sim 40$.\r\nFor a subshell with significant population --- e.g., an half-filled subshell \r\nwith large $j$ --- the first two terms of the expansion provide a very good \r\napproximation while adding many more terms do not improve the approximation \r\nat all. Conversely, in the cases with a large number of subshells each one \r\nwith a small population, the quality of the Gram-Charlier expansion improves \r\nas more terms are added. Similar conclusions holds for ``exotic'' \r\nconfigurations for which the $M$-distribution shows a broad plateau, which \r\ncan be fairly reproduced by including several tens of terms.\r\n\r\nIt has been verified that configurations with a large number of electrons \r\nare better represented by Edgeworth expansion for a moderate value of the \r\ntruncation index, reaching a best-approximation plateau before the \r\nGram-Charlier expansion. Furthermore, a better accuracy is achieved for \r\nconfigurations with a large number of electrons. However both expansions \r\nappear to be asymptotic and not convergent, with an earlier divergence for \r\nthe Edgeworth series. Such conclusions are similar to what we obtained when \r\nconsidering the statistics of configurations inside a superconfiguration. \r\nA physically important application of this analysis is that it leads to \r\nuseful information on the distribution of total angular momentum $J$ and on \r\nline numbers. This will be considered in a forthcoming paper. \r\n\r\n\r\n", "meta": {"timestamp": "2021-06-25T02:18:52", "yymm": "2106", "arxiv_id": "2106.13009", "language": "en", "url": "https://arxiv.org/abs/2106.13009"}}
{"text": "\\section{\\label{sec:Intro}Introduction}\n\\noindent In a He gas discharge, two long-lived, excited (``metastable'') atomic levels are formed by electron-impact excitation from the $1^1$S$_0$ electronic ground level: the $2^3$S$_1$ level (electronic energy $E = 19.8\\,$eV \\cite{NIST_ASD}, natural lifetime $\\tau = 7870\\,$s \\cite{Hodgman2009}) and the $2^1$S$_0$ level ($E = 20.6\\,$eV \\cite{NIST_ASD}, $\\tau  = 19.7\\,$ms \\cite{VanDyck1971}). In the following, the metastable He atoms are referred to as spin-polarized, when only a single magnetic sub-level of He($2^3$S$_1$) is populated. Such spin-polarized metastable He (He$^{\\mathrm{SP}}$) is used for a wide range of applications. Special interest is currently devoted to He magnetometry for the quantum sensing of very small magnetic fields, e.g. see Refs.\\ \\cite{Heil2017, Wang2020}. In metastable atom electron spectroscopy \\cite{Harada1997}, also referred to as metastable de-excitation spectroscopy, He$^{\\mathrm{SP}}$ has, for example, been used for probing surface magnetism \\cite{Onellion1984}. In atom optics, He$^{\\mathrm{SP}}$ has found applications in nanolithography, as well as in atomic waveguides and beamsplitters for atom interferometry \\cite{Baldwin2005, Vassen2012}. He$^{\\mathrm{SP}}$ also serves as a source of polarized electrons \\cite{McCusker1969,McCusker1972} and ions \\cite{Schearer1969}, e.g., for atomic and high-energy nuclear scattering experiments. Besides that, spin-polarized samples of $^3$He($2^3$S$_1$) are used for biomedical imaging, e.g., to visualize the human lung \\cite{Walker1997, Kauczor1998, Nikolaou2015}.\n\nSupersonic beams of He$^{\\mathrm{SP}}$ are typically produced by optical pumping \\cite{Happer1972}, as well as by magnetic (de-)focusing and magnetic deflection. Optical pumping of $^4$He($2^3$S$_1$) via the $2^3$P$_{J^{'}}-2^3$S$_1$ transition (where $J^{'} = 0, 1, 2$) at a wavelength of  $\\lambda = 1083\\,$nm has first been achieved by Franken, Colegrove and Schaerer using a helium lamp \\cite{Franken1958, Colegrove1960, Schearer1961}. A few years later, also the optical pumping of the $2^3$S$_1$ level of the $^3$He isotope has been demonstrated using a similar setup \\cite{Phillips1962, Colegrove1963}. The more recent use of narrowband laser radiation has proven to be particularly efficient for the optical pumping of He$^{\\mathrm{SP}}$ \\cite{Giberson1982, Lynn1990, Wallace1995, Granitza1995}.\n\nApart from that, also the interaction of a spin with an inhomogeneous magnetic field has been used for producing spin-polarized atomic beams of $^3$He($2^3$S$_1$) and $^4$He($2^3$S$_1$), respectively. These level-preparation techniques include Stern-Gerlach deflection \\cite{Kato2012,Rubin2004,Zheng2017,Zheng2019,Chen2020,Smiciklas2010} , magnetic hexapole focusing \\cite{Jardine2001, Woestenenk2001, Watanabe2006, Chaustowski2007,Kurahashi2008,Kurahashi2021,Baum1988} and Zeeman deceleration \\cite{Dulitz2015a, Cremers2017b}.\n\n\n\nA comparison between the different techniques for He$^{\\mathrm{SP}}$ preparation in a supersonic beam is of paramount importance for experimental design considerations. In this article, we describe the results of a comparative study aimed at the laser optical pumping of $^4$He($2^3$S$_1$) into a single $M_{J^{''}}$ sub-level (where $M_{J^{''}} = -1, 0, +1$) and at the magnetic hexapole focusing (defocusing) of the $M_{J^{''}} = +1$ ($M_{J^{''}} = -1$) sub-level of $^4$He($2^3$S$_1$) using an array of two magnetic hexapoles. We have determined the efficiency for $M_{J^{''}}$-sub-level selection using low-cost fluorescence and surface-ionization detectors, respectively, which can easily be implemented in other experiments.\n\n\\section{\\label{sec:Setup}Experiments}\n\\noindent Major parts of the experimental setup have already been described elsewhere \\cite{Grzesiak2019,Guan2019}. Briefly, a pulsed $^4$He beam is produced by a supersonic expansion of $^4$He gas from a high-pressure reservoir ($30-40\\,$bar) into the vacuum using a home-built CRUCS valve \\cite{Grzesiak2018} ($30\\,\\mu$s pulse duration). An electron-seeded plate discharge (attached to the front plate of the valve) is used to excite an $\\approx 4\\cdot10^{-5}$ fraction of He atoms from the $1^1$S$_0$ electronic ground level to the two metastable levels, 2$^1$S$_0$ and 2$^3$S$_1$, referred to as He$^*$ hereafter \\cite{Grzesiak2019}.\nAfter passing through an $1\\,$mm-diameter skimmer at a distance of $130\\,$mm from the valve exit, the supersonic beam enters a second vacuum chamber, in which a specific magnetic sub-level of the 2$^3$S$_1$ level is prepared using laser optical pumping or selected using magnetic hexapole focusing (see below). The distance between the skimmer tip and the center of the optical pumping region (hexapole magnets) is $228\\,$mm ($331\\,$mm).\nThe He$^*$ flux and the He$^*$ beam velocity are determined using Faraday cup detection at well-known positions along the supersonic beam axis $y$.\n \nFor the experiments on optical pumping, the pulsed valve is operated at room temperature resulting in a supersonic beam of He$^*$ with a mean longitudinal velocity of $v = (1844 \\pm 6)\\,$m/s ($250\\,$m/s full width at half maximum, FWHM). For the experiments on magnetic hexapole focusing, the pulsed valve is cooled by a cryocooler (CTI Cryogenics, 350CP), and the valve temperature is actively stabilized to $42\\,$K using PID-controlled resistive heating. This results in a supersonic beam of He$^*$ with a mean longitudinal velocity of $v = (830 \\pm 17)\\,$m/s ($\\approx 130\\,$m/s FWHM).\n\n\\subsection{\\label{sec:setup.optPump} Laser optical pumping}\n\\noindent The energy-level schemes and the experimental setup used for laser optical pumping are shown in Figs.\\ \\ref{fig:Setup} (a) and (b), respectively. Optical pumping is achieved by laser excitation via the $2^3$P$_1-2^3$S$_1$ transition or via the $2^3$P$_2-2^3$S$_1$ transition at $\\lambda =$ 1083 nm, respectively. The laser light for optical pumping is generated by a combination of a fiber laser and a fiber amplifier (NKT Photonics, Koheras BOOSTIK Y10 PM FM, $2.2\\,$W output power, $10\\,$kHz line width). The laser frequency is stabilized using frequency-modulated, saturated absorption spectroscopy in a He gas discharge cell. Since the frequency difference between the $2^3$P$_1$ and $2^3$P$_2$ spin-orbit levels is only $\\Delta f \\approx 2\\,$GHz \\cite{NIST_ASD}, the laser frequency can be changed in between the different transitions without effort.\n\nThe laser light is guided to the vacuum chamber using a polarization-maintaining single-mode fiber, where it is collimated to a beam diameter of $2 w_0 \\approx 14\\,$mm ($w_0$ is the Gaussian beam waist). Before the laser beam enters the vacuum chamber, it passes a polarizing beam splitter for polarization clean-up, and a quarter wave plate for the production of circularly polarized light. Inside the vacuum chamber, the laser beam crosses the supersonic beam at right angles and parallel to the direction of the magnetic field produced by two coils in near-Helmholtz configuration (radius $R = 55\\,$cm). The thus produced homogeneous magnetic field of $ B_z \\leq 4.5\\,\\text{G}$\nprovides a uniform quantization axis for all He atoms in the supersonic beam.\n\nThe level-preparation efficiency is determined by measuring the laser induced-fluorescence (LIF) of the He atoms in the direction perpendicular to the supersonic beam and the laser beam. The fluorescence light is collected and focused onto an InGaAs photodiode (Hamamatsu, $1\\,$mm active area diameter, photosensivity of $R_{\\text{PD}} = 0.63\\,$A/W at $\\lambda = 1080\\,\\text{nm}$) using two anti-reflection-coated, aspheric lenses (Thorlabs, $75\\,$mm diameter, $60\\,$mm focal length). Due to the symmetric lens configuration (as shown in Fig.\\ \\ref{fig:Setup} (b)), the fluorescence collection region in the $yz$ plane is expected to be of the same size as the detection region, which is given by the active area of the photodiode. The output of the photodiode is amplified using a home-built transimpedance amplifier with a gain of $G_{\\text{PD}} \\approx 5 \\cdot 10^{5}\\,\\text{V}/\\text{A}$. A rotatable linear polarizer (Thorlabs, extinction ratio $> 400:1$ at $\\lambda = 1083\\,\\text{nm}$) is mounted in between the lenses in order to analyze the polarization of the fluorescence light. All the optical components of the fluorescence detector are placed into a single lens tube system to ensure the correct alignment of the optical parts. The entire detector assembly is positioned on a translational stage outside the vacuum chamber which can be moved along the $y$ axis.\n\nUnder normal operating conditions, the number of He atoms in the $2^3$S$_1$ level is $\\approx 10^9$/pulse, as inferred from the signal on the Faraday-cup detector \\cite{Grzesiak2019}. For excitation via the $2^3$P$_2-2^3$S$_1$ transition, the time-dependent signal of the photodiode has a peak voltage of $U_{\\text{PD,max}} \\approx 41\\,\\text{mV}$ and an FWHM of 27 $\\mu$s. The peak flux of detected photons is then inferred from $U_{\\text{PD,max}}$ using\n\\begin{align}\n\t\\dot{N}_{\\text{ph,max}} = \\frac{U_{\\text{PD,max}} }{h \\nu R_\\text{PD} G_{\\text{PD}}} \\approx 7 \\cdot 10^{11}\\,\\frac{\\text{photons}}{\\text{s}},\n\\end{align}\nwhere $h$ is Planck's constant and $\\nu$ is the corresponding transition frequency. From these measurements, we infer a root-mean-square noise amplitude of $U_{\\text{noise}} = 6.4\\,\\text{mV}$ for a single measurement which improves to $U_{\\text{noise}} = 0.4\\,\\text{mV}$ by averaging over 300 gas pulses. This results in a signal-to-noise ratio of \n\\begin{align}\n\tSNR = \\frac{U_{\\text{PD,max}}^2}{U_{\\text{noise}}^2} =\n\t\\begin{cases}\n\t16.2\\,\\text{dB} \\quad \\text{single measurement}\\\\\n\t40.3\\,\\text{dB} \\quad \\text{300 averages}\n\t\\end{cases}\n\\end{align}\nAt $SNR = 0\\,\\text{dB}$, we thus infer a detection limit of $\\dot{N}_{\\text{ph,lim}} \\approx 1 \\cdot 10^{11}\\,$photons/s ($\\dot{N}_{\\text{ph,lim}} \\approx 7 \\cdot 10^{9}\\,$photons/s) for a single measurement (300 averages).\n\n\n\\begin{figure}[hbt!]\n\n\t\\includegraphics{fig1.pdf}\n\t\\caption{\\label{fig:Setup}\t\n\t(a) Left: He energy levels relevant for the experiments described in the main text. The level energies are taken from Ref.\\ \\cite{NIST_ASD}. Right: Transitions relevant for the laser optical pumping of He($2^3$S$_1$) via the $2^3$P$_0$ (top), the $2^3$P$_1$ (middle) and the $2^3$P$_2$ (bottom) levels, respectively. The relative transition strengths for $\\sigma^+$, $\\pi$ and $\\sigma^-$ excitation are labeled in pink, green and blue color, respectively.\n\t(b) Schematic illustration of the experimental setup used for optical pumping and fluorescence detection. HHC = Helmholtz coil, PBS = polarizing beam splitter, $\\lambda/4$ = quarter wave plate, PD = photodiode, P = polarizer, L = aspheric lens.\n\t(c) Schematic drawing of a magnetic hexapole array in Halbach configuration.\n\t(d) Sketch of the detection system for magnetic hexapole focusing including the two Halbach arrays (HA), the wire detector (W) and a Faraday cup detector (FC). All dimensions in (b)-(d) are given in units of mm, and they are not to scale.}\n\\end{figure}\n\n\\subsection{\\label{sec:setup.magDefl} Magnetic hexapole focusing}\n\\noindent For the magnetic focusing of He(2$^3$S$_1$, $M_{J^{''}} = +1$), we use a set of two Halbach arrays \\cite{Halbach1980, Halbach1981} in hexapole configuration, sketched in Fig.\\ \\ref{fig:Setup} (c), whose design has already been described previously \\cite{Dulitz2014a, Dulitz2016}. Each hexapole array consists of 12 magnetized segments (Arnold Magnetic Technologies, NdFeB, N42SH, remanence of $B_0 = 1.3\\,$T) which are glued into an aluminium housing and placed on a position-adjustable rail at a center-to-center distance of $14.6\\,$mm.\n\nTo determine the focusing properties of the magnet assembly, a thin stainless-steel wire (diameter $d_{\\mathrm{wire}} = 0.2\\,$mm, labelled as ``W'' in Fig.\\ \\ref{fig:Setup} (d)) is used as a position-sensitive Faraday-cup-type detector. Its position along the $y$ and $x$ axes can be varied by a maximum of $180\\,$mm and $50\\,$mm, respectively, using a set of two precision linear translators. A second Faraday-cup detector (labelled as ``FC'' in Fig.\\ \\ref{fig:Setup} (d)), i.e., a stainless-steel plate of $30\\,$mm diameter, is placed behind the wire detector to determine the He$^*$ beam velocity. \n\n\\section{Results and Discussion}\n\\subsection{\\label{sec:optPump} Laser optical pumping}\n\\noindent In general, the level preparation efficiency relies on the polarization state of the laser radiation, on the energy-level structure of the involved electronic levels and on the transition strengths for photon absorption and emission. If atoms are excited with $\\sigma^{+ (-)}$-polarized light, the change in angular momentum between the upper and lower level is $\\Delta M_{J^{''}} = M_{J^{'}}-M_{J^{''}} = +1\\,(-1)$ for every photon-scattering event, where $M_{J^{'}}$ and $M_{J^{''}}$ are the magnetic projection quantum numbers for the upper and the lower magnetic sub-levels, respectively. For excitation with $\\pi$-polarized light, $\\Delta M_{J^{''}} = 0$.\n\nThe transition strengths for the $2^3$P$_{J^{'}}-2^3$S$_1$ transitions (where $J^{'} = 0, 1, 2$) in He are shown in Fig.\\ \\ref{fig:Setup} (a). As can be seen from the figure, all $M_{J^{'}}- M_{J^{''}}$ transitions strengths for the $2^3$P$_2-2^3$S$_1$ transition are non-zero. Hence, in this case, multiple excitation cycles with $\\sigma^{+ (-)}$-polarized light lead to equal populations in the 2$^3$S$_1$, $M_{J^{''}} = +1\\,(-1)$ and 2$^3$P$_2$, $M_{J^{'}} = +2\\,(-2)$ sub-levels, respectively. In this case, photon emission via this transition continues to occur as long as the atoms are subject to laser excitation. In contrast to that, the emission of photons ceases after a few pumping cycles for $\\sigma^{+(-)}$ excitation of the $2^3$P$_1-2^3$S$_1$ transition, since all population is trapped in the 2$^3$S$_1$, $M_{J^{''}} = +1\\,(-1)$ sub-level. Likewise, the excitation of the $2^3$P$_1-2^3$S$_1$ transition using $\\pi$-polarized light leads to a transfer of population into the $2^3$S$_1, M_{J^{''}} = 0$ sub-level and photon emission stops as a result of the zero transition strength for the $2^3$P$_1, M_{J^{'}}=0 - 2^3$S$_1, M_{J^{''}}=0$ transition.\n\nAs can be inferred from Fig.\\ \\ref{fig:Setup} (a), the selective population of a single $M_{J^{''}}$ sub-level in He($2^3$S$_1$) via the $2^3$P$_0-2^3$S$_1$ transition is more complicated, as it requires two different laser polarization states. Therefore, we have focused our experimental work on the optical pumping of He($2^3$S$_1$) via the $2^3$P$_2-2^3$S$_1$ and $2^3$P$_1-2^3$S$_1$ transitions, respectively. In the following, we provide a brief description of the different optical pumping schemes and the methods used for analyzing and optimizing the sub-level preparation efficiencies. Furthermore, we compare our results with literature values.\n\n\\subsubsection{\\label{sec:optPumpAnalysis3P2} Optical pumping via the $2^3$P$_2-2^3$S$_1$ transition}\n\\noindent As stated above, the excitation of the $2^3$P$_2-2^3$S$_1$ transition with $\\sigma^{+ (-)}$-polarized light leads (after a few excitation cycles) to the optical cycling between the $2^3$S$_1$, $M_{J^{''}} = +1\\,(-1)$ sub-level and the 2$^3$P$_2$, $M_{J^{'}} = +2\\,(-2)$ sub-level. In this case, the atomic fluorescence only consists of $\\sigma^{+ (-)}$-polarized light as $\\Delta M = M_{J^{'}} - M_{J^{''}} = +1\\, (-1)$. During the first optical pumping cycles, and for a non-perfect circular polarization of the input light, also sub-levels with $M_{J^{'}} \\neq +2\\,(-2)$ are populated and can decay back to the $2^3$S$_1$ level while emitting $\\sigma^{- (+)}$- and $\\pi$-polarized photons as well. Therefore, the polarization purity of the emitted fluorescence provides information about the sub-level preparation efficiency. Since the polarization of fluorescence photons are given with respect to the quantization axis, which is the pointing of the external magnetic field vector, fluorescence photons of $\\sigma^{+/-}$ polarization that are detected along an axis perpendicular to the quantization axis, are projected onto a linear polarization. The direction of this projected linear polarization is again perpendicular to the quantization axis. Furthermore, fluorescence photons of $\\pi$ polarization are linearly polarized parallel to the quantization axis. Therefore, we analyze the purity of the emitted light using a linear polarizer plate and the fluorescence intensity can be ascribed to $\\sigma^{+/-}$ polarization ($\\pi$ polarization) if the transmission axis of the polarizer $P$ is perpendicular (parallel) to the magnetic field component $B_z$, respectively \\cite{Hubele2015}. For example, the results of fluorescence measurements at different polarizer angles are shown in Fig.\\ \\ref{fig:P2FLmodulation}.\n\nIn order to compare the sub-level preparation efficiencies, we define\n\\begin{equation}\n\\eta_i = \\frac{\\rho(M_{J^{''}_i})}{\\sum_{i}{\\rho(M_{J^{''}_i})}},\n\\end{equation}\nfor producing a specific magnetic sub-level population $\\rho(M_{J^{''}_i})$ of He(2$^3$S$_1$) (where $i = -1, 0, +1$). For the $2^3$P$_2-2^3$S$_1$ transition, the efficiency for optical pumping into the $2^3$S$_1, M_{J^{''}} = +1\\,(-1)$ sub-level is thus obtained using\n\\begin{align}\n\\eta_{+1\\,(- 1)} = 1 - \\frac{I_{\\mathrm{F}}(P \\parallel B_z)}{I_{\\mathrm{F}}(P \\parallel B_z) + I_{\\mathrm{F}}(P \\perp B_z)},\n\\end{align}\nwhere $I_{\\mathrm{F}}(P \\parallel B_z)$ and $I_{\\mathrm{F}}(P \\perp B_z)$ are the fluorescence intensities for emission at polarizer axes $P \\parallel B_z$ and $P \\perp B_z$, respectively.\n\\begin{figure}[hbt!]\n\t\\includegraphics[width=8cm]{fig2.eps\n\n\t\t\\caption{\\label{fig:P2FLmodulation} Fluorescence intensity as a function of polarizer angle for excitation with $\\sigma^+$-polarized light via the $2^3$P$_2-2^3$S$_1$ transition. The experimental data points are shown as red circles. The black curve is a sine fit to the data. The dashed vertical lines represent the two angles at which the transmission axis of the polarizer $P$ is perpendicular or parallel to the magnetic field component $B_z$, respectively. Here, an efficiency of $\\eta_{+1} \\approx 93\\,$\\% is determined from the fit to the data.}\n\\end{figure}\n\n\\subsubsection{\\label{sec:optPumpAnalysis3P1} Optical pumping via the $2^3$S$_1 - 2^3$P$_1$ transition}\n\\noindent Excitation via the $2^3$P$_1-2^3$S$_1$ transition allows for the selective optical pumping into each of the $M_{J^{''}}$ sub-levels in He($2^3$S$_1$). When the atoms are excited with pure $\\sigma^{+(-)}$-polarized light, all population is pumped into the $2^3$S$_1, M_{J^{''}} = +1\\,(-1)$ sub-level. Since this is a dark sub-level, fluorescence emission should only occur in the first few pumping cycles. However, by using a mixture of $\\sigma^{+}$- and $\\sigma^{-}$-polarized excitation light, the dark sub-level is remixed, so that optical cycling (and thus fluorescence emission) continues to occur. In the present configuration, the input polarization is changed by varying the angle $\\Phi$ of the quarter wave plate compared to the axis of the incident linear laser polarization. The observed change of the fluorescence intensity as a function of quarter wave plate angle is shown in Fig.\\ \\ref{fig:P1FLmodulation}. The efficiency for pumping into the $2^3$S$_1, M_{J^{''}} = +1\\,(-1)$ sub-level is determined using\n\\begin{align}\n\\eta_{+1\\,(- 1)} = 1 - \\frac{I_{\\text{F}}(\\sigma^{+(-)})}{I_{\\text{F}}(\\sigma^{+} + \\sigma^{-})},\n\\end{align}\nwhere $I_{\\mathrm{F}}(\\sigma^{+(-)})$ and $I_{\\mathrm{F}}(\\sigma^{+}+\\sigma^{-})$ are the fluorescence intensities for excitation with pure $\\sigma^{+(-)}$ polarization and with a mixture of $\\sigma^{+}$ and $\\sigma^{-}$ polarization, respectively.\n\\begin{figure}[hbt!]\n\t\\includegraphics[width=8cm]{fig3.eps\n\n\t\\caption{\\label{fig:P1FLmodulation} Red circles: Measured fluorescence intensity for excitation via the $2^3$P$_1-2^3$S$_1$ transition as a function of quarter wave plate angle $\\Phi$. The quarter wave plate angles for excitation with pure $\\sigma^+$-polarized light and with an equal mixture of $\\sigma^+$- and $\\sigma^-$-polarized light are indicated as dashed vertical lines. The inset shows the results of a measurement in a region around $\\Phi = 45^\\circ$ taken under different experimental conditions\n\t}\n\\end{figure}\n\nFor optical pumping into the $2^3$S$_1(M_{J^{''}} = 0)$ sub-level, we have used an additional coil pair in near-Helmholtz configuration (radius of $76\\,$mm, distance of $255\\,$mm), placed at right angles to the other Helmholtz-coil pair, to generate a well-defined quantization axis along the $x$ direction. As a result, the laser beam direction is perpendicular to the magnetic field component $B_x$. In addition to that, the quarter wave plate is replaced by a half wave plate. By rotating the half wave plate, the angle of polarization is adjusted to be either parallel or perpendicular to $B_x$. In the latter case, the excitation light is projected onto an equal mixture of $\\sigma^{+}$ and $\\sigma^{-}$ input polarization which again causes a remixing of the otherwise dark sub-level $2^3$S$_1, M_{J^{''}} = 0$. Fig.\\ \\ref{fig:P1M0FLmodulation} shows the change of the fluorescence intensity as a function of half wave plate angle. For pumping into the $2^3$S$_1, M_{J^{''}} = 0$ sub-level, the sub-level preparation efficiency is thus obtained using\n\\begin{align}\n\\eta_{0} = 1 - \\frac{I_{\\text{F}}(\\pi \\parallel B_x)}{I_{\\text{F}}(\\pi \\perp B_x)},\n\\end{align}\nwhere $I_{\\mathrm{F}}(\\pi \\parallel B_x)$ and $I_{\\mathrm{F}}(\\pi \\perp B_x)$ are the fluorescence intensities for excitation using $\\pi$-polarized light in a direction parallel and perpendicular to the magnetic field component $B_x$, respectively.\n\\begin{figure}[hbt!]\n\t\\includegraphics[width=8cm]{fig4.eps\n\n\t\\caption{\\label{fig:P1M0FLmodulation} Red circles: Measured fluorescence intensity for excitation via the $2^3$P$_1-2^3$S$_1$ transition as a function of half wave plate angle $\\Phi$. The half wave plate angles for excitation with $\\pi$-polarized light parallel and perpendicular to the magnetic field direction $B_x$ are indicated as dashed vertical lines.\n\t}\n\\end{figure}\n\n\\subsubsection{\\label{sec:optPumpPosition} Optimization of the sub-level preparation efficiency}\n\\noindent We have identified several parameters which strongly affect the sub-level preparation efficiency: the interaction time between the excitation laser light and the sample, the laser intensity, the magnetic field strength and the purity of the input polarization.\n\nDuring the excitation process, an atom typically scatters several photons before it is pumped to the designated magnetic sub-level. Since the radiative lifetime of the 2$^3$P$_{J^{'}}$ levels in He is long compared to typical optical pumping transitions in other atoms ($\\tau = 1/\\Gamma = 97.89\\,\\text{ns}$ \\cite{NIST_ASD}), a comparably long interaction time between the laser beam and the sample has to be achieved. In our case, we have found that a large 1/$e^2$ laser beam diameter of $2 w_0 \\approx 14\\,\\text{mm}$ is most practical for this purpose. For a supersonic beam with a mean velocity of $1844\\,$m/s, this beam diameter translates into an interaction time of $\\Delta t_{\\mathrm{int}} = 7.6\\,\\mu\\text{s} \\gg \\tau$.\n\nWe have studied the influence of the interaction time on the sub-level preparation efficiency $\\eta_i$ by monitoring the fluorescence intensity at different fluorescence detector positions along the $y$ axis. As can be seen from the colored markers in Fig.\\ \\ref{fig:optPumpPosition}, the efficiency $\\eta_{+1}$ for $\\sigma^+$ excitation of the $2^3$P$_2-2^3$S$_1$ and $2^3$P$_1-2^3$S$_1$ transitions, respectively, increases to a nearly constant value as the detector is moved towards the midpoint of the excitation laser beam. This confirms that, in our experiment, the interaction time does not limit the sub-level preparation efficiency. \n\nWe have simulated the population transfer process using rate-equation calculations. A detailed description of the rate-equation model can be found in App. \\ref{app:ratEqns}. The best fit to our experimental data for excitation via the $2^3$P$_2-2^3$S$_1$ transition and via the $2^3$P$_1-2^3$S$_1$ transition, respectively, is found by assuming that the excitation light is a mixture of $95\\,$\\% $\\sigma^+$- and $5\\,$\\% $\\sigma^-$-polarized light. The admixture of wrongly polarized light also explains why the observed sub-level preparation efficiency is below $100\\,\\%$. In addition to that, as can be seen from Fig.\\ \\ref{fig:Setup} (a), the relative transition strengths for optical pumping with wrongly polarized light is $1/6$ for the $2^3$P$_1-2^3$S$_1$ transition, while it is only $1/30$ for the $2^3$P$_2-2^3$S$_1$ transition. Thus, optical pumping via the $2^3$P$_1-2^3$S$_1$ transition is more sensitive to wrongly polarized excitation light which explains the observed difference in the sub-level preparation efficiency. In our setup, such an admixture of wrong input polarization might be caused by imperfections of the quarter wave plate or by the birefringence of the vacuum window.\n\\begin{figure}[hbt!]\n\t\\includegraphics[width=8cm]{fig5.eps\n\n\t\\caption{\\label{fig:optPumpPosition} sub-level preparation efficiencies $\\eta_{+1}$ for $\\sigma^+$ excitation of the $2^3$P$_2-2^3$S$_1$ and $2^3$P$_1-2^3$S$_1$ transitions (see legend), respectively, at different positions of the fluorescence detector along the $y$ axis. The origin of the position axis denotes the midpoint of the laser beam.  Experimental values are shown as markers, and the results of the rate-equation calculations are shown as solid lines. In the calculations, a mixture of $95\\,$\\% $\\sigma^+$- and $5\\,$\\% $\\sigma^-$-polarized light is assumed for both excitation schemes.}\n\\end{figure}\n\nSecondly, the laser intensity has to be high enough so that the laser-induced power broadening compensates for a detuning of the laser frequency from the atomic resonance. This detuning is caused by the Doppler broadening due to the transverse velocity of the atoms ($\\Delta_{\\text{Doppler}} \\approx 12\\,\\text{MHz}$ FWHM) and by the Zeeman shift of the atomic levels ($\\Delta_{\\mathrm{Zeeman}} < 14\\,\\text{MHz}$). The FWHM of the power broadening can be expressed as\n\\begin{align}\n\\Delta_{\\text{power}} = \\frac{\\Gamma}{2 \\pi} \\cdot \\sqrt{1 + \\frac{I}{I_\\text{sat}}},\n\\end{align}\nwhere $I$ is the intensity of the laser light and $I_{\\text{sat}} \\approx 0.16\\,$mW/cm$^2$ (assuming a two-level system) is the saturation intensity of the transition. Therefore, in order to compensate for the Doppler broadening and for the Zeeman shift, the laser intensity has to be $I \\geq 12 \\,\\frac{\\text{mW}}{\\text{cm}^2}$, corresponding to a laser power of $\\geq 9\\,\\text{mW}$ for our experiments. From Fig.\\ \\ref{fig:optPumpPower}, we can see that the sub-level preparation efficiency for $\\sigma^+$ excitation of the $2^3$P$_2-2^3$S$_1$ transition is constant for laser powers $P > 50\\,\\text{mW}$. Unfortunately, measurements of the sub-level preparation efficiency at lower laser powers suffer from low signal intensities and are thus less representative.\nFor $\\sigma^+$ excitation of the $2^3$P$_1-2^3$S$_1$ transition, we observe that more than 300 mW of laser power are required to reach a constant sub-level preparation efficiency.\nThis power difference might be attributed to a weaker power broadening of the $2^3$P$_1-2^3$S$_1$ line compared to the $2^3$P$_2-2^3$S$_1$ line as a result of a higher saturation intensity for this transition. As both transitions have the same line width, the same initial level and approximately the same transition frequency, we can see from Eq. \\eqref{eq:dmd} that the squared dipole matrix elements $\\left|\\mu_{J^{'}}\\right|^2$ are proportional to the degeneracy factors $2J^{'} + 1$. As $I_{\\text{sat}} \\propto 1/ \\left| \\mu_{J^{'}} \\right|^2$, it follows that $I_{\\text{sat}}(2^3$P$_1-2^3$S$_1) / I_{\\text{sat}}(2^3$P$_2-2^3$S$_1) = \\left|\\mu_2\\right|^2 / \\left|\\mu_1 \\right|^2=  5/3$.\n\\begin{figure}[hbt!]\n\t\\vspace{0.5cm}\n\t\\includegraphics[width=8cm]{fig6.eps\n\n\t\\caption{\\label{fig:optPumpPower} sub-level preparation efficiency $\\eta_{+1}$ for $\\sigma^+$ excitation of the $2^3$P$_2-2^3$S$_1$ and $2^3$P$_1-2^3$S$_1$ transitions (see legend), respectively, at different laser powers. The data are taken at a $6\\,$mm distance downstream from the midpoint of the laser beam in order to represent the efficiencies at equilibrium.}\n\\end{figure}\n\nThirdly, the magnetic bias field has to be large enough to ensure a uniform quantization axis within the optical pumping region so that the contributions of stray fields along other spatial directions is small. \n\nIn Fig.\\ \\ref{fig:optPumpBfiled}, a scan of the sub-level preparation efficiency $\\eta_{+1}$ for $\\sigma^+$ excitation of the $2^3$P$_2-2^3$S$_1$ and $2^3$P$_1-2^3$S$_1$ transitions, respectively, is shown as a function of the magnetic field component $B_z$. The highest efficiency is achieved at field strengths between $2\\,\\mathrm{G} \\leq B_z \\leq 3\\,\\mathrm{G}$ for both transitions. This magnetic field range is in line with previous observations reported in the literature \\cite{Gillot2013,Wallace1995,Schearer1990a}. At magnetic field strengths $B_z > 3\\,\\mathrm{G}$, the sub-level preparation efficiency for excitation via the $2^3$P$_1-2^3$S$_1$ ($2^3$P$_2-2^3$S$_1$) transition is decreased (remains constant) compared to the optimum $B_z$ field range. This is consistent with a decreased scattering rate at higher magnetic fields caused by the increased Zeeman detuning. \n\\begin{figure}[hbt!]\n\t\\includegraphics[width=8cm]{fig7.eps\n\n\t\\caption{\\label{fig:optPumpBfiled} sub-level preparation efficiency $\\eta_{+1}$\n\t\tfor $\\sigma^+$ excitation of the $2^3$P$_2-2^3$S$_1$ and $2^3$P$_1-2^3$S$_1$ transitions (see legend), respectively, at different bias magnetic field strengths. The data are taken at a $6\\,$mm distance downstream from the midpoint of the laser beam in order to represent the efficiencies at equilibrium.}\n\\end{figure}\n\nWe have also analyzed the influence of stray magnetic fields along the $x$ and $y$ directions. Using a high-accuracy, three-axis Gauss probe (Stefan Mayer Instruments, $\\leq 1\\,\\mathrm{G}$, $0.05\\,$mG resolution), we obtain $B_x \\approx 0.2\\,\\text{G}$ and $B_y \\approx 0.1\\,\\text{G}$. At $B_z = 3\\,\\text{G}$, this results in an angle of $\\theta = \\sqrt{B_x^2+B_y^2}/B_z \\approx 80\\,\\text{mrad}$ \nbetween the magnetic field and the $z$ axis (cf. Gillot et al. \\cite{Gillot2013}). We have observed that a further compensation of the magnetic stray fields using additional coils along the $x$ axis (resulting in $\\theta < 40\\,\\text{mrad}$) does not result in an improved sub-level preparation efficiency. In addition to that, a non-perfect alignment of the laser propagation direction parallel to the quantization axis can induce a similar limit to the achievable sub-level preparation efficiency as the presence of magnetic stray fields. Furthermore, small magnetic-field inhomogeneities within the interaction region, resulting from e.g. a not perfect Helmholtz coil arrangement or electronic devices in the laboratory, may also limit the sub-level-preparation efficiency.\n\nIn summary, we conclude that the imperfect polarization of the laser light (see discussion above) is the main limiting factor for the sub-level preparation efficiency.\n\n\\subsubsection{\\label{sec:compOPlit} Comparison with literature values}\n\\noindent In Tab.\\ \\ref{tab:optPumpEfficiencies}, we present a summary of the maximum sub-level preparation efficiencies $\\eta_{i,\\mathrm{max}}$ obtained from our experimental data, and a comparison with literature values. As can be seen from the table, our $\\eta_{i,\\mathrm{max}}$ values are in good agreement with previous results for the laser optical pumping of He($2^3$S$_1$). To the best of our knowledge, we are the first to obtain a maximum efficiency $> 90\\,$\\% for optical pumping into $M_{J^{''}} = 0$. The only previous attempt to selectively populate $M_{J^{''}} = 0$ has been by Giberson et al. \\cite{Giberson1982} using linearly polarized light resonant with the $2^3$P$_0-2^3$S$_1$ transition and propagating along the quantization axis.\n\nFor optical pumping into the spin-stretched sub-levels ($M_{J^{''}} = \\pm 1$), we report a somewhat lower maximum efficiency than previous groups which we attribute to the aforementioned imperfect laser polarization in our experiments. In addition, we see a deviation of $\\eta_{i,\\mathrm{max}}$ for optical pumping with $\\sigma^+$ and $\\sigma^-$-polarized light especially while exciting via the $2^3$P$_1-2^3$S$_1$ transition. This might be induced by a systematic asymmetry in our setup resulting from e.g. small magnetic-field inhomogeneities as discussed above.\n\\begin{table}[ht!]\n\t\\caption{\\label{tab:optPumpEfficiencies} Summary of maximum efficiencies $\\eta_{i,\\mathrm{max}}$ obtained for the laser optical pumping of He($2^3$S$_1$) into selected $M_{J^{''}}$ sub-levels in our experiment, and comparison with literature values. The given uncertainties (two standard deviations) of our experimental results are statistical only.}\n\t\\begin{threeparttable}\n\t\t\\begin{tabular}{@{}lcccccc@{}}\n\t\t\t\\toprule[0.7pt]\n\t\t\t&& \\multicolumn{5}{c}{$\\eta_{i,\\mathrm{max}}$ (in \\%)}\\\\\n\t\t\t\\cmidrule[0.7pt]{3-7}\n\t\t\t&& $M_{J^{''}} = +1$ && $M_{J^{''}} = 0$ && $M_{J^{''}} = -1$ \\\\\n\t\t\t\\midrule[0.7pt]\n\t\t\t$2^3$P$_2-2^3$S$_1$ transition &&  &&  && \\\\\n\t\t\tThis work && $94 \\pm 3$ && -- && $90 \\pm 3$ \\\\\n\t\t\tGranitza et al. (1995) \\cite{Granitza1995} && $98.5$ && -- && $98.5$ \\\\\n\t\t\tLynn et al. (1990) \\cite{Lynn1990} && $96$ && -- && $96$ \\\\\n\t\t\tGiberson et al. (1982) \\cite{Giberson1982} && $\\approx 66$ && -- && $\\approx 66$ \\\\\n\t\t\t\\midrule[0.7pt]\n\t\t\t$2^3$P$_1-2^3$S$_1$ transition &&  &&  && \\\\\n\t\t\tThis work && $87 \\pm 5$ && $93 \\pm 4$ && $75 \\pm 5$ \\\\\n\t\t\tGranitza et al. (1995) \\cite{Granitza1995} && $<98.5$\\tnote{a} && -- && $<98.5$\\tnote{a} \\\\\n\t\t\tWallace et al. (1995) \\cite{Wallace1995} && $97$ && -- && $97$ \\\\\n\t\t\t\\midrule[0.7pt]\n\t\t\t$2^3$P$_0-2^3$S$_1$ transition &&  &&  && \\\\\n\t\t\tKato et al. (2012) \\cite{Kato2012} && $>99$ && -- && $>99$ \\\\\n\t\t\tSchearer \\& Tin  (1990) \\cite{Schearer1990a} && -- && -- && $96$ \\\\\n\t\t\tGiberson et al. (1982) \\cite{Giberson1982} && -- && $56$ && -- \\\\\n\t\t\t\\bottomrule[0.7pt]\n\t\t\\end{tabular}\n\t\t\\begin{tablenotes}\n\t\t\t\\item[a] No specific values given.\n\t\t\\end{tablenotes}\n\t\\end{threeparttable}\n\\end{table}\n\n\\subsection{\\label{sec:magDefl} Magnetic hexapole focusing}\n\\noindent The red circles in Fig.\\ \\ref{fig:Halbachres} show the results of a series of measurements which were obtained using the setup for the magnetic hexapole focusing of He($2^3$S$_1$, $M_{J^{''}} = +1$) (cf. Figs.\\ \\ref{fig:Setup} (c) and (d)). In order to interpret these results, we did numerical three-dimensional particle trajectory simulations in MATLAB. For these simulations, we use random number distributions for the particle positions and velocities (deduced from the experimental data obtained at the Faraday-cup detector) and a velocity-Verlet algorithm. An intial number of $5\\cdot10^6$ particles in each Zeeman sub-level of He($2^3$S$_1$) and He($2^1$S$_0$) are propagated at a time. The magnetic field by the two Halbach arrays is implemented using the analytical expressions given in Ref.\\ \\cite{Dulitz2016}. Particles are removed from the simulation if their transverse position inside a Halbach array exceeds the 3.0 mm inner radius of the assembly (cf. Fig.\\ \\ref{fig:Setup} (c)).\n\nIn each $xy$ detection plane, the output of the trajectory simulation (black lines in Fig.\\ \\ref{fig:Halbachres}) is analyzed over a certain interval of $x$ positions corresponding to the diameter of the wire detector. The experimental results are matched to the simulated data by comparing the ratio of areas beneath two Gaussian distributions fitted to the datasets (not shown). Very good agreement between the experimental and simulated datasets is achieved by using an effective remanence of $B_{0,\\mathrm{eff}} = 1.0 \\, \\mathrm{T} < B_{0}$ and an effective wire diameter of $d_{\\mathrm{wire, eff}} = 5.0 \\, \\mathrm{mm} > d_{\\mathrm{wire}}$ in the simulations. The decreased remanence compared to $B_0$ could be due to the demagnetization of the material as a result of the prolonged storage time of the magnets. Likewise, deviations from the ideal Halbach configuration may also be possible as a result of manufacturing defects.\n\n\nThe analysis of the simulated results suggests that the strong increase of the He$^*$ signal intensity around $x = 0$ is due to the transverse focusing of the $M_{J^{''}} = +1$ sub-level of the $2^3$S$_1$ level. The strongest signal increase, corresponding to the focal point of the device, is at a distance of $\\approx 110\\,$mm from the center of the two Halbach arrays. The remaining signal intensity is mainly due to a mixture of He atoms in the $2^3$S$_1$, $M_{J^{''}} = 0$ and $2^1$S$_0$, $M_{J^{''}} = 0$ sub-levels. This is also consistent with previous observations \\cite{Watanabe2006}. At time $t_0 = 0$, we assume a He($2^1$S$_0$)/He($2^3$S$_1$) ratio of $66\\,$\\% which is in line with the results of previous measurements in our laboratory \\cite{Guan2019}. At the focal point, the signal contribution by He atoms in the $2^3$S$_1$, $M_{J^{''}} = -1$ sub-level is decreased by more than a factor of 7 compared to the signal intensity by atoms in the $2^3$S$_1$, $M_{J^{''}} = 0$ sub-level. This is a result of the strong transverse magnetic defocusing forces which are exerted onto the atoms in the $M_{J^{''}} = -1$ sub-level.\n\nThe output of the simulation also provides an estimate of the sub-level selection efficiency for He($2^3$S$_1$, $M_{J^{''}} = +1$), $\\eta_{+1}$. Under the conditions of our experiment, $\\eta_{+1}$ is nearly constant over a region of $\\Delta y \\approx 20\\,$mm around the focal point. However, the efficiency strongly depends on the He beam diameter considered for the analysis. If we assume that the supersonic beam is collimated to the diameter of the wire detector (i.e., $0.2\\,$mm) just in front of this device, we obtain a maximum efficiency $\\eta_{+1,\\mathrm{max}} = 99\\,$\\% at the focal point. If we assume the same He beam diameter as in the optical pumping experiments described above (i.e., $2.9\\,$mm), the maximum efficiency $\\eta_{+1,\\mathrm{max}}$ at the focal point is reduced to $84\\,$\\%. To further improve the sub-level selectivity, we suggest the use of a bent magnetic guide \\cite{Beardmore2009, Mazur2014, Osterwalder2015, Dulitz2016, Toscano2018} or by the use of a central stop behind the Halbach arrays  \\cite{Chaustowski2007,Kurahashi2008,Kurahashi2021,Baum1988}.\n\n\\begin{figure}[hbt!]\n\n\n\t\\includegraphics[width=8cm]{fig8.pdf}\n\t\\caption{\\label{fig:Halbachres} Red circles: Measured He$^*$ signal intensities on the wire detector at different positions $y$ along the supersonic beam axis and at different transverse positions $x$ after magnetic hexapole focusing. The $y$-axis scale is given relative to the center of the two Halbach arrays. Black lines: He$^*$ signal intensities obtained from a numerical particle trajectory simulation.\n\t}\n\\end{figure}\n\n\\section{Conclusion}\n\\noindent We conclude that both laser optical pumping and magnetic hexapole focusing are very efficient methods for the selective preparation of magnetic sub-levels of He(2$^3$S$_1$) in a supersonic beam. We find that optical pumping into the spin-stretched sub-levels of He(2$^3$S$_1$) via the $2^3$P$_2-2^3$S$_1$ transition is more efficient than excitation via the $2^3$P$_1-2^3$S$_1$ transition. The best performance is achieved for $\\sigma^{+(-)}$ excitation via the $2^3$P$_2-2^3$S$_1$ transition yielding a maximum efficiency of $94 \\pm 3\\,$\\% ($90 \\pm 3\\,$\\%) for optical pumping into $M_{J^{''}} = +1$ ($M_{J^{''}} = -1$).\n\nMagnetic hexapole focusing is observed to be highly sub-level selective at low forward velocities of the supersonic beam. At $v = 830\\,$m/s and at the focal point of the hexapole lens system, we infer that up to $99\\,$\\% of the metastable atoms are in the $M_{J^{''}} = +1$ sub-level, if an $0.2\\,$mm-diameter region around the center of the supersonic beam axis is considered. The magnetic-hexapole-sub-level-selection technique is attractive, because it allows for the quantum-state manipulation of all atomic and molecular species with non-zero spin. Compared to optical pumping, the mechanical setup for magnetic focusing is rather simple, especially when commercial magnets are used \\cite{Osterwalder2015}.\n\nHowever, optical pumping has several advantages compared to magnetic hexapole focusing. While magnetic focusing is limited to the preparation of sub-level-selected samples in low-field-seeking sub-levels only, optical pumping allows for the selective population of all $M_{J^{''}}$ sub-levels, as shown here for the $2^3$P$_1-2^3$S$_1$ transition in He. For optical excitation with $\\pi$-polarized light, we obtain an efficiency of $93 \\pm 4\\,$\\% for population transfer into $M_{J^{''}} = 0$. The creation of a pure $M_{J^{''}} = 0$ sub-level might be possible by using magnetic focusing as well but would require a strong overfocusing of the low-field seeking quantum states. In our experiments, this may be realized by further reducing the forward velocity of the He* atoms or by using a longer hexapole magnet array. However, we observe that the number of metastable helium atoms decreases by a factor of $\\approx 2$ when the valve temperature is decreased from $300\\,$K to $50\\,$K. At the same time, the peak He$^*$ flux within the gas pulse decreases by a factor of $\\approx 50$, as the longer flight time to the detection region leads to a larger longitudinal spreading of the beam. Optical pumping can be applied independently of the velocity of the atoms as long as the discussed requirements for reaching the equilibrium sub-level efficiency are fulfilled. Thus, this technique results in a greater flexibility in choosing the valve temperature and, as mentioned above, running the valve at higher temperatures leads to much higher peak fluxes of $M_{J^{''}}$-sub-level-selected atoms. These high peak fluxes are particularly important for applications which benefit from high local densities, such as collision experiments. Besides that, optical pumping relies on a transfer of population from a statistical mixture of $M_{J^{''}}$ sub-levels into a single sub-level, whereas magnetic hexapole focusing relies on the spatial focusing (defocusing) of the desired (unwanted) $M_{J^{''}}$-sub-level population. Further transmission losses are due to an aperture which has to be inserted into the beam path behind the magnet assembly in order to eliminate contributions by the $2^3$S$_1$, $M_{J^{''}} = 0$ and $2^1$S$_0$, $M_{J^{''}} = 0$ sub-levels, whose motion is not influenced by a magnetic field.\n\nIn the future, we will use the presented sources of $M_{J^{''}}$-sub-level-selected He(2$^3$S$_1$) for quantum-state-controlled Penning-ionization studies \\cite{Grzesiak2019}. Furthermore, magnetic-sub-level-selected beams of He(2$^3$S$_1$) are useful as a starting point for the generation of coherent superposition states. The coherent control of Penning and associative ionization cross sections with such superposition states, for instance, involving the $M_{J^{''}} = 0$ sub-level of He(2$^3$S$_1$), has been predicted \\cite{Omiste2018}. In addition to that, helium is of particular interest for high-precision tests of few-electron quantum electrodynamics theory, as it is the simplest two-electron atom \\cite{Morton2006,Pachucki2017}. Accurate transition frequency measurements have been performed on ultracold trapped samples \\cite{vanRooij2011,Notermans2014,Rengelink2018} as well as on atomic beams \\cite{Pastor2004,Pastor2012,Zheng2017} of He(2$^3$S$_1$) atoms. The measurement of transitions with zero first-order Zeeman shift (i.e., $M_{J^{'}} = 0 \\leftarrow 2^3$S$_1,\\, M_{J^{''}} = 0$ transitions) would greatly reduce the experimental uncertainty.\n\n\\begin{acknowledgments}\n\\noindent We thank J. Toscano (JILA) and B. Heazlewood (University of Oxford) for the loan of the magnetic hexapole arrays and for fruitful discussions. This work is supported financially by the German Research Foundation (Project No. DU1804/1-1), by the Fonds der Chemischen Industrie (Liebig Fellowship to K. Dulitz) and by the University of Freiburg (Research Innovation Fund).\n\\end{acknowledgments}\n", "meta": {"timestamp": "2021-06-25T02:19:02", "yymm": "2106", "arxiv_id": "2106.13016", "language": "en", "url": "https://arxiv.org/abs/2106.13016"}}
{"text": "\\section{Introduction}\n\nA central goal of modern neuroscience research is to measure and quantify behavior of laboratory animals in order to enable correction studies to neuronal activity.\n\n\\begin{figure}[htb!]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figs/Fig1.pdf}\n\\caption{A: Exemplary input data of unconstrained mouse behavior tracked with 2D keypoint detectors from 5 perspectives. B: Outline of the inference pipeline; first, we obtain high-quality triangulation data from the 5-camera setup. This data is then used to train the lift pose model for 3D inference from single camera views. }\n\\label{fig1}\n\\end{figure}\n\n\nAnimal pose estimation in two dimensions has been recently made possible using convolutional neural networks that allow keypoint detections throughout a recorded video based on the training data labeled by the user \\cite{deeplabcut, pereira_fast_2019}.\nIf the goal is to obtain the animal pose in three dimensions, it is possible to combine the 2D keypoint detections from multiple synchronously operated cameras, and triangulate the points into a global 3D space \\cite{hartley_multiple_2004}.\nHowever, prior to the triangulation, the cameras need to be calibrated using a target, which is often an erroneous process in practice that can result in skewed projections of points into the global space.\nMoreover, during many experiments in behavioral neuroscience, it is difficult to establish continuous 3D tracking of keypoints over time as often it can not be guaranteed that two or more cameras have a view on all tracked body parts.\nThis can occur due to different reasons; difficulties in mounting the cameras in the desired angles, occlusion of objects or others animals, and self occlusion of the animal itself.\nTo overcome such shortcomings for 3D pose estimation in humans, lift pose models have been developed, which aim to infer the 3D pose directly from a single camera view \\cite{simplebaseline, videopose, cao2017realtime,sun_view-invariant_2020}.\n\n\nThis work contains two contributions. First, we present a simple and robust procedure for triangulation of freely moving rodents from multiple cameras that are orthogonal towards a camera positioned underneath the plane of movement (Figure \\ref{fig2}).\nSecond, we evaluate two model architectures that have been proposed for 3D pose lifting of human poses previously, one a linear residual network \\cite{simplebaseline} and the other a dilated temporal convolutional residual network \\cite{videopose}, aiming to establish which of both models works better for our triangulated rodent pose data. \nMoreover, we evaluate the choice of the temporal window setting on the performance of the temporal convolutional model as well as the choice of the viewing perspective on the performance of both models.\nThe aim of this work is to pave the way for robust detection of 3D rodent poses from single poses, allowing for studies of behavior in complex laboratory as well as naturalistic environments.\n\n\\section{Related work}\nPreviously 3D pose estimation on humans was proposed by Martinez \\etal \\cite{simplebaseline} where a simple linear residual network was trained on HumanEva and Human3.6M dataset. This architecture has already been evaluated by Gosztolai \\etal \\cite{liftpose} for 3D pose estimation in freely moving monkeys as well as rodents in a constrained behavioral setup. Pose lifting of human data using temporal 1-dimensional dilated convolutional neural networks was previously proposed by Pavllo \\etal \\cite{videopose}. View-invariant human pose estimation from embedding spaces was recently proposed by Sun \\etal \\cite{sun_view-invariant_2020}.\nApproaches to triangulation of animals include Anipose \\cite{anipose} and more recently the DANNCE framework that employs 3D convolutions for improving detections of 3D poses captured from a multi-camera setup \\cite{dunn_geometric_2021}.\n\n\n\n\\section{Geometric camera calibration}\n\nClassical triangulation of points is based on the projection of 2D planes onto a global 3D space. This requires calibration with a flat calibration target, typically a checker-board, that needs to be visible to all cameras simultaneously in order to identify the direction of the projection.\nHere we propose a simple and robust triangulation process based on orthogonality of cameras that are unable to view the classical calibration target simultaneously in their spatial configuration (Figure \\ref{fig2}).\n\nThis approach is motivated by the fact that the most variability of rodent movement is visible from the bottom perspective \\cite{luxem_identifying_2020}, from which the $x,y$ position of many body parts can be directly detected given reliable 2d pose estimation.\nThe additional orthogonal cameras are then needed to determine the $z$ coordinate for each keypoint, which can be given by the evaluation of a polynomial function determined during the calibration process.\n\nTo perform the experiments, we let one C57BL/6J wildtype animal freely explore a circular arena with a transparent Plexiglas floor.\nWe recorded 60 minutes of unconstrained behavior using 5 synchronous cameras operating at 50 Hz that have been mounted as shown in Figure \\ref{fig2}.\nWe then labeled 8 body parts in $1e3$ images and trained a ResNet CNN model provided in the \\textit{DeepLabCut 2.2} \\cite{deeplabcut} to detect the keypoints from the bottom view as well as side view recordings (Figure \\ref{fig1}).\nThe image augmentation routines supplied in the \\textit{DeepLabCut} package were used to improve the generalization of the network. \nThe keypoint coordinates were egocentrically aligned, so that the nosetip and tailbase marker form a vector parallel to the $x$-axis  and the center of the animal body is approximately at the origin of the $x$ and $y$ axes.\n\n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[width=2.3in]{figs/Fig2.pdf}\n\\caption{Multi camera setup for geometric triangulation.}\n\\label{fig2}\n\\end{figure}\n\nThe triangulation procedure was performed as follows. The algorithm iterated over the tuples of detected $x,y$ coordinates obtained from the bottom perspective. For each timestep and keypoint, the algorithm then searched for detections from the side camera that were obtained from the ResNet at a high accuracy ($p > 0.95$). \nThe height of the pixel on the side frame was converted into a physical $z$ value via the evaluation of a polynomial function, which was obtained a priori using a cylindrical calibration object with physical markers at three heights that was moved over the arena (Figure \\ref{fig3}). \nIf more then one side camera reliably detected a keypoint, an average of the detections was assigned to the $z$ value of the particular keypoint.\nIn total, in about $86\\%$ of the time frames all body parts could be detected in one of the side cameras. The remaining points were interpolated using an exponentially weighted moving average filter. Finally, the mean centered and z-scored values have been used for training of the pose-lifting models.\n\n\n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figs/Fig3.pdf}\n\\caption{Polynomial fitting function used for translating the $y$ coordinate of the calibration marker heights to the physical height in the arena, given the distance of the calibration target to the camera.}\n\\label{fig3}\n\\end{figure}\n\n\\section{Lifting models}\n\n\\subsection{Linear Model }\nThe \\textit{Linear model} \\cite{simplebaseline} architecture consists of a linear layer followed by a batchnorm layer, a ReLU activation function and dropout with 0.25 probability (Figure \\ref{fig4}). The linear layer is then followed by a residual block consisting of two linear layers followed by batchnorm layer, ReLU and dropout, where the input and output of this block are connected by a residual connection. \n\n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figs/Fig4.pdf}\n\\caption{Main Model Architecture of the \\textit{Linear model}.}\n\\label{fig4}\n\\end{figure}\n\n\n\nWe used the MPJPE (Mean Per Joint Position Error) to compute the loss at every iteration,\n\\begin{equation}\n \\text{MPJPE} = \\frac{1}{N} \\frac{1}{K} \\sum_{i=1}^{N}\\sum_{k=1}^{K} \\parallel f(x) - y\\parallel,\n\\label{mpjpe}\n\\end{equation} \nwhere $N$ is the total number of samples in the dataset , $K$ is the total number of keypoints we are considering for our experiments, $f(x)$ is the predicted 3D pose coordinate by the model and $y$ is our triangulated 3D coordinate used as target data \\cite{20202DT3}.\n\n\nThe model operates on keypoints detected in a single time step, \\ie $z$ is predicted from the tuple $(x,y)$.\nWe furthermore added the decaying momentum to the batch normalization layers to make the model more comparable to the \\textit{Temporal Convolutional Model}. The momentum parameter decides how much of the statistics of the input variables for a layer is used to normalize the input distribution between the layers. The model was trained using the Adam Optimizer and uses the Kaiming Initializers for initialization of the weights.\n\n\\subsection{Temporal Convolutional Model}\nThe \\textit{Temporal Convolutional model}\\cite{videopose} architecture consists of 1-dimensional convolutional layer followed by a batch normalisation layer, ReLU and dropout (Figure \\ref{fig5}). This arrangement is followed by $N$ number of residual blocks where each of consists of a combination of 1-dimensional convolution layer, a batch norm layer, ReLU and dropout followed repeatedly by the same combination again. Finally there exists another 1-dimensional convolutional layer before the output layer. \n\n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figs/Fig5.pdf}\n\\caption{Model Architecture of the \\textit{Temporal Convolutional model}.}\n\\label{fig5}\n\\end{figure}\n\n\nAs proposed in the original publication \\cite{videopose}, the model exists in two variants. One variant of this model uses strided convolutions for training, as it is takes better care of unused intermediate results in the hidden layers.  The other variant uses dilated convolutions which uses all the intermediate results for the prediction of the 3D pose output. In our case, we carried out experiments using strided convolutions both for training and prediction of 3D pose coordinates.\nFurthermore, we used symmetric convolutions only in the model, as causal convolutions are rather practical for real-time inference scenarios\\cite{videopose}.\n\n\n   \n   \n   \n\n\nFor prediction of 3D poses at a particular time step $t$ we consider a temporal window of size $T$ . This temporal window contains input coordinates from timestep $(t-T)$ to timestep $(t+T)$.\nHence as input we consider the past timestep $(t-T)$ together with future timestep $(t+T)$ input coordinates along with the present timestep $t$ coordinates. Hence, the network predicts the current timestep $t$ as output.\nThe 1-dimensional input kernel convolves vertically along the axis of data samples. The individual keypoints or features for each instance act as different channels for the convolution.\nThe model was trained using the Adam optimizer and the loss was the MPJPE loss \\eqref{mpjpe}.\n\n\\section{Results}\n\nIn this section we compare the performance of models, the \\textit{Linear model} and the \\textit{Temporal Convolutional Model}, on the described dataset.\nFor comparison, we consider three scenarios: the first scenario predicts the $z$ coordinate given the tuple $(x,y)$. The second scenario predicts the $y$ coordinate given the tuple $(x,z)$, resembling the depth inference from the side view of the rodent. In the third scenario also predicts the $y$ coordinate from $(x,z)$ tuples, however the perspective is rotated by $45 \\degree$ along the $x$-axis, resembling a diagonal top-down view on the animal as it could be obtained under realistic experimental conditions.\n\nBefore using the data we shuffled the data in order to improve the generalization over the test set.\nFor the \\textit{Temporal Convolution model}, we initially divided the data into chunks of the temporal window size and re-arranged them in a random order.\nFor the \\textit{Linear model}, we initially shuffled the data at every time step.\nEach model evaluation was repeated for 10 times, where each run was started with random initialization weights and a new random test/train split ($20\\%/80\\%$).\nThe evaluation criteria for each model and setting was the final test error that was obtained after 150 training epochs.\nBoth models were evaluated at the same input data for the MPJPE loss. In both models, the initial learning rate was 0.001 and the decay rate of per epoch was 0.95. \n\n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[width=0.9 \\linewidth]{figs/Fig6.pdf}\n\\caption{Mean test error of the \\textit{Temporal Convolutional Model} obtained at increasing temporal window size. The error bars show the loss standard deviation of the 10 shuffled evaluation runs.}\n\\label{fig6}\n\\end{figure}\n\nFor the \\textit{Temporal Convolutional Model} we first determined the optimal temporal window in terms of the test loss.\nAs shown in Figure \\ref{fig6}, the lowest test loss was obtained for window setting 135, which is 2.7 seconds in physical time.\nNote that the number of trainable parameters was 6.75 million for the model at temporal window size 15 and 16.8 million for the model at a temporal window size of 243.\n\n\\begin{table}[ht]\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{p{0.2\\linewidth}p{0.2\\linewidth}p{0.2\\linewidth}}\n\\hline\\hline\n\\\\Scenario & Linear & Temporal Convolutions \\\\ [1ex]\n\\hline\n\\\\  $(x,y) \\rightarrow z$ & \\vtop{\\hbox{\\strut Train: $4.17 \\pm 1.519$}\\hbox{\\strut Test: $1.47 \\pm 0.031$}} & \\vtop{\\hbox{\\strut Train: $0.72 \\pm 0.008$ }\\hbox{\\strut Test: $0.46 \\pm 0.006$}} \\\\  [1ex]\n\\hline\n\\\\ $(x,z) \\rightarrow y$  & \\vtop{\\hbox{\\strut Train: $3.40 \\pm 0.482$}\\hbox{\\strut Test: $1.34 \\pm 0.064$}} & \\vtop{\\hbox{\\strut Train: $1.01 \\pm 0.005$ }\\hbox{\\strut Test: $0.89 \\pm 0.007$}} \\\\  [1ex]\n\\hline\n\\\\ \\vtop{\\hbox{\\strut$(x,z) \\rightarrow y$}\\hbox{\\strut $45 \\degree$ rotation}}  & \\vtop{\\hbox{\\strut Train: $3.17 \\pm 0.519$}\\hbox{\\strut Test: $0.99 \\pm 0.037$}} & \\vtop{\\hbox{\\strut Train: $0.61 \\pm 0.006$}\\hbox{\\strut Test: $0.45 \\pm 0.003$}} \\\\  [1ex]\n\\hline\n\\\\  $(z,y) \\rightarrow x$  & \\vtop{\\hbox{\\strut Train: $3.18 \\pm 0.337$}\\hbox{\\strut Test: $1.21 \\pm 0.025$}} & \\vtop{\\hbox{\\strut Train: $0.69 \\pm 0.003$}\\hbox{\\strut Test: $0.53 \\pm 0.004$}} \\\\  [1ex]\n\\hline\n\\end{tabular}}\n\\caption{Performance evaluation of the \\textit{Linear model} and \\textit{Temporal Convolutional model} for different prediction directions.}\n\\label{table:nonlin}\n\\end{table}\n\nFinaly, the results for predictions from different perspectives for both models are in Table \\ref{table:nonlin}.\nIn this table, we evaluated the \\textit{Temporal Convolutional model} at the optimal window size of 135 timesteps.\nWe found that under all perspectives, the \\textit{Temporal Convolutional model} performed better than the \\textit{Linear model}.\nMoreoever, we found that both models achieve the lowest test loss at the prediction of $(x,z) \\rightarrow y$ (side view along the mouse body), when the perspective is rotated at 45 degrees around the x-axis.\nThis is followed by the $(x,y) \\rightarrow z$ (bottom perspective) for the \\textit{Temporal Convolutional model} and the $(x,z) \\rightarrow y$ prediction for the \\textit{Linear model}.\n\n\n\n\\section{Conclusion and outlook}\nIn this work, we present a simple and robust procedure for triangulation of freely moving rodents from multiple cameras that are orthogonal towards a camera positioned underneath the plane of movement. \nUsing the triangulated data, we trained and evaluated two types of deep lift posing models that are able to predict the depth coordinate from single camera views, a linear ResNet model and a temporal convolutional ResNet model.\nWe show that the \\textit{Temporal Convolutional model} attained a lower test loss at all viewing angles then the \\textit{Linear model}.\n\nDuring the evaluation of our dataset, we also found that some viewing directions could be more effectively predicted than others.\nFor example, the diagonal top-down view yielded a lower test-loss than the orthogonal side view. \nWe believe that due to the body symmetry some body parts (paws, ears) are hard to distinguish by the network in this perspective.\nInterestingly, the $z$ height of body parts given the $(x,y)$ coordinates from the bottom view can be rather efficiently estimated by both models, in comparison to the other directions.\n\nIn regards to previous literature, Wiltschko. al. \\cite{autocorel} state that the auto correlation of mouse pose dynamics decrease after a period of approximately 500 ms. Interestingly, the optimal temporal window size determined for the \\textit{Temporal Convolutional model} was significantly larger (3.7 s).\nThis suggests that additional behavioral information can be extracted at longer time spans, adding onto the discussion on the relevance of multi-scale dependencies in behavioral models \\cite{datta_computational_2019}.\n\nFor future work, we aim to implement semi-supervised learning as suggested in \\cite{videopose} and \\cite{sun_view-invariant_2020} to establish view-invariant pose-lifting for freely moving rodents.\nMoreover, we need to evaluate the model under different scenarios, such es occlusions by objects or other animals.\nFinally, we hope that this work paves the way to robust 3d pose estimation in complex laboratory environments, allowing for behavior quantification based on 3D pose data for a variety of experimental designs. \n\n{\\small\n\\bibliographystyle{ieee_fullname}\n", "meta": {"timestamp": "2021-06-25T02:18:10", "yymm": "2106", "arxiv_id": "2106.12993", "language": "en", "url": "https://arxiv.org/abs/2106.12993"}}
{"text": "\\section{Introduction}\nIn this paper, a \\textit{(metric) tree} is a compact, connected metric space with the property that each pair of distinct points forms the endpoints of a unique arc. We show that all metric trees that satisfy two simple geometric properties can be embedded in a Euclidean space with bounded distortion.\n\nThe two properties that we impose are \\textit{doubling} and \\textit{bounded turning}. Recall that a metric space is called doubling if each ball in the space can be covered by $N$ balls of half the radius, for some fixed constant $N$. A metric space is called bounded turning if each pair of points $x,y$ in the space are contained in a compact, connected set whose diameter is bounded by $Cd(x,y)$, for some fixed constant $C$. In the case of metric trees, this is equivalent to saying that the unique arc joining a pair of points has diameter comparable to the distance between those points.\n\n\n\nThe class of trees satisfying these two properties were studied in detail in \\cite{Kinneberg, BM1, BM2, DV}, and given the following name:\n\n\\begin{definition}\nA \\textit{quasiconformal tree} is a metric tree that is doubling and bounded turning.\n\\end{definition}\n\nWe refer the reader to \\cite{BM1, DV} for more discussion on the history of quasiconformal trees, and the ways in which they arise naturally in metric geometry and complex analysis.\n\nOur main theorem says that all quasiconformal trees bi-Lipschitz embed in some Euclidean space in a quantitative fashion: \n\n\\begin{theorem}\\label{thm:main}\nIf $T$ is a quasiconformal tree, then $T$ admits a bi-Lipschitz embedding into some $\\mathbb{R}^k$. The dimension $k$ and the bi-Lipschitz constant of the embedding depend only on the doubling and bounded turning constants of $T$.\n\\end{theorem}\n\nThis answers a question posed explicitly in \\cite[Question 1.6]{DV}. As a reminder, a bi-Lipschitz embedding (recalled precisely in Section \\ref{sec:prelim}) is an embedding that preserves all distances up to a fixed constant factor. The ``bi-Lipschitz embedding problem'' (the classification of metric spaces that admit a bi-Lipschitz embedding into some $\\mathbb{R}^n$), is one of the most well-known problems in the field of analysis on metric spaces. (See, e.g., \\cite[Open Problem 12.3]{Heinonen}.)\n\nThe problem for quasiconformal trees is especially interesting as quasiconformal trees stand between two types of metric spaces with different behaviors. On the one hand, quasiarcs (i.e. doubling and bounded turning arcs) are a special case of quasiconformal trees and are known to always bi-Lipschitz embed into Euclidean spaces \\cite[Proposition 8.1]{DV}. On the other hand, if we generalize further to what we might call ``metric graphs'' -- compact, path-connected metric spaces of topological dimension 1 -- then there exist examples that are doubling and bounded turning, generalizing quasiconformal trees, but that do not admit such embeddings. For examples, see \\cite[Theorem 4.1]{Laakso} and \\cite[Theorem 2.3]{LP}.\n\nBi-Lipschitz embeddability of quasiconformal trees was previously known in only two special cases. First, Gupta, Krauthgamer, and Lee \\cite{GKL} (see also \\cite{GT}) proved that if a doubling tree is \\emph{geodesic} (that is, the distance between any two points of the tree is equal to the length of the unique arc that joins them), then the tree bi-Lipschitz embeds into some Euclidean space ${\\mathbb{R}}^{N}$ with $N$ depending only on the doubling constant of the tree; see also \\cite{LNP} for a different proof. Note that the geodesic property is much stronger than the bounded turning property: in geodesic trees, every branch is necessarily isometric to a line segment, while in quasiconformal trees the branches may be fractal curves (e.g., the von Koch snowflake). Second, in \\cite{DV} the first and third named authors proved that a quasiconformal tree bi-Lipschitz embeds in some Euclidean space if and only if the set of \\emph{leaves} of the tree bi-Lipschitz embeds in some Euclidean space; see Section \\ref{sec:prelim} for definitions. While the latter result is useful in cases where the embedabbility of the set of leaves is clear (e.g. the set of leaves is uniformly disconnected), there are examples of quasiconformal trees whose leaves are dense in the tree, and the result is inconclusive.\n\n\n\nOur new construction in Theorem \\ref{thm:main} owes some ideas to the well-known theorem of Assouad \\cite{Assouad} that every doubling metric space admits a ``snowflake'' embedding into some Euclidean space, but new ideas are needed to improve snowflake to bi-Lipschitz in the case of quasiconformal trees. \n\nThe crucial new idea is a novel partition of the tree into infinitely many simpler parts. Our partition bears a resemblance to the ``path partition'' used by Matou\\v{s}ek in \\cite[p. 231]{Matousek} to produce bi-Lipschitz embeddings of (discrete) \\textit{geodesic} trees into $\\ell_p$-spaces, but there are also significant differences. Other decompositions of quasiconformal trees have been produced in \\cite{BM1,DV}, but they are less similar to our construction. \n\nBefore ending the introduction, we note that, while the doubling property is necessary for the bi-Lipschitz embedabbility of any metric space into a Euclidean space, the bounded turning property is not. It is, however, a natural condition to consider and it cannot be removed from the statement of Theorem \\ref{thm:main} (even for arcs), as the following example shows.\n\n\\begin{example}\nLet $C\\subseteq \\mathbb{R}$ be the standard Cantor set and $F\\subseteq \\mathbb{R}$ be a Cantor set of positive $1$-dimensional Lebesgue measure. Let $E = F\\times F\\times F\\subseteq \\mathbb{R}^3$, a Cantor set of positive $3$-dimensional Lebesgue measure. By \\cite[Corollary 2]{Mcmillan}, $E$ is a \\textit{tame} Cantor set, and so there is a homeomorphism from $\\mathbb{R}^3$ to itself that sends $C\\times \\{0\\} \\times \\{0\\}$ onto $E$. In particular, there is a topological arc $\\Gamma\\subseteq \\mathbb{R}^3$ containing $E$.\n\nNow equip $\\mathbb{R}^3$ with the Heisenberg group metric $d_\\mathbb{H}$. (See, e.g., \\cite[p. 76]{Heinonen} for an introduction to the Heisenberg group.) It is a consequence of the celebrated differentiation theorem of Pansu \\cite{Pansu} that the Heisenberg group (and any positive-measure subset), although doubling, admits no bi-Lipschitz embedding into any Euclidean space \\cite[p. 99]{Heinonen}. Thus, $\\Gamma\\subseteq (\\mathbb{R}^3, d_{\\mathbb{H}})$ is a doubling arc with no bi-Lipschitz embedding into any Euclidean space. \n\\end{example}\n\n\\subsection{Outline of the paper}\nSection \\ref{sec:prelim} contains the basic definitions and notation used in the paper. In Section \\ref{sec:decomposition}, we describe our novel way of decomposing a quasiconformal tree into useful pieces. Finally, Section \\ref{sec:embedding} defines the embedding and proves that it is bi-Lipschitz.\n\n\\section{Preliminaries}\\label{sec:prelim}\n\\subsection{Metric spaces and mappings}\nWe generally denote metrics on metric spaces by $d$. The \\textit{diameter} of a subset $E$ in a metric space $X$ is\n$$ {\\rm diam}(E) = \\sup\\{d(x,y):x,y\\in E\\}.$$\n\nGiven $\\epsilon>0$, an \\textit{$\\epsilon$-net} in a metric space $X$ is a subset $N\\subseteq X$ such that $d(x,y)\\geq \\epsilon$ for all $x,y\\in N$ and ${\\rm dist}(p, N) < \\epsilon$ for all $p\\in X$. Each metric space has $\\epsilon$-nets for every $\\epsilon>0$. Moreover, each $\\epsilon$-separated set in $X$ (that is, a set satisfying only the first condition to be a net) can be extended to an $\\epsilon$-net of $X$. \n\nA mapping $f\\colon X\\rightarrow Y$ between two metric spaces is Lipschitz (or $L$-Lipschitz to emphasize the constant) if there is a constant $L$ such that\n$$ d(f(x), f(y)) \\leq L \\cdot d(x,y) \\text{ for all } x,y\\in X.$$\nThe mapping $f$ is bi-Lipschitz (or $(a,b)$-bi-Lipschitz to emphasize the constants) if there are constants $a,b>0$ such that\n$$ ad(x,y) \\leq d(f(x),f(y)) \\leq bd(x,y) \\text{ for all } x,y \\in X.$$\n\n\\subsection{Trees}\nIf $T$ is a metric tree, we write $\\mathcal{L}(T)$ for the set of leaves of $T$. A point $x$ is called a leaf of $T$ if $T\\setminus\\{x\\}$ is connected.\n\nIf $x$ and $y$ are points in a metric tree $T$, we write $[x,y]$ for the unique topological arc joining $x$ and $y$ in $T$, with the understanding that $[x,y]=\\{x\\}=\\{y\\}$ if $x=y$. Often it is convenient to orient an arc $[x,y]$ ($x\\neq y$), so we assume that $[x,y]$ is equipped with a a continuous parametrization $\\gamma\\colon [0,1] \\rightarrow [x,y]$ such that $\\gamma(0)=x$ and $\\gamma(1)=y$. This allows us to order subsets of $[x,y]$; we call this the ``natural order'' along the arc. If $K$ is a non-empty compact subset of $[x,y]$, we often speak of the ``first'' or ``last'' point of $K$ with respect to this ordering.\n\n\n\n\n\\section{Decomposing the tree}\\label{sec:decomposition}\n\nIn this section, we give a new way to decompose a quasiconformal tree into useful pieces. This differs from other decompositions given in, e.g., \\cite{BM1, DV}.\n\nFor the remainder of this section, we fix a tree $T$ that is doubling with constant $D$ and bounded turning with constant $1$. (One can always reduce to this case; see Section \\ref{sec:embedding}.) Because $T$ is assumed to be $1$-bounded turning, we have ${\\rm diam}(T)={\\rm diam}(\\mathcal{L}(T))$, and we rescale so that ${\\rm diam}(T)={\\rm diam}(\\mathcal{L}(T))=1$. Finally, we will assume that $\\mathcal{L}(T)$ -- the set of leaves of $T$ -- is finite. This assumption is not strictly necessary, and none of the constants below will depend on the cardinality of $\\mathcal{L}(T)$, but it avoids certain technical difficulties. In the course of proving Theorem \\ref{thm:main}, we will reduce to this case regardless. (See Section \\ref{sec:embedding}.) Remark \\ref{rmk:infinitecase} explains some of the annoyances that finiteness prevents.\n\nWe fix a collection $\\{N_n\\}_{n\\geq 1}$ of $2^{-n}$-nets in $\\mathcal{L}(T)$ with the property that $N_n \\subseteq N_m$ if $n\\leq m$. The doubling property of $T$ implies that each $N_n$ is finite. The assumption that $\\mathcal{L}(T)$ is finite also implies that $N_n=\\mathcal{L}(T)$ for all $n$ sufficiently large.  \n\nWe then define $T_n\\subseteq T$ to be the ``convex hull'' of $N_n$, i.e., \n$$ T_n = \\bigcup_{a,b\\in N_n} [a,b].$$\nOf course, $T_n \\subseteq T_{n+1}$ for each $n$.\n\n\nFor each $n\\geq 2$, the compact set $\\overline{T_{n} \\setminus T_{n-1}}$ is the disjoint union of finitely many compact connected components, which we denote $\\{K^j_{n}\\}_{j\\in J_{n}}$. For convenience, we also set $K^1_1=T_1$ and $J_1=\\{1\\}$. Under our finiteness assumption, it is the case that $J_n = \\emptyset$ for all $n$ sufficiently large, but none of our bounds will depend on this threshold. \n\nIn the next two lemmas we collect some basic properties of the sets $K^j_{n}$. \n\n\\begin{lemma}\\label{lem:Kproperties}\nThe following properties of the sets $K^j_n$ hold:\n\\begin{enumerate}[(i)]\n\\item\\label{eq:Ki} For each $n\\in\\mathbb{N}$ and $j\\in J_n$, the set $K_{n}^j$ is a quasiconformal tree.\n\\item\\label{eq:Kii}  If $n\\geq 2$, then $K^j_n$ intersects $T_{n-1}$ in exactly one point. Moreover, for each $n \\in \\mathbb{N}$, $T_n \\cap \\overline{T_{n+1}\\setminus T_n}$ contains a finite number of points.\n\\item If $n\\in\\mathbb{N}$ and $j,j' \\in J_n$ with $j\\neq j'$, then $K_n^{j} \\cap K_n^{j'} = \\emptyset$. If $n,m \\in \\mathbb{N}$ with $n\\neq m$, $j\\in J_n$, and $j'\\in J_m$, then $K^i_n$ and $K^j_m$ intersect in at most one point.\n\\item\\label{eq:Kiii} For each $n\\in \\mathbb{N}$ and $j\\in J_n$, the set $\\mathcal{L}(K^j_n)$ is contained in $T_{n-1} \\cup (N_n\\setminus N_{n-1})$ and contains at least one element of $N_n\\setminus N_{n-1}$.  Moreover, the set $K_n^j \\cap \\mathcal{L}(T)$ is disjoint from $K_m^i$ for every $m\\in\\mathbb{N}, i\\in J_m$.\n\\item\\label{eq:Kv} We have\n$$ T \\subseteq \\bigcup_{n\\geq 1} \\bigcup_{j\\in J_n} K^j_n,$$\ni.e., every point of $T$ is contained in some $K^j_n$. \n\\end{enumerate}\n\\end{lemma}\n\n\\begin{proof}\nFor (i), we note that each $K_{n}^j$ is a compact connected subset of a metric tree, hence it is a metric tree itself. Moreover, each $K_{n}^j$ is 1-bounded turning as a subset of $T$, and $D$-doubling as a subset of a $D$-doubling space. Therefore, each $K_{n}^j$ is a quasiconformal tree.\n\nFor (ii), we start by noting that each component of $T_n \\setminus T_{n-1}$ contains at least one point in $N_{n}\\setminus N_{n-1}$. To see this, note that each $x\\in T_n \\setminus T_{n-1}$ is contained on an arc $[a,b]$ between two leaves, with $b\\in N_n \\setminus N_{n-1}$. If $a\\in N_{n-1}$, then since $T_n$ is a tree, we must have $[x,b] \\subset T_n\\setminus T_{n-1}$. If $a \\in N_{n}\\setminus N_{n-1}$ then both arcs $[a,x]$ and $[x,b]$ can not intersect $T_{n-1}$. Either way the component containing $x$ will contain either $a$ or $b$ and that point lies in $N_n\\setminus N_{n-1}$. \n \nConversely, for any point $x\\in N_{n}\\setminus N_{n-1}$ there exists unique component of $T_{n} \\setminus T_{n-1}$ containing $x$. Therefore, we can enumerate the components of $T_{n}\\setminus T_{n-1}$ as $X_1,\\dots,X_m$ for some $m\\leq \\card(N_{n}\\setminus N_{n-1})$. Moreover, for every $i\\in\\{1,\\dots,m\\}$ we have that $\\overline{X_i}$ is a metric tree  and $\\overline{X_i}\\cap T_{n-1}$ contains exactly one point. Now, for each $i\\in J_n$ we have that $K_{n}^j = \\overline{X_{i_1}}\\cup\\cdots \\cup \\overline{X_{i_j}}$ for some $i_1,\\dots,i_j \\in \\{1,\\dots,m\\}$ with $\\overline{X_{i_1}},\\dots,\\overline{X_{i_j}}$ all intersecting $T_{n-1}$ (and each other) at one single point.\n\nTherefore, we showed that $\\card(J_n) \\leq \\card(N_{n}\\setminus N_{n-1})$ and that for any $j\\in J_n$, $K^j_{n}$ intersects $T_{n-1}$ in exactly one point. It follows that $T_{n-1} \\cap \\overline{T_{n}\\setminus T_{n-1}}$ contains at most $\\card(N_{n}\\setminus N_{n-1})$ many points.\n\nFor (iii), if $j,j' \\in J_n$ with $j\\neq j'$, then $K_n^{j} \\cap K_n^{j'} = \\emptyset$ since each $K_{n}^i$ is a distinct component of $\\overline{T_{n+1} \\setminus T_n}$. Fix now positive integers $n< m$ and $j\\in J_n$ and $j'\\in J_m$. Then \n\\[ K_{n}^j \\cap K_{m}^{j'} \\subset K_{m}^{j'} \\cap T_n \\subset K_{m}^{j'} \\cap T_{m-1}\\]\nwhere the latter intersection contains exactly one point by (ii).\n\nFor (iv), observe that \n\\[ \\mathcal{L}(K_n^j) \\subset (K_n^j \\cap T_{n-1}) \\cup (\\mathcal{L}(T_{n})\\setminus T_{n-1}) = (K_n^j \\cap T_{n-1}) \\cup (N_n\\setminus N_{n-1}).\\]\n\nBy the proof of (ii), we know that $K_n^j$ contains a point of $N_n \\setminus N_{n-1}$. Thus, $\\mathcal{L}(K_n^j)$ also contains a point of $N_n \\setminus N_{n-1}$.\n\nFor the ``moreover'' statement in (iv), fix a point $x \\in  K_n^j \\cap \\mathcal{L}(T) \\subseteq N_n \\setminus N_{n-1}$. By (iii) we have that $x \\not\\in K_n^{j'}$ for any $j'\\in J_n \\setminus \\{j\\}$. If $m\\neq n$ and $j'\\in J_m$, then $K_m^{j'}\\cap \\mathcal{L}(T) \\subseteq N_m \\setminus N_{m-1}$ and therefore cannot contain $x$. This completes the proof of (iv).\n\nFor the last claim, we know that $N_n =\\mathcal{L}(T)$ for all $n$ sufficiently large, and therefore $T_n = T$ for all $n$ sufficiently large. Given $x\\in T$, choose $n\\in\\mathbb{N}$ to be the first index such that $x\\in T_n \\setminus T_{n-1}$ (viewing $T_0=\\emptyset$). Then $x$ must be in some component of $\\overline{T_n \\setminus T_{n-1}}$, i.e., some $K_n^j$. \n\\end{proof}\n\n\n\n\\begin{remark} \\label{rmk:infinitecase} The simplification gained by assuming that $\\mathcal{L}(T)$ is finite is most prominent in the proof of (v) in the previous lemma, which then makes Lemma \\ref{lem:monotone} somewhat easier to state and prove.\n\nIf $\\mathcal{L}(T)$ were infinite, then part (v) above would have to be modified to $T \\setminus \\mathcal{L}(T) \\subset \\bigcup_{n\\geq 1} \\bigcup_{j\\in J_n} K_n^j$. To see this, consider $x\\in T \\setminus \\mathcal{L}(T)$. Since $x\\not\\in \\mathcal{L}(T)$, then $T\\setminus \\{x\\}$ contains at least two components. Let $A$ be a component of $T\\setminus\\{x\\}$ containing a point $a\\in N_1$, and $B$ another component of $T\\setminus\\{x\\}$.  The set $B$ contains at least one leaf of $T$ and is open in $T$; therefore, it contains a point $b\\in N_n$ for some $n$ large. The arc $[a,b]$ contains $x$ and is contained in $T_n$. Therefore $x\\in T_n$ for some $n$, and so $x\\in K_m^j$ for some $m,j$. \\end{remark}\n\n\n\\begin{lemma}\\label{lem:components2}\nEach tree $K^j_{n}$ has diameter at most $2^{2-n}$. Moreover, for each $n\\in \\mathbb{N}$ and $j\\in J_n$, $K^j_n$ is a union of at most $C$ arcs, where $C$ depends only on $D$.\n\\end{lemma} \n\n\\begin{proof}\nFor $n=1$, the first claim of the lemma is clear. For the second claim we have $\\card(\\mathcal{L}(K_1^1)) = \\card(N_1)$, so $K^1_1$ is a union of at most $\\card(N_1)$ arcs. Note also that $\\card(N_1) \\leq C$ for some $C$ depending only on $D$. \n\nAssume for the rest that $n\\geq 2$. Fix $j\\in J_n$ and let $x$ be the unique point in $T_{n-1} \\cap K_n^j$.\n\nFor the first claim, let $[a,b]\\subseteq K^j_{n}$ with $a\\neq b$. The arc $[a,b]$ extends to an arc $[a',b']$ with $a',b'\\in \\mathcal{L}(K^j_{n})$. We consider two cases.\n\n\\emph{Case 1.} Suppose that one of the points $a',b'$, say $b'$, is the point $x$; then $a'\\in N_n \\setminus N_{n-1}$ by Lemma \\ref{lem:Kproperties}. Choose $z\\in N_{n-1}\\subseteq T_{n-1}$ such that $d(z,a')< 2^{1-n}$. Then,\n\\[ {\\rm diam}{[a,b]} \\leq {\\rm diam}{[a',x]} \\leq {\\rm diam}{[a',z]} = d(a',z) < 2^{1-n}.\\]\n\n\\emph{Case 2.} Suppose that $a',b' \\in N_{n}$. By Case 1 and the triangle inequality,\n\\[ {\\rm diam}{[a,b]} \\leq {\\rm diam}{[a',b']} = d(a',b') \\leq d(a',x) + d(b',x) < 2^{2-n}.\\]\nIt follows that the diameter of any arc in $K_n^j$ is at most $2^{2-n}$.\n\nWe now show the second claim. By the first claim, we have that $(N_n\\setminus N_{n-1})\\cap K_n^j \\subseteq \\overline{B}(x, 2^{2-n})$. Since $N_n \\setminus N_{n-1}$ is a $2^{-n}$-separated set, there exists $C>1$ depending only on $D$ such that\n\\[ \\card{ \\mathcal{L}(K_{n}^j)} \\leq 1+ \\card{((N_n\\setminus N_{n-1})\\cap K_n^j)} \\leq C+1.\\]\nTherefore, $K^{j}_n$ is the union of at most $C$ arcs. \n\\end{proof}\n\n\n\n\nSet $r^1_1\\in N_1 \\subseteq K^1_1$ to be an arbitrary root for $K^1_1=T_1$, and let \n$$ r^j_n = \\text{ the unique element of } K_n^j \\cap T_{n-1},$$\n(as provided by Lemma \\ref{lem:Kproperties}) for each $n\\geq 1$ and $j\\in J_n$.\n\nWe now form a graph that encodes how close the sets $K^n_j$ are to each other. Given a constant $S\\geq 1$, consider the graph $G_S=(V,E_S)$ with vertex set\n$$ V = \\{(n,i) : n\\geq 1, i\\in J_n\\}$$\nand edge set\n$$ E_S = \\{ \\{(n,i), (m,j)\\} : n\\geq m \\text{ and } {\\rm dist}(K_n^i, K_m^j) \\leq S2^{-\\max\\{n,m\\}}\\},$$\nIf $v,w\\in V$, we use the notation $v\\sim w$ to indicate that there is an edge between $v$ to $w$ in $E_S$, with $S$ understood from context.\n\n\nWe remark that the valence of the graph $G_S$ (the maximum of the valences of its vertices) may be arbitrarily large. However, we show in the next lemma that it is possible to color the vertices of $G_S$ with a fixed number of colors so that adjacent vertices have different colors.\n\n\\begin{lemma}\\label{lem:coloring}\nFor each $S\\geq 1$, there is a positive integer $A=A(S,D)$ and a ``coloring'' $\\chi \\colon V \\rightarrow \\{1, \\dots, A\\}$ such that if $v,w\\in V$ and $v\\sim w$, then $\\chi(v) \\neq \\chi(w)$.\n\\end{lemma}\n\\begin{proof}\nGiven $S$, let $A=2A'+1$, where $A'=A'(S,D)$ is the maximum number of $r$-separated points in any ball of radius $200Sr$ in $T$. This is finite and depends only on $S$ and the doubling constant $D$. We now construct $\\chi$ inductively. Order $V$ so that $(n,i)<(m,j)$ if and only if $n<m$ or $n=m$ and $i<j$. Set $\\chi((1,1))=1$.\n\nSuppose we have defined $\\chi$ for all $v<v_0 = (n_0, i_0)$. We will show that the collection of all $v=(m,j)\\in V$ such that $v<v_0$ and $v_0 \\sim v$ has strictly fewer than $A$ elements. In that case, we may color $v_0$ by a color which does not match any of these, and the proof is complete.\n\nConsider first the collection of all $v=(m,j)\\in V$ such that $v<v_0$, $v_0 \\sim v$ and ${\\rm diam}(K_m^j)\\leq 10S2^{-n}$. By Lemma \\ref{lem:Kproperties}, each such $K_m^j$ contains a distinct point $n_m^j\\in N_m\\subseteq N_n$. Each such $n_m^j$ satisfies\n$$ d(n_m^j, r_{n_0}^{i_0}) \\leq {\\rm diam}(K_{n_0}^{i_0}) + S2^{-n}+ {\\rm diam}(K_m^j) \\leq (1+11S)2^{-n}.$$\nSince these points are $2^{-n}$-separated, there can be at most $A'<\\frac{A}{2}$ such points, and hence less than $A/2$ such $K_m^j$.\n\nNext we bound the total number of $v=(m,j)\\in V$ such that $v<v_0$, $v_0 \\sim v$ and ${\\rm diam}(K_m^j)> 10S2^{-n}$. Consider such a $K_m^j$. Each such $K_m^j$ contains a point $q_m^j$ such that ${\\rm dist}(q_m^j, K_{n_0}^{i_0})< 10S2^{-n}$ and $q_m^j$ is at least distance $\\frac{1}{10} 2^{-n}$ from $r_m^j$. \n\nIf $(m,j)\\neq (m',j')$ for two such vertices in our graph, then $d(q_m^j, q_{m'}^{j'}) \\geq \\frac{1}{10}2^{-n}$: any path from $q_m^j$ to $q_{m'}^{j'}$ must pass through either $r_m^j$ or $r_{m'}^{j'}$, and so the bounded turning condition and the second defining property of $q_m^j$ yield this bound.\n\nThus, the collection of all such points $q_m^j$ forms a $\\frac{1}{10}2^{-n}$-separated set inside $B(r_{n_0}^{i_0}, (1+10S)2^{-n})$. It follows that the collection of such $K_m^j$ has again at most $A'<A/2$ elements.\n\nThus, the total number of $v=(m,j)\\in V$ such that $v<v_0$ and $v_0 \\sim v$ is strictly less than $A$. This completes the proof.\n\\end{proof}\n\nA simple consequence of the previous lemma is the following bound on how many sets $K_m^j$, with $m$ small, can intersect an arc.\n\n\\begin{lemma}\\label{lem:arcintersection}\nThere is a constant $M=M(D)$ such that if $d(x,y)\\leq 2^{-n}$, then $[x,y]$ can intersect at most $M$ distinct sets $K^m_j$ with $m\\leq n$. \n\\end{lemma}\n\\begin{proof}\nLet $S=1$ and apply Lemma \\ref{lem:coloring}.\n\nNotice that if $K_{\\ell}^j$ and $K_m^i$ intersect $[x,y]$ and have $\\ell, m\\leq n$, then  \n$${\\rm dist}(K_{\\ell}^j, K_m^i) \\leq {\\rm diam}([x,y]) \\leq 2^{-n} \\leq S2^{-\\max\\{\\ell,m\\}},$$\nand so there is an edge $(\\ell,j)\\sim (m,i)$ in the graph $G_S$.\n\nThus, if $K_{\\ell}^j$ and $K_m^i$ intersect $[x,y]$ and have $\\ell, m\\leq n$, then they are adjacent in $G_S$. Lemma \\ref{lem:coloring} therefore says that the number of such sets is bounded by a constant depending only on $D$, which completes the proof. \n\\end{proof}\n\nNext, we examine more closely the way in which an arc $\\gamma$ in $T$ can be covered by the sets $K_m^j$. Let us say that $\\gamma$ \\textit{traverses} $K_m^j$ if $\\gamma\\cap K_m^j$ contains more than one point (in which case $\\gamma\\cap K_m^j$ is a sub-arc of $\\gamma$).\n\nEach point $x\\in \\gamma$ must be contained in a set $K_m^j$ that $\\gamma$ traverses. Indeed, let $\\{x_i\\}$ be a sequence of points in $\\gamma$, all distinct from $x$, that converges to $x$. By our finiteness assumption, there are only finitely many sets $K_m^j$, and by Lemma \\ref{lem:Kproperties}(v), each $x_i$ is in one of them. Therefore, a subsequence of $\\{x_i\\}$ is contained in a fixed $K_m^j$, which must also contain $x$ by compactness and be traversed by $\\gamma$.  \n\nIn addition, note that if $\\gamma$ traverses both $K_m^j$ and $K_n^i$, then $\\gamma$ must intersect one of them ``first'' in the order of parametrization of the arc.\n\n\\begin{lemma}\\label{lem:monotone}\nLet $\\gamma$ be an arc in $T$. Let $\\{K_{m(i)}^{j(i)}\\}_{i\\in I}$ be the collection of sets $K_m^j$ that are traversed by $\\gamma$, where the index set $I$ is a finite set $\\{1, \\dots, n\\}$.  Order the sets in the order along which they intersect $\\gamma$.\n\nThen there is an index $i_0\\in I$ such that $m(i) > m(i+1)$ for all $i<i_0$ and $m(i) < m(i+1)$ for all $i\\geq i_0$. \n\\end{lemma}\n\\begin{proof}\nChoose $i_0\\in I$ such that $m(i_0)\\leq m(i)$ for all $i\\in I$. \n\nWe show by induction that for any $i< i_0$ we have $m(i+1) < m(i)$; the case $i> i_0$ is similar. First, we have that $m(i_0-1) \\geq m(i_0)$. By (iii) of Lemma \\ref{lem:Kproperties}, we have that $m(i_0-1) \\neq m(i_0)$, so  $m(i_0-1) > m(i_0)$. Suppose now that\n\\[ m(i_0-l) < m(i_0-l+1) < \\cdots < m(i_0).\\]\nLet $p$ be the unique point in $\\gamma \\cap K_{m(i_0-l)}^{j(i_0-l)} \\cap K_{m(i_0-l+1)}^{j(i_0-l+1)}$ and let $q$ be the unique point in $\\gamma \\cap K_{m(i_0-l-1)}^{j(i_0-l-1)} \\cap K_{m(i_0-l)}^{j(i_0-l)}$. Since $\\gamma$ traverses $K_{m(i_0-l)}^{j(i_0-l)}$, we have that $p\\neq q$. Moreover, by (ii) of Lemma \\ref{lem:Kproperties} we have that $K_{m(i_0-l)}^{j(i_0-l)} \\cap T_{m(i_0-l+1)} = \\{p\\}$. Therefore, $m(i_0-l-1) \\geq m(i_0-l)$ and by (iii) of Lemma \\ref{lem:Kproperties}, we have that $m(i_0-l-1) > m(i_0-l)$, and so the inductive step is complete.\n\\end{proof}\n\n\\section{Construction of the embedding and proof of Theorem \\ref{thm:main}}\\label{sec:embedding} \n\nLet $T$ be a quasiconformal tree. In this section, we prove Theorem \\ref{thm:main} by showing that $T$ admits a bi-Lipschitz embedding into some Euclidean space, with distortion and dimension depending only on the doubling and bounded turning constants of $T$.\n\nAs a first simplification, we may modify $T$ by a bi-Lipschitz deformation (whose distortion depends only on the bounded turning constant) so that it is bounded turning with constant $1$ (See \\cite[Lemma 2.5]{BM1}.)  Because $T$ is $1$-bounded turning, we have ${\\rm diam}(T)={\\rm diam}(\\mathcal{L}(T))$. We may also rescale so that ${\\rm diam}(T)={\\rm diam}(\\mathcal{L}(T))=1$. Let $D$ be the doubling constant of $T$, after these modifications. Throughout this section, in all statements and proofs we make these assumptions.\n\nFinally, we assume without loss of generality that the set $\\mathcal{L}(T)$ of leaves of $T$ is finite. This assumption is justified by the following basic fact in metric embeddings: A compact metric space $X$ admits an $(a,b)$-bi-Lipschitz embedding into $\\mathbb{R}^k$ if and only if every finite subset of $X$ admits an $(a,b)$-bi-Lipschitz embedding into $\\mathbb{R}^k$. (See, e.g., \\cite[equation (1)]{NN} or \\cite[Lemma 4.9]{Assouad}.) Thus, if we prove Theorem \\ref{thm:main} only for trees with finitely many leaves and we wish to embed an arbitrary quasiconformal tree $T$, we may apply the theorem to the convex hull of every finite subset of $T$ (which are all uniformly doubling, bounded turning trees), and conclude that Theorem \\ref{thm:main} holds for $T$ by the ``basic fact'' mentioned above.\n\nTo prove Theorem \\ref{thm:main}, it thus suffices to embed a $D$-doubling, $1$-bounded-turning tree $T$ with finite leaf set into some $\\mathbb{R}^k$, with $k$ and the distortion depending only on $D$. These assumptions on $T$ are now in force. We first apply the construction of the previous section to obtain sets $K_n^j$ satisfying the properties of Section \\ref{sec:decomposition}.\n\nOur first step is to construct ``local'' bi-Lipschitz embeddings on each piece $K_n^j$.\n\\begin{lemma}\\label{lem:localembedding}\nThere are constants $L=L(D)\\geq 1$ and $d=d(D)\\in\\mathbb{N}$ such that, for each $n\\geq 1$ and $j\\in J_n$, there is a $(1,L)$-bi-Lipschitz embedding\n$$f_n^j\\colon K_n^j \\rightarrow \\mathbb{R}^d$$\nsuch that $f_n^j(r_n^j)=0$.\n\nMoreover, this embedding extends to an L-Lipschitz map $f_n^j\\colon T \\rightarrow \\mathbb{R}^d$ that is constant on each component of $T\\setminus K_{n}^j$. In particular, $f_n^j$ is constant on $K_m^i$ if $(m,i)\\neq (n,j)$.\n\\end{lemma}\n\n\\begin{proof}\nFix any $K_n^j$. The set $K_n^j$ is a union of at most $C=C(D)$ arcs, by Lemma \\ref{lem:components2}. Each such arc is $D$-doubling and $1$-bounded turning, and therefore admits a bi-Lipschitz embedding into $\\mathbb{R}^{d_0}$, $d_0=d_0(D)$ and bi-Lipschitz constants depending only on $D$. (This is \\cite[Proposition 8.1]{DV}.) By \\cite[Theorem 8.2]{LP}, we see that $K_n^j$ then admits an bi-Lipschitz embedding $f_n^j$ into some $\\mathbb{R}^{d}$, for $d=d(D)$ and constants dpending on $D$ and $C=C(D)$. By rescaling and translating this embedding, we can ensure that it is $(1,L)$ bi-Lipschitz for $L=L(D)$ and maps $r_n^j$ to the origin.\n\nTo construct the Lipschitz extension, let $K$ be a component of $T\\setminus K_n^j$. Then $\\overline{K}\\cap K_n^j$ consists of exactly one point, $p$. Indeed, this intersection contains at least one point as $T$ is connected. Suppose it contained distinct points $p\\neq q$, in which case it would also contain the arc $[p,q]$. Let $p',q'$ be a points of $K$ with distance at most $d(p,q)/10$ from $p,q$, respectively. Then $[p',q'] \\cup [q',q] \\cup [q,p]$ would contain an arc joining $p$ and $p'$ of diameter at least $d(p,q)/2 > d(p,p')$, contradicting the bounded turning property.\n\nWe thus set $f_n^j(x) = f_n^j(p)$ for all $x\\in K$. This extends $f_n^j$ to all of $T$ in a way that it constant on each component of $T\\setminus K_n^j$.\n\nTo prove that the extension is $L$-Lipschitz, consider $x, y\\in T$. If $[x,y]$ is disjoint from $K_n^j$, then $x$ and $y$ lie in the same component of $T\\setminus K_n^j$ and thus $|f(x)-f(y)|=0$. Otherwise, let $p$ and $q$ be the first and last points, respectively, on $[x,y]\\cap K_n^j$. Then either $p=x$ or $p$ is the unique point in both $K_n^j$ and in the component of $T\\setminus K_n^j$ containing $x$; in either case, we have $f_n^j(x)=f_n^j(p)$. Similarly, $f_n^j(y)=f_n^j(q)$. Therefore,\n$$ |f_n^j(x)-f_n^j(y)|=|f_n^j(p)-f_n^j(q)| \\leq L d(p,q) \\leq L{\\rm diam}([p,q]) \\leq L{\\rm diam}([x,y]) \\leq Ld(x,y),$$\nusing the known fact that $f_n^j$ is $L$-Lipschitz on $K_n^j$ itself.\n\nFor the final, ``in particular'' statement, suppose $(n,j)\\neq (m,i)$. By Lemma \\ref{lem:Kproperties}, $K_n^j \\cap K_m^i$ contains at most one point, $p$. If $x,y\\in K_m^i$ and $[x,y]$ does not contain this point (or there is no such point), then $[x,y]$ lies in a component of $T\\setminus K_n^j$ and so $f_n^j(x)=f_n^j(y)$. Otherwise, $x$ and $p$ lie in the closure of a component of $T\\setminus K_n^j$, and the same holds for $p$ and $y$, and so $f_n^j(x)=f_n^j(p)=f_n^j(y)$. Thus, $f_n^j$ is constant on $K_m^i$.\n\\end{proof}\n\n\\begin{lemma}\\label{lem:localzero}\nIf $x\\in T$, $n\\in\\mathbb{N}$, $j\\in J_n$, then $f_n^j(x)=0$ unless the arc $[r_1^1, x]$ traverses $K_n^j$.\n\\end{lemma}\n\\begin{proof}\nFirst, observe that $f_n^j(r_1^1)=0$ for every $n\\in\\mathbb{N}$ and $j\\in J_n$. If $n=1$ this is by definition of $f_1^1$. Otherwise, this is because $f_n^j(r_n^j)=0$ by definition of $f_n^j$ and $r_n^j$ and $r_1^1$ are joined by an arc in $T_{n-1}$ whose only intersection with $K_n^j$ is $r_n^j$. Therefore, $r_n^j$ and $r_1^1$ lie in the closure of a common component of $T\\setminus K_n^j$.\n\nNow suppose that $[r_1^1, x]$ does not traverse $K_n^j$. \n\nIf $[r_1^1,x]$ is disjoint from $K_n^j$, then $r_1^1$ and $x$ are in the same component of $T\\setminus K_n^j$ and $f_n^j(x)=f_n^j(r_1^1)=0$.\n\nIf $[r_1^1,x]$ is not disjoint from $K_n^j$, then it intersects $K_n^j$ in exactly one point, $y$. It follows that $f_n^j(r_1^1)=f_n^j(y)$ and $f_n^j(y)=f_n^j(x)$, hence $f_n^j(x)=0$.\n\\end{proof}\n\nWe fix constants before proceeding. Let $M=M(D)$ be as in Lemma \\ref{lem:arcintersection}, and let $L=L(D), d=d(D)$ be as in Lemma \\ref{lem:localembedding}. Let $N\\in\\mathbb{N}$ be chosen sufficiently large depending only on $L$ and $M$ (hence only on $D$); specifically, we will require that\n$$ \\frac{1}{2\\sqrt{2N+M+1}} - L2^{3-N} > 0.$$\nLet $S=2^{N}$. Finally, let $A=A(D)$ be the number of colors produced by Lemma \\ref{lem:coloring} using this choice of $S$.\n\nWe now define our bi-Lipschitz embedding $f\\colon T \\rightarrow \\mathbb{R}^{Ad}$. We use the $L$-Lipschitz mappings $f_n^j\\colon T \\rightarrow \\mathbb{R}^d$ defined in Lemma \\ref{lem:localembedding}. View $\\mathbb{R}^{Ad}$ as $\\bigoplus_{\\ell=1}^A \\mathbb{R}^d_\\ell$, with each $\\mathbb{R}^d_\\ell$ a copy of $\\mathbb{R}^d$.  Making a simple adjustment to $f_n^j$, we post-compose with an isometric embedding so that \n$$ f_n^j \\colon T \\rightarrow \\mathbb{R}^d_{\\chi((n,j))} \\subseteq \\mathbb{R}^{Ad},$$\nwhere $\\chi$ is the coloring from Lemma \\ref{lem:coloring}, and $f_n^j(r_n^j) = 0$.\n\nWe now define $f\\colon T \\rightarrow \\mathbb{R}^{Ad}$ by\n\\begin{equation}\\label{eq:fdef}\nf(x) = \\sum_{n\\in \\mathbb{N}} \\sum_{j\\in J_n} f_n^j(x).\n\\end{equation}\nBecause of our assumption that the leaf set of $T$ is finite, only finitely many of the sets $J_n$ are non-empty, and hence this sum is always finite. \n\n\n\\begin{proof}[Proof of Theorem \\ref{thm:main}] \n\nRecall that, by the discussion at the beginning of this section, we have reduced the embedding problem to the case of a $1$-bounded turning tree $T$ with $\\mathcal{L}(T)$ finite and ${\\rm diam}(T)={\\rm diam}(\\mathcal{L}(T))=1$. We will show that, under these assumptions, $f$ as defined in \\eqref{eq:fdef} is $(a,b)$-bi-Lipschitz with constants $a$ and $b$, depending only on $D$, to be described below.\n\nFix $x,y \\in T$ with $x\\neq y$. \n\nLet $\\{K_{m(i)}^{j(i)}\\}_{i\\in I}$ be the (finite) collection of sets $K_m^j$ that are traversed by $[x,y]$. Here we set $I$ to be a finite index set $\\{1, 2, \\dots, \\max(I)\\}$, and the sets are ordered in the natural order along the arc $[x,y]$. By Lemma \\ref{lem:monotone}, there is an index $i_0\\in I$ such that $m(i) > m(i+1)$ for all $i<i_0$ and $m(i) < m(i+1)$ for all $i\\geq i_0$. \n\nFor each $i\\in I$, let $p_i$ be the first point on $[x,y] \\cap K_{m(i)}^{j(i)}$. We also add one additional point at the end of $I$ and set $p_{\\max(I)}=y$. Because $x$ is contained in a set traversed by $\\gamma$ (see the remark above Lemma \\ref{lem:monotone}), we also have $p_{1} = x$.\n\nWe can now write a telescoping sum:\n\\begin{equation}\\label{eq:telescope}\n f(x) - f(y) = \\sum_{i\\in I} \\left(f(p_i) - f(p_{i+1})\\right).\n\\end{equation}\n\nLet $n\\in {\\mathbb{N}}$ be such that\n$$ 2^{-n-1} \\leq d(x,y) \\leq 2^{-n}.$$\n\n\nDefine indices $i_*$ and $i^*$ in $I$ as follows:\n$$ i_* = \\min\\{ i\\in I: m(i)\\leq n\\} \\text{ and } i^* = \\max\\{i\\in I: m(i)\\leq n\\}.$$\n(We set $i_*=i^*=i_0$ if the associated set is empty.)\nBy Lemmas \\ref{lem:arcintersection} and \\ref{lem:monotone}, we have $i^*-i_* \\leq M=M(D)$. \n\nThe restriction of $f$ to a single $K_m^j$ is $(1,L)$-bi-Lipschitz, because $f_m^j$ is $(1,L)$-bi-Lipschitz on $K_m^j$ and each $f_n^i$ for $(n,i)\\neq (m,j)$ is constant on $K_m^j$. (See Lemma \\ref{lem:localembedding}.) Note also that $p_i$ and $p_{i+1}$ are both always in $K_{m(i)}^{j(i)}$. Therefore, we always have\n\\begin{equation}\\label{eq:fbound1}\n d(p_i, p_{i+1}) \\leq |f(p_i) - f(p_{i+1})|\\leq L{\\rm diam}(K_{m(i)}^{j(i)}) \\leq L2^{2-m(i)}.\n\\end{equation}\nIn addition, the bounded turning condition and the fact that $p_i\\in [x,y]$ ensures that we also always have\n\\begin{equation}\\label{eq:fbound2}\n|f(p_i) - f(p_{i+1})| \\leq Ld(p_i, p_{i+1}) \\leq Ld(x,y).\n\\end{equation}\n\nUsing equations \\eqref{eq:fbound1} and \\eqref{eq:fbound2}, we obtain:\n\\begin{align*}\n|f(x) - f(y)| &\\leq \\sum_{i\\in I} |f(p_i) - f(p_{i+1})|\\\\\n&\\leq \\sum_{i\\in I, i<i_*}  |f(p_i) - f(p_{i+1})| + \\sum_{i\\in I, i_*\\leq i \\leq i^*}  |f(p_i) - f(p_{i+1})| + \\sum_{i\\in I, i>i^*}  |f(p_i) - f(p_{i+1})|\\\\\n&\\leq L\\left(\\sum_{i\\in I, i<i_*} 2^{2-m(i)} + \\sum_{i\\in I, i_*\\leq i \\leq i^*} d(x,y)  + \\sum_{i\\in I, i>i^*}  2^{2-m(i)}\\right)\\\\\n&\\lesssim L 2^{-m(i_*-1)} + Md(x,y) + L 2^{-m(i^*+1)}\\\\\n&\\lesssim L 2^{-n} + Md(x,y) + L2^{-n}\\\\\n&\\lesssim d(x,y) \n\\end{align*}\nThis proves that $f$ is Lipschitz (with constant depending only on $D$).\n\nFor the lower bound, we similarly break up the sum in \\eqref{eq:telescope} into three pieces, but at different points. Define indices $\\underline{i}$ and $\\overline{i}$ in $I$ as follows:\n$$ \\underline{i} = \\min\\{ i\\in I: m(i)\\leq n+N\\} \\text{ and } \\overline{i} = \\max\\{i\\in I: m(i)\\leq n+N\\}.$$\n(We interpret $\\underline{i}=\\overline{i}=i_0$if the associated set is empty, although in fact the proof shows that this cannot happen with our choice of $N$.)\nNote that $\\overline{i}-\\underline{i}\\leq 2N + M$ by Lemmas \\ref{lem:arcintersection} and \\ref{lem:monotone}.\n\nNote that \n\\begin{equation}\\label{eq:tails}\n\\sum_{i>\\overline{i}} d(p_i, p_{i+1}) \\leq \\sum_{i>\\overline{i}} {\\rm diam}(K_{m(i)}^{j(i)}) \\leq \\sum_{i>\\overline{i}} 2^{2-m(i)} \\leq 2^{2-(n+N)}.\n\\end{equation}\nThe same bound holds for the sum over $i< \\underline{i}$.\n\n\nNow we note that if $\\underline{i}\\leq i<i' \\leq \\overline{i}$, then $f(p_i)-f(p_{i+1})$ and $f(p_{i'})-f(p_{i'+1})$ lie in orthogonal subspaces $\\mathbb{R}^d_\\ell$ and $\\mathbb{R}^d_{\\ell'}$. To see this, note that $p_i$ and $p_{i+1}$ are both in $K_{m(i)}^{j(i)}$, and similarly for $p_{i'}$ and $p_{i'+1}$. Therefore, recalling the last statement of Lemma \\ref{lem:localembedding},\n$$ f(p_i) - f(p_{i+1}) \\in \\mathbb{R}^d_\\ell,$$\nwhere $\\ell = \\chi(m(i),j(i))$. Similarly, $f(p_i) - f(p_{i+1}) \\in \\mathbb{R}^d_{\\ell'}$, where $\\ell' = \\chi(m(i'),j(i'))$. To see that $\\ell\\neq \\ell'$, observe that\n$$ {\\rm dist}(K_{m(i)}^{j(i)}, K_{m(i')}^{j(i')}) \\leq d(x,y) \\leq 2^{-n} \\leq 2^N 2^{-\\max\\{m(i), m(i')\\}} = S2^{-\\max\\{m(i), m(i')\\}},$$\nso $\\chi((m(i),j(i)) \\neq \\chi((m(i'),j(i'))$ by the defining property of the coloring $\\chi$ from Lemma \\ref{lem:coloring}, and our choice of $S=2^N$.\n\n\nTherefore, by using \\eqref{eq:fbound1},  the Cauchy--Schwarz inequality, the fact that $\\sum_I d(p_i,p_{i+1})\\geq d(x,y)$, and  \\eqref{eq:tails}, we obtain  \n\\begin{align*}\n\\left| \\sum_{\\underline{i}\\leq i \\leq \\overline{i}} f(p_i)-f(p_{i+1}) \\right| &= \\left(\\sum_{\\underline{i}\\leq i \\leq \\overline{i}} |f(p_i)-f(p_{i+1})|^2 \\right)^{1/2}\\\\ \n&\\geq \\left(\\sum_{\\underline{i}\\leq i \\leq \\overline{i}} d(p_i, p_{i+1})^2\\right)^{1/2}\\\\\n&\\geq \\frac{1}{\\sqrt{\\overline{i}-\\underline{i}+1}} \\sum_{\\underline{i}\\leq i \\leq \\overline{i}} d(p_i, p_{i+1})\\\\\n&\\geq \\frac{1}{\\sqrt{\\overline{i}-\\underline{i}+1}}\\left( d(x,y) - \\sum_{i<\\underline{i} \\text{ or } i > \\overline{i}} d(p_i, p_{i+1})\\right)\\\\\n&\\geq \\frac{1}{\\sqrt{2N+M+1}}( d(x,y)  - 2^{3-(n+N)})\\\\\n&\\geq \\left(\\frac{1}{\\sqrt{2N+M+1}} - 2^{3-N}\\right)d(x,y)\\\\\n&\\geq \\frac{1}{2\\sqrt{2N+M+1}} d(x,y).\n\\end{align*}\n\nAgain using \\eqref{eq:tails} and the fact that the restriction of $f$ to each $K_m^j$ is $L$-Lipschitz, we therefore have\n\\begin{align*}\n|f(x)-f(y)| &\\geq \\left|\\sum_{\\underline{i}\\leq i \\leq \\overline{i}} f(p_i)-f(p_{i+1})\\right|  - \\sum_{i<\\underline{i} \\text{ or } i > \\overline{i}} |f(p_i)-f(p_{i+1})|\\\\\n&\\geq \\left|\\sum_{\\underline{i}\\leq i \\leq \\overline{i}} f(p_i)-f(p_{i+1})\\right|  - L\\sum_{i<\\underline{i} \\text{ or } i > \\overline{i}} d(p_i,p_{i+1})\\\\\n&\\geq \\frac{1}{2\\sqrt{2N+M+1}} d(x,y) -  L2^{3-(n+N)}\\\\\n&\\geq \\left(\\frac{1}{2\\sqrt{2N+M+1}} - L2^{3-N}\\right) d(x,y)\\\\\n&\\gtrsim d(x,y)\n\\end{align*}\nby our choice of $N$.\n\\end{proof}\n\n\n", "meta": {"timestamp": "2021-06-25T02:18:49", "yymm": "2106", "arxiv_id": "2106.13007", "language": "en", "url": "https://arxiv.org/abs/2106.13007"}}
{"text": "\\section{Introduction}\\label{intro}\nSince Dubrovin introduced the notion of Frobenius manifold in the beginning of 90's of the last century \\cite{dubrovin1993integrable, dubrovin1996geometry} inspired by the development of 2d topological field theory \\cite{dijkgraaf1991notes,dijkgraaf1991topological, kontsevich1992intersection,witten1990structure, witten1990two}, its relationship with hierarchies of integrable PDEs has become an important research subject in the theory of integrable systems and its applications in the study of Gromov-Witten invariants, singularity theory and quantum field theory, see some of the related works  \n\\cite{buryak2015double,\ndubrovin1998bihamiltonian, \ndubrovin1999frobenius, dubrovin2001normal, dubrovin2004virasoro, \neguchi1997quantum,\neguchi1995genus,fan2013witten,\ngetzler2001toda,givental2001gromov,  \ngivental2001semisimple, \ngivental2005simple, \nliu2015bcfg, milanov2016gromov,zhang2002cp} and references therein.\nIn \\cite{dubrovin2001normal}, Dubrovin and Zhang, the third author of the present paper, proposed a project to classify hierarchies of integrable PDEs which possess hydrodynamic limits and satisfy the following properties: each integrable hierarchy has a bihamiltonian structure, a tau function (also called a tau structure), and an infinite set of Virasoro symmetries that act linearly on the tau function. The bihamiltonian structure of the integrable hierarchy is assumed to be represented as a formal power series w.r.t. the dispersion parameter $\\varepsilon$, its coefficients are given by homogeneous differential polynomials of the dependent variables of the integrable hierarchy, and its dispersionless limit\nis a bihamiltonian structure of hydrodynamic type. The existence of a bihamiltonian structure and of a tau function implies that the dispersionless limit of the integrable hierarchy can be described by a Frobenius manifold structure or a degenerate one. On the other hand, starting from any Frobenius manifold, one can construct an integrable hierarchy of hydrodynamic type which is called the Principal Hierarchy of the Frobenius manifold. The flat metric and the intersection form of the Frobenius manifold yield a bihamiltonian structure of the Principal Hierarchy which also possesses a tau function and an infinite set of Virasoro symmetries. The Virasoro symmetries, apart from the first two, of the Principal Hierarchy do not act linearly on the tau function. Under the semisimplicity condition of the Frobenius manifold, the requirement of linear actions of the Virasoro symmetries on the tau function leads to a dispersionful deformation, called the topological deformation, of the Principal hierarchy. As it is shown in \\cite{dubrovin2001normal}, the topological deformation of the Principal Hierarchy is constructed via a quasi-Miura transformation determined by the loop equation of the semisimple Frobenius manifold.\n\nTo classify dispersionful deformations of the Principal Hierarchy, Dubrovin and Zhang introduced in \\cite{dubrovin2001normal} the notion of bihamiltonian cohomology $\\bh^k(M; P_0, P_1)$ for a bihamiltonian structure $(P_0, P_1)$ of hydrodynamic type defined on the jet space of a smooth manifold $M$. The cohomology groups $\\bh^2(M; P_0, P_1)$ and $\\bh^3(M; P_0, P_1)$ characterize the equivalence classes of infinitesimal deformations of $(P_0, P_1)$ under Miura type transformations and the obstruction of extending an infinitesimal deformations to a full deformation respectively. Here we emphasize that the deformed bihamiltonian structures are required to have the property that the coefficients of the powers of the deformation parameter are given by homogeneous differential polynomials of the dependent variables, this property is called the {\\em polynomiality} of the deformed Hamiltonian structures. \n\nIn \\cite{lorenzoni2002deformations} Lorenzoni studied the deformations of the bihamiltonian structure of the dispersionless KdV hierarchy and showed,\nup to the fourth order approximation of the deformation parameter, that they are parametrized by a function of one variable.\nThe very first computation of the bihamiltonian cohomology groups for a semisimple bihamiltonian structure of hydrodynamic type is given in \\cite{ DLZ-1,liu2005deformations}, from which the cohomology group $BH^2_{\\ge 2}(M; P_0, P_1)$ can be obtained, and it shows that the equivalence classes of the infinitesimal deformations of $(P_0, P_1)$ with $n$ dependent variables are parametrized by a set of smooth functions $c_1(u_1), \\dots, c_n(u_n)$, where $u_1, \\dots, u_n$ are the canonical coordinates of the semisimple bihamiltonian structure of hydrodynamic type. These functions are called the central invariants of the deformed bihamiltonian structure. It is also conjectured that the cohomology group $\\bh^3(M; P_0, P_1)$ is trivial, and that any infinitesimal deformation of $(P_0, P_1)$ can be extended to a full deformation. \n\nAn important observation toward computing the cohomology group $\\bh^3(M; P_1, P_2)$ was made by the first and third author of the present paper in \\cite{liu2013bihamiltonian}, where they showed that one can work in the space of differential polynomials instead of the space of the local functionals to compute the bihamiltonian cohomology. By using this idea they proved the afore mentioned conjecture for the bihamiltonian structure of hydrodynamic type associated to the one dimensional Frobenius manifold (or the dispersionless KdV hierarchy). Base on the method of \\cite{liu2013bihamiltonian} and on a clever utilizing of the technique of spectral sequences and some other tools from homological algebra, Carlet, Posthuma and Shadrin proved this conjecture for a general semisimple bihamiltonian structure of hydrodynamic type in \\cite{carlet2018deformations}. \n \nA deformation of the bihamiltonian structure of the Principal hierarchy of a semisimple Frobenius manifold yields a deformation of the integrable hierarchy. It is shown in \\cite{dubrovin2018bihamiltonian} that the deformed integrable hierarchy possesses a tau structure when the central invariants of the deformed bihamiltonian structure are constant functions. \nHowever, it remains to be unclear whether this deformed integrable hierarchy also possesses an infinite set of Virasoro symmetries. \nIn particular, it is conjectured that the deformation of the Principal Hierarchy with all the central invariants being equal to $\\frac1{24}$ possesses an infinite set of Virasoro symmetries which act linearly on the tau function of the deformed integrable hierarchy, or in other words, it coincides with the topological deformation of the Principal Hierarchy. \nAn approach to prove the validity of this conjecture is as follows: one needs to show that the quasi-Miura transformation, which transforms the Principal Hierarchy to its topological deformation, also transforms the bihamiltonian structure of hydrodynamic type to a deformed bihamiltonian structure which satisfies the polynomiality property. In \\cite{buryak2012deformations, buryak2012polynomial} Buryak, Posthuma and Shadrin proved the polynomiality property of the first deformed Hamiltonian structure of the Principal Hierarchy, and in the recent preprint \n\\cite{iglesias2021bi} Iglesias and Shadrin provide a certain evidence of the validity of the polynomiality property of the second Hamiltonian structure.\n\nThe main purpose of the present paper and of its subsequents is to show that the deformation of the Principal Hierarchy of a semisimple Frobenius manifold which is associated to a deformed bihamiltonian structure with constant central invariants possesses an infinite set of Virasoro symmetries, and these Virasoro symmetries can be lifted to the actions on the tau function of the deformed integrable hierarchy; moreover, for the case when all the central invariants equal to $\\frac1{24}$, the Virasoro symmetries act linearly on the tau function, and so this deformed integrable hierarchy is equivalent to the topological deformation of the Principal Hierarchy. In particular, this implies that the topological deformation of the Principal Hierarchy of a semisimple Frobenius manifold possesses a bihamiltonian structure which satisfies the polynomiality property. To this end, we first generalize in the present paper the notion of bihamiltonian cohomology by considering a certain cohomology, called the variational bihamiltonian cohomology, on the space of variational $1$-forms of the infinite jet space of the dependent variables, and apply it to the study of Virasoro symmetries of the deformed integrable hierarchy in the subsequent papers.\n\nThe paper is organized as follows. In Sect.\\,\\ref{ms}, we recall the basic notions of bihamiltonian structures in terms of local functionals on the infinite jet bundle of a super manifold, and we present the main results of this paper. In Sect.\\,\\ref{vbc}, we first define the variational Hamiltonian cohomology and prove the triviality of the cohomology groups, and then we define the variational bihamiltonian cohomology. Sect.\\,\\ref{vbh23} and Sect.\\,\\ref{vsh} are both devoted to the computation of the variational bihamiltonian cohomology groups. As an illustration of the application of this theory, we classify the conformal bihamiltonian structures and their deformations in Sect.\\,\\ref{conformal}. In Sect.\\,\\ref{con} we give some concluding remarks.\n\n\n\\section{Basic notions and Main results}\\label{ms}\nIn this section, we first recall some basic definitions and constructions on bihamiltonian structures in terms of the language of super variables. The readers may refer to \\cite{liu2011jacobi, liu2013bihamiltonian} and the  lecture note \\cite{liu2018lecture} for a detailed introduction to this topic. \n\nLet $M$ be an $n$-dimensional smooth manifold, and $\\hat M = \\Pi(T^*M)$ be the super manifold of dimension $(n\\mid n)$ obtained by reversing the parity of the fiber of the cotangent bundle of $M$. Locally, one may choose a coordinate system $(u^1,\\cdots,u^n)$ for an open neighborhood $U\\subset M$ and we will use  $(\\theta_1,\\cdots,\\theta_n)$ to denote the coordinate system of the fibers. The variables $\\theta_i$ are fermion, meaning that they satisfy the anti-commutation relations $\\theta_i\\theta_j + \\theta_j\\theta_i = 0$. Let $J^\\infty(\\hat M)$ be the infinite jet bundle of $\\hat M$ and let $\\hat{\\mathcal A}$ be the ring of differential polynomials. In local coordinates, we can write it as\n\\[\\hat{\\mathcal A} = C^\\infty(U)[[u^{i,s+1}, \\theta^s_i\\mid i = 1,\\cdots,n;s\\geq 0]].\\]\nHere and henceforth we denote $u^{i,0} = u^i$ and ${\\theta_i^0} = \\theta_i$. The jet variables $u^{i,s}, \\theta_i^s$ for $s\\geq 1\t$ may be regarded as $s$-th order derivatives with respect to an independent variable, which we usually denote as $x$,  when regarding $u^i$ and $\\theta_i$ as dependent variables. This identification is possible due to the existence of the global vector field\n\\begin{equation}\\label{dx-zh}\n\\partial_x = \\sum_{s\\geq 0}u^{i,s+1}\\diff{}{u^{i,s}}+\\theta_i^{s+1}\\diff{}{\\theta_i^s}.\n\\end{equation}\nSometimes we will use a prime to denote $\\partial_x$, i.e. $f':=\\partial_xf$. The elements in the quotient space $\\hat{\\mathcal F} = \\hat{\\mathcal A}/\\partial_x$ are called local functionals and for an element $f\\in\\hat{\\mathcal A}$, we will use $\\int f$ to denote its class in $\\hat{\\mathcal F}$. Let us define two degrees $\\deg_{x}$ and $\\deg_\\theta$ on the space $\\hm A$, which are called respectively the differential gradation and the super gradation. On the generators of $\\hm A$, these degrees are defined by\n\\[\n\\deg_x u^{i,s} = \\deg_x \\theta^{s}_i = s;\\quad \\deg_\\theta u^{i,s} = 0,\\quad \\deg_\\theta \\theta_i^s = 1.\n\\]\nWe denote the space of homogeneous elements by\n\\[\n\\hat{\\mathcal A}_d = \\{f\\in\\hat{\\mathcal A}\\mid \\deg_x f = d\\},\\quad \\hat{\\mathcal A}^p = \\{f\\in\\hat{\\mathcal A}\\mid  \\deg_\\theta f = p\\},\\quad \\hat{\\mathcal A}^p_d = \\hat{\\mathcal A}^p\\cap \\hat{\\mathcal A}_d.\n\\]\nIt is clear that the vector field $\\partial_x$ is homogeneous with respect to both degrees, hence the gradations on $\\hat{\\mathcal A}$ induce a gradation on $\\hat{\\mathcal F}$ and we will denote the space of homogeneous elements by $\\hat{\\mathcal F}_d$, $\\hat{\\mathcal F}^p$ and $\\hat{\\mathcal F}^p_d$ accordingly.\n\nA nontrivial construction called the Schouten-Nijenhuis bracket equips the space $\\hat{\\mathcal F}$ with a graded Lie algebra structure. Define the variational derivative of an element in $\\hat{\\mathcal A}$ by\n\\[\n\\vard{f}{u^i} = \\sum_{s\\geq 0}(-\\partial_x)^s\\diff{f}{u^{i,s}},\\quad \\vard{f}{\\theta_i^s} = \\sum_{s\\geq 0}(-\\partial_x)^s\\diff{f}{\\theta_i^s}.\n\\]\nIt can be shown that the variational derivative annihilates the image of $\\partial_x$ so we can define the variational derivative of a local functional as follows:\n\\[\\vard{F}{u^i} = \\int \\vard{f}{u^i},\\quad \\vard{F}{\\theta_i} = \\int \\vard{f}{\\theta_i},\\quad F = \\int f\\in\\hat{\\mathcal F}.\n\\]\nThen the Schouten-Nijenhuis bracket is defined to be the following bilinear map \n\\[[-,\\ -]: \\hat{\\mathcal F}^p\\times \\hat{\\mathcal F}^q\\to\\hat{\\mathcal F}^{p+q-1},\\quad\n[P,Q] = \\int \\sum_i \\vard{P}{\\theta_i}\\vard{Q}{u^i}+(-1)^p\\vard{P}{u^i}\\vard{Q}{\\theta_i}.\n\\]\nThis bracket defines a graded Lie algebra structure on $\\hm F$, whose sign convention is different from the usual definition, for more details one may refer to \\cite{liu2011jacobi}.\n\nFor a local functional $P\\in\\hat{\\mathcal F}^p$, we can associate a derivation on $\\hat{\\mathcal A}$ defined by\n\\begin{equation}\n\\label{def-d}\nD_P = \\sum_{i}\\sum_{s\\geq 0}\\partial_x^s\\left(\\vard{P}{\\theta_i}\\right)\\diff{}{u^{i,s}}+(-1)^p\\partial_x^s\\left(\\vard{P}{u^i}\\right)\\diff{}{\\theta^{s}_i}.\n\\end{equation}\nWe see that $D_P$ is a derivation of super degree $p-1$ and that $[D_P,\\partial_x] = 0$. The following property is important for the construction of the cohomologies that will be given later in this and the next section:\n\\begin{equation}\n\\label{comm-d}\n(-1)^{p-1}D_{[P,Q]} = D_P\\comp D_Q-(-1)^{(p-1)(q-1)}D_Q\\comp D_P;\\quad P\\in\\hat{\\mathcal F}^p,\\ Q\\in\\hat{\\mathcal F}^q.\n\\end{equation}\nThis relation shows that the map $D$ induces a (graded) Lie algebra homomorphism\n\\[\\hm F\\to\\mathrm{Der}(\\hat{\\mathcal A}):\\quad P\\mapsto D_P,\\]\nhere the natural graded Lie algebra structure on the space of derivations on $\\hat{\\mathcal A}$, denoted by $\\mathrm{Der}(\\hat{\\mathcal A})$, is given by the graded commutators.\nIn what follows, we will also call an element of $\\mathrm{Der}(\\hat{\\mathcal A})$ a vector field on $J^\\infty(\\hat M)$.\n\nUsing the above notions, a Hamiltonian structure is defined to be a local functional $P\\in\\hat{\\mathcal F}^2$ satisfying $[P,P] = 0$, and it is called of hydrodynamic type if $P\\in \\hat{\\mathcal F}^2_1$. A bihamiltonian structure is a pair of Hamiltonian structure $(P_0, P_1)$ satisfying an additional compatibility condition $[P_0, P_1] = 0$. Now assume that we have a bihamiltonian structure $(P_0,P_1)$ of hydrodynamic type \\cite{liu2018lecture}, locally we represent them as follows:\n\\[\nP_a = \\frac 12\\int\\sum g^{ij}_a(u)\\theta_i\\theta_j^1+\\Gamma^{ij}_{a,k}(u)u^{k,1}\\theta_i\\theta_j,\\quad a = 1,2.\n\\]\nThis bihamiltonian structure is called semisimple if the roots $\\lambda^1(u),\\cdots,\\lambda^n(u)$ of the characteristic equation $\\det(g_1^{ij}-\\lambda g_0^{ij}) = 0$ are distinct and not constant.\n\nIt is proved in \\cite{ferapontov2001compatible} that if $(P_0, P_1)$ is semisimple, then the roots $\\lambda^1(u),\\cdots,\\lambda^n(u)$ can serve as local coordinates of the manifold $M$ and in such a coordinate system $(P_0, P_1)$ can be represented in the following forms:\n\\[\nP_0 = \\frac 12\\int \\sum f^i(\\lambda)\\theta_i\\theta_i^1+ A^{ij}\\theta_i\\theta_j,\\quad P_1 = \\frac 12\\int \\sum g^i(\\lambda)\\theta_i\\theta_i^1+ B^{ij}\\theta_i\\theta_j,\n\\]\nwhere $f^i$ are non-vanishing functions, $g^i = \\lambda^if^i$ and the functions $A^{ij}$ and $B^{ij}$ are given by\n\\[\nA^{ij} = \\frac 12\\left(\\frac{f^i}{f^j}\\diff{f^j}{\\lambda^i}\\lambda^{j,1}-\\frac{f^j}{f^i}\\diff{f^i}{\\lambda^j}\\lambda^{i,1}\\right),\\quad B^{ij} = \\frac 12\\left(\\frac{g^i}{f^j}\\diff{f^j}{\\lambda^i}\\lambda^{j,1}-\\frac{g^j}{f^i}\\diff{f^i}{\\lambda^j}\\lambda^{i,1}\\right).\n\\]\nThe coordinates $\\lambda^1,\\cdots,\\lambda^n$ are called the canonical coordinates of $(P_0, P_1)$.\n\nIn what follows, when we consider semisimple bihamiltonian structures of hydrodynamic type, it is always assumed that we choose the canonical coordinates as local coordinates. We will use $u^1,\\cdots,u^n$ instead of $\\lambda^1,\\cdots,\\lambda^n$ to denote them. In these coordinates, we represent a bihamiltonian structure $(P_0, P_1)$ of hydrodynamic type as\n\\begin{equation}\n\\label{norm-p}\nP_0 = \\frac 12\\int \\sum f^i(u)\\theta_i\\theta_i^1+ A^{ij}\\theta_i\\theta_j,\\quad P_1 = \\frac 12\\int \\sum u^if^i(u)\\theta_i\\theta_i^1+ B^{ij}\\theta_i\\theta_j.\n\\end{equation}\n\nGiven a hydrodynamic bihamiltonian structure $(P_0,P_1)$, we construct the variational bihamiltonian cohomology as follows. Consider the space of 1-forms $\\Omega$ of the infinite jet space $J^\\infty(\\hat M)$, locally it is an $\\hm A$-module generated by $\\delta u^{i,s}$ and $\\delta\\theta_i^s$ for $i = 1,\\cdots, n$ and $s\\geq 0$:\n\\[\n\\Omega = \\biggl\\{\\sum_{i;s\\geq 0}g_{i,s}\\delta u^{i,s}+h^i_s\\delta\\theta_i^s\\mid g_{i,s}, h^i_s\\in\\hm A\\biggr\\}.\n\\]\nEach derivation of $\\hm A$ induces an action on $\\Omega$ by the Lie derivative (see Sect.\\,\\ref{vbc} for details). In particular we consider the action of Lie derivative of $\\partial_x$ on $\\Omega$, which we still denote by $\\partial_x$, and we denote by $\\bar\\Omega$ the quotient space $\\Omega/{\\partial_x\\Omega}$. We can verify that the actions on $\\bar\\Omega$ given by the Lie derivatives of $D_{P_0}$ and $D_{P_1}$, which we denote by $\\tilde D_0$ and $\\tilde D_1$ respectively, equip $\\bar\\Omega$ a structure of double complex. We grade the space $\\bar\\Omega$ similarly as we do for the space $\\hm F$ by setting \n\\[\n\\deg_x \\delta u^{i,s} = \\deg_x \\delta\\theta^{s}_i = s;\\quad \\deg_\\theta \\delta u^{i,s} = 0,\\quad \\deg_\\theta \\delta \\theta_i^s = 1.\n\\]\nThen we define the variational bihamiltonian cohomology groups as follows:\n\\[\\vbh^p_d(\\bar\\Omega,\\tilde D_0,\\tilde D_1) = \\frac{\\bar\\Omega_d^p\\cap\\ker \\tilde D_0\\cap\\ker\\tilde D_1}{\\bar\\Omega_d^p\\cap\\ima\\tilde D_0\\tilde D_1}.\\]\n\nWe prove the following theorem in Sect.\\! \\ref{vbh23} and Sect.\\! \\ref{vsh}.\n\\begin{Th}\n\\label{vbh-res}\nThe variational bihamiltonian cohomology groups for a semisimple bihamiltonian structure $(P_0,P_1)$ of hydrodynamic type have the following properties:\n\\begin{enumerate}\n\\item[\\rm{i)}] $\\vbh^2_3(\\bar\\Omega,\\tilde D_0,\\tilde D_1)\\cong\\oplus_{i=1}^nC^\\infty(\\mathbb R);$\n\\item[\\rm{ii)}] If $d\\geq 2$ and if both $(p,d)$ and $(p+1,d)$ are NOT in the index set $I$, then $\\vbh^p_d(\\bar\\Omega,\\tilde D_0,\\tilde D_1) = 0$, where the index set is defined by $I = I_1\\cup I_2\\cup I_3$ with\n\\begin{align*}\nI_1 &= \\{(i, j)\\mid j = 0,1;\\ i = j+1,\\cdots,j+n+1\\};\\\\\nI_2 &= \\{(i,j)\\mid j = 2,\\cdots,n;\\ i = j,\\cdots,j+n+1\\};\\\\\nI_3 &= \\{(i,j)\\mid j = n+1,n+2,n+3;\\ i = j,\\cdots,j+n\\}.\n\\end{align*}\n\\item[\\rm{iii)}] $\\vbh^1_2(\\bar\\Omega,\\tilde D_0,\\tilde D_1) = 0.$\n\\end{enumerate}\n\\end{Th}\n\nTo illustrate the application of the variational bihamiltonian cohomology, in Sect.\\! \\ref{conformal} we study the conformal property of bihamiltonian structures which is shared by the bihamiltonian structures associated with Frobenius manifolds. Let us introduce the notations\n\\begin{align*}\n&\\mathrm{Der}(\\hat{\\mathcal A})^p:=\\{X\\in \\mathrm{Der}(\\hat{\\mathcal A})\\mid X(\\hm A^k)\\subset \\hm A^{k+p}\\},\\\\\n&\\mathrm{Der}(\\hat{\\mathcal A})^p_d:=\\{X\\in \\mathrm{Der}(\\hat{\\mathcal A})\\mid X(\\hm A^k_l)\\subset \\hm A^{k+p}_{l+d}\\}.\n\\end{align*}\nThen the conformal property of a bihamiltonian structure can be described as the follows.\n\\begin{Def}\n\\label{def-conf}\\label{def-2-1-zh}\nA bihamiltonian structure $(P_0, P_1)$ is called conformal if there exists a nonzero derivation $E \\in \\mathrm{Der}(\\hat{\\mathcal A})^0$ and real numbers $\\mu,\\lambda_0,\\lambda_1$ such that:\n\\begin{equation}\\label{eq-def-2-1-zh}\n\\left[E,\\partial_x\\right] = \\mu\\partial_x,\\quad \\left[E, D_{P_a}\\right] = \\lambda_aD_{P_a},\\quad a = 0,1.\n\\end{equation}\n\\end{Def}\n\n\\begin{Ex}\nThe bihamiltonian structure of the dispersionless KdV hierarchy is given by \n\\[\nP_0 = \\frac 12\\int\\theta\\qth^1;\\quad P_1 = \\frac 12 \\int u\\theta\\qth^1.\n\\]\nOne can check that it is a conformal bihamiltonian structure with $E$ given by\n\\[\nE = \\sum_{s\\geq 0}(\\lambda_1-\\lambda_0+s\\mu)u^{(s)}\\diff{}{u^{(s)}}+(\\lambda_1+(s-1)\\mu)\\theta^s\\diff{}{\\theta^s}.\n\\]\n\\end{Ex}\n\\begin{Th}\n\\label{g0-conf}\nA semisimple bihamiltonian structure $(P_0, P_1)$ of hydrodynamic type is conformal if and only if there exist real numbers $d^1,\\cdots,d^n$ such that the functions $f^1,\\cdots,f^n$ given in \\eqref{norm-p} satisfy the following identities:\n\\begin{align}\n\\label{hom-f}\n&\\sum_j u^j\\diff{f^i}{u^j} = d^i f^i,\\quad \\forall\\ i;\\\\\n\\label{irre-f}\n&(d^i-d^j)\\diff{f^j}{u^i} = 0,\\quad \\forall\\ i, j.\n\\end{align}\nIn such a case, if we require that the derivation $E$ has differential degree 0, then $E$ is an Euler type vector field  given by\n\\begin{equation}\n\\label{gen-e}\nE = \\sum_{i;s\\geq 0}(\\lambda_1-\\lambda_0+s\\mu)u^{i,s}\\diff{}{u^{i,s}}+(\\lambda_1-(\\lambda_1-\\lambda_0)d^i+(s-1)\\mu)\\theta_i^s\\diff{}{\\theta_i^s}.\n\\end{equation}\n\\end{Th}\n\n\nGiven a semisimple and conformal bihamiltonian structure  of hydrodynamic type, we consider whether its deformations are still conformal. It turns out that there is only a certain family of deformations that preserve the conformal property, and the  equivalence classes of these deformations under Miura type transformations are parametrized by $n$ constants, as it is shown by the following theorem.\n\\begin{Th}\n\\label{g1-conf} \nLet $(P_0,P_1)$ be a semisimple and conformal bihamiltonian structure of hydrodynamic type, then its deformation $(\\tilde P_0,\\tilde P_1)$ is conformal if and only if the central invariants are given by:\n\\begin{equation}\n\\label{con-ci}\nc_i(u^i) = C_i(u^i)^{m_i},\\quad m_i = \\frac{\\lambda_1-\\lambda_0-2\\mu-(\\lambda_1-\\lambda_0)d^i}{\\lambda_1-\\lambda_0},\n\\end{equation}\nwhere $C_i$ are arbitrary constants.\n\\end{Th}\n\n\n\n\\section{Variational Bihamiltonian Cohomology}\\label{vbc}\nIn this section, we start with the definition of variational Hamiltonian cohomology for a single Hamiltonian structure of hydrodynamic type and prove its triviality. Then we define the variational bihamiltonian cohomology and make some necessary algebraic preparations for computing the cohomology groups.\n\n\\subsection{Variational forms on the infinite jet bundle.} We recall the constructions of the variational forms on the infinite jet bundle $J^\\infty(\\hat M)$ (cf. \\cite{dubrovin2001normal} and reference therein). Let $\\mathcal E$ be the space of differential forms on $J^\\infty(\\hat M)$, locally is just the space\n\\[\n\\mathcal E = \\wedge^* \\left(\\mathrm{Span}_{\\hm A}\\left\\{\\delta\\theta_i^s, \\delta u^{i,s}\\mid i = 1,\\cdots,n,\\, s\\ge 0 \\right\\}\\right).\n\\]\nHere we adopt Deligne's sign rule for these variational forms, for example\n\\[\n\\theta_i^s\\delta\\theta_j^t = -\\delta\\theta_j^t\\theta_i^s,\\quad \\delta\\theta_i^s\\delta\\theta_j^t = \\delta\\theta_j^t\\delta\\theta_i^s,\\quad \\delta u^{i,s}\\delta\\theta_j^t = -\\delta\\theta_j^t\\delta u^{i,s},\\quad \\delta u^{i,s}\\delta u^{j,t} = -\\delta u^{j,t}\\delta u^{i,s}.\n\\]\n\nAny vector field $X\\in\\mathrm{Der}(\\hm A)$ induces an action on $\\mathcal E$ by the Lie derivative, which is defined by the Cartan formula\n\\[\\mathcal L_X = \\delta\\comp\\iota_X+\\iota_X\\comp\\delta,\\]\nwhere the de Rham differential is given by\n\\[\n\\delta = \\sum_{i;s\\geq 0}\\delta u^{i,s}\\diff{}{u^{i,s}}+\\delta\\theta_i^s\\diff{}{\\theta_i^s}.\n\\]\nIn particular, the vector field $\\partial_x$ defined in \\eqref{dx-zh} induces an action on $\\mathcal E$, which we still denote by $\\partial_x$. We define the space of local functionals of forms by\n\\[\\bar{\\mathcal E} = \\mathcal E/{\\partial_x \\mathcal E},\\] \nand represent its elements in the form $\\int f$ with $f\\in\\mathcal E$.\nIn addition, if a vector field $X\\in\\mathrm{Der}(\\hm A)$ commutes with $\\partial_x$, then its Lie derivative $\\mathcal L_X$ also commutes with $\\partial_x$ acting on $\\mathcal E$, hence it induces an action on $\\bm E$ which we still denote by $\\mathcal L_X$.\n\n\\begin{Ex}\n\\label{1-form}\nThe space of 1-forms is given by\n\\[\n\\mathcal E^1 = \\biggl\\{\\sum_{i;s\\geq 0}g_{i,s}\\delta u^{i,s}+h^i_s\\delta\\theta_i^s\\mid g_{i,s}, h^i_s\\in\\hm A\\biggr\\}.\n\\]\nLet us grade $\\mathcal E^1$ as we do for $\\hm A$ as follows:\n\\begin{align*}\n(\\mathcal E^1)_d = \\biggl\\{\\sum_{i;s\\geq 0}g_{i,s}\\delta u^{i,s}+h^i_s\\delta\\theta_i^s\\mid g_{i,s}, h^i_s\\in\\hm A_{d-s}\\biggr\\},\\\\\n(\\mathcal E^1)^p = \\biggl\\{\\sum_{i;s\\geq 0}g_{i,s}\\delta u^{i,s}+h^i_s\\delta\\theta_i^s\\mid g_{i,s}\\in\\hm A^p, h^i_s\\in\\hm A^{p-1}\\biggr\\}.\n\\end{align*}\nFor any element $\\omega\\in\\bm E^1$, we can uniquely represent it in the form\n\\[\n\\omega = \\int \\sum_i g_i\\delta u^i+h^i\\delta\\theta_i,\\quad g_i,h^i\\in\\hm A.\n\\]\nSo we can identify the space $\\bm E^1$ with the space of $\\hm A$-valued differential 1-forms on $\\hat M$.\n\\end{Ex}\n\\begin{Ex}\\label{ex-3-2-zh}\nLet $X\\in\\mathrm{Der}(\\hm A)^q$ and $\\omega\\in(\\mathcal E^1)^p$ be given by\n\\[\\omega = \\sum_{i;s\\geq 0}g_{i,s}\\delta u^{i,s}+h^i_s\\delta\\theta_i^s,\\quad g_{i,s}\\in\\hm A^p,\\ h^i_s\\in\\hm A^{p-1},\\]\nthen the action of $\\mathcal L_X$ on $\\omega$ has the expression\n\\[\\mathcal L_X\\omega = \\sum_{i;s\\geq 0}X(g_{i,s})\\delta u^{i,s}+(-1)^{pq}g_{i,s}\\delta \\left(X(u^{i,s})\\right)+X(h^i_s)\\delta\\theta_i^s+(-1)^{(p-1)q}h^i_s\\delta \\left(X(\\theta_i^s)\\right).\\]\n\\end{Ex}\n\nLet us consider the space $\\mathrm{Der}(\\hm A)^{\\partial}$ of derivations on $\\hm A$ which commute with $\\partial_x$. Take an element $X\\in \\mathrm{Der}(\\hm A)^{\\partial}$, since $[X,\\ \\partial_x] = 0$  we can regard $X$ as an $\\hm A$-valued vector field on $\\hat M$ instead of on $J^\\infty(\\hat M)$. Note that $\\hat M$ admits a canonical symplectic structure\n\\[\\varpi = \\sum_i \\delta u^i\\wedge\\delta\\theta_i,\\]\nhence the space of the $\\hm A$-valued vector fields on $\\hat M$ is canonically identified with the space of $\\hm A$-valued differential 1-forms on $\\hat M$, which is the same as the space $\\bm E^1$ as we explained in the Example \\ref{1-form}. Let us write down this identification explicitly. An element $X$ of $\\mathrm{Der}(\\hm A)^{\\partial}$ can be represented as\n\\[X = \\sum_{i;s\\geq 0}\\partial_x^s\\left(X(u^i)\\right)\\diff{}{u^{i,s}}+\\partial_x^s\\left(X\\left(\\theta_i\\right)\\right)\\diff{}{\\theta^{s}_i}.\\]\nRestricting it on $\\hat M$, we get the following $\\hm A$-valued vector field on $\\hat M$ which we still denote by $X$:\n\\[X = \\sum_i X(u^i)\\diff{}{u^i}+X(\\theta_i)\\diff{}{\\theta_i}.\\]\nUsing the canonical symplectic structure $\\varpi$, we identify $X$ with the 1-form\n\\[W = \\iota_X \\varpi = \\sum_i X(u^i)\\delta\\theta_i-X(\\theta_i)\\delta u^i,\\]\nwhich corresponds to a unique element $\\omega\\in\\bm E^1$ given by\n\\[\\omega = \\int  \\sum_i X(u^i)\\delta\\theta_i-X(\\theta_i)\\delta u^i.\\]\nWe note that for an element $X \\in (\\mathrm{Der}(\\hm A)^{\\partial})^p_d$, the corresponding element $\\omega$ belongs to $(\\bm E^1)^{p+1}_d$. Let us denote this correspondence by\n\\begin{equation}\n\\label{s3-t1}\n\\Phi: \\mathrm{Der}(\\hm A)^{\\partial}\\to \\bm E^1;\\quad \\Phi: X\\mapsto \\int \\iota_\\varpi \\left(X|_{\\hat M}\\right).\n\\end{equation}\n\nNow let $P$ be a Hamiltonian structure of hydrodynamic type. By using the identity \\eqref{comm-d}, we see that the space $\\mathrm{Der}(\\hm A)^{\\partial}$ becomes a cochain complex with the differential given by adjoint action of $D_P$. The question is that when we identify elements in $\\mathrm{Der}(\\hm A)^{\\partial}$ as  elements in $\\bm E^1$, then how the action of $D_P$ induces a differential on $\\bm E^1$?\n\n\\begin{Lem}\nLet $P$ be a Hamiltonian structure of hydrodynamic type, then the following identity holds true for any $X \\in \\mathrm{Der}(\\hm A)^{\\partial}$:\n\\begin{equation}\n\\label{s3-t2}\n\\Phi\\left([D_P,\\ X]\\right) = \\mathcal L_{D_P}\\Phi(X).\n\\end{equation}\n\\end{Lem}\n\\begin{proof}\nSince both the definition \\eqref{s3-t1} and the identity \\eqref{s3-t2} are coordinate free, we can choose a system of local coordinates $(v^1,\\cdots, v^n; \\phi_1,\\cdots,\\phi_n)$ on $\\hat M$ such that the Hamiltonian structure $P$ has the expression \\cite{dubrovin1996hamiltonian}\n\\[P = \\frac 12\\int\\eta^{\\alpha\\beta}\\phi_\\alpha\\phi_\\beta^1,\\]\nhere $(\\eta^{\\alpha\\beta})$ is a constant non-degenerate matrix, and summation over repeated lower and upper Greek indices is assumed. We call such coordinates the flat coordinates of $P$.\n\nNow we assume that $X$ is an element of $\\mathrm{Der}(\\hm A)^{\\partial}$ with super degree $p$, which means that $X(\\hm A^q)\\subset \\hm A^{p+q}$, then it is straightforward to show that\n\\begin{equation*}[D_P, X]v^\\alpha = D_P\\left(X(v^\\alpha)\\right)+(-1)^{p+1}\\eta^{\\alpha\\beta}(X(\\phi_\\beta^1)),\\quad [D_P, X]\\phi_\\alpha = D_P(X(\\phi_\\alpha)).\\end{equation*}\nThen we arrive at\n\\[\n\\Phi\\left([D_P, X]\\right) = \\int\\left(D_P\\left(X(v^\\alpha)\\right)+(-1)^{p+1}\\eta^{\\alpha\\beta}(X(\\phi_\\beta^1))\\right)\\delta\\phi_\\alpha-D_P(X(\\phi_\\alpha))\\delta v^\\alpha.\n\\]\nOn the other hand we have\n\\begin{align*}\n\\mathcal L_{D_P}\\Phi(X) &= \\mathcal L_{D_P}\\int X(v^\\alpha)\\delta\\phi_\\alpha-X(\\phi_\\alpha)\\delta v^\\alpha\\\\\n&=\\int D_P\\left(X(v^\\alpha)\\right)\\delta\\phi_\\alpha-D_P(X(\\phi_\\alpha))\\delta v^\\alpha-(-1)^{p+1}X(\\phi_\\alpha)\\delta\\left(\\eta^{\\alpha\\beta}\\phi_\\beta^1\\right)\\\\\n&= \\int\\left(D_P\\left(X(v^\\alpha)\\right)+(-1)^{p+1}\\eta^{\\alpha\\beta}(X(\\phi_\\beta^1))\\right)\\delta\\phi_\\alpha-D_P(X(\\phi_\\alpha))\\delta v^\\alpha.\n\\end{align*}\nTherefore we prove the lemma.\n\\end{proof}\n\\subsection{Variational Hamiltonian cohomology and its triviality}\nFrom now on we will use $\\Omega$ to denote the space of 1-froms $\\mathcal E^1$ and $\\bar\\Omega$ to denote its quotient space $\\bar{\\mathcal E}^1$. The space of homogeneous elements with differential degree $d$ and super degree $p$ will be denoted by $\\Omega^p_d$ and $\\bar\\Omega^p_d$ respectively.\n\nLet $P$ be a Hamiltonian structure of hydrodynamic type, we will use $\\tilde D_P$ to denote the action $\\mathcal L_{D_P}$ on the space $\\Omega$ and $\\bar\\Omega$. By using the identity \\eqref{s3-t2} we conclude that $\\tilde D_P\\comp\\tilde D_P = 0$, so $\\tilde D_P$ is a differential on the spaces $\\Omega$ and $\\bar\\Omega$.\n\\begin{Def}\nThe variational Hamiltonian cohomology of $\\Omega$ (and of $\\bar \\Omega$, respectively) is defined to be the cohomology of the complex $(\\Omega^\\bullet,\\tilde D_P)$ (and of $(\\bar\\Omega^\\bullet,\\tilde D_P), respectively$) given by\n\\[\nH^p_d(\\Omega,\\tilde D_P) = \\frac{\\Omega^p_d\\cap\\ker\\tilde D_P}{\\Omega^p_d\\cap\\ima\\tilde D_P};\\quad H^p_d(\\bar\\Omega,\\tilde D_P) = \\frac{\\bar\\Omega^p_d\\cap\\ker\\tilde D_P}{\\bar\\Omega^p_d\\cap\\ima\\tilde D_P}.\n\\]\n\\end{Def} \n\nBy using the fundamental facts of the homological algebra we have the following lemma.\n\\begin{Lem}\n\\label{s3-t3}\nThe short exact sequence\n\\xym{\n   0\\ar[r]&\\Omega\\ar[r]^{\\partial_x}&\\Omega\\ar[r]^{\\pi}&\\bar\\Omega\\ar[r] & 0}\ninduces the following long exact sequence of the cohomology groups for $d\\geq 1$:\n\\[\n\\cdots \\to H^p_d(\\Omega,\\tilde D_P)\\to H^p_d(\\bar\\Omega,\\tilde D_P)\\to H^{p+1}_d(\\Omega,\\tilde D_P)\\to\\cdots.\n\\]\n\\end{Lem}\n\\begin{Rem}\nOn the space $\\hm A$, the map $\\partial_x$ has the kernel $\\mathbb R$, however on the space $\\Omega$ the map $\\partial_x$ is injective.\n\\end{Rem}\n\n\\begin{Th}[Triviality of the variational Hamiltonian cohomology]\nWe have \n\\[H^p_d(\\Omega,\\tilde D_P) = 0\\]\nfor $p>0,\\ d>0$.\n\\end{Th}\n\\begin{proof}\nWe choose locally a system of flat coordinates $(v^1,\\cdots, v^n; \\phi_1,\\cdots,\\phi_n)$ on $\\hat M$ such that $P = \\frac 12\\int \\eta^{\\alpha\\beta}\\phi_\\alpha\\phi_\\beta^1$. Then for a 1-form\n\\begin{equation*}\n\\omega = \\sum_{s\\geq 0} f_{\\alpha,s}\\delta v^{\\alpha,s}+g^\\alpha_s\\delta\\phi_\\alpha^s\\in\\Omega^p_d\n\\end{equation*}\nwe have\n\\begin{equation*}\n\\tilde D_P\\omega = \\sum_{s\\geq 0} D_P(f_{\\alpha,s})\\delta v^{\\alpha,s}+(-1)^p\\eta^{\\alpha\\beta}f_{\\alpha,s}\\delta\\phi_\\beta^{s+1}+D_P(g^\\alpha_s)\\delta\\phi_\\alpha^s.\n\\end{equation*}\nIf $\\omega\\in\\ker \\tilde D_P$, then we see that\n\\begin{equation*}\nD_P(g^\\alpha_0)= 0;\\quad D_P(g_{s+1}^\\alpha)+(-1)^p\\eta^{\\alpha\\beta}f_{\\beta,s} = 0,\\ s\\geq 0,\n\\end{equation*}\nso we can write $\\omega$ in the form\n\\begin{equation*}\n\\omega = \\tilde D_P \\left(\\sum_{s\\geq 0}(-1)^{p-1}\\eta_{\\alpha\\beta}g^\\alpha_{s+1}\\delta v^{\\beta,s}\\right)+ g^\\alpha_0\\delta\\phi_\\alpha.\n\\end{equation*}\nFor $g^\\alpha_0\\in\\hm A^{p-1}_d$, by using the triviality of the Hamiltonian cohomology (see, for example \\cite{liu2018lecture,liu2011jacobi}) we can represent it as $g^\\alpha_0 = D_P(h^\\alpha_0)$. Thus the cocycle $\\omega$ must also be a coboundary. The theorem is proved.\n\\end{proof}\nFrom Lemma \\ref{s3-t3} we have the following corollary.\n\\begin{Cor}\nWe have $H^p_d(\\bar\\Omega,\\tilde D_P) = 0$ for $p>0,\\ d>0$.\n\\end{Cor}\n\n\\subsection{Definition of the variational bihamiltonian cohomology}\nWe proceed to define the variational bihamiltonian cohomology and discuss its relations with the bihamiltonian cohomology. Let $(P_0,P_1)$ be a semisimple bihamiltonian structure of hydrodynamic type and $u^1,\\cdots,u^n$ be its canonical coordinates. We will use $\\tilde D_0$ and $\\tilde D_1$ to denote $\\tilde D_{P_0}$ and $\\tilde D_{P_1}$ respectively. By using the identity \\eqref{s3-t2} we have $\\tilde D_i\\tilde D_j+\\tilde D_j\\tilde D_i = 0$ for $i,j = 0,1$.\n\\begin{Def}\nThe variational bihamiltonian cohomology for $(P_0,P_1)$ is defined to be the following groups:\n\\begin{align}\n\\vbh^p_d(\\Omega,\\tilde D_0,\\tilde D_1) = \\frac{\\Omega_d^p\\cap\\ker \\tilde D_0\\cap\\ker\\tilde D_1}{\\Omega_d^p\\cap\\ima\\tilde D_0\\tilde D_1};\\\\ \n\\vbh^p_d(\\bar\\Omega,\\tilde D_0,\\tilde D_1) = \\frac{\\bar\\Omega_d^p\\cap\\ker \\tilde D_0\\cap\\ker\\tilde D_1}{\\bar\\Omega_d^p\\cap\\ima\\tilde D_0\\tilde D_1}.\n\\end{align}\n\\end{Def}\n\nThe following lemmas are important for computing the cohomology groups.\n\\begin{Lem}\n\\label{lem-lambda}\nThe cohomology groups of the cochain complex $\\Omega[\\lambda] = \\Omega\\otimes \\mathbb R[\\lambda]$ with differential $\\partial_\\lambda = \\tilde D_1-\\lambda\\tilde D_0$ is isomorphic to the variational bihamiltonian cohomology, i.e.\n\\begin{equation}\n{H}^p_d(\\Omega[\\lambda],\\partial_\\lambda)\\cong \\vbh^p_d(\\Omega,\\tilde D_0,\\tilde D_1),\\quad d\\geq 2.\n\\end{equation}\nSimilarly, we have the following isomorphisms for the corresponding quotient spaces:\n\\begin{equation}\n{H}^p_d(\\bar\\Omega[\\lambda],\\partial_\\lambda)\\cong \\vbh^p_d(\\bar\\Omega,\\tilde D_0,\\tilde D_1),\\quad d\\geq 2.\n\\end{equation}\n\\end{Lem}\n\\begin{proof}\nThe lemma follows from the triviality of the variational hamiltonian cohomology. For details, one may refer to the proof of Lemma 4.4 of \\cite{liu2013bihamiltonian}.\n\\end{proof}\n\n\\begin{Lem}[Salamander lemma]\n\\label{sal}\n We have the following isomorphism induced by $\\tilde D_0$ for $p>0, d>0$:\n\\begin{align*}\n\\tilde D_0:\\quad \\frac{\\bar\\Omega^p_d\\cap\\ker \\tilde D_1\\tilde D_0}{\\bar\\Omega^p_d\\cap(\\ima\\tilde D_0+\\ima\\tilde D_1)}\\xrightarrow[]{\\cong} \\vbh^{p+1}_{d+1}(\\bar\\Omega,\\tilde D_0,\\tilde D_1).\n\\end{align*}\n\\end{Lem}\n\\begin{proof}\nThe lemma follows easily from the triviality of the variational Hamiltonian cohomology.\n\\end{proof}\n\n\nThe definition of the variational bihamiltonian cohomology is comparable with the definition of bihamiltonian cohomology. On the space $\\hm A$ there are differentials $D_0$ and $D_1$ which are defined in \\eqref{def-d} by the bihamiltonian structure $(P_0, P_1)$, and on the space $\\hm F$ there are the induced differential which are denoted by $d_0$ and $d_1$. The bihamiltonian cohomology is then given by\n\\begin{align*}\n\\bh^p_d(\\hm A,D_0,D_1) = \\frac{\\hm A_d^p\\cap\\ker  D_0\\cap\\ker D_1}{\\hm A_d^p\\cap\\ima D_0 D_1}\\cong H^p_d(\\hm A[\\lambda],D_1-\\lambda D_0);\\\\ \\bh^p_d(\\hm F,d_0,d_1) = \\frac{\\hm F_d^p\\cap\\ker d_0\\cap\\ker d_1}{\\hm F_d^p\\cap\\ima d_0d_1}\\cong H^p_d(\\hm F[\\lambda],d_1-\\lambda d_0).\n\\end{align*}\n\nThere is a natural cochain map \n\\begin{equation}\n\\label{bhvbh}\n\\delta:\\quad \\hm F[\\lambda]\\to \\bar\\Omega[\\lambda]\n\\end{equation}\nbetween the complexes $\\hm A[\\lambda]$ and $\\Omega[\\lambda]$ given by the de Rham differential $\\delta$.\nThis map can be further illustrated as follows: We first note that the space $\\hm F$ can be identified with \n\\[\n\\mathrm{Der}(\\hm A)^D:=\\{X\\in\\mathrm{Der}(\\hm A)\\mid \\exists\\ Y\\in\\hm F, X = D_Y\\}\n\\]\nby using the map \\eqref{def-d}. By applying the identity \\ref{comm-d}, we see that the differentials $d_0, d_1$ on $\\hm F$ induce differentials $\\mathrm{ad}_{D_0}, \\mathrm{ad}_{D_1}$ on $\\mathrm{Der}(\\hm A)^D$. Since it is obvious that $\\mathrm{Der}(\\hm A)^D\\subseteq \\mathrm{Der}(\\hm A)^\\partial$, the identity \\eqref{s3-t2} shows that the map \\eqref{bhvbh} can be viewed as a natural embedding (with a change of signs, see the remark below):\n\\[i:\\mathrm{Der}(\\hm A[\\lambda])^D\\hookrightarrow \\mathrm{Der}(\\hm A[\\lambda])^\\partial.\\]\nTherefore we conclude that the bihamiltonian cohomology can be viewed as the cohomology on the space $\\mathrm{Der}(\\hm A[\\lambda])^D$, while the variational bihamiltonian cohomology is the cohomology on the space $\\mathrm{Der}(\\hm A[\\lambda])^\\partial$. In this sense, the bihamiltonian cohomology is just a restriction of the variational bihamiltonian cohomology onto a subcomplex.\n\n\\begin{Rem}\nThe map $\\delta: \\hm F\\to\\bar \\Omega$ can be viewed as a generalization of the correspondence between the hamiltonian functions and the hamiltonian vector field on a finite dimensional symplectic manifold. In our case, this correspondence is twisted by a sign:\n\\[\n\\delta X = (-1)^{p-1}\\Phi(D_X),\\quad X\\in\\hm F^p.\n\\] \n\\end{Rem}\n\n\n\\section{The cohomology group $\\vbh^2_3$}\n\\label{vbh23}\n\nIn this section, we compute the bihamiltonian cohomology group $\\vbh^2_3(\\bar\\Omega,\\tilde D_0,\\tilde D_1)$. The computation of other cohomology groups will be covered in the next section. We fix a semisimple bihamiltonian $(P_0,P_1)$ of hydrodynamic type and work in the canonical coordinates such that the bihamiltonian structure is given by \\eqref{norm-p}.\nNote that if we define a derivation\n\\begin{align*}\nD(g^1,\\cdots,g^n) = &\\sum_{s\\geq 0;i}\\partial_x^s(g^i\\theta_i^1)\\diff{}{u^{i,s}}\n\\\\&+\\frac{1}{2}\\sum_{s\\geq 0;i,j}\\partial_x^s\\left(\\partial_jg^iu^{j,1}\\theta_i+g^i\\frac{\\partial_ig^j}{g^j}u^{j,1}\\theta_j-g^j\\frac{\\partial_jg^i}{g^i}u^{i,1}\\theta_j\\right)\\diff{}{u^{i,s}}\\\\\n&+\\frac{1}{2}\\sum_{s\\geq 0;i,j}\\partial_x^s\\left(\\partial_ig^j\\theta_j\\theta_j^1+g^j\\frac{\\partial_jg^i}{g^i}\\theta_i\\theta_j^1-g^j\\frac{\\partial_jg^i}{g^i}\\theta_j\\theta_i^1\\right)\\diff{}{\\theta_i^s}\\\\\n&+\\frac{1}{2}\\sum_{s\\geq 0;i,j,k}\\partial_x^s\\left(\\partial_i\\left(g^k\\frac{\\partial_kg^j}{g^j}\\right)u^{j,1}\\theta_k\\theta_j-\\partial_j\\left(g^k\\frac{\\partial_kg^i}{g^i}\\right)u^{j,1}\\theta_k\\theta_i\\right)\\diff{}{\\theta_i^s},\n\\end{align*}\nfor a set of functions $g^1,\\cdots,g^n$, then we know that\n\\[\nD_{P_0} = D(f^1,\\dots,f^n),\\quad D_{P_1} = D(u^1f^1,\\dots,u^nf^n).\n\\]\nHere and henceforth we will use $\\partial_i$ to denote $\\diff{}{u^i}$. \n\nFrom Lemma \\ref{sal} it follows that in order to compute the cohomology group $\\vbh^2_3(\\bar\\Omega,\\tilde D_0,\\tilde D_1)$ we only need to consider the spaces \n\\[\\mathcal Z:= \\bar\\Omega^1_2\\cap \\ker(\\tilde D_0\\tilde D_1),\\quad\n\\mathcal B:= \\bar\\Omega^1_2\\cap(\\ima\\tilde D_0+\\ima\\tilde D_1).\\]\nFor an element $\\omega\\in\\bar \\Omega^1_2$, we can represent it uniquely in the form\n\\begin{equation}\n\\label{s3-t4}\n\\omega = \\int \\sum_i X^i\\delta u^i+Y^i\\delta\\theta_i,\\quad X^i\\in\\hm A^1_2,\\quad Y^i\\in\\hm A^0_2,\n\\end{equation}\nwhere the differential polynomials can be written as\n\\begin{align}\n\\label{s3-t5}\nX^i &= \\sum_j X^{(i)}_{j}\\theta_j^2+\\sum_{j,k}\\left(X^{(i)}_{kj}u^{j,1}\\theta_k^1+Z^{(i)}_{jk}u^{k,2}\\theta_j\\right)+\\sum_{j,k,l}Z^{(i)}_{j;kl}u^{k,1}u^{l,1}\\theta_j;\\\\\n\\label{s3-t6}\nY^i &= \\sum_j Y^{(i)}_ju^{j,2}+\\sum_{j,k}Y^{(i)}_{jk}u^{j,1}u^{k,1}.\n\\end{align}\n\\begin{Def}\\label{ind-zh}\nFor an element $\\omega\\in\\bar\\Omega^1_2$ which is represented in the form \\eqref{s3-t4}--\\eqref{s3-t6}, the indices $ind_i(\\omega)$ for $i = 1,\\cdots, n$ with respect to a semisimple bihamiltonian structure $(P_0,P_1)$ of hydrodynamic type are defined to be the functions\n\\[\nind_i(\\omega):=\\frac{1}{f^i}(X^{(i)}_i+Y^{(i)}_i).\n\\]\n\\end{Def}\nWe will see later that the indices defined above are generalizations of the central invariants of deformations of bihamiltonian structures of hydrodynamic type. To compute the cohomology group $\\vbh^2_3(\\bar\\Omega)\\cong \\mathcal Z/\\mathcal B$, we need the following two lemmas.\n\\begin{Lem}\n\\label{s3-t7}\nFor any given cocycle $\\omega\\in\\mathcal Z$, the index $ind_i(\\omega)$ is a function of single variable $u^i$ for any $i=1,\\cdots,n$.\n\\end{Lem}\n\\begin{Lem}\n\\label{s3-t8}\nA cocycle $\\omega\\in\\mathcal Z$ is a coboundary, i.e. $\\omega\\in\\mathcal B$, if and only if  \n\\[ind_i(\\omega) = 0,\\quad i = 1,\\cdots,n.\\]\n\\end{Lem}\n\\begin{Th}\n\\label{h23}\nThe quotient space $\\mathcal Z/\\mathcal B\\cong\\oplus_{i=1}^nC^\\infty(\\mathbb R)$.\n\\end{Th}\n\\begin{proof}\nGiven $n$ functions of single variable $c_1(u^1),c_2(u^2),\\cdots,c_n(u^n)$, we construct an element $\\tau\\in\\mathcal Z$ as follows:\n\\[\n\\tau = \\int \\delta\\biggl(D_1\\sum_i c_i(u^i)u^{i,1}\\log u^{i,1}-D_0\\sum_i u^ic_i(u^i)u^{i,1}\\log u^{i,1}\\biggr).\n\\]\nOne can check directly, or refer to \\cite{liu2005deformations}, to confirm the fact that actually $\\tau$ is a 1-form with differential polynomial coefficients and that $\\tau\\in\\mathcal Z$.\n\nBy a straightforward computation we can show that \n\\[ind_i(\\tau) = -3c_i(u^i),\\quad i = 1,\\cdots,n.\\]\nTherefore, it follows from Lemma \\ref{s3-t7} that we can choose suitable $c_i(u^i)$ such that \n\\[ind_i(\\omega-\\tau) = 0\\] for any given cocycle $\\omega\\in\\mathcal Z$. Then we apply Lemma \\ref{s3-t8} to conclude that the class $[\\omega]$ coincides with the class $[\\tau]$ in the space $\\mathcal Z/\\mathcal B$. Lemma \\ref{s3-t8} also implies that $\\tau$ is a coboundary if and only if $c_i(u^i) = 0$, hence different choices of functions $c_1(u^1),\\cdots,c_n(u^n)$ give different classes $[\\tau]$ in $\\mathcal Z/\\mathcal B$. The theorem is proved.\n\\end{proof}\n\nThe remaining part of this section is devoted to the proof of Lemma \\ref{s3-t7} and Lemma \\ref{s3-t8}. The strategy of our proof is: We first prove the `only if' part of Lemma \\ref{s3-t8}, then we prove Lemma \\ref{s3-t7}. Finally we prove the `if' part of Lemma \\ref{s3-t8}.\n\n\\begin{proof}[\\textbf{Proof of Lemma \\ref{s3-t8} (Part 1)}] We are to show that if $\\omega\\in\\mathcal B$ then its indices vanish. Take $\\alpha\\in\\bar\\Omega^0_1$ and write it uniquely as\n\\[\n\\alpha = \\int \\sum_{i,j}\\alpha^{(i)}_ju^{j,1}\\delta u^i,\\]\nthen we have\n\\begin{align}\n\\label{s3-t9}\n\\tilde D_0\\alpha &= \\int \\sum_{i,j}D_0(\\alpha^{(i)}_ju^{j,1})\\delta u^i+\\alpha^{(i)}_ju^{j,1}\\delta(D_0(u^i))\\\\\n\\nonumber\n&=\\int \\sum_{i,j}\\left(\\alpha^{(i)}_jf^j\\theta_j^2+\\cdots\\right)\\delta u^i+\\left(-\\alpha^{(i)}_jf^iu^{j,2}+\\cdots\\right)\\delta\\theta_i.\n\\end{align}\nHere $\\cdots$ stands for the terms that make no contribution to the index. Then from Definition \\ref{ind-zh} it follows that $ind_i(\\tilde D_0\\alpha) = 0$. Similarly we have $ind_i(\\tilde D_1\\alpha) = 0$. Hence the indices of a coboundary must vanish.\n\\end{proof}\n\nThe above proof shows that the indices can be defined for a class $[\\omega]\\in\\mathcal Z/\\mathcal B$. To prove Lemma \\ref{s3-t7}, we first choose a `normal form' for every class $[\\omega]\\in\\mathcal Z/\\mathcal B$ that will simplify the computation.\n\n\\begin{Lem}\n\\label{s3-t10}\nFor any given class $[\\sigma]\\in\\mathcal Z/\\mathcal B$, there exists a unique $\\omega\\in\\mathcal Z$ which can be represented in the form \\eqref{s3-t4}--\\eqref{s3-t6} and satisfies the following conditions:\n\\begin{enumerate}\n\\item[\\rm(1)] $[\\omega] = [\\sigma]$;\n\\item[\\rm(2)] $X^{(i)}_j = 0$ for $j\\neq i$;\n\\item[\\rm(3)] $Y^{(i)}_j = 0$ for any $i,j$;\n\\item[\\rm(4)] $Y^{(i)}_{ii} = 0$.\n\\end{enumerate}\nSuch a form $\\omega$ is called the normal form of the class $[\\sigma]$.\n\\end{Lem}\n\\begin{proof}\nWe first take $\\omega = \\sigma$, then the first condition is satisfied. We then adjust $\\omega$ by adding elements of $\\mathcal B$ such that other conditions are also satisfied. Firstly, according to \\eqref{s3-t9}, we change $\\omega$ to $\\tilde\\omega = \\omega+\\tilde D_0\\gamma$ with\n\\[\n\\gamma = \\int \\sum_{i,j}\\frac{Y^{(i)}_j}{f^i}u^{j,1}\\delta u^i,\n\\]\nthen the third condition is satisfied. As for other conditions, we want to find $\\alpha,\\beta\\in\\bar \\Omega^0_1$ such that $\\tilde\\omega+\\tilde D_0\\alpha+\\tilde D_1\\beta$ satisfies all the four conditions.\n\nLet us write $\\alpha = \\int\\sum_i\\alpha_i\\delta u^i$ and $\\beta = \\int\\sum_i\\beta_i\\delta u^i$ for some $\\alpha_i,\\beta_i\\in\\hm A^0_1$. We can uniquely represent $\\tilde D_0\\alpha$ and $\\tilde D_1\\beta$ as follows:\n\\begin{equation}\n\\tilde D_0\\alpha = \\int \\sum_i A_i\\delta u^i+W_i\\delta\\theta_i;\\quad\\tilde D_1\\beta = \\int \\sum_i B_i\\delta u^i+R_i\\delta\\theta_i,\n\\end{equation}\nwhere the differential polynomials $W_i$ and $R_i$ are given by\n\\begin{align*}\nW_i =& -(\\alpha_if^i)^\\prime+\\frac 12\\sum_j\\left(\\alpha_i\\aij ji u^{j,1}+\\alpha_j\\bij ji u^{i,1}-\\alpha_j\\bij ij u^{j,1}\\right);\\\\ \nR_i =& -(\\beta_iu^if^i)^\\prime+\\frac 12\\sum_j\\left(\\beta_iu^i\\aij ji u^{j,1}+\\beta_ju^j\\bij ji u^{i,1}-\\beta_ju^i\\bij ij u^{j,1}\\right)\\\\\n&+\\frac 12 \\beta_if^iu^{i,1}.\n\\end{align*}\nNote that we should make sure that the third condition is satisfied, so if we further write $\\alpha_i = \\sum_j\\alpha^{(i)}_ju^{j,1}$ and $\\beta_i = \\sum_j\\beta^{(i)}_ju^{j,1}$, and compare the coefficients of $u^{j,2}$ of $W_i$ and $R_i$, we arrive at $u^i\\beta^{(i)}_{j}+\\alpha^{(i)}_{j} = 0$, hence we must take $\\alpha_i = -u^i\\beta_i$. Since the coefficients of $(u^{i,1})^2$ in $W_i+R_i$ are given by $\\beta_{i}^{(i)}f^i$, we can choose suitable functions $\\beta_{i}^{(i)}$ such that the condition (4) is satisfied.\nFinally we compute the coefficients of $\\theta_j^2$ in $A_i+B_i$ to obtain\n\\[\n\\beta^{(i)}_j(u^j-u^i)f^j,\n\\]\nso for $j\\neq i$ we can choose suitable functions $\\beta^{(i)}_j$ such that the condition (2) is satisfied. Thus we find a form $\\omega$ which satisfy all the four conditions, and the above computation also shows that such an $\\omega$ is unique. The lemma is proved.\n\\end{proof}\n\nAs a byproduct of the above computation, we have the following theorem. \n\\begin{Th}\n\\label{vbh12}\nWe have $\\vbh^1_2(\\bar\\Omega,\\tilde D_0,\\tilde D_1) = 0$.\n\\end{Th} \n\\begin{proof}\nRecall that by definition $\\vbh^1_2(\\bar\\Omega,\\tilde D_0,\\tilde D_1) = \\bar\\Omega^1_2\\cap\\ker\\tilde D_0\\cap\\ker\\tilde D_1$. Now take any cocycle $\\omega$, by using the triviality of the variational Hamiltonian cohomology we can find $\\alpha, \\beta\\in \\bar\\Omega^0_1$ such that \n\\begin{equation}\n\\omega = \\tilde D_0\\alpha = \\tilde D_1\\beta.\n\\end{equation}\nLet us show that $\\alpha=\\beta=0$. To this end we write $\\alpha = \\int\\sum_i\\alpha_i\\delta u^i$ and $\\beta = \\int\\sum_i\\beta_i\\delta u^i$ for some $\\alpha_i,\\beta_i\\in\\hm A^0_1$, and represent $\\tilde D_0\\alpha$, $\\tilde D_1\\beta$, as we do in the proof of Lemma \\ref{s3-t10}, in the form\n\\begin{equation}\n\\tilde D_0\\alpha = \\int \\sum_i A_i\\delta u^i+W_i\\delta\\theta_i;\\quad\\tilde D_1\\beta = \\int \\sum_i B_i\\delta u^i+R_i\\delta\\theta_i.\n\\end{equation}\nFrom the coefficients of $u^{j,2}$ in $W_i$ and $R_i$ it follows that $\\alpha_i = u^i\\beta_i$, and from the coefficients of $(u^{i,1})^2$ in $W_i$ and $R_i$ we conclude that $\\beta^{(i)}_i = 0$.\nFinally, from the coefficients of $\\theta^{2}_j$ in $A_i$ and $B_i$ for $j\\neq i$ it follows that $\\beta^{(i)}_j = 0$ when $j\\neq i$. Therefore $\\alpha = \\beta = 0$ and the theorem is proved.\n\\end{proof}\n\nNow let us come back to prepare the proof of Lemma \\ref{s3-t7}. Take a class $[\\omega]\\in\\mathcal Z/\\mathcal B$ with $\\omega$ being its normal form which can be represented as\n\\begin{equation}\n\\label{norm-1}\n\\omega = \\int \\sum_i X^i\\delta u^i+Y^i\\delta\\theta_i,\\quad X^i\\in\\hm A^1_2,\\quad Y^i\\in\\hm A^0_2,\n\\end{equation}\nwhere the differential polynomials can be written in the form\n\\begin{align}\n\\label{norm-2}\nX^i &=  X^{(i)}_{i}\\theta_i^2+\\sum_{j,k}\\left(X^{(i)}_{kj}u^{j,1}\\theta_k^1+Z^{(i)}_{jk}u^{k,2}\\theta_j\\right)+\\sum_{j,k,l}Z^{(i)}_{j;kl}u^{k,1}u^{l,1}\\theta_j;\\\\\n\\label{norm-3}\nY^i &= \\sum_{j,k}Y^{(i)}_{jk}u^{j,1}u^{k,1},\\quad Y^{(i)}_{ii} = 0.\n\\end{align}\nFurther more, we require that\n\\begin{equation}\n\\label{norm-4}\nZ^{(i)}_{j;kl} = Z^{(i)}_{j;lk},\\quad Y^{(i)}_{jk}=Y^{(i)}_{kj}.\\end{equation}\nDue to the part of Lemma \\ref{s3-t8} that we just proved, different representatives of a class in $\\mathcal Z/\\mathcal B$ have the same indices, hence to prove Lemma \\ref{s3-t7} we only need to show that  $ind_i(\\omega) = X^{(i)}_i/f_i$ is a function of $u^i$ for each $i=1,\\dots,n$.\n\nSince $\\omega\\in\\mathcal Z$, from the triviality of variational Hamiltonian cohomology it follows the existence of $\\alpha\\in\\bar\\Omega^1_2$ such that $\\tilde D_0\\omega = \\tilde D_1\\alpha$. Such an $\\alpha$ is unique up to the addition of an image of $\\tilde D_1$, hence we can make a particular choice of $\\alpha$ in a similar way as we choose the normal form of $\\omega$. More explicitly, it is not difficult to see that there exists a unique $\\alpha$ such that\n\\begin{equation}\n\\label{norm-5}\n\\alpha = \\int \\sum_i P^i\\delta u^i+Q^i\\delta\\theta_i,\\quad P^i\\in\\hm A^1_2,\\quad Q^i\\in\\hm A^0_2,\n\\end{equation}\nwhere the differential polynomials can be written as\n\\begin{align}\n\\label{norm-6}\nP^i &=  \\sum_j P^{(i)}_{j}\\theta_j^2+\\sum_{j,k}\\left(P^{(i)}_{kj}u^{j,1}\\theta_k^1+W^{(i)}_{jk}u^{k,2}\\theta_j\\right)+\\sum_{j,k,l}W^{(i)}_{j;kl}u^{k,1}u^{l,1}\\theta_j,\\\\\n\\label{norm-7}\nQ^i &= \\sum_{j,k}Q^{(i)}_{jk}u^{j,1}u^{k,1}\n\\end{align}\nwith\n\\begin{equation}\n\\label{norm-8}\nW^{(i)}_{j;kl} = W^{(i)}_{j;lk},\\quad Q^{(i)}_{jk}=Q^{(i)}_{kj}.\\end{equation}\n\n\\begin{proof}[\\textbf{Proof of Lemma \\ref{s3-t7}}]\nConsider a class $[\\omega]\\in\\mathcal Z/\\mathcal B$ where $\\omega$ is given by the normal form \\eqref{norm-1}--\\eqref{norm-4}. Let $\\alpha\\in\\bar\\Omega^1_2$ be the unique element given by \\eqref{norm-5}--\\eqref{norm-8} such that $\\tilde D_0\\omega = \\tilde D_1\\alpha$. For the bihamiltonian structure $(P_0, P_1)$ given in \\eqref{norm-p}, we denote\n\\begin{equation}\n\\label{aij}\na_{ij} = \\frac{1}{2}\\partial_if^j,\\quad b_{ij} = \\frac{1}{2}f^i\\frac{\\partial_if^j}{f^j}.\n\\end{equation}\nLet us first compute the differential polynomials $M^i,N^i,S^i,T^i$ that are defined by\n\\[\n\\tilde D_0\\omega = \\int \\sum_i M^i\\delta u^i+N^i\\delta\\theta_i,\\quad \\tilde D_1\\alpha = \\int \\sum_i S^i\\delta u^i+T^i\\delta\\theta_i.\n\\]\nIn what follows of the proof, we will omit the symbol of summations and use $j,k,l$ to denote the indices that should be summed over $1,\\cdots, n$. The index $i$ is a fixed index and do not participate in the summation. It is straightforward to obtain\n\\begin{align*}\nM^i &= D_0(X^i)-\\partial_if^jX^j\\theta_j^1-X^k\\partial_ia_{jk}u^{j,1}\\theta_k+\\left(X^ja_{ij}\\theta_j\\right)'-X^k\\partial_ib_{kj}u^{j,1}\\theta_j\\\\\n&+\\left(X^jb_{ji}\\theta_i\\right)'+X^k\\partial_ib_{jk}u^{k,1}\\theta_j-\\lrb{X^ib_{ji}\\theta_j}'+Y^k\\partial_ia_{kj}\\theta_j\\theta_j^1\n+Y^k\\partial_ib_{jk}\\theta_k\\theta_j^1\\\\\n&-Y^k\\partial_ib_{jk}\\theta_j\\theta_k^1+Y^l\\partial_i\\partial_lb_{kj}u^{j,1}\\theta_k\\theta_j-\\left(Y^j\\partial_jb_{ki}\\theta_k\\theta_i\\right)'\\\\\n&-Y^l\\partial_i\\partial_jb_{kl}u^{j,1}\\theta_k\\theta_l+\\left(Y^j\\partial_ib_{kj}\\theta_k\\theta_j\\right)',\n\\end{align*}\n\\begin{align*}\nS^i &= D_1(P^i)-P^j\\partial_i\\lrb{u^jf^j}\\theta_j^1-P^k\\partial_i\\kk{u^ka_{jk}}u^{j,1}\\theta_k+\\kk{P^ju^ja_{ij}\\theta_j}'\\\\\n&-P^k\\partial_i\\kk{u^kb_{kj}}u^{j,1}\\theta_j+\\kk{P^ju^jb_{ji}\\theta_i}'+P^k\\partial_i\\kk{u^jb_{jk}}u^{k,1}\\theta_j-\\kk{P^iu^jb_{ji}\\theta_j}'\\\\\n&-P^ja_{ij}u^{j,1}\\theta_j+\\kk{\\frac 12 P^if^i\\theta_i}'+Q^k\\partial_i\\kk{u^ja_{kj}}\\theta_j\\theta_j^1+Q^k\\partial_i\\kk{u^jb_{jk}}\\theta_k\\theta_j^1\\\\\n&-Q^k\\partial_i\\kk{u^jb_{jk}}\\theta_j\\theta_k^1+Q^ja_{ij}\\theta_j\\theta_j^1+Q^l\\partial_i\\partial_l\\kk{u^kb_{kj}}u^{j,1}\\theta_k\\theta_j\\\\\n&-\\kk{Q^j\\partial_j\\kk{u^kb_{ki}}\\theta_k\\theta_i}'-Q^l\\partial_i\\partial_j\\kk{u^kb_{kl}}u^{j,1}\\theta_k\\theta_l+\\kk{Q^j\\partial_i\\kk{u^kb_{kj}}\\theta_k\\theta_j}',\\\\\nN^i &= D_0(Y^i)+\\kk{X^if^i}'-X^ia_{ji}u^{j,1}-X^jb_{ji}u^{i,1}+X^jb_{ij}u^{j,1}-\\kk{Y^ja_{ji}\\theta_i}'\\\\\n&-Y^ja_{ji}\\theta_i^1-\\kk{Y_jb_{ij}\\theta_j}'-Y^ib_{ji}\\theta_j^1+\\kk{Y^ib_{ji}\\theta_j}'+Y^jb_{ij}\\theta_j^1\\\\\n&+Y^j\\partial_jb_{ki}u^{i,1}\\theta_k-Y^k\\partial_kb_{ij}u^{j,1}\\theta_j-Y^i\\partial_jb_{ki}u^{j,1}\\theta_k+Y^k\\partial_jb_{ik}u^{j,1}\\theta_k,\\\\\nT^i &= D_1(Q^i)+\\kk{P^iu^if^i}'-P^iu^ia_{ji}u^{j,1}-P^ju^jb_{ji}u^{i,1}+P^ju^ib_{ij}u^{j,1}-\\frac 12 P^if^iu^{i,1}\\\\\n&-\\kk{Q_ju^ia_{ji}\\theta_i}'-Q^ju^ia_{ji}\\theta_i^1-\\kk{Q^ju^ib_{ij}\\theta_j}'-Q^iu^jb_{ji}\\theta_j^1+\\kk{Q_iu^jb_{ji}\\theta_j}'\\\\\n&+Q^ju^ib_{ij}\\theta_j^1+Q^j\\partial_j\\kk{u^kb_{ki}}u^{i,1}\\theta_k-Q^k\\partial_k\\kk{u^ib_{ij}}u^{j,1}\\theta_j\\\\\n&-Q^i\\partial_j\\kk{u^kb_{ki}}u^{j,1}\\theta_k+Q^k\\partial_j\\kk{u^ib_{ik}}u^{j,1}\\theta_k-\\kk{\\frac 12 Q^if^i\\theta_i}'-\\frac 12 Q^if^i\\theta_i^1.\n\\end{align*}\n\nBy comparing the coefficients of $\\theta_j^3$ of both sides of the equation $N^i = T^i$ we obtain \n\\begin{equation}\n\\label{cond-p1-1}\nP^{(i)}_j = 0,\\quad i\\neq j;\\quad X^{(i)}_i = u^iP^{(i)}_i.\n\\end{equation}\nComparing the coefficients of $u^{k,3}\\theta_j$ on both sides of the equation $N^i = T^i$, we conclude that\n\\ee{\n\\label{cond-p1-2}\nZ^{(i)}_{jk} = u^iW^{(i)}_{jk},\\quad\\forall\\ i,j,k.\n}\n\nNext, from the coefficients of $\\theta_j^1\\theta_i^2$ in $M^i$ and $S^i$ for $j\\neq i$, and from \\eqref{cond-p1-1} it follows that\n\\ee{\n\\label{cond-p5}\n\\kk{\\partial_jP^{(i)}_if^j-2P^{(i)}_ib_{ji}}(u^i-u^j) = \\kk{X^{(i)}_{ji}-P^{(i)}_{ji}u^i}f^i,\\quad i\\neq j.\n}\nSimilarly, from the coefficients of $u^{j,2}\\theta_k^1$ in $N^i$ and $T^i$, and from the identity \\eqref{cond-p1-2} it follows that\n\\ee{\n\\label{cond-p8-3}\nX^{(i)}_{kj} = u^iP^{(i)}_{kj},\\quad\\forall\\ i,j,k.\n}\nThe identity \\eqref{cond-p5} and \\eqref{cond-p8-3} lead to\n\\[\\partial_j\\frac{P^{(i)}_i}{f^i} = 0,\\quad i\\neq j.\\]\nThus for each $i$ the function\n\\[\nind_i(\\omega) = \\frac{X^{(i)}_i}{f^i} = u^i\\frac{P^{(i)}_i}{f^i}.\n\\]\ndepends only on $u^i$. The lemma is proved.\n\\end{proof}\n\nFinally, let us complete the proof of Lemma \\ref{s3-t8}. Before proceeding to the proof, we first derive some relations satisfied by the coefficients in $\\omega$ and $\\alpha$. In the rest of this section,  we will continue to use the notations which are used in the proof of Lemma \\ref{s3-t7}.\n\nBy comparing the coefficients of $\\theta_k\\theta_j^3$, $\\theta_j\\theta_j^3$ and $\\theta_i\\theta_j^3$ in $M^i$ and $S^i$ and by using the relations \\eqref{cond-p1-1} and \\eqref{cond-p1-2} we obtain\n\\ee{\n\\label{cond-p2-1}\nZ^{(i)}_{kj} = W^{(i)}_{kj} = 0,\\quad i\\neq j\\neq k\\neq i.\n}\n\\ee{\n\\label{cond-p2-2}\nP^{(i)}_ia_{ij} = W^{(i)}_{jj}f^j,\\quad i\\neq j.\n}\n\\ee{\n\\label{cond-p3}\nP^{(i)}_ib_{ji} = W^{(i)}_{ij}f^j,\\quad i\\neq j.\n}\nWe also compare the coefficients of $\\theta_k^1\\theta_j^2$, $\\theta_i^1\\theta_j^2$ and $\\theta_j^1\\theta_j^2$ in $M^i$ and $S^i$ to arrive at\n\\ee{\\label{cond-p4-1}X^{(i)}_{kj} = P^{(i)}_{kj}u^j,\\quad i\\neq j\\neq k\\neq i.}\n\\ee{\\label{cond-p4-2}\n3P^{(i)}_i(u^i-u^j)b_{ji} = (X^{(i)}_{ij}-P^{(i)}_{ij}u^j)f^j,\\quad i\\neq j.\n}\n\\ee{\\label{cond-p4-3}\nP^{(i)}_i(u^i-u^j)a_{ij} = (X^{(i)}_{jj}-P^{(i)}_{jj}u^j)f^j,\\quad i\\neq j.\n}\nBy comparing the coefficients of $u^{k,1}\\theta_j^2$, $u^{j,1}\\theta_j^2$, $u^{i,1}\\theta_j^2$, $u^{j,1}\\theta_i^2$ and $u^{i,1}\\theta_i^2$ in $N^i$ and $T^i$, and by using the relation \\eqref{cond-p8-3} we arrive at the following identities:\n\\ee{\\label{cond-p7-1}\nY^{(i)}_{jk} = Q^{(i)}_{jk}u^j,\\quad i\\neq j\\neq k\\neq i.\n}\n\\ee{\\label{cond-p7-2}\n2Y^{(i)}_{jj}f^j+X^{(j)}_jb_{ij} = 2Q^{(i)}_{jj}u^jf^j+P^{(j)}_ju^ib_{ij},\\quad i\\neq j.\n}\n\\ee{\\label{cond-p7-3}\nY^{(i)}_{ji} = Q^{(i)}_{ji}u^j,\\quad i\\neq j.\n}\n\\ee{\\label{cond-p8-1}\nY^{(i)}_{ij} = Q^{(i)}_{ij}u^i,\\quad i\\neq j.\n}\n\\ee{\\label{cond-p8-2}\n2Y^{(i)}_{ii} = 2Q^{(i)}_{ii}u^i+\\frac 12 P^{(i)}_if^i.\n}\n\\begin{proof}[\\textbf{Proof of Lemma \\ref{s3-t8} (Part 2)}] We need to show that a cocycle with vanishing indices must be a coboundary. Take a class $[\\omega]\\in\\mathcal Z/\\mathcal B$ with $\\omega$ being its normal form and $ind_i(\\omega) = 0$. Let $\\alpha\\in\\bar\\Omega^1_2$ be the unique element given by \\eqref{norm-5}--\\eqref{norm-8} such that $\\tilde D_0\\omega = \\tilde D_1\\alpha$. We will prove that $\\alpha=\\omega=0$.\n\nFrom $ind_i(\\omega) = 0$ and \\eqref{cond-p1-1} we know that $X^{(i)}_i = P^{(i)}_i = 0$. Then the identities \\eqref{cond-p1-2} and \\eqref{cond-p2-1}--\\eqref{cond-p3} show that the coefficients $W^{(i)}_{jk}$ vanish when $k\\ne j$ and $Z^{(i)}_{ji} = u^iW^{(i)}_{ji}$; the identities \\eqref{cond-p4-1}--\\eqref{cond-p4-3} together with \\eqref{cond-p8-3} show that the coefficients $P^{(i)}_{jk}$ vanish when $k\\ne i$ and $X^{(i)}_{ji} = u^iP^{(i)}_{ji}$. Similarly, we conclude from the identities \\eqref{norm-4}, \\eqref{norm-8} and \\eqref{cond-p7-1}--\\eqref{cond-p8-2} that the coefficients $Q^{(i)}_{jk}=0$ when $k\\ne j$ and $Y^{(i)}_{jj} = u^jQ^{(i)}_{jj}$. In particular, from \\eqref{norm-4} we know that $Q^{(i)}_{ii} = 0$.\n\nTo simplify the notations, we use $Y^{(i)}_j$ and $Q^{(i)}_j$ to denote $Y^{(i)}_{jj}$ and $Q^{(i)}_{jj}$. Therefore we can represent the coefficients of $\\omega$ and $\\alpha$ that are given in \\eqref{norm-2}, \\eqref{norm-3} and \\eqref{norm-6}, \\eqref{norm-7} as follows:\n\\begin{align*}\nX^i &=  \\sum_{j}\\left(X^{(i)}_{ji}u^{i,1}\\theta_j^1+Z^{(i)}_{ji}u^{i,2}\\theta_j\\right)+\\sum_{j,k,l}Z^{(i)}_{l;jk}u^{k,1}u^{j,1}\\theta_l,\\\\\nP^i &=  \\sum_{j}\\left(P^{(i)}_{ji}u^{i,1}\\theta_j^1+W^{(i)}_{ji}u^{i,2}\\theta_j\\right)+\\sum_{j,k,l}W^{(i)}_{l;jk}u^{k,1}u^{j,1}\\theta_l,\\\\\nY^i &= \\sum_jY^{(i)}_j(u^{j,1})^2,\\quad Q^i = \\sum_jQ^{(i)}_j(u^{j,1})^2,\n\\end{align*}\nwhere the coefficients satisfy the conditions\n\\begin{align}\n\\label{pc-1}\n&X^{(i)}_{ji} = u^iP^{(i)}_{ji},\\quad Z^{(i)}_{ji} = u^iW^{(i)}_{ji};\\\\\n\\label{pc-2}\n&Y^{(i)}_{j} = u^jQ^{(i)}_{j},\\quad Y^{(i)}_i = Q^{(i)}_{i} = 0.\n\\end{align}\n\nLet us first compare the coefficients of $u^{i,1}u^{i,2}\\theta_j$ in $N^i$ and $T^i$ to arrive at\n\\ee{\\label{np4}\n2Z^{(i)}_{j;ii} = 2u^iW^{(i)}_{j;ii}-\\frac 12 W^{(i)}_{ji},\\quad\\forall\\ i,j.\n}\nAt the same time we compute the coefficients of $u^{i,1}\\theta_j\\theta_i^2$ in $M^i$ and $S^i$, and we obtain\n\\ee{\\label{np7-0}-2Z^{(i)}_{ji}\\partial_if^i-2Z^{(i)}_{j;ii}f^i = -2W^{(i)}_{ji}\\partial_i(u^if^i)-2W^{(i)}_{l;ii}u^if^i-\\frac 12 W^{(i)}_{ji}f^i,\\quad\\forall\\ i,j.}\nThen it follows from the identities \\eqref{pc-1}, \\eqref{np4} and \\eqref{np7-0} that \n\\ee{\\label{np7}Z^{(i)}_{ji} = W^{(i)}_{ji} = 0,\\quad\\forall\\ i,j.}\n\nNext we compare the coefficients of $u^{k,1}u^{j,2}\\theta_l$ in $N^i$ and $T^i$ for distinct $j$, $k$ and we obtain the relations\n\\ee{\\label{np2}\nZ^{(i)}_{l;jk} = u^iW^{(i)}_{l;jk},\\quad j\\neq k,\\quad\\forall\\ i,l.\n}\nFor $j\\neq i$, we compare the coefficients of $u^{j,1}\\theta_l\\theta_k^2$ in $M^i$ and $S^i$ and we arrive at\n\\ee{\\label{np5-1}Z^{(i)}_{l;jk} = u^kW^{(i)}_{l;jk},\\quad i\\neq j,\\quad\\forall\\ k,l.}\nSince $Z^{i}_{l;jk} = Z^{(i)}_{l;kj}$, $W^{i}_{l;jk} = W^{(i)}_{l;kj}$, the identities \\eqref{np2} and \\eqref{np5-1} imply  that the only possibly non-vanishing coefficients are $Z^{(i)}_{l;jj}$ and $W^{(i)}_{l;jj}$; moreover, from the identities \\eqref{np4} and \\eqref{np7} we conclude that\n\\ee{\\label{np5-2}Z^{(i)}_{l;jj} = u^jW^{(i)}_{l;jj},\\quad\\forall\\ i,j,l.}\n\nWe proceed to compute the coefficients of $(u^{i,1})^2\\theta_j^1$ in $N^i$ and $T^i$. By using \\eqref{pc-1} and \\eqref{pc-2}, it is easy to deduce that\n\\ee{\\label{np14-2}X^{(i)}_{ji} = P^{(i)}_{ji} = 0,\\quad\\forall\\ i,j.}\nThanks to this equation, we can also compute  the coefficients of $(u^{j,1})^2\\theta_j^1$ of $N^i$ and $T^i$ and obtain the relations\n\\ee{\\label{np14-0}f^iZ^{(i)}_{j;jj} = u^if^iW^{(i)}_{j;jj}+2Q^{(i)}_jf^j,\\quad i\\neq j. }\nOn the other hand, it follows from \\eqref{np7} and the coefficients of $u^{j,1}u^{j,2}\\theta_j$ in $N^i$ and $T^i$ for $i\\neq j$ that\n\\ee{\\label{np3-2}\n2f^iZ^{(i)}_{j;jj} = 2u^if^iW^{(i)}_{j;jj}+Q^{(i)}_jf^j,\\quad i\\neq j.\n}\nThus the relations \\eqref{np14-0} and \\eqref{np3-2} together with \\eqref{pc-2} lead to\n\\ee{\\label{np14-1}Y^i = Q^i = 0,\\quad\\forall\\ i.}\n\nNow thanks to \\eqref{np7}, \\eqref{np14-2} and \\eqref{np14-1}, we are able to easily compare the coefficients of $u^{j,1}u^{j,2}\\theta_k$ and $u^{j,1}u^{j,2}\\theta_i$ in $N^i$ and $T^i$ respectively and we arrive at\n\\aaa{\\label{np3-1}\nZ^{(i)}_{k;jj}\n&=u^iW^{(i)}_{k;jj},\\quad i\\neq j\\neq k\\neq i;\\\\\n\\label{np3-3}\nZ^{(i)}_{i;jj}\n&=u^iW^{(i)}_{i;jj},\\quad i\\neq j,\n}\nTherefore by combining the identities \\eqref{np5-2}, \\eqref{np3-1} and \\eqref{np3-3} we conclude that\n\\[\nX^i = \\sum_jZ^{(i)}_{j;ii}(u^{i,1})^2\\theta_j,\\quad P^i = \\sum_jW^{(i)}_{j;ii}(u^{i,1})^2\\theta_j,\\quad Z^{(i)}_{j;ii} = u^iW^{(i)}_{j;ii}.\\]\n\nand comparing the coefficients of $(u^{i,1})^3$ we deduce that $P^i=0$,  and it follows from the above relation that $X^i$ also vanish. The lemma is proved.\n\n\\end{proof}\n\n\\section{Vanishing theorem of the variational bihamiltonian cohomology}\\label{vsh}\n\\subsection{Vanishing theorem and the strategy of computation.}\nIn this section, we compute the general variational bihamiltonian cohomology groups $\\vbh^p_d(\\bar\\Omega,\\tilde D_0,\\tilde D_1)$. The main result we are to obtain is the following theorem.\n\\begin{Th}\n\\label{thm-van}\nThe cohomology group $H^p_d(\\Omega[\\lambda],\\partial_\\lambda)$ vanishes unless the bidegree $(p,d)$ belongs to the following two cases:\n\\begin{align*}\n&\\mathrm{Case\\, 1:}\\quad d = 0,\\cdots,n;\\quad p = d+1,\\cdots,d+n+1,\\\\\n&\\mathrm{Case\\, 2:}\\quad d = 2,\\cdots,n+3;\\quad p = d,\\cdots,d+n.\n\\end{align*}\n\\end{Th}\n\\begin{proof}[Proof of Theorem \\ref{vbh-res}]\nSimilar to Lemma \\ref{s3-t3}, we have the following long exact sequence:\n\\begin{equation*}\n\\cdots\\rightarrow H^p_d(\\Omega[\\lambda],\\partial_\\lambda)\\rightarrow H^p_d(\\bar\\Omega[\\lambda],\\partial_\\lambda)\\rightarrow H^{p+1}_d(\\Omega[\\lambda],\\partial_\\lambda)\\rightarrow\\cdots.\n\\end{equation*}\nThen Theorem \\ref{vbh-res} follows from Theorem \\ref{thm-van} together with Lemma \\ref{lem-lambda},\nTheorem \\ref{h23} and Theorem \\ref{vbh12}.\n\\end{proof}\n\nThe strategy to prove the above theorem is inspired by the work \\cite{carlet2018deformations,carlet2018central}, where some appropriate spectral sequences are used to compute the bihamiltonian cohomology of a bihamiltonian structure of hydrodynamic type. Our computation in this section can be viewed as a certain generalization of that of \\cite{carlet2018central}, therefore we will use the same notations as the ones used in \\cite{carlet2018central} whenever possible.\n\nFor a semisimple bihamiltonian structure $(P_0,P_1)$ of hydrodynamic type, we will work in its canonical coordinates $u^1,\\cdots, u^n$. Given $\\omega\\in\\Omega^p$, we represent it in the form\n\\[\\omega = \\sum_{i;s\\geq 0}g_{i,s}\\delta u^{i,s}+h^i_s\\delta\\theta_i^s,\\quad g_{i,s}\\in\\hm A^p,\\ h^i_s\\in\\hm A^{p-1},\\]\nthen by using the formula given in Example \\ref{ex-3-2-zh} we have\n\\[\\tilde D_a\\omega = \\sum_{i;s\\geq 0}(D_{P_a}g_{i,s})\\delta u^{i,s}+(-1)^pg_{i,s}\\delta (D_{P_a}u^{i,s})+(D_{P_a}h^i_s)\\delta\\theta_i^s+(-1)^{p-1}h^i_s\\delta (D_{P_a}\\theta_i^s)\\]\nfor $a=0, 1$. \nWe are going to compute the cohomology of the complex $(\\Omega[\\lambda],\\partial_\\lambda=\\tilde D_1-\\lambda\\tilde D_0)$ in what follows.\n\nLet us define a gradation, called the $u$-degree, on $\\Omega$ by setting \n\\begin{equation*}\n\\deg_u u^{i,s+1} = 1,\\ \\deg_u \\delta u^{i,s} = 1,\\quad i=1,\\cdots,n;\\quad s\\geq 0,\n\\end{equation*}\nand the $u$-degrees of other generators are set to be 0. Let us denote the super degree of an element $\\omega$ of $\\Omega$ by $\\deg_\\theta\\omega$. We filtrate the complex $\\Omega[\\lambda]$ by defining\n\\[\nF^k\\Omega[\\lambda] = \\{\\omega\\in\\Omega[\\lambda]\\mid\\deg_u\\omega+\\deg_\\theta\\omega\\geq k\\},\n\\]\nthen we have a filtration\n\\[\\cdots F^{k+1}\\Omega[\\lambda]\\subset F^k\\Omega[\\lambda]\\subset\\cdots F^0\\Omega[\\lambda] = \\Omega[\\lambda].\\]\nWe also decompose the differential in the following way:\n\\[\\partial_\\lambda = \\Delta_{-1}+\\Delta_0+\\cdots,\\quad \\deg_u\\Delta_k = k.\\]\nNote that $\\deg_\\theta\\partial_\\lambda = 1$, therefore each $\\Delta_k$ preserves the filtration. Now let $(^1E_r,d_r), r\\geq 0$ be the spectral sequence induced by the filtration, then by the standard construction $(^1E_0,d_0) = (\\Omega[\\lambda],\\Delta_{-1})$, and $^1E_k$ is given by the cohomology of $^1E_{k-1}$ for $k\\geq 1$. It is clear that when restricted to $\\Omega^p_d[\\lambda]$, the above filtration is bounded and therefore this guarantees the convergence of the spectral sequence. Note that the differential $d_r$ with $r=0$ or $1$ is not the\ninduced differential on $\\hm F$ used in Sect.\\,\\ref{vbc}.\n\nWe will compute $^1E_1$ and $^1E_2$ in the following subsections, to this end we need to construct some other spectral sequences as in \\cite{carlet2018central}. The following standard fact is also used in the computation.\n\\begin{Lem}\n\\label{ha-lem}\nLet $(\\mathcal C^\\bullet,d)$ be a cochain complex in an Abelian category. Assume that for each $p$ we have the decomposition $\\mathcal C^p = \\mathcal A^p\\oplus \\mathcal B^p$ with $\\mathcal A^\\bullet$ being a subcomplex, and $\\mathcal A^\\bullet$ is acyclic. Then \n\\begin{equation}\nH^p(\\mathcal C^\\bullet,d)\\cong H^p(\\mathcal B^\\bullet,\\pi_B\\circ d).\n\\end{equation}\nHere $\\pi_B$ is the projection from $\\mathcal C^\\bullet$ to $\\mathcal B^\\bullet$.\n\\end{Lem}\n\n\n\\subsection{Computation of $^1E_1$.}\\label{subsec-5-2-zh} We first write down $\\Delta_{-1}$ in the explicit form\n\\begin{align}\n\\label{Qd-1}\n\\Delta_{-1} &= \\sum_{s\\geq 1;i}(-\\lambda+u^i)f^i\\theta_i^{s+1}\\diff{}{u^{i,s}}+\\sum_{s\\geq 0;i}(-\\lambda+u^i)f^i\\delta\\theta_i^{s+1}\\diff{}{\\delta u^{i,s}}\\\\\n\\nonumber\n&:=\\sum_i(-\\lambda+u^i)f^i\\delta\\hat d_i,\n\\end{align}\nhere we introduce the de Rham-type differential\n\\begin{equation}\n\\label{delta-di}\n\\delta\\hat d_i = \\sum_{s\\geq 1}\\theta_i^{s+1}\\diff{}{u^{i,s}}+\\sum_{s\\geq 0}\\delta\\theta_i^{s+1}\\diff{}{\\delta u^{i,s}}.\n\\end{equation}\nWe will also use the notation\n\\begin{equation}\n\\label{di}\n\\hat d_i = \\sum_{s\\geq 1}\\theta_i^{s+1}\\diff{}{u^{i,s}}.\n\\end{equation}\n\\begin{Lem}\nEach summand of the following decomposition is a cochain subcomplex with respect to $\\Delta_{-1}$:\n\\begin{equation}\n\\label{decomp1}\n\\Omega[\\lambda] = \\hm{A}[\\lambda]\\{\\delta\\theta_i\\mid i=1,\\cdots,n\\}\\oplus\\bigoplus_{i=1}^n\\hm{A}[\\lambda]\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\mid s\\geq 0\\}.\n\\end{equation}\nHere we use the notation $R\\{g_1, g_2, \\dots\\}$ to denote the free $R$-module generated by $g_1, g_2, \\dots$.\n\\end{Lem}\n\nIn what follows, we will use the same notations as in \\cite{carlet2018deformations} to denote the following subspaces:\n\\begin{align*}\n\\hm{C} &= C^\\infty(u)[\\theta_1,\\cdots,\\theta_n,\\theta_1^1,\\cdots,\\theta_n^1],\\\\\n\\hm{C}_i &= \\hm{C}[[ u^{i,s},\\theta_i^{s+1}\\mid s\\geq 1]],\n\\end{align*}\nand use $\\hm{C}_i^{nt}$ to denote the subspace of $\\hm{C}_i$ spanned by nontrivial monomials, i.e., all monomials that contain at least one of the variables $u^{i,s}$, $\\theta_i^{s+1}$ for $s\\geq 1$. We also use $\\hm{M}$ to denote the subspace of $\\hm A$ spanned by monomials which contain at least one of the mixed quadratic expressions:\n\\begin{equation*}\nu^{i,s}u^{j,t};\\quad u^{i,s}\\theta_j^{t+1};\\quad \\theta_i^{s+1}\\theta_j^{t+1}\n\\end{equation*}\nfor some $i\\neq j$ and $s,t\\geq 1$. Then it is easy to see that there is a decomposition\n\\begin{equation}\n\\label{decomp2}\n\\hm A = \\hm C\\oplus\\kk{\\bigoplus_{i=1}^n\\hm C_i^{nt}}\\oplus\\hm M,\n\\end{equation}\nand each summand is preserved under the action of $\\hat d_i$ given by \\eqref{di}.\n\\begin{Lem}\nWe have\n\\begin{equation*}\nH(\\hm{A}[\\lambda]\\{\\delta\\theta\\},\\Delta_{-1}) = \\hm{C}[\\lambda]\\{\\delta\\theta\\}\\oplus\\bigoplus_{i=1}^n\\frac{\\hat d_i(\\hm C_i[\\lambda])}{(-\\lambda+u^i)\\hat d_i(\\hm C_i[\\lambda])}\\{\\delta\\theta\\}.\n\\end{equation*}\nHere $\\{\\delta\\theta\\}$ is the abbreviation of $\\{\\delta\\theta_i,\\ i=1,\\cdots,n\\}$.\n\\end{Lem}\n\\begin{proof}\nIt is clear from the formula \\eqref{Qd-1} that\n\\begin{equation*}\nH(\\hm{A}[\\lambda]\\{\\delta\\theta\\},\\Delta_{-1}) = H(\\hm{A}[\\lambda],\\Delta_{-1})\\{\\delta\\theta\\},\n\\end{equation*}\nwhere the action of $\\Delta_{-1}$ on $\\hm A[\\lambda]$ is just $\\sum_i (-\\lambda+u^i)f^i\\hat d_i$. Hence the lemma is proved by applying the result in \\cite{carlet2018deformations}.\n\\end{proof}\n\nAccording to the decomposition \\eqref{decomp1}, we still need to compute cohomology\n\\[H(\\hm{A}[\\lambda]\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\mid s\\geq 0\\},\\Delta_{-1}).\\]\nTo this end, we construct a second spectral sequence $^2E$. For a fixed index $i$, we define the $u^i$-degree by setting\n\\begin{equation*}\n\\deg_{u^i}u^{i,s+1}= 1,\\quad \\deg_{u^i}\\delta u^{i,s} = 1,\\quad s\\geq 0,\n\\end{equation*}\nand other generators have $u^i$-degree zero. Accordingly, we decompose the differential $\\Delta_{-1}$ as follows\n\\begin{equation*}\n\\Delta_{-1}=\\Delta_{-1,-1}+\\Delta_{-1,0},\\quad \\deg_{u^i}\\Delta_{-1,k} = k,\n\\end{equation*}\nwhere\n\\[\n\\Delta_{-1,-1} = (-\\lambda+u^i)f^i\\delta\\hat d_i,\\quad \\Delta_{-1,0} = \\sum_{j\\neq i}(-\\lambda+u^j)f^j\\delta\\hat d_j.\n\\]\nSimilar to our construction of $^1E$, we filtrate $\\hm{A}[\\lambda]\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\mid s\\geq 0\\}$ with $\\deg_{u^i}+\\deg_{\\theta}$, and construct the associated spectral sequence $^2E$. Then we have\n\\[(^2E_0,d_0) = (\\hm{A}[\\lambda]\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\mid s\\geq 0\\},\\Delta_{-1,-1})\\]\nand $^2E_1 = H(^2E_0,d_0)$, with differential $d_1 = \\Delta_{-1,0}$. Since on the $^2E_2$ page the differential becomes 0, hence this spectral sequence becomes convergent on this page, i.e.\n\\[H(\\hm{A}[\\lambda]\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\mid s\\geq 0\\}, \\Delta_{-1}) \\cong{^2E_2}.\\]\n\nLet us compute $^2E_1 = H(^2E_0,d_0)$. Take any element\n\\begin{equation}\n\\label{s5-t1}\n\\omega = \\sum_{s\\geq 0}p_s\\delta u^{i,s}+q_s\\delta\\theta_i^{s+1}\\in {^2E_0},\n\\end{equation}\nwhere we assume that $\\omega$ is homogeneous of degree $p$ with respect to the $\\theta$-degree. Then it is easy to see that\n\\begin{equation*}\nd_0\\omega = (-\\lambda+u^i)f^i\\sum_{s\\geq 0}\\hat d_i(p_s)\\delta u^{i,s}+\\left(\\hat d_i(q_s)+(-1)^pp_s\\right)\\delta\\theta_i^{s+1}.\n\\end{equation*}\nSo if $d_0\\omega = 0$, we have $p_s = (-1)^{p+1}\\hat d_i(q_s)$, and we can write\n\\begin{equation*}\n\\omega = \\delta\\hat d_i\\biggl(\\sum_{s\\geq 0}(-1)^{p+1}q_s\\delta u^{i,s}\\biggr).\n\\end{equation*}\nLet us denote \\[\\hm B_i = \\hm A\\{\\delta u^{i,s},\\ s\\geq 0\\},\\] then the above computation shows that $\\ker d_0\\subset \\delta\\hat d_i(\\hm B_i)[\\lambda]$. The inverse inclusion $\\delta\\hat d_i(\\hm B_i)[\\lambda]\\subset \\ker d_0$ is clear, hence we have $\\ker d_0=\\delta\\hat d_i(\\hm B_i)[\\lambda]$. \n\nTo compute the image of the differential $d_0$, we take another element $\\omega\\in{^2E_0}$ represented in the form of \\eqref{s5-t1}. Observe that\n\\begin{align*}\nd_0\\omega &= (-\\lambda+u^i)f^i\\sum_{s\\geq 0}\\hat d_i(p_s)\\delta u^{i,s}+\\left(\\hat d_i(q_s)+(-1)^pp_s\\right)\\delta\\theta_i^{s+1}\\\\\n&=(-\\lambda+u^i)f^i\\delta\\hat d_i\\sum_{s\\geq 0}\\left(p_s+(-1)^p\\hat d_i(q_s)\\right)\\delta u^{i,s},\n\\end{align*}\nwe see that $\\ima d_0 = (-\\lambda+u^i)\\delta\\hat d_i(\\hm B_i)[\\lambda]$. Therefore we obtain the following lemma.\n\\begin{Lem} We have\n\\begin{equation*}\n^2E_1 = \\frac{\\delta\\hat d_i(\\hm B_i)[\\lambda]}{(-\\lambda+u^i)\\delta\\hat d_i(\\hm B_i)[\\lambda]}.\n\\end{equation*}\n\\end{Lem}\n\nNext let us compute $^2E_2 = H(^2E_1,d_1)$. We use the following identification:\n\\begin{equation*}\n^2E_1 = \\frac{\\delta\\hat d_i(\\hm B_i)[\\lambda]}{(-\\lambda+u^i)\\delta\\hat d_i(\\hm B_i)[\\lambda]}\\cong \\delta\\hat d_i(\\hm B_i)\n\\end{equation*}\nby identifying $\\lambda$ with $u^i$. Under this identification, we have\n\\begin{equation*}\nd_1 = \\sum_j(u^j-u^i)f^j\\delta\\hat d_j.\n\\end{equation*}\nThus we can represent an element of $^2E_1$ as follows:\n\\begin{equation*}\n\\omega = \\delta\\hat d_i \\sum_{s\\geq 0}p_s\\delta u^{i,s} = \\sum_{s\\geq 0}\\hat d_i(p_s)\\delta u^{i,s}+(-1)^pp_s\\delta\\theta_i^{s+1}\\in {^2E_1}.\n\\end{equation*}\nIf $\\omega$ is a cocycle, i.e. $d_1\\omega = 0$, by using the fact that $\\delta\\hat d_i$ commutes with $d_1$ we obtain\n\\begin{equation}\n\\label{s5-t2}\n\\sum_{j}(u^j-u^i)f^j\\hat d_j(p_s) = 0.\n\\end{equation}\nLet us consider the following decomposition according to \\eqref{decomp2} as follows:\n\\begin{equation*}\np_s = \\hat p_s+\\sum_k  p_s^k+p_s^m,\n\\end{equation*}\nwhere $\\hat p_s \\in \\hm C$, $ p_s^j \\in \\hm C_j^{nt}$ and $p_s^m\\in\\hm M$. Then from the equation \\eqref{s5-t2} and the obvious fact that $\\hat d_j$ annihilates $p_s^k$ unless $k=j$, we arrive at the following equations:\n\\begin{equation}\n\\left(u^k-u^i\\right)f^k\\hat d_k(p_s^k) = 0,\\quad \\sum_{j}(u^j-u^i)f^j\\hat d_j(p_s^m) = 0.\\label{ukuifk}\n\\end{equation}\nTherefore for $k\\neq i$, we have $\\hat d_k(p_s^k) = 0$ and by Poincar\\'e Lemma (Lemma 12 in \\cite{carlet2018deformations}), this implies $p_s^k = \\hat d_k(q_s^k)$ for some $q_s^k\\in\\hm C_k^{nt}$. For the term $p_s^m\\in\\hm M$, after a rescaling by a non-zero factor, we may rewrite the second equation of \\eqref{ukuifk} as $\\sum_{j\\neq i}\\hat d_j(p_s^m) = 0$, then by Proposition 11 in \\cite{carlet2018deformations} (or a simplified version, Lemma 3.4 in \\cite{carlet2018central}), we see that $p_s^m = \\sum_{j\\neq i}\\hat d_j(q_s^m)+h_s$, where $q_s^m\\in\\hm M$ and $h_s\\in\\hm C_i$. But such $h_s$ must vanish since we have $h_s = p_s^m - \\sum_{j\\neq i}\\hat d_j(q_s^m)\\in\\hm M$.\n\nTo conclude, we can rewrite $p_s$ as\n\\begin{equation*}\np_s = \\hat p_s+p_s^i+\\sum_{j\\neq i}\\hat d_i q_s^j+d_1(q_s^m) = \\hat p_s+p_s^i+ d_1\\left(\\sum_{j\\neq i}\\frac{q_s^j}{(u^j-u^i)f^j}+q_s^m\\right).\n\\end{equation*}Using again the fact that $\\delta\\hat d_i$ and $d_1$ commutes, we see that\n\\begin{align*}\n\\omega &= \\delta\\hat d_i (\\sum_{s\\geq 0}p_s\\delta u^{i,s})\\\\\n&= \\delta\\hat d_i\\sum_{s\\geq 0}(\\hat p_s+p_s^i)\\delta u^{i,s}-d_1\\delta\\hat d_i\\sum_{s\\geq 0}\\sum_{j\\neq i}\\left(\\frac{q_s^j}{(u^j-u^i)f^j}+q_s^m\\right)\\delta u^{i,s}.\n\\end{align*}\nSo we see that for any $\\omega \\in\\ker d_1$, there is $\\eta \\in \\hm C_i\\{\\delta u^{i,s}\\mid s\\geq 0\\}$ such that $[\\omega] = [\\delta\\hat d_i(\\eta)]$ in $^2E_2$. Let us denote $\\hm H_i =\\hm C_i\\{\\delta u^{i,s},\\ s\\geq 0\\}$. It is obvious that each element in $\\delta\\hat d_i(\\hm H_i)$ is annihilated by $d_1$ and different elements define different classes in $^2E_2$, hence we arrive at\n\\begin{equation*}\n^2E_2 \\cong \\delta\\hat d_i(\\hm H_i).\n\\end{equation*}\n\nTo summarize all the results above, we obtain the following theorem.\n\\begin{Th}\\label{thm-5-6-zh}\nThe first page $^1E_1$ of the spectral sequence $^1E$ can be described as the direct sum of the following spaces:\n\\begin{equation*}\n^1E_1\\cong \\hm{C}[\\lambda]\\{\\delta\\theta\\}\\oplus\\bigoplus_{i=1}^n\\frac{\\hat d_i(\\hm C_i[\\lambda])}{(-\\lambda+u^i)\\hat d_i(\\hm C_i[\\lambda])}\\{\\delta\\theta\\}\\oplus\\bigoplus_{i=1}^n\\delta\\hat d_i(\\hm H_i).\n\\end{equation*}\n\\end{Th}\n\\subsection{Computation of $^1E_2$.}\nWe will find suitable bidegrees $(p,d)$ such that the cohomology $^1E_2 = H^p_d(^1E_1,\\Delta_0) = 0$, this means that the spectral sequence $^1E$ collapses on the second page for these bidegrees. Then we conclude that for the same bidegrees $H^p_d(\\Omega[\\lambda],\\partial_\\lambda) = 0$. \n\nWe first write down explicitly the formula for $\\Delta_0$. To avoid lengthy expressions, we will split $\\Delta_0$ into $\\partial/\\partial u^{i,s}$ part, $\\partial/\\partial \\delta u^{i,s}$ part, $\\partial/\\partial \\theta_i^s$ part and $\\partial/\\partial \\delta\\theta_i^s$ part. In the following formulae, the index $i$ is fixed and does not participate in the summation. These formulae are comparable to those given in \\cite{carlet2018deformations}.\n\nThe $\\partial/\\partial u^{i,s}$ part of $\\Delta_0$ reads\n\\begin{align*}\n&\\sum_{s\\geq 1}\\sum_{t=1}^s\\binom{s}{t}u^{i,t}f^i\\theta_i^{s-t+1}\\diff{}{u^{i,s}}\n+\\sum_{s\\geq 1;j}\\sum_{t=1}^s\\binom{s}{t}(-\\lambda+u^i)\\aij ji u^{j,t}\\theta_i^{s-t+1}\\diff{}{u^{i,s}}\\\\\n+&\\frac 12\\sum_{s\\geq 1}\\sum_{t=0}^s\\binom{s}{t}u^{i,t+1}f^i\\theta_i^{s-t}\\diff{}{u^{i,s}}\n+\\frac 12\\sum_{s\\geq 1;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^i)\\aij ji u^{j,t+1}\\theta_i^{s-t}\\diff{}{u^{i,s}}\\\\\n+&\\frac 12\\sum_{s\\geq 1;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^i)\\bij ij u^{j,t+1}\\theta_j^{s-t}\\diff{}{u^{i,s}}\\\\\n-&\\frac 12 \\sum_{s\\geq 1;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^j)\\bij ji u^{i,t+1}\\theta_j^{s-t}\\diff{}{u^{i,s}}+(\\lambda+u^i)f^i\\theta_i^1\\diff{}{u^i}.\n\\end{align*}\nThe $\\partial/\\partial \\delta u^{i,s}$ part part of $\\Delta_0$ reads:\n\\begin{align*}\n&\\sum_{s\\geq 0}\\sum_{t=0}^s\\binom{s}{t}f^i\\theta_i^{s-t+1}\\delta u^{i,t}\\diff{}{\\delta u^{i,s}}\n+\\sum_{s\\geq 0;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^i)\\aij ji \\theta_i^{s-t+1}\\delta u^{j,t}\\diff{}{\\delta u^{i,s}}\\\\\n+&\\sum_{s\\geq 1}\\sum_{t=1}^s\\binom{s}{t}f^iu^{i,t}\\delta\\theta_i^{s-t+1}\\diff{}{\\delta u^{i,s}}\n+\\sum_{s\\geq 1;j}\\sum_{t=1}^s\\binom{s}{t}(-\\lambda+u^i)\\aij ji u^{j,t}\\delta\\theta_i^{s-t+1}\\diff{}{\\delta u^{i,s}}\\\\\n+&\\frac 12\\sum_{s\\geq 0}\\sum_{t=0}^s\\binom{s}{t}f^i\\delta(u^{i,t+1}\\theta_i^{s-t})\\diff{}{\\delta u^{i,s}}\n+\\frac 12\\sum_{s\\geq 0;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^i)\\aij ji \\delta(u^{j,t+1}\\theta_i^{s-t})\\diff{}{\\delta u^{i,s}}\\\\\n+&\\frac 12\\sum_{s\\geq 1;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^i)\\bij ij \\delta(u^{j,t+1}\\theta_j^{s-t})\\diff{}{\\delta u^{i,s}}\\\\\n-&\\frac 12 \\sum_{s\\geq 1;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^j)\\bij ji \\delta(u^{i,t+1}\\theta_j^{s-t})\\diff{}{\\delta u^{i,s}}.\n\\end{align*}\nThe $\\partial/\\partial \\theta_i^s$ part part of $\\Delta_0$ reads\n\\begin{align*}\n&\\frac 12 \\sum_{s\\geq 0}\\sum_{t=0}^s\\binom{s}{t}f^i\\theta_i^t\\theta_i^{1+s-t}\\diff{}{\\theta_i^s}+\\frac 12\\sum_{s\\geq 0;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^j)\\aij ij \\theta_j^t\\theta_j^{1+s-t}\\diff{}{\\theta_i^s}\\\\\n+&\\frac 12 \\sum_{s\\geq 0;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^j)\\bij ji \\theta_i^t\\theta_j^{1+s-t}\\diff{}{\\theta_i^s}\\\\\n-&\\frac 12\\sum_{s\\geq 0;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^j)\\bij ji \\theta_j^t\\theta_i^{1+s-t}\\diff{}{\\theta_i^s}.\n\\end{align*}\nThe $\\partial/\\partial \\delta\\theta_i^s$ part part of $\\Delta_0$ reads\n\\begin{align*}\n&\\frac 12 \\sum_{s\\geq 0}\\sum_{t=0}^s\\binom{s}{t}f^i\\delta(\\theta_i^t\\theta_i^{1+s-t})\\diff{}{\\delta\\theta_i^s}+\\frac 12\\sum_{s\\geq 0;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^j)\\aij ij \\delta(\\theta_j^t\\theta_j^{1+s-t})\\diff{}{\\delta\\theta_i^s}\\\\\n+&\\frac 12 \\sum_{s\\geq 0;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^j)\\bij ji \\delta(\\theta_i^t\\theta_j^{1+s-t})\\diff{}{\\delta\\theta_i^s}\\\\\n-&\\frac 12\\sum_{s\\geq 0;j}\\sum_{t=0}^s\\binom{s}{t}(-\\lambda+u^j)\\bij ji \\delta(\\theta_j^t\\theta_i^{1+s-t})\\diff{}{\\delta\\theta_i^s}.\n\\end{align*}\n\nTo compute the cohomology $H({^1E_1},\\Delta_0)$, we introduce a third spectral sequence $^3E$ by defining the $\\theta^1$-degree\n$\\deg_{\\theta^1}\\theta_i^1 = 1$\nfor $i=1,\\cdots, n$, and other generators have $\\deg_{\\theta^1}$ zero. Then we filtrate ${^1E_1}$ via\n\\[F^r({^1E_1}) = \\{\\deg_{\\theta_1}-\\deg_\\theta \\omega\\leq -r\\mid\\omega\\in {^1E_1}\\}.\\]\nWe also have the decomposition\n\\begin{equation*}\n\\Delta_0 = \\Delta_{0,1}+\\Delta_{0,0}+\\Delta_{0,-1},\\quad \\deg_{\\theta^1}\\Delta_{0,k} = k.\n\\end{equation*}\nThe zeroth page  $(^3E_0,d_0)$ is given by $(^1E_1,\\Delta_{0,1})$. We want to determine the bidegrees $(p,d)$ such that $H^p_d(^3E_0,d_0) = 0$, then we conclude that for the same degrees $H^p_d(^1E_1,\\Delta_0) = 0$. We first write down the explicit formula for $\\Delta_{0,1}$.\n\nThe $\\partial/\\partial u^{i,s}$ part of $\\Delta_{0,1}$ reads\n\\begin{align*}\n&(-\\lambda+u^i)f^i\\theta_i^1\\diff{}{u^i}+\\sum_{s\\geq 1}(\\frac s2+1)f^i\\theta_i^1u^{i,s}\\diff{}{u^{i,s}}+\\sum_{s\\geq 1;j}(\\frac s2+1)(-\\lambda+u^i)\\aij ji \\theta_i^1u^{j,s}\\diff{}{u^{i,s}}\\\\\n+&\\frac 12 \\sum_{s\\geq 1;j}s(-\\lambda+u^i)\\bij ij \\theta_j^1u^{j,s}\\diff{}{u^{i,s}}-\\frac 12 \\sum_{s\\geq 1;j}s(-\\lambda+u^j)\\bij ji \\theta_j^1u^{i,s}\\diff{}{u^{i,s}}.\n\\end{align*}\nThe $\\partial/\\partial \\delta u^{i,s}$ part of $\\Delta_{0,1}$ reads\n\\begin{align*}\n&\\sum_{s\\geq 0}f^i\\theta_i^1\\delta u^{i,s}\\diff{}{\\delta u^{i,s}}+\\sum_{s\\geq 0;j}(-\\lambda+u^i)\\aij ji \\theta_i^1\\delta u^{j,s}\\diff{}{u^{i,s}}\\\\\n+&\\frac 12\\sum_{s\\geq 0;j}s(-\\lambda+u^i)\\aij ji \\theta_i^1\\delta u^{j,s}\\diff{}{\\delta u^{i,s}}+\\frac 12 \\sum_{s\\geq 0}sf^i\\theta_i^1\\delta u^{i,s}\\diff{}{\\delta u^{i,s}}\\\\\n+&\\frac 12 \\sum_{s\\geq 0;j}s(-\\lambda+u^i)\\bij ij \\theta_j^1\\delta u^{j,s}\\diff{}{\\delta u^{i,s}}-\\frac 12 \\sum_{s\\geq 0;j}s(-\\lambda+u^j)\\bij ji\\theta_j^1\\delta u^{i,s}\\diff{}{\\delta u^{i,s}}.\n\\end{align*}\nThe $\\partial/\\partial \\theta_i^s$ part part of $\\Delta_{0,1}$\n\\begin{align*}\n&\\frac 12\\sum_{s\\geq 0;j}(-\\lambda+u^j)\\aij ij (s-1)\\theta_j^1\\theta_j^s\\diff{}{\\theta_i^s}+\\frac 12\\sum_{s\\geq 0}f^i(s-1)\\theta_i^1\\theta_i^s\\diff{}{\\theta_i^s}\\\\\n+&\\frac 12 \\sum_{\\substack{s\\geq 0;j\\\\ s\\neq 1}}(\\lambda+u^j)\\bij ji (s+1)(\\theta_i^s\\theta_j^1-\\theta_j^s\\theta_i^1)\\diff{}{\\theta_i^s}+\\sum_j(-\\lambda+u^j)\\bij ji \\theta_i^1\\theta_j^1\\diff{}{\\theta_i^1}\n\\end{align*}\nThe $\\partial/\\partial \\delta\\theta_i^s$ part part of $\\Delta_0$\n\\begin{align*}\n&\\frac 12\\sum_{s\\geq 0;j}(-\\lambda+u^j)\\aij ij (s-1)\\theta_j^1\\delta\\theta_j^s\\diff{}{\\delta\\theta_i^s}+\\frac 12\\sum_{s\\geq 0}f^i(s-1)\\theta_i^1\\delta\\theta_i^s\\diff{}{\\theta_i^s}\\\\\n+&\\frac 12 \\sum_{\\substack{s\\geq 0;j\\\\ s\\neq 1}}(\\lambda+u^j)\\bij ji (s+1)(\\delta\\theta_i^s\\theta_j^1-\\delta\\theta_j^s\\theta_i^1)\\diff{}{\\theta_i^s}+\\sum_j(-\\lambda+u^j)\\bij ji \\delta(\\theta_i^1\\theta_j^1)\\diff{}{\\theta_i^1}\n\\end{align*}\n\nTo simplify the above expressions, we perform a rescaling on the generators of $\\Omega$ (see \\cite{carlet2018central}) as follows:\n\\begin{align*}\n&\\Psi\\colon u^{i,s}\\mapsto (f^i)^{\\frac s2}u^{i,s};\\quad \\theta_i^s\\mapsto (f^i)^{\\frac{s+1}{2}}\\theta_i^s;\\ s\\geq 0;\\\\\n&\\Psi\\colon \\delta u^{i,s}\\mapsto (f^i)^{\\frac s2}\\delta u^{i,s};\\quad \\delta\\theta_i^s\\mapsto (f^i)^{\\frac{s+1}{2}}\\delta\\theta_i^s;\\ s\\geq 0.\n\\end{align*}\nNote that this is NOT induced by a change of coordinate, but just an isomorphism of the space $\\Omega$. The expression of $\\Delta_{0,1}$ will be simplified after being conjugated by $\\Psi$, and since $\\Psi$ leaves all the decomposition of the complex invariant, this will not affect the computation of cohomology groups.\n\nThe following identities are easy to verify and helpful for our computation of the conjugated operator $\\tilde\\Delta_{0,1}=\\Psi^{-1}\\Delta_{0,1}\\Psi$:\n\\begin{align*}\n&\\Psi^{-1}u^{i,s}\\Psi = (f^i)^{-\\frac s2}u^{i,s};\\quad \\Psi^{-1}\\theta_i^s\\Psi = (f^i)^{-\\frac{s+1}{2}}\\theta_i^s;\\\\\n&\\Psi^{-1}\\delta u^{i,s}\\Psi = (f^i)^{-\\frac s2}u^{i,s};\\quad \\Psi^{-1}\\delta\\theta_i^s\\Psi = (f^i)^{-\\frac{s+1}{2}}\\theta_i^s;\\\\\n&\\Psi^{-1}\\diff{}{u^{i,s}}\\Psi = (f^i)^{\\frac s2}\\diff{}{u^{i,s}}, s\\geq 1;\\quad \\Psi^{-1}\\diff{}{\\theta_i^s}\\Psi = (f^i)^{\\frac{s+1}{2}}\\diff{}{\\theta_i^s}, s\\geq 0;\\\\\n&\\Psi^{-1}\\diff{}{\\delta u^{i,s}}\\Psi = (f^i)^{\\frac s2}\\diff{}{\\delta u^{i,s}}, s\\geq 0;\\quad \\Psi^{-1}\\diff{}{\\delta\\theta_i^s}\\Psi = (f^i)^{\\frac{s+1}{2}}\\diff{}{\\delta\\theta_i^s}, s\\geq 0;\\\\\n&\\Psi^{-1}\\diff{}{u^{i}}\\Psi =\\diff{}{u^i}+\\sum_{s\\geq 0;j}\\frac{\\aij ij}{f^j}\\left(\\frac s2 u^{j,s}\\diff{}{u^{j,s}}+\\frac{s+1}{2}\\theta_j^s\\diff{}{\\theta_j^s}\\right).\n\\end{align*}\nThen by using the rotation coefficients \n\\begin{equation*}\n\\gamma_{ij} = -\\frac 12 \\left(\\frac{f^i}{f^j}\\right)^{1/2}\\frac{\\aij ij}{f^j}\n\\end{equation*}\ndefined for the diagonal metric $(f^1,\\cdots,f^n)$,\nwe can represent $\\tilde \\Delta_{0,1}$ in the form\n\\[ \\tilde \\Delta_{0,1}= \\phi_1+\\phi_2+\\phi_3,\\]\nwhere\n\\begin{align*}\n\\phi_1 =& -\\sum_{s\\geq 1;i,j}(-\\lambda+u^i)\\left(\\frac{f^i}{f^j}\\right)^{\\frac{s+1}{2}}\\left((s+2)\\gamma_{ji}\\theta_i^1+s\\gamma_{ij}\\theta_j^1\\right)u^{j,s}\\diff{}{u^{i,s}}\\\\\n&-\\sum_{s\\geq 0;i,j}(-\\lambda+u^i)\\left(\\frac{f^i}{f^j}\\right)^{\\frac{s+1}{2}}\\left((s+2)\\gamma_{ji}\\theta_i^1+s\\gamma_{ij}\\theta_j^1\\right)\\delta u^{j,s}\\diff{}{\\delta u^{i,s}}\\\\\n&+\\sum_{s\\geq 2;i,j}(-\\lambda+u^j)\\left(\\frac{f^i}{f^j}\\right)^{\\frac{s}{2}}\\left((1-s)\\gamma_{ij}\\theta_j^1-(1+s)\\gamma_{ji}\\theta_i^1\\right)\\theta_j^s\\diff{}{\\theta_i^s},\\\\\n\\phi_2=& \\sum_{s\\geq 2;i,j}(-\\lambda+u^j)\\left(\\frac{f^i}{f^j}\\right)^{\\frac{s}{2}}\\left((1-s)\\gamma_{ij}\\theta_j^1-(1+s)\\gamma_{ji}\\theta_i^1\\right)\\delta\\theta_j^s\\diff{}{\\delta\\theta_i^s}\\\\\n&+\\sum_{i,j}(-\\lambda+u^j)\\frac{\\partial_jf^i}{f^j}\\theta_i^1\\delta\\theta_j^1\\diff{}{\\delta\\theta_i^1},\\\\\n\\phi_3 =& -\\frac 12\\sum_{s\\geq 0;i,j}(-\\lambda+u^j)\\frac{\\partial_jf^i}{f^j}\\theta_j^1\\left(s\\delta u^{i,s}\\diff{}{\\delta u^{i,s}}+(s+1)\\delta\\theta_i^s\\diff{}{\\delta\\theta_i^s}\\right)\\\\\n&+\\sum_{i,j}(-\\lambda+u^j)(\\gamma_{ij}\\theta_j^1-\\gamma{ji}\\theta_i^1)\\left(\\theta_j\\diff{}{\\theta_i}+\\delta\\theta_j\\diff{}{\\delta\\theta_i}\\right)\\\\\n&+\\sum_i\\theta_i^1\\mathcal E_i+\\sum_i(-\\lambda+u^i)\\theta_i^1\\diff{}{u^i},\n\\end{align*}\nhere $\\mathcal E_i$ is an Euler-type vector filed given by\n\\begin{equation*}\n\\mathcal E_i = \\sum_{s\\geq 1}\\left(\\frac s2+1\\right)u^{i,s}\\diff{}{u^{i,s}}+\\sum_{s\\geq 0}\\left(\\frac s2+1\\right)\\delta u^{i,s}\\diff{}{\\delta u^{i,s}}+\\sum_{s \\geq 0}\\frac{s-1}{2}\\left(\\theta_i^s\\diff{}{\\theta_i^s}+\\delta\\theta_i^s\\diff{}{\\delta\\theta_i^s}\\right).\n\\end{equation*}\n\nWe first simplify $\\tilde \\Delta_{0,1}$ by the following observation (which is parallel to Lemma 3.6 of \\cite{carlet2018central}).\n\\begin{Lem}\nBoth $\\phi_1$ and $\\phi_2$ act trivially on $^1E_1$.\n\\end{Lem}\n\\begin{proof}\nWe first recall from Theorem \\ref{thm-5-6-zh} that \n\\begin{equation}\n\\label{decomp3}\n^1E_1\\cong \\hm{C}[\\lambda]\\{\\delta\\theta\\}\\oplus\\bigoplus_{i=1}^n\\frac{\\hat d_i(\\hm C_i[\\lambda])}{(-\\lambda+u^i)\\hat d_i(\\hm C_i[\\lambda])}\\{\\delta\\theta\\}\\oplus\\bigoplus_{i=1}^n\\delta\\hat d_i(\\hm H_i).\n\\end{equation}\nThe vanishing of the action of $\\phi_1$ on the cohomology \n\\begin{equation*}\n\\hm{C}[\\lambda]\\{\\delta\\theta\\}\\oplus\\bigoplus_{i=1}^N\\frac{\\hat d_i(\\hm C_i[\\lambda])}{(-\\lambda+u^i)\\hat d_i(\\hm C_i[\\lambda])}\\{\\delta\\theta\\}\n\\end{equation*}\nof $\\hm A[\\lambda]\\{\\delta\\theta\\}$ is a direct consequence of Lemma 3.6 of \\cite{carlet2018central}, and the vanishing of that of $\\phi_2$ is obvious. \n\nNext we consider action of $\\phi_1$ and $\\phi_2$ on\n\\[H(\\hm A[\\lambda]\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\},\\Delta_{-1})=\\delta\\hat d_i(\\hm H_i)\\]\nfor a fixed index $i$. By identifying $\\lambda$ with $u^i$, we can represent $\\omega$ in the form\n\\begin{equation*}\n\\omega = \\delta\\hat d_i(\\sum_{s\\geq 0}p_s\\delta u^{i,s}) = \\sum_{s\\geq 0}\\hat d_i(p_s)\\delta u^{i,s}+(-1)^pp_s\\delta\\theta_i^{s+1},\\quad p_s\\in\\hm C_i.\n\\end{equation*} \nUnder such an identification, the action of $\\phi_1$ can be represented as\n\\begin{align*}\n\\phi_1 =& -\\sum_{s\\geq 1;k,j}(-u^i+u^k)\\left(\\frac{f^k}{f^j}\\right)^{\\frac{s+1}{2}}\\left((s+2)\\gamma_{jk}\\theta_k^1+s\\gamma_{kj}\\theta_j^1\\right)u^{j,s}\\diff{}{u^{k,s}}\\\\\n&-\\sum_{s\\geq 0;k,j}(-u^i+u^k)\\left(\\frac{f^k}{f^j}\\right)^{\\frac{s+1}{2}}\\left((s+2)\\gamma_{jk}\\theta_i^1+s\\gamma_{kj}\\theta_j^1\\right)\\delta u^{j,s}\\diff{}{\\delta u^{k,s}}\\\\\n&+\\sum_{s\\geq 2;k,j}(-u^i+u^j)\\left(\\frac{f^k}{f^j}\\right)^{\\frac{s}{2}}\\left((1-s)\\gamma_{kj}\\theta_j^1-(1+s)\\gamma_{jk}\\theta_k^1\\right)\\theta_j^s\\diff{}{\\theta_k^s}.\n\\end{align*}\nHence it is clear that \n\\[\\phi_1\\omega \\in\\bigoplus_{j\\neq i}\\hm C_j^{nt}\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\mid s\\geq 0\\}.\\] \nFor the action of $\\phi_2$ on $\\omega$, we first regard $\\omega$ as an element of $\\delta \\hat d_i(\\hm H_i)[\\lambda]$, then we observe that \n\\[\\phi_2\\omega\\in\\bigoplus_{j} (-\\lambda+u^j)\\hm C_i[\\lambda]\\{\\delta \\theta_j^{s+1}\\mid s\\geq 0\\}.\\] We further make the decomposition $\\phi_2\\omega=\\alpha_1+\\alpha_2$, where \n\\[\\alpha_1\\in\\bigoplus_{j\\neq i} (-\\lambda+u^j)\\hm C_i[\\lambda]\\{\\delta \\theta_j^{s+1}\\mid s\\geq 0\\},\\quad \n\\alpha_2\\in (-\\lambda+u^i)\\hm C_i[\\lambda]\\{\\delta \\theta_i^{s+1}\\mid s\\geq 0\\}.\\] \nFinally it is easy to see that from the definition of $\\phi_3$ that \n\\[\\phi_3\\omega\\in\\hm C_i[\\lambda]\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\mid  s\\geq 0\\}.\\] \n\n It follows from\n\\begin{equation*}\n\\Delta_{0,1}\\Delta_{-1}+\\Delta_{-1}\\Delta_{0,1} = 0\n\\end{equation*}\nthat $\\tilde \\Delta_{0,1}\\omega=(\\phi_1+\\phi_2+\\phi_3)\\omega$ still lies in the cohomology group. Since the subspaces\n\\begin{align*}\n&\\bigoplus_{j\\neq i}\\hm C_j^{nt}\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\mid s\\geq 0\\},\\\\ &\\bigoplus_{j\\neq i} (-\\lambda+u^j)\\hm C_i[\\lambda]\\{\\delta \\theta_j^{s+1}\\mid s\\geq 0\\},\\\\ &\\hm C_i[\\lambda]\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\mid s\\geq 0\\}\n\\end{align*}\nare disjoint and they are all invariant under the action $\\Delta_{-1}$, we conclude that $\\phi_1\\omega$, $\\alpha_1$ and $\\alpha_2+\\phi_3\\omega$ lie in the cohomology group.\n\nFrom our computation of $^1E_1$ given in \\ref{subsec-5-2-zh}, it follows that terms in the subspace \n\\[\\bigoplus_{j\\neq i}\\hm C_j^{nt}\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\mid s\\geq 0\\}\\] \nvanish in $^1E_1$ and hence action of $\\phi_1$ vanishes. Similarly, from the computation of the spectral sequence $^2E$ given in \\ref{subsec-5-2-zh}, it follows that the terms in the subspace \n\\[\\bigoplus_{j\\neq i} (-\\lambda+u^j)\\hm C_i[\\lambda]\\{\\delta \\theta_j^{s+1}\\mid s\\geq 0\\}\\] \nalso vanish, hence $\\alpha_1 = 0$. For the same reason, the multiples of $(-\\lambda+u^i)$ in the subspace $\\hm C_i[\\lambda]\\{\\delta u^{i,s},\\delta\\theta_i^{s+1}\\mid s\\geq 0\\}$ are trivial in the cohomology as well, hence in particular $\\alpha_2 = 0$ which implies the vanishing of $\\phi_2$. The lemma is proved.\n\\end{proof}\n\nThis above lemma shows that $\\tilde\\Delta_{0,1} = \\phi_3$ and therefore each summand in the decomposition \\eqref{decomp3} of $^1E_1$ \nis preserved by $\\tilde \\Delta_{0,1}$. In what follows, we will show that for some bidegrees $(p,d)$ the action of $\\tilde\\Delta_{0,1}$ is acyclic on each summand when restricted to the elements with super degree $p$ and differential degree $d$.\n\\begin{Lem}\n\\label{s5-t3}\nWe have $H^p_d(\\hm{C}[\\lambda]\\{\\delta\\theta\\},\\tilde\\Delta_{0,1}) = 0$ unless\n\\begin{equation*}\nd = 0,\\cdots,n;\\quad p = d+1,\\cdots,d+n+1.\n\\end{equation*}\n\\end{Lem}\n\\begin{proof}\nIndeed, possible bidegrees $(p,d)$ of elements of  in $\\hm C[\\lambda]\\{\\delta\\theta\\}$ are precisely those excluded in the lemma. The lemma is proved.\n\\end{proof}\n\nTo compute the cohomology of $\\tilde \\Delta_{0,1}$ on the space\n\\begin{equation*}\n\\frac{\\hat d_i(\\hm C_i[\\lambda])}{(-\\lambda+u^i)\\hat d_i(\\hm C_i[\\lambda])}\\{\\delta\\theta\\},\n\\end{equation*}\nlet us first identify this space with $\\hat d_i(\\hm C_i)\\{\\delta\\theta\\}$ by sending $\\lambda$ to $u^i$. After this identification, the action of $\\tilde\\Delta_{0,1}$ reads (we keep the same notation)\n\\begin{align*}\n\\tilde\\Delta_{0,1}=&\\sum_j\\theta_j^1\\mathcal E_j-\\frac 12\\sum_{j,k}(-u^i+u^j)\\frac{\\partial_jf^k}{f^j}\\theta_j^1\\delta\\theta_k\\diff{}{\\delta\\theta_k}+\\sum_j(-u^i+u^j)f^j\\theta_j^1\\diff{}{u^j}\\\\\n&+\\sum_{j,k}(-u^i+u^j)(\\gamma_{kj}\\theta_j^1-\\gamma_{jk}\\theta_k^1)\\left(\\theta_j\\diff{}{\\theta_k}+\\delta\\theta_j\\diff{}{\\delta\\theta_k}\\right).\n\\end{align*}\n\n\\begin{Lem}\n\\label{lem1}\nWe have $H^p_d(\\hat d_i(\\hm{C_i})\\{\\delta\\theta\\},\\tilde\\Delta_{0,1}) = 0$ unless\n\\begin{equation}\nd = 2,\\cdots,n+3;\\quad p = d,\\cdots,d+n.\n\\end{equation}\n\\end{Lem}\n\\begin{proof}\nTo compute the cohomology, we introduce a forth spectral sequence $^4E$ given by a filtration of $\\hat d_i(\\hm{C_i})\\{\\delta\\theta\\}$ using the $\\theta_i^1$-degree, which is defined by\n $\\deg_{\\theta_i^1}\\theta_i^1 = 1$ and by setting the degrees of other generators to be zero. By decomposing the differential $\\tilde\\Delta_{0,1}$\n with respect to the $\\theta_i^1$-degree, we conclude that the zeroth page of this spectral sequence is given by $^4E_0 = \\hat d_i(\\hm{C_i})\\{\\delta\\theta\\}$ with the differential $\\mathcal D_i$ given by\n\\begin{equation*}\n\\mathcal D_i = \\theta_i^1\\mathcal E_i+\\sum_j(u^i-u^j)\\gamma_{ji}\\theta_i^1\\left(\\theta_j\\diff{}{\\theta_i}+\\delta\\theta_j\\diff{}{\\delta\\theta_i}\\right).\n\\end{equation*}\n\nThe idea to compute $H(^4E_0, \\mathcal D_i)$ is as follows. We first make the decomposition\n\\begin{equation*}\nd_i(\\hm{C_i})\\{\\delta\\theta\\} = d_i(\\hm{C_i})\\{\\delta\\theta_i\\}\\oplus\\bigoplus_{j\\neq i}d_i(\\hm{C_i})\\{\\delta\\theta_j\\}.\n\\end{equation*}\nNote that $\\bigoplus_{j\\neq i}d_i(\\hm{C_i})\\{\\delta\\theta_j\\}$ is an invariant subspace of $\\mathcal D_i$ while $d_i(\\hm{C_i})\\{\\delta\\theta_i\\}$ is not. Nevertheless, if we can find proper bidegrees $(p,d)$ such that $\\mathcal D_i$ is acyclic on $\\bigoplus_{j\\neq i}d_i(\\hm{C_i})\\{\\delta\\theta_j\\}\\cap\\Omega^p_d$, then from Lemma \\ref{ha-lem} we know that the cohomology of $d_i(\\hm{C_i})\\{\\delta\\theta\\}\\cap\\Omega^p_d$ is given by the cohomology of the space $d_i(\\hm{C_i})\\{\\delta\\theta_i\\}\\cap\\Omega^p_d$ with the differential given by the projection of $\\mathcal D_i$.\n\nTake any monomial $\\mathfrak m$ in $u^{i,s},\\theta_i^{s+1}$ for $s\\geq 1$ and any monomial $g\\in\\hm C$. For $j\\neq i$, we have\n\\begin{equation*}\n\\mathcal D_i(g\\hat d_i(\\mathfrak m)\\delta\\theta_j) = \\theta_i^1\\biggl(\\sum_j(u^i-u^j)\\gamma_{ji}\\theta_j\\diff{g}{\\theta_i}+(w_i(g)+w_i(\\mathfrak m)-1)g\\biggr)\\hat d_i(\\mathfrak m)\\delta\\theta_j,\n\\end{equation*}\nhere for a monomial $\\omega\\in\\Omega$, the rational number $w_i(\\omega)$ is defined by ${\\mathcal E}_i \\omega=w_i(\\omega)\\omega$.\n\nSo for any fixed $\\mathfrak m$ and $j\\neq i$, the subspace $\\hm C\\hat d_i(\\mathfrak m)\\{\\delta\\theta_j\\}$ is invariant under the action of $\\mathcal D_i$ , and we can concentrate first on computing the cohomology of this subspace. \nThe following argument is basically the same as that of Lemma 3.10 of \\cite{carlet2018central}, nonetheless we still write it down for the convenience of readers. Let us decompose $\\hm C$ in the form \n\\begin{equation*}\n\\hm C = \\hm C^i_0\\oplus\\theta_i\\hm C_0^i,\n\\end{equation*}\nwhere $\\hm C^i_0$ is the subspace of $\\hm C$ spanned by monomials that do not contain $\\theta_i$. For $g \\in \\hm C^i_0$, the action of $\\mathcal D_i$ is given by\n\\begin{equation}\\label{diform-zh}\n\\mathcal D_i(g\\hat d_i(\\mathfrak m)\\delta\\theta_j) = \\theta_i^1(w_i(\\mathfrak m)-1)g\\hat d_i(\\mathfrak m)\\delta\\theta_j,\n\\end{equation}\ntherefore the subspace $\\hm C^i_0\\hat d_i(\\mathfrak m)\\{\\delta\\theta_j\\}$ is acyclic. Indeed, since $w_i(\\mathfrak m)\\ge\\frac32$ if $\\hat d_i(\\mathfrak m) \\neq 0$, so a nonzero cocycle of $\\mathcal D_i$ must contain $\\theta_i^1$. Assume we have a cocycly $\\theta_i^1h$ for some $h$, then it follows from \\eqref{diform-zh} the existence of a suitable constant $c$ such that $\\theta_i^1h = c\\mathcal D_i(h)$.\n\nDue to Lemma \\ref{ha-lem}, in order to compute the cohomology of $\\hm C\\hat d_i(\\mathfrak m)\\{\\delta\\theta_j\\}$ we only need to compute the one for the subspace $\\theta_i\\hm C_0^i\\{\\delta\\theta_j\\}$. The projection of $\\mathcal D_i$ on $\\theta_i\\hm C_0^i\\hat d_i(\\mathfrak m)$ is just a multiplication by $\\theta_i^1(w_i(\\mathfrak m)-\\frac{3}{2})$, which is acyclic if $w_i(\\mathfrak m)\\neq\\frac{3}{2}$. So the nontrivial cocycles are given by elements of $\\theta_i\\hm C_0^i\\hat d_i( u^{i,1})\\delta\\theta_j = \\hm C_0^i\\theta_i\\theta_i^2\\delta\\theta_j$, which have the following possible bidegrees: \n\\begin{equation*}\nd = 2,\\cdots,2+n;\\quad p = d+1,\\cdots,d+n.\n\\end{equation*}\nThus unless a bidegree $(p,d)$ takes the values given above, the subcomplex \n\\[\\bigoplus_{j\\neq i}d_i(\\hm{C_i})\\{\\delta\\theta_j\\}\\cap\\Omega^p_d\\] \nis acyclic. \n\nThus to compute the cohomology of ${^4 E_0}$, we only need to consider the cohomology of the space $d_i(\\hm{C_i})\\{\\delta\\theta_i\\}$ due to Lemma \\ref{ha-lem}. The differential is the projection of $\\mathcal D_i$\nwhich can be represented as\n\\begin{equation*}\n\\theta_i^1\\mathcal E_i+\\sum_j(u^i-u^j)\\gamma_{ji}\\theta_i^1\\theta_j\\diff{}{\\theta_i}.\n\\end{equation*}\nNow by repeating the argument above we see that the nontrivial cocycles are of the form $\\hm C^i_0\\theta_i^2\\delta\\theta_i$ or $\\hm C^i_0\\theta_i\\theta_i^3\\delta\\theta_i$. By counting the possible bidegrees of these elements, we complete the proof of the lemma.\n\\end{proof}\n\\begin{Rem}\nBy a more careful analysis, we can prove that actually \n\\[H^{2n+3}_{n+3}(\\hat d_i(\\hm{C_i})\\{\\delta\\theta\\},\\tilde\\Delta_{0,1}) = 0.\\]\nBut this is not important for our consideration of the deformation problem.\n\\end{Rem}\n\nFinally we are to compute the cohomology of $\\tilde \\Delta_{0,1}$ on the space $\\delta\\hat d_i(\\mathcal H_i)$. Recall that on this space we have to identify $\\lambda$ with $u^i$, and therefore the differential reads\n\\begin{align*}\n\\tilde \\Delta_{0,1} =& -\\frac 12\\sum_{s\\geq 0;k,j}(-u^i+u^j)\\frac{\\partial_jf^k}{f^j}\\theta_j^1\\left(s\\delta u^{k,s}\\diff{}{\\delta u^{k,s}}+(s+1)\\delta\\theta_k^s\\diff{}{\\delta\\theta_k^s}\\right)\\\\\n&+\\sum_{k,j}(-u^i+u^j)(\\gamma_{kj}\\theta_j^1-\\gamma_{jk}\\theta_k^1)\\theta_j\\diff{}{\\theta_k}+\\sum_j\\theta_j^1\\mathcal E_j+\\sum_j(-u^i+u^j)\\theta_j^1\\diff{}{u^j}.\n\\end{align*}\n\\begin{Lem}\n\\label{s5-t4}\nWe have $H^p_d(\\delta\\hat d_i(\\mathcal H_i),\\tilde \\Delta_{0,1}) = 0$ unless\n\\begin{equation*}\nd = 3,\\cdots,n+3;\\quad p = d,\\cdots d+n-1.\n\\end{equation*}\n\\end{Lem}\n\\begin{proof}\nThe idea is very similar to the proof of Lemma \\ref{lem1}. We introduce another spectral sequence by filtrating $\\delta\\hat d_i(\\mathcal H_i)$ using $\\deg_{\\theta_i^1}$. The differential on the zeroth page reads\n\\begin{equation*}\n\\mathcal D_i = \\theta_i^1\\mathcal E_i+\\sum_j(u^i-u^j)\\gamma_{ji}\\theta_i^1\\theta_j\\diff{}{\\theta_i}.\n\\end{equation*}\nLet us denote $\\psi = \\sum_j(u^i-u^j)\\gamma_{ji}\\theta_i^1\\theta_j\\diff{}{\\theta_i}$. For any monomial $\\mathfrak m$ in $u^{i,s},\\theta_i^{s+1}$ with $s\\geq 1$, and any monomial $g\\in\\hm C$, we have\n\\begin{align*}\n\\mathcal D_i\\delta\\hat d_i(g\\mathfrak m\\delta u^{i,s}) =& \\mathcal D_i(g\\hat d_i(\\mathfrak m)\\delta u^{i,s}+(-1)^pg\\mathfrak m\\delta\\theta_i^{s+1})\\\\\n=&\\theta_i^1\\left(w_i(g)+w_i(\\mathfrak m)-1+\\frac s2 +1\\right)g\\hat d_i(\\mathfrak m)\\delta u^{i,s}\\\\\n&+(-1)^p\\theta_i^1\\left(w_i(g)+w_i(\\mathfrak m)+\\frac s2\\right)g\\mathfrak m\\delta\\theta_i^{s+1}\\\\\n&+\\theta_i^1\\left(\\psi(g)\\hat d_i(\\mathfrak m)\\delta u^{i,s}+(-1)^p\\psi(g)\\mathfrak m\\delta\\theta_i^{s+1}\\right)\\\\\n=&-\\delta\\hat d_i\\left(\\theta_i^1\\psi(g)\\mathfrak m\\delta u^{i,s}+\\theta_i^1\\left(w_i(g)+w_i(\\mathfrak m)+\\frac s2\\right)g\\mathfrak m\\delta u^{i,s}\\right).\n\\end{align*}\nSo the subspace $\\hm C\\delta\\hat d_i(\\mathfrak m\\delta u^{i,s})$ is invariant under the action of $\\mathcal D_i$. Similarly, we make the decomposition  $\\hm C = \\hm C^i_0\\oplus\\theta_i\\hm C^i_0$. For $g\\in\\hm C_i^0$, it is easy to see that\n\\begin{equation*}\n\\mathcal D_i\\delta\\hat d_i(g\\mathfrak m\\delta u^{i,s})  = -\\delta\\hat d_i\\left(\\theta_i^1\\left(w_i(\\mathfrak m)+\\frac s2\\right)g\\mathfrak m\\delta u^{i,s}\\right).\n\\end{equation*}\nOn the other hand, for a monomial in $u^{i,s},\\theta_i^{s+1}$ with $s\\geq 1$, we must have that $w_i(\\mathfrak m)\\geq \\frac 12$, thus the subspace $\\hm C^i_0\\delta\\hat d_i(\\mathfrak m\\delta u^{i,s})$ is acyclic. By applying Lemma \\ref{ha-lem} again, we know that we only need to consider the cohomology of $\\theta_i\\hm C^i_0\\delta\\hat d_i(\\mathfrak m\\delta u^{i,s})$ with the differential being the projection of $\\mathcal D_i$ given by the multiplication of\n\\begin{equation*}\n-\\theta_i^1\\left(w_i(\\mathfrak m)+\\frac s2-\\frac 12\\right).\n\\end{equation*}\nSo the only possible nontrivial cocycle is given by the case when $s = 0$ and $w_i(\\mathfrak m) = \\frac 12$, i.e. when the monomial $\\mathfrak m$ is of the form\n\\begin{equation*}\n\\theta_i\\hm C^i_0\\delta\\hat d_i(\\theta_i^2\\delta u^{i}) = \\hm C^i_0\\theta_i\\theta_i^2\\delta\\theta_i^1.\n\\end{equation*}\nBy counting the possible bidegrees of such elements, we complete the proof of the lemma.\n\\end{proof}\n\nLet us summarize all the results obtained above. We first construct a spectral sequence $^1E$ to compute the cohomology group $H^p_d(\\Omega[\\lambda],\\partial_\\lambda)$. Then we transform the computation to finding suitable bidegrees $(p,d)$ such that the second page $^1E_2 = 0$ when restricted to $\\Omega^p_d$. To this end we introduce a third spectral sequence $^3E$, and we conclude that the first page $^3E_1$ vanishes for suitable bidegrees in Lemmas \\ref{s5-t3}--Lemma \\ref{s5-t4}, thus $^3E$ converges to $^1E_2$ and so $^1E_2$ vanishes. Consequently $^1E$ converges to $H^p_d(\\Omega[\\lambda],\\partial_\\lambda)$. In this way, we prove  Theorem \\ref{thm-van}.\n\nThe following proposition is an illustration of applications of the variational bihamiltonian cohomology.\n\\begin{Prop}\n\\label{biham-flow}\nLet $(P_0, P_1)$ be a semisimple bihamiltonian structure of hydrodynamic type, and let $X\\in\\mathrm{Der}(\\hm A)_{1}^{0}$ commute with $\\partial_x$, $D_{P_0}$ and $D_{P_1}$. Then for any deformation $(\\tilde P_0,\\tilde P_1)$ of $(P_0, P_1)$, there exists a unique $\\tilde X \\in\\mathrm{Der}(\\hm A)^0_{\\geq 1}$ with leading term given by $X$ such that $\\tilde X$ commutes with $\\partial_x$, $D_{\\tilde P_0}$ and $D_{\\tilde P_1}$.\n\\end{Prop}\n\\begin{proof}\nLet us decompose $\\tilde X$ and $\\tilde P_a$ according to the differential degree as follows:\n\\begin{align*}\n\\tilde X &= X^{[0]}+\\sum_{k\\geq 1}X^{[k]},\\quad X^{[k]}\\in \\mathrm{Der}(\\hm A)_{k+1}^{0},\\ X^{[0]} = X;\\\\\n\\tilde P_a &= P_a+\\sum_{k\\geq 1}P_a^{[k]},\\quad P_a^{[k]}\\in\\hm F^2_{k+1},\\quad a=0,1.\n\\end{align*}\nThe condition that $\\tilde X$ commutes with $D_{\\tilde P_a}$ is equivalent to the following equations:\n\\begin{equation}\\label{eq-hvf-zh}\n\\fk{D_{ P_a}}{X^{[k]}}+\\sum_{l=1}^{k}\\fk{D_{\\tilde P_a^{[l]}}}{X^{[k-l]}} = 0,\\quad k\\geq 1,\\ a = 0,1.\n\\end{equation}\n\nTo prove the uniqueness of $\\tilde X$, we only need to show that if $X = 0$ then $\\tilde X = 0$. Indeed, if $X = 0$, then we have the following equation for $X^{[1]}$:\n\\[\\fk{D_{P_a}}{X^{[1]}} = 0,\n\\]\nwhich is equivalent to $\\tilde D_a\\mathcal X^{[1]} = 0$. Here \n$\\mathcal X^{[k]} \\in\\bar\\Omega^1_{k+1}$ is the 1-form corresponds to $X^{[k]}$.\nRecall that \n\\[\n\\vbh^1_{d}(\\bar\\Omega,\\tilde D_0,\\tilde D_1) = \\bar\\Omega^1_d\\cap\\ker\\tilde D_0\\cap\\ker\\tilde D_1,\n\\]\nthen by the vanishing of $\\vbh^1_{2}(\\bar\\Omega,\\tilde D_0,\\tilde D_1)$ given by Theorem \\ref{vbh12}, we conclude that $X^{[1]} = 0$. In a similar way,  we can prove recursively that $X^{[k]} = 0$ and the uniqueness is proved.\n\nThe proof of the existence is very similar to the one given in Sect.\\,\\ref{e-deform} and we omit the details here. For a sketch of the proof, we may first assume $P_a^{[1]} = 0$ and hence we can choose $X^{[1]} = 0$, then by using $\\vbh^2_{\\geq 4}(\\bar\\Omega,\\tilde D_0,\\tilde D_1) = 0$, we can recursively solve the equations \\eqref{eq-hvf-zh} to obtain $X^{[\\geq 2]}$. The proposition is proved.\n\\end{proof}\n\\begin{Rem}\nThis result is a generalization of the fact that a bihamiltonian vector filed is uniquely determined by its leading term. This generalized version can be applied to the case when the flows are not in the space $\\mathrm{Der}(\\hm A)^D$. Typical examples of such kind of flows are given by Virasoro symmetries.\n\\end{Rem}\n\n\\section{Conformal bihamiltonian structures}\n\\label{conformal}\n\n\\subsection{Conformal Bihamiltonian Structures of hydrodynamic type}\\label{cbs}\nThis section is devoted to the proof of the Theorem \\ref{g0-conf}. To this end, we consider a semisimple bihamiltonian structure $(P_0,P_1)$ of hydrodynamic type which is represented as in \\eqref{norm-p}. \n\nAssume that the bihamiltonian structure $(P_0,P_1)$ is conformal, we are going to find a derivation $E$ such that it satisfied the equation \\eqref{eq-def-2-1-zh}. We  first make the following decomposition \n\\[E = E^{[0]}+E^{[\\geq 1]},\\quad E^{[0]}\\in\\mathrm{Der}(\\hm A)^0_0,\\,E^{[\\geq 1]}\\in\\mathrm{Der}(\\hm A)^0_{\\geq 1}.\\]\nAccording to the equation \\eqref{eq-def-2-1-zh}, it is easy to see that $E^{[\\geq 1]}$ is a vector field that commutes with $D_{P_0}$ and $D_{P_1}$ and thus it is an element of $\\vbh^1_{\\ge 1}(\\bar\\Omega,\\tilde D_0,\\tilde D_1)$. In what follows we assume that $E\\in\\mathrm{Der}(\\hm A)^0_0$.\n\nDenote $E(u^i) = F^i$ and $E(\\theta_i) = \\sum_j G^j_i\\theta_j$, where $F^i$ and $G^j_i$ are some smooth functions  of $u^1,\\dots, u^n$. We are to solve these functions from the equations given in \\eqref{eq-def-2-1-zh}. \n\nFirstly, from the equation $[E, \\partial_x] = \\mu\\partial_x$ it follows that \n\\[\nE(u^{i,s}) = \\partial_x^s(F^i)+s\\mu u^{i,s},\\quad E(\\theta^{s}_i) = \\sum_j \\partial_x^s(G^j_i\\theta_j)+s\\mu \\theta_i^s.\n\\]\nBy comparing the $\\theta_j^1$ coefficients $(j\\neq i)$ of both sides of the equation \n\\begin{equation}\\label{eq-6-1-zh}\n[E, D_{P_a}]u^i = \\lambda_aD_{P_a}(u^i)\n\\end{equation}\nfor $a = 0,1$\nwe obtain the following equations:\n\\begin{align*}\n&f^iG^j_i-(\\partial_jF^i)f^j = 0,\\quad j \\neq i\\\\\n&u^if^iG^j_i-(\\partial_jF^i)u^jf^j = 0,\\quad j \\neq i.\n\\end{align*}\nFrom these equations it follows that $G^j_i = 0$ for $j\\neq i$. We also compare the $\\theta_i^1$ coefficients of both sides of the equation \\eqref{eq-6-1-zh} to obtain \n\\begin{align*}\n&\\sum_j\\aij ji F^j+f^i(G^i_i+\\mu)-\\partial_iF^if^i = \\lambda_0f^i,\\\\\n&\\sum_j\\partial_j(u^if^i) F^j+u^if^i(G^i_i+\\mu)-\\partial_iF^iu^if^i = \\lambda_1u^if^i.\n\\end{align*}\nWe solve these equations to arrive at\n\\[\nF^i = (\\lambda_1-\\lambda_0)u^i,\\quad G^i_i = \\lambda_1-\\mu-\\frac{1}{f^i}\\sum_j(\\lambda_1-\\lambda_0)u^j\\partial_jf^i.\n\\]\nTogether with the fact that $G^j_i = 0$ for $j\\neq i$, we conclude that \n\\[\nE(u^i) = (\\lambda_1-\\lambda_0)u^i,\\quad E(\\theta_i) = \\left(\\lambda_1-\\mu-\\frac{1}{f^i}E(f^i)\\right)\\theta_i.\n\\]\nFor simplicity, we will denote $E(\\theta_i) = G^i\\theta_i$.\n\nNext we compare the $\\theta_i$ coefficients of the equation \n\\begin{equation}\\label{eq-6-2-zh}\n[E, D_{P_0}]u^i = \\lambda_0D_{P_0}(u^i)\n\\end{equation}\nto arrive at the following equation: \n\\begin{align*}\n\\sum_{j,k}&\\frac{f^i\\partial_j\\partial_kf^i-\\aij ji\\aij ki}{f^i}(\\lambda_0-\\lambda_1)u^ku^{j,1}+\\sum_j \\aij ji(\\lambda_0-\\lambda_1)u^{j,1}\\\\\n&+\\frac 12 \\sum_{j,k}\\partial_k\\partial_jf^i(\\lambda_1-\\lambda_0)u^ku^{j,1}+\\frac 12\\sum_j \\aij jiu^{j,1}G^i = \\frac 12\\sum_j(\\lambda_0-\\mu)\\aij ji u^{j,1}.\n\\end{align*}\nBy further comparing the $u^{j,1}$ coefficients for both sides of the above equation, one can show that the above equation is actually equivalent to \n\\begin{equation}\n\\label{s2-t1}\nE\\left(\\frac{\\aij ji}{f^i}\\right) = (\\lambda_0-\\lambda_1)\\left(\\frac{\\aij ji}{f^i}\\right),\\quad \\forall i,j.\n\\end{equation}\nFor $j\\neq i$, compare the coefficients of $u^{i,1}\\theta_j$ and $u^{j,1}\\theta_j$ on both sides of the equation \\eqref{eq-6-2-zh}, one obtains the following equations:\n\\begin{align}\n\\label{s2-t2}\n&E\\left(\\bij ij\\right)+\\bij ij G^j = (\\lambda_0-\\mu)\\bij ij, \\quad j\\neq i.\\\\\n\\label{s2-t3}\n&E\\left(\\bij ji\\right)+\\bij ji G^j = (\\lambda_0-\\mu)\\bij ji, \\quad j\\neq i.\n\\end{align}\n\n Finally let us compare the coefficients on both sides of the equation \n \\[[E, D_{P_0}]\\theta_i = \\lambda_0D_{P_0}(\\theta_i).\\] \n For $j\\neq i$, we consider first the coefficients of $\\theta_i\\theta_j^1$ and obtain that\n\\begin{equation}\n\\label{s2-t4}\nE\\left(\\bij ji\\right)+\\bij ji G^j = (\\lambda_0-\\mu)\\bij ji-\\partial_jG^if^j, \\quad j\\neq i.\n\\end{equation}\nTherefore it is easy to conclude that $\\partial_jG^i = 0$ for $j\\neq i$ by comparing \\eqref{s2-t3} and \\eqref{s2-t4}. We also compare the coefficients of $\\theta_i\\theta_i^1$, after a straightforward computation, we arrive at an equation\n\\begin{equation}\n\\label{s2-t5}\nE\\left(\\frac{\\aij ii}{f^i}\\right) = (\\lambda_0-\\lambda_1)\\left(\\frac{\\aij ii}{f^i}\\right)-\\partial_iG^i.\n\\end{equation}\nHence we conclude that $\\partial_iG^i = 0$ by comparing \\eqref{s2-t5} with \\eqref{s2-t1}, and $G^i$ must be a constant. This means that there exists real numbers $\\alpha^i$ such that $E(f^i) = \\alpha_i f^i$, which gives the condition \\eqref{hom-f} with $d^i$ determined by $\\alpha^i = (\\lambda_1-\\lambda_0)d^i$. Substitute the expression for $G^j$ into \\eqref{s2-t2}, we obtain the condition \\eqref{irre-f}, and hence the `only if' part of the Theorem \\ref{g0-conf} is proved.\n\nThe `if' part of the Theorem \\ref{g0-conf} can be checked easily by a straightforward computation and hence the theorem is proved.\n\n\\subsection{Deformed Conformal Bihamiltonian Structures}\\label{e-deform}\nIn this section, we will use the theory of variational bihamiltonian cohomology to prove Theorem \\ref{g1-conf}. Let $(P_0,P_1)$ be a conformal semisimple bihamiltonian structure of hydrodynamic type as described in Theorem \\ref{g0-conf} and $(\\tilde P_0, \\tilde P_1)$ be any of its deformation. We are going to find a deformation $\\tilde E\\in\\mathrm{Der}(\\hm A)^0$, such that \n\\begin{equation}\n\\label{def-tildeE}\n\\left[\\tilde E, \\partial_x\\right] = \\mu\\partial_x;\\quad \\left[\\tilde E, D_{\\tilde P_a}\\right] = \\lambda_aD_{\\tilde P_a},\\quad a = 0,1.\n\\end{equation}\n\n\n\nWe first decompose $\\tilde P_a$ and $\\tilde E$ as follows:\n\\[\n\\tilde P_a = \\sum_{k\\geq 0}\\tilde P_a^{[k]},\\ \\tilde P_a^{[k]}\\in\\hm F^2_{k+1};\\quad \\tilde E = \\sum_{k\\geq 0}E^{[k]},\\ E^{[k]}\\in\\mathrm{Der}(\\hm A)^0_k.\n\\]\nWe also make the same decomposition for both sides of the equation $[\\tilde E, D_{\\tilde P_a}] = \\lambda_aD_{\\tilde P_a}$ to obtain\n\\begin{equation}\n\\label{s6-t1}\n\\sum_{i=0}^l\\fk{E^{[i]}}{D_{P_a^{[l-i]}}} = \\lambda_aD_{P_a^{[l]}},\\quad a = 0,1;\\ l\\geq 1.\n\\end{equation}\nAccording to the result of bihamiltonian cohomology, we can assume (by doing a Miura transformation if necessary) that $P_a^{[1]} = 0$, hence we can choose $E^{[1]} = 0$. Let us proceed to find the deformations $E^{[\\geq 2]}$. Let us denote\n\\[\nW_a^{[l]} = \\sum_{i=0}^{l-1}\\fk{E^{[i]}}{D_{P_a^{[l-i]}}}-\\lambda_aD_{P_a^{[l]}},\n\\]\nthen we can recursively determine the deformation by solving the equation\n\\[\n\\fk{D_{P_a^{[0]}}}{E^{[l]}} = W_a^{[l]}.\n\\]\n\\begin{Lem}\n\\label{s6-close}\nAssume that we have found $E^{[i]}$ for all $i<k$ such that the equations in \\eqref{s6-t1} are satisfied for $l<k$. Then we have the following identity:\n\\[\n\\fk{W_b^{[k]}}{D_{P_a^{[0]}}}+\\fk{W_a^{[k]}}{D_{P_b^{[0]}}} = 0,\\quad a, b = 0, 1.\n\\]\n\\end{Lem}\n\\begin{proof}\nThe proof is a straightforward computation by using the equations\n\\[\n\\fk{E^{[i]}}{D_{P_a^{[0]}}} = W_a^{[i]},\\, i<k;\\quad \\fk{D_{\\tilde P_a}}{D_{\\tilde P_b}} = 0,\n\\]\nand the (graded) Jacobi identities.\n\\end{proof}\n\nIn order to apply the theory of variational bihamiltonian cohomology, we need the following fact.\n\\begin{Lem}We have\n$\\fk{E^{[l]}}{\\partial_x} = 0$ and $\\fk{W_a^{[l]}}{\\partial_x} = 0$ for $l\\geq 1$.\n\\end{Lem}\n\\begin{proof}\nThis follows directly from the assumption $[\\tilde E,\\partial_x] = \\mu\\partial_x$.\n\\end{proof}\nAs a consequence of the above lemma, we can regard the vector fields $E^{[l]}$ and $W_a^{[l]}$ as elements of the space $\\bar\\Omega$. We denote the corresponding 1-forms by $\\mathcal E^{[l]}$ and $\\mathcal W_a^{[l]}$ respectively. More precisely, we have\n\\[\n\\mathcal E^{[l]}\\in\\bar\\Omega^1_{l},\\quad \\mathcal W_a^{[l]}\\in\\bar\\Omega^2_{l+1}.\n\\]\n\nNow the equations we are to solve can be rewritten in the form\n\\begin{equation}\\label{eq-6-3-zh}\n\\tilde D_a\\mathcal E^{[l]} = \\mathcal W_a^{[l]},\\quad a = 0,1,\\ l\\geq 2,\n\\end{equation}\nand Lemma \\ref{s6-close} gives the following conditions\n\\[\n\\tilde D_a\\mathcal W_a^{[l]} = 0,\\quad a = 0,1.\n\\]\nThus by using the triviality of the variational Hamiltonian cohomology we can find an element $\\gamma^{[l]}\\in\\bar\\Omega^1_l$ such that\n\\[\n\\tilde D_0 \\gamma^{[l]}= \\mathcal W_0^{[l]}.\n\\]\nThen the solution of $\\mathcal E^{[l]}$ can be represented by\n\\[\n\\mathcal E^{[l]} = \\gamma^{[l]}+\\tilde D_0 \\alpha^{[l]},\\quad \\alpha^{[l]}\\in\\Omega^0_{l-1}.\n\\]\nThe 1-form $\\alpha^{[l]}$ should be determined by the equation\n\\begin{equation*}\n\\tilde D_1\\mathcal E^{[l]} = \\tilde D_1\\kk{\\gamma^{[l]}+\\tilde D_0 \\alpha^{[l]}} = \\mathcal W_1^{[l]}.\n\\end{equation*}\n By using Lemma \\ref{s6-close} again, we see that $\\mathcal W_1^{[l]}-\\tilde D_1\\gamma^{[l]}$ lies in $\\ker\\tilde D_0\\cap\\ker\\tilde D_1$. Therefore from the fact that $\\vbh^2_{\\geq 4}(\\bar\\Omega,\\tilde D_0,\\tilde D_1) = 0$ it follows that we can always solve the above equation to obtain $\\alpha^{[l]}$ for $l\\geq 3$. \n\nNow let us try to find $\\mathcal E^{[2]}$ by solving the equations in \\eqref{eq-6-3-zh} for $l=2$. We will work in the canonical coordinates $u^1,\\dots, u^n$ of $(P_0,P_1)$. According to the results in \\cite{liu2005deformations}, we can choose a Miura type transformation  such that $\\tilde P_0 = P_0$, $P_1^{[1]}=0$  and the derivation $D_{P_1^{[2]}}$ is given by the 1-form $\\tilde D_0\\mathcal T$, where $\\mathcal T$ is given by\n\\[\n\\mathcal T = \\int\\delta\\kk{D_{P_1}\\sum_ic_i(u^i)u^{i,1}\\log u^{i,1}-D_{P_0}\\sum_iu^ic_i(u^i)u^{i,1}\\log u^{i,1}}.\n\\]\nHere the functions $c_i(u^i)$ are the central invariants of the deformed bihamiltonian structure $(\\tilde P_0, \\tilde P_1)$ . Since $P_0^{[2]} = 0$, we can choose $\\gamma^{[2]} = 0$. Then we have \n\\[\nW_1^{[2]} = \\fk{E^{[0]}}{D_{P_1^{[2]}}}-\\lambda_1D_{P_1^{[2]}},\n\\]\nhere $E^{[0]}=E$ is described as in Theorem \\ref{g0-conf}. Let us use $T$ to denote the derivation given by the 1-form $\\mathcal T$, then we have\n\\begin{align*}\nW_1^{[2]}&=\\fk{E^{[0]}}{\\fk{D_{P_0}}{T}}-\\lambda_1\\fk{D_{P_0}}{T}\\\\\n&=-\\fk{D_{P_0}}{\\fk{T}{E^{[0]}}}-\\fk{T}{\\fk{E^{[0]}}{D_{P_0}}}-\\lambda_1\\fk{D_{P_0}}{T}\\\\\n&=-\\fk{D_{P_0}}{\\fk{T}{E^{[0]}}}+\\lambda_0\\fk{D_{P_0}}{T}-\\lambda_1\\fk{D_{P_0}}{T}\\\\\n&=\\fk{D_{P_0}}{(\\lambda_0-\\lambda_1)T-\\fk{T}{E^{[0]}}}.\n\\end{align*}\nDenote by $\\beta\\in\\bar\\Omega^1_2$ the 1-form corresponding to the derivation $(\\lambda_0-\\lambda_1)T-\\fk{T}{E^{[0]}}$, then we see that\n\\[\n\\tilde D_1\\mathcal E^{[2]} = \\tilde D_1\\tilde D_0\\alpha^{[2]} = \\mathcal W_1^{[2]} = \\tilde D_0 \\beta.\n\\]\nTo solve this equation for $\\mathcal E^{[2]}$, we need to check that $[\\tilde D_0\\beta]\\in \\vbh^2_3(\\bar\\Omega,\\tilde D_0,\\tilde D_1)$ is trivial. According to Lemma \\ref{s3-t8} we only need to check that the indices $ind_i(\\beta)$ for $i = 1,\\cdots,n$ vanish.\n\nWe first note that \n\\[ind_i(\\mathcal T) = -3c_i(u^i).\\]\nIn another word, if we represent $\\mathcal T$ in the form\n\\[\n\\mathcal T = \\int \\sum_i X^i\\delta u^i+Y^i\\delta\\theta_i,\\quad X^i\\in\\hm A^1_2,\\quad Y^i\\in\\hm A^0_2,\n\\]\nwhere $X^i$ and $Y^i$ are given by\n\\begin{align*}\nX^i &= \\sum_j X^{(i)}_{j}\\theta_j^2+\\sum_{j,k}\\left(X^{(i)}_{kj}u^{j,1}\\theta_k^1+Z^{(i)}_{jk}u^{k,2}\\theta_j\\right)+\\sum_{j,k,l}Z^{(i)}_{j;kl}u^{k,1}u^{l,1}\\theta_j,\\\\\nY^i &= \\sum_j Y^{(i)}_ju^{j,2}+\\sum_{j,k}Y^{(i)}_{jk}u^{j,1}u^{k,1},\n\\end{align*}\nthen we must have $X^{(i)}_i+Y^{(i)}_i = -3c^i(u^i)f^i$.\nFrom the explicit formula \\eqref{gen-e} for $E^{[0]}$ it follows that\n\\begin{align*}\n\\fk{T}{E^{[0]}}u^i =& (\\lambda_1-\\lambda_0)X^{(i)}_iu^{i,2}-E^{[0]}\\kk{X^{(i)}_i}u^{i,2}\\\\&-X^{(i)}_i(\\lambda_1-\\lambda_0+2\\mu)u^{i,2}+\\cdots\\\\=&-2\\mu X^{(i)}_iu^{i,2}-E^{[0]}\\kk{X^{(i)}_i}u^{i,2}+\\cdots;\\\\\n\\fk{T}{E^{[0]}}\\theta_i =& -\\kk{\\lambda_1-(\\lambda_1-\\lambda_0)d^i-\\mu}Y^{(i)}_i\\theta_i^2+E^{[0]}\\kk{Y^{(i)}_i}\\theta_i^2\\\\&+\\kk{\\lambda_1-(\\lambda_1-\\lambda_0)d^i+\\mu}Y^{(i)}_i\\theta_i^2+\\cdots\\\\=&2\\mu Y^{(i)}_i\\theta_i^2+E^{[0]}\\kk{Y^{(i)}_i}\\theta_i^2+\\cdots.\n\\end{align*}\nHere we omit all the terms that do not contribute to the computation of indices. Then by the definition of the index we conclude that \n\\[\nind_i(\\beta) =3\\kk{\\lambda_1-\\lambda_0-2\\mu-(\\lambda_1-\\lambda_0)d^i}c_i(u^i)-3E^{[0]}\\kk{c_i(u^i)}. \n\\]\nThe equation $ind_i(\\beta) = 0$ is actually an ODE for $c_i(u^i)$, which can \nbe easily solved to give the solution\n\\[c_i(u^i) = C_i(u^i)^{m_i},\\quad m_i = \\frac{\\lambda_1-\\lambda_0-2\\mu-(\\lambda_1-\\lambda_0)d^i}{\\lambda_1-\\lambda_0},\\]\nwhere $C_i$ are arbitrary constants. \nThus we prove the Theorem \\ref{g1-conf}.\n\\section{Conclusion}\\label{con}\nIn this paper, we propose a generalization of the bihamiltonian cohomology, called the variational bihamiltonian cohomology, to deal with more general bihamiltonian flows. The eventual goal for developing the theory of variational bihamiltonian cohomology is to prove the following conjecture.\n\\begin{Conj}\nFor any deformation of the bihamiltonian structure of the Principal Hierarchy associated to a semisimple Frobenius manifold with constant central invariants, the Virasoro symmetries $\\diff{}{s_m}$ can be deformed to be  symmetries of the deformed integrable hierarchy. In particular, when the central invariants are all equal to $\\frac{1}{24}$, the Virasoro symmetries can be represented by linear actions on the tau function.\n\\end{Conj}\n\nDue to the Virasoro commutation relation, we only need to prove this conjecture for $m = -1, 2$. The case $m = -1$ is proved in \\cite{dubrovin2018bihamiltonian}, and the classification theorem on the conformal bihamiltonian structures presented in this paper can be viewed as a proof of this conjecture for $m = 0$. To verify this conjecture for $ m = 2$, some new tools should be developed, in particular, we must construct the super tau cover for any tau-symmetric bihamiltonian integrable hierarchy, generalizing the results given in \\cite{liu2020super}.\n\nApart from the above-mentioned goal, there are some other interesting problems concerning the variational bihamiltonian cohomology itself. For example, the natural map\n\\[\n\\delta:\\hm F\\to\\bar\\Omega\n\\]\ninduces an isomorphism from the bihamiltonian cohomology to the variational bihamiltonian cohomology for most of the bidegrees $(p,d)$, in particular, for the bidegress where both the bihamiltonian cohomology and the variational bihamiltonian cohomology vanish and for the bidegree $(p,d) = (2, 3)$. Then it is natural to ask if this map is indeed a quasi-isomorphism of the complex, and if so can we obtain a homotopy inverse? Also, we can generalize the notion of variational bihamiltonian cohomology to include all the possible differential forms on $J^\\infty(\\hm M)$, then we have:\n\\xym{\\hm F\\ar[r]^\\delta &\\bar{\\mathcal E}^1\n\\ar[r]^\\delta &\\bar{\\mathcal E}^2\\ar[r]^\\delta &\\bar{\\mathcal E}^3\\ar[r]^\\delta&\\cdots}\nIt is then interesting to consider the corresponding bihamiltonian cohomology on each $\\bar{\\mathcal E}^k$ and to ask if each $\\delta$ is an quasi-isomorphism. We conjecture that the cohomology on $\\bar{\\mathcal E}^k$ is related to the general $\\mathcal W$-symmetry of order $k+1$. In another word, the cohomology on $\\hm F$ controls the flows that do not explicitly contain time variables, and the cohomology on $\\bar{\\mathcal E}^k$ controls the flows that explicitly depend on at most $k$ time variables.\n\n", "meta": {"timestamp": "2021-06-25T02:19:44", "yymm": "2106", "arxiv_id": "2106.13038", "language": "en", "url": "https://arxiv.org/abs/2106.13038"}}
{"text": "\\section{Introduction}\\label{sec:Intro}\nIn this article we dedicate our studies to answer the question of whether a data set of univariate real numbers belongs to the famous family of Cauchy distributions. The Cauchy distribution is undoubtedly the standard example for a distribution without existing mean value and was studied in the mathematical world for more than 300 years, having wide applicability in diverse fields ranging from modeling resonances in physics (then often called Lorentz distribution) to cryptocurrencies in finance, see \\cite{S:2020}. It is also known as the Breit-Wigner distribution, for an extensive historical overview, see \\cite{S:1974}. To be precise, we write shorthand C$(\\alpha,\\beta)$, $\\alpha \\in\\R$, $\\beta > 0$, for the Cauchy distribution with location parameter $\\alpha$ and scale parameter $\\beta$, having density\n\\begin{equation*}\nf(x,\\alpha,\\beta) = \\frac{1}{\\pi}\\frac{\\beta}{\\beta^2+(x-\\alpha)^2},\\quad x\\in\\R.\n\\end{equation*}\nFor a detailed discussion on this family, see \\cite{JKB:1994}, Chapter 16. The Cauchy distribution is a so called heavy tailed distribution and is a member of the stable distributions, see \\cite{N:2020}. Note that $X \\sim$ C$(\\alpha, \\beta)$ if, and only if, $\\frac{X-\\alpha}{\\beta} \\sim$ C$(0,1)$ and hence the Cauchy distribution belongs to the location-scale family of distributions. In the following we denote the family of Cauchy distributions by $\\mathcal{C}:=\\{\\mbox{C}(\\alpha,\\beta):\\,\\alpha\\in\\R,\\beta>0\\}$, a family of distributions which is closed under translation and rescaling. We test the composite hypothesis\n\\begin{equation}\\label{eq:H0}\nH_0:\\;\\mathbb{P}^X\\in\\mathcal{C}\n\\end{equation}\nagainst general alternatives on the basis of independent identical copies $X_1,\\ldots,X_n$ of $X$. This testing problem has been considered in the literature: \\cite{GH:2000} propose a test procedure based on the empirical characteristic function and \\cite{MT:2005} extend this test by considering alternative estimation methods. More recently, \\cite{MZ:2017} propose to use the likelihood ratio as in \\cite{Z:2002} as well as the Kullback-Leibler distance, an idea that is extended in \\cite{MZ:2019}. A quantile based method is proposed in \\cite{R:2001} and compared to the classical omnibus procedures. An empirical power study of goodness-of-fit tests for the Cauchy model based on the empirical distribution function as the Kolmogorov-Smirnov test, the Cram\\'{e}r-von Mises test, the Kuiper test, the Anderson Darling and the Watson test is found in \\cite{ODYM:2001}. In \\cite{L:2005}, two tests for the standard Cauchy distribution based on characterizations are given; however, they are not designed for the composite hypothesis \\eqref{eq:H0}.\n\nThe novel procedure is based on the following new characterization of the standard Cauchy distribution.\n\\begin{theorem}\\label{thm:Char}\nLet $X$ be a random variable with absolutely continuous density $p$ and $\\E\\left[ \\frac{|X|}{1+X^2}\\right] < \\infty$. Then $X$ has a Cauchy Distribution C$(0,1)$ if, and only if \n\\begin{equation}\\label{eq:meanchar}\n\\E \\left[\\left(it-\\frac{2X}{1+X^2}\\right)\\exp(itX)\\right]= 0\n\\end{equation}\nholds for all $t\\in \\R$, where $i$ denotes the imaginary unit.\n\\end{theorem}\n\\begin{proof}\nFor $X\\sim\\mbox{C}(0,1)$ direct calculation shows the assertion. Let $X$ be a random variable with absolutely continuous density function $p$ such that \n\\begin{equation*}\n\\E \\left[\\left(it-\\frac{2X}{1+X^2}\\right)\\exp(itX)\\right]= 0\n\\end{equation*}\nholds for all $t\\in \\R$. Note that since $-it\\E[\\exp(itX)]$ is the Fourier-Stieltjes transform of the derivative of $p$ we have\n\\begin{equation*}\n0=\\E \\left[\\left(it-\\frac{2X}{1+X^2}\\right)\\exp(itX)\\right]=\\int_{-\\infty}^\\infty \\left(-p'(x)-\\frac{2x}{1+x^2}p(x)\\right)\\exp(itx)\\,\\mbox{d}x\n\\end{equation*}\nfor all $t\\in\\R$. By properties of the Fourier-Stieltjes transform we hence note that $p$ must satisfy the ordinary differential equation\n\\begin{equation*}\np'(x)+\\frac{2x}{1+x^2}p(x)=0\n\\end{equation*}\nfor almost all $x\\in\\R$. The only solution satisfying $\\int_{-\\infty}^\\infty p(x)\\mbox{d}x=1$ is $p(x)=f(x,0,1)$, $x\\in\\R$, and $X\\sim\\mbox{C}(0,1)$ follows.\n\\end{proof}\n\\begin{remark}\nNote that the characterization in Theorem \\ref{thm:Char} is related to the spectral representation of the Stein operator of the so called density approach, for details on Stein operators, see \\cite{SReview:2021}.\n\\end{remark}\n\n\\subsection{A new class of goodness of fit tests for the Cauchy distribution}\nNote that the testing problem under discussion is invariant with respect to transformations of the kind $x\\to ax+b, \\, x\\in\\R$, where $a\\in\\R$ and $b>0$. Consequently, a decision in favor or against $H_0$ should be the same for  $X_1,\\ldots,X_n$ and  $aX_1+b,\\ldots,aX_n+b$. This goal is achieved if the test statistic $T_n$, say, is based on the scaled residuals $Y_{n,1},...,Y_{n,n}$, given by\n\\begin{equation}\n\\label{Y}\nY_{n,j}=\\frac{X_j-\\widehat{\\alpha}_n}{\\widehat{\\beta}_n}, \\quad j=1,\\ldots,n.\n\\end{equation}\nHere, $\\widehat{\\alpha}_n=\\widehat{\\alpha}_n(X_1,...,X_n)$ and $\\widehat{\\beta}_n=\\widehat{\\beta}_n(X_1,...,X_n)$ denote consistent estimators of $\\alpha\\in\\R$ and $\\beta>0$ such that \n\\begin{eqnarray}\n\\label{eq:alpha}\n\\widehat{\\alpha}_n(aX_1+b,...,aX_n+b)&=&a\\widehat{\\alpha}_n(X_1,...,X_n)+b,\\\\\\label{eq:beta}\n\\widehat{\\beta}_n(aX_1+b,...,aX_n+b)&=&a\\widehat{\\beta}_n(X_1,...,X_n),\n\\end{eqnarray}\nholds for each $a>0$ and $b\\in\\R$. By (\\ref{eq:alpha}) and (\\ref{eq:beta}) it is easy to see that $Y_{n,j}$, $j=1,\\ldots,n,$ do not depend on the location nor on the scale parameter. Hence, $T_n$ has the property\n$T_n(aX_1+b,\\ldots,aX_n+b)=T_n(X_1,\\ldots,X_n)$, and we may and do assume $\\alpha=0$ and $\\beta=1$ in the following. Specifically, we choose the test statistic\n\\begin{equation}\n\\label{eq:statistic}\nT_{n}=n\\int_{-\\infty}^{\\infty}\\Big\\vert\\frac{1}{n}\\sum_{j=1}^n\\Big(it-\\frac{2Y_{n,j}}{1+Y_{n,j}^2}\\Big)e^{itY_{n,j}}\\Big\\vert^2\\omega(t)\\;\\mbox{d}t,\n\\end{equation}\nwhich is the weighted $L^2$-distance from (\\ref{eq:meanchar}) to 0. Here, $\\omega:\\R\\rightarrow\\R$ denotes a weight function such that\n\\begin{equation}\n\\label{omega}\n\\omega(t)=\\omega(-t),\\ t\\in\\R,\\text{ and }\\ \\int_{-\\infty}^{\\infty}\\omega(t)\\;\\mbox{d}t<\\infty,\n\\end{equation}\nand $|\\cdot|$ is the complex absolute value. For the particular choice $\\omega(t)=\\omega_a(t)=\\exp(-a|t|)$, $t\\in\\R$, $a>0$, of the weight function, we get the integration-free, numerical stable formula\n\\begin{align}\n\\label{eq:EF}\nT_{n,a}=\\frac{1}{n}\\sum_{j,k=1}^n\\bigg(&\\frac{8aY_{n,j}Y_{n,k}}{(1+Y_{n,j}^2)(1+Y_{n,k}^2)((Y_{n,j}-Y_{n,k})^2+a^2)}\n-\\frac{16aY_{n,j}(Y_{n,j}-Y_{n,k})}{(1+Y_{n,j}^2)((Y_{n,j}-Y_{n,k})^2+a^2)^2}\\nonumber\\\\\\\n&+\\frac{4a^3-12a(Y_{n,j}-Y_{n,k})^2}{((Y_{n,j}-Y_{n,k})^2+a^2)^3}\\bigg),\n\\end{align}\nand hence a whole family of tests depending on the so called tuning parameter $a>0$. \nThe next result reveals the limit behavior of $T_{n,a}$ for $a\\rightarrow 0$ and $a\\rightarrow\\infty$.\n\n\\begin{theorem}\\label{thmlimit} \nFor fixed $n$, we have\n\\begin{align} \\label{stat:lim1}\n \\lim_{a\\to 0}  a \\bigg( T_{n,a}-\\frac{4}{a^3} \\bigg) &= \\frac{8}{n} \\sum_{j=1}^n \\frac{Y_{n,j}^2}{(1+Y_{n,j}^2)^2},\n\\end{align}\nand\n\\begin{align} \\label{stat:lim2}\n\\lim_{a\\to\\infty} aT_{n,a} &= \\frac{8}{n} \\Bigg( \\sum_{j=1}^n \\frac{Y_{n,j}}{1+Y_{n,j}^2} \\Bigg)^2.\n\\end{align}\n\\end{theorem}\n\n\\begin{proof}\nSplitting the sum in (\\ref{eq:EF}) in a diagonal and a non-diagonal part results in \n\\begin{align*}\nT_{n,a}= \\frac{1}{n}\\sum_{j,k=1}^n R_{j,k,a}\n=\\frac{1}{n}\\sum_{j=1}^n R_{j,j,a} +\\frac{1}{n}\\sum_{j\\neq k} R_{j,k,a}\n=T_{n,a}^{d}+T_{n,a}^{nd},\n\\end{align*}\nsay. Since $\\lim_{a\\to 0}T_{n,a}^{nd}=0$, we obtain\n\\begin{align*}\n\\lim_{a\\to 0}  aT_{n,a} &= \\lim_{a\\to 0}  aT_{n,a}^{d} \n= \\frac{8}{n} \\sum_{j=1}^n \\left\\{ \\frac{Y_{n,j}^2}{(1+Y_{n,j}^2)^2} + \\frac{4}{a^2} \\right\\},\n\\end{align*}\nand the first assertion follows.\n\nObserve that $T_{n,a}  =  \\int_0^\\infty g(t) \\exp(-at) \\: dt$,\nwhere $g(u) = n (h_1^2(t)+h_2^2(t))$, and $h_1$ and $h_2$ are defined by\n\\begin{align*}\nh_1(t) &=  \\frac{1}{n} \\sum_{j=1}^n \\bigg\\{ \\bigg(\\frac{2Y_{n,j}}{1+Y_{n,j}^2}+t\\bigg) \\cos(tY_{n,j})\n + \\bigg(t-\\frac{2Y_{n,j}}{1+Y_{n,j}^2}\\bigg) \\sin(tY_{n,j}) \\bigg\\}, \\\\\nh_2(t) &=  \\frac{1}{n} \\sum_{j=1}^n \\bigg\\{ \\bigg(\\frac{2Y_{n,j}}{1+Y_{n,j}^2}-t\\bigg) \\cos(tY_{n,j})\n + \\bigg(t+\\frac{2Y_{n,j}}{1+Y_{n,j}^2}\\bigg) \\sin(tY_{n,j}) \\bigg\\}.\n \\end{align*}\nSince\n\\begin{align*}\n\\lim_{t\\to 0} h_1(t) &= \\lim_{t\\to 0} h_2(t) \n\\ = \\frac{2}{n} \\sum_{j=1}^n \\frac{Y_{n,j}}{1+Y_{n,j}^2},\n \\end{align*}\nthe second assertion follows from an Abelian theorem on Laplace transforms (see \\cite{W:1959}, p.182).\n\\end{proof}\n\nThe rest of the paper is organized as follows. In Section 2, the limit distribution of the test statistic $T_{n,a}$ is derived in a Hilbert space framework under the null hypothesis and under fixed alternatives. Furthermore, we derive the limit distribution of $T_{n,0}$. Consistency of the new tests against a large class of alternatives is shown in Section 3. An extensive Monte Carlo simulation study in Section 4 shows that the test is competitive to the state of the art procedures. Finally, in Section 4, the tests are applied to log-returns of cryptocurrencies.\n\n\\section{Limit distribution under the null hypothesis}\nThe asymptotic theory is derived in the Hilbert space $\\mathbb{H}$ of measurable, square integrable functions $\\mathbb{H}=L^2(\\mathbb{R},\\mathcal{B}, \\omega(t)\\mbox{d}t)$, where $\\mathcal{B}$ is the Borel-$\\sigma$-field of $\\mathbb{R}$. Notice that the functions figuring within the integral in the definition of $T_{n}$ are $(\\mathcal{A} \\otimes \\mathcal{B}, \\mathcal{B})$-measurable random elements of $\\mathbb{H}$. We denote by\n\\begin{equation*}\n\t\\|f\\|_{\\mathbb{H}} = \\left( \\int_{-\\infty}^\\infty \\big|f(t)\\big|^2 \\, \\omega(t) \\, \\mathrm{d}t \\right)^{1/2}, \\qquad \\langle f, g \\rangle_{\\mathbb{H}}=\\int_{-\\infty}^\\infty f(t)g(t) \\, \\omega(t) \\, \\mathrm{d}t\n\\end{equation*}\nthe usual norm and inner product in $\\mathbb{H}$.  In the following, we assume that the estimators $\\widehat{\\alpha}_n$ and $\\widehat{\\beta}_n$ allow linear representations \n\\begin{align}\n    \\sqrt{n} \\widehat{\\alpha}_n&= \\frac{1}{\\sqrt{n}} \\sum_{j = 1}^n \\psi_1 (X_{j}) + o_{\\mathbb{P}}(1),\\label{eq:psi_11}\\\\\n    \\sqrt{n} (\\widehat{\\beta}_n - 1) &= \\frac{1}{\\sqrt{n}} \\sum_{j = 1}^n \\psi_2 (X_{j}) + o_{\\mathbb{P}}(1),\\label{eq:psi_21}\n\\end{align}\nwhere $o_\\mathbb{P}(1)$ denotes a term that converges to 0 in probability, and $\\psi_1$ und $\\psi_2$ are measurable functions with\n\\begin{equation*}\n    \\mathbb{E}[\\psi_1 (X_{1})] = \\mathbb{E}[\\psi_2 (X_{1})] = 0, \\quad\\mbox{and}\\quad \\mathbb{E}[\\psi_1^2 (X_{1})] < \\infty,\\;  \\mathbb{E}[\\psi_2^2 (X_{1})] < \\infty, \\label{eq:psi2}\n\\end{equation*}\nsee Remark \\ref{rem:est} for examples of estimation procedures satisfying these assumptions. By the symmetry of the weight function $\\omega(\\cdot)$ straightforward calculations show\n\\begin{equation*}\n    T_n=\\int_{-\\infty}^\\infty Z_n^2(t)\\,\\omega(t)\\mbox{d}t,\n\\end{equation*}\nwhere\n\\begin{equation}\\label{eq:Zn}\n    Z_n(t)=\\frac{1}{\\sqrt{n}}\\sum_{j=1}^n\\Big(\\frac{2Y_{n,j}}{1+Y_{n,j}^2}+t\\Big)\\cos(tY_{n,j})+\\Big(t-\\frac{2Y_{n,j}}{1+Y_{n,j}^2}\\Big)\\sin(tY_{n,j}),\\ \\ t\\in\\R,\n\\end{equation}\nis a real-valued $(\\mathcal{A} \\otimes \\mathcal{B}, \\mathcal{B})$-measurable random element of $\\mathbb{H}$.\n\n\\begin{theorem}\\label{thm:asympVer}\nLet $X_1,...,X_n$ be i.i.d. $\\mbox{C}(0,1)$ distributed random variables and $Z_n$ be the random element of $\\mathbb{H}$ as in \\eqref{eq:Zn}. Then there exists a centred Gaussian random process $\\mathcal{Z}$ in $\\mathbb{H}$ with covariance kernel\n\\begin{align*}\nK(s,t)&=\\frac{1}{2}\\big(s^2+t^2+\\vert s-t\\vert+1\\big)e^{-\\vert s-t\\vert}\\\\\n&-\\frac{1}{2}(t^2+\\vert t\\vert+1)e^{-\\vert t\\vert}\\E\\bigg[\\bigg(\\bigg(\\frac{2X_1}{1+X_1^2}+s\\bigg)\\cos(sX_1)+\\bigg(s-\\frac{2X_1}{1+X_1^2}\\bigg)\\sin(sX_1)\\bigg)\\psi_1(X_1)\\bigg]  \\\\\n&+\\frac{1}{2}t(\\vert t\\vert+1)e^{-\\vert t\\vert}\\E\\bigg[\\bigg(\\bigg(\\frac{2X_1}{1+X_1^2}+s\\bigg)\\cos(sX_1)+\\bigg(s-\\frac{2X_1}{1+X_1^2}\\bigg)\\sin(sX_1)\\bigg)\\psi_2(X_1)\\bigg]\\\\\n&-\\frac{1}{2}(s^2+\\vert s\\vert+1)e^{-\\vert s\\vert}\\E\\bigg[\\bigg(\\bigg(\\frac{2X_1}{1+X_1^2}+t\\bigg)\\cos(tX_1)+\\bigg(t-\\frac{2X_1}{1+X_1^2}\\bigg)\\sin(tX_1)\\bigg)\\psi_1(X_1)\\bigg]\\\\\n&+\\frac{1}{2}s(\\vert s\\vert+1)e^{-\\vert s\\vert}\\E\\bigg[\\bigg(\\bigg(\\frac{2X_1}{1+X_1^2}+t\\bigg)\\cos(tX_1)+\\bigg(t-\\frac{2X_1}{1+X_1^2}\\bigg)\\sin(tX_1)\\bigg)\\psi_2(X_1)\\bigg]\\\\\n&+\\frac{1}{4}(s^2+\\vert s\\vert+1)(t^2+\\vert t\\vert+1)e^{-\\vert s\\vert-\\vert t\\vert}\\E[\\psi_1^2(X_1)]+\\frac{1}{4}s(\\vert s\\vert+1)t(\\vert t\\vert+1)e^{-\\vert s\\vert-\\vert t\\vert}\\E[\\psi_2^2(X_1)],\\quad s,t\\in\\R,\n\\end{align*}\nsuch that $Z_n\\overset{D}{\\longrightarrow}\\mathcal{Z}$ in $\\mathbb{H}$ as $n\\rightarrow\\infty$. \n\\end{theorem}\nA proof of Theorem \\ref{thm:asympVer} is found in Appendix \\ref{app:proofasy1}. An application of the continuous mapping theorem states the following corollary.\n\n\\begin{corollary}\\label{cor:H0asy}\nWe have as $n\\rightarrow\\infty$ \n\\[T_n\\overset{D}{\\longrightarrow}\\int_{-\\infty}^\\infty \\mathcal{Z}^2(t)\\omega(t)dt=\\Vert \\mathcal{Z}\\Vert_{\\mathbb{H}}^2.\\]\n\\end{corollary}\nIt is well known that the distribution of $\\Vert \\mathcal{Z}\\Vert_{\\mathbb{H}}^2$ is that of $\\sum_{j=1}^\\infty \\lambda_jN_j^2$, where $N_1,N_2,\\ldots$ are i.i.d. standard normal random variables and $(\\lambda_j)$ is a decreasing sequence of positive eigenvalues of the integral operator\n\\begin{equation*}\n    \\mathcal{K}g(s)=\\int_{-\\infty}^\\infty K(s,t)g(t)\\omega(t)\\,\\mbox{d}t.\n\\end{equation*}\nDue to the complexitiy of the covariance kernel $K$ it seems hopeless to solve the integral equation $\\mathcal{K}g(s)=\\lambda g(s)$ and find explicit values of $\\lambda_j$, $j\\ge1$. For a numerical approximation method we refer to Subsection 3.3 in \\cite{MT:2005}. Note that \n\\begin{equation}\n    \\E\\Vert \\mathcal{Z}\\Vert_{\\mathbb{H}}^2=\\int_{-\\infty}^\\infty K(t,t)\\omega(t)\\,\\mbox{d}t\n\\end{equation}\nand \n\\begin{equation}\n    \\mbox{Var}\\Vert \\mathcal{Z}\\Vert_{\\mathbb{H}}^2=2\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty K^2(s,t)\\omega(t)\\omega(s)\\,\\mbox{d}t\\mbox{d}s\n\\end{equation}\ncan be derived for specific estimation procedures and weight functions. Such results in the theory of goodness-of-fit tests for the Cauchy family are sparse, for some explicit formulae for mean values, see \\cite{GH:2000} and \\cite{MT:2005}.\n\n\\begin{remark}\\label{rem:est} The generality of Theorem \\ref{thm:asympVer} in view of the linear representations of the estimators and hence dependence on the functions $\\psi_1$ and $\\psi_2$ leads to explicit covariance kernels for different parameter estimation procedures. To estimate $\\alpha$ and $\\beta$ we choose the following location and scale estimators $\\widehat{\\alpha}_n$ and $\\widehat{\\beta}_n$, which all satisfy \\eqref{eq:alpha} and \\eqref{eq:beta} respectively. For a compact notation, we write $\\psi(x)=(\\psi_1(x),\\psi_2(x))^\\top$ and $I_2$ for the 2-dimensional identity matrix. Some derivations were partially provided by the computer algebra system Maple, see \\cite{Maple2019}.\n\\begin{enumerate}\n\\item \\textbf{Median and interquartile-distance estimators:} Let $\\xi_p$, $p\\in(0,1)$, denote the $p$-quantile of the underlying distribution $F$, $\\widehat{\\xi}_{p,n}$ the sample $p$-quantile, and $X_{(1)}\\le\\cdots\\le X_{(n)}$ the order statistics of $X_1,\\ldots,X_n$. With $\\lfloor\\cdot\\rfloor$ denoting the floor function, let\n\\begin{equation}\\label{eq:medest}\n\\widehat{\\alpha}_n=\\begin{cases}\n\\frac{1}{2}(X_{(\\frac{n}{2})}+X_{(\\frac{n}{2}+1)}), & \\text{if}\\,n\\,\\mbox{even,}\\\\\nX_{(\\lfloor\\frac{n}{2}\\rfloor+1)}, & \\text{otherwise,}\n\\end{cases}\n\\end{equation}\nbe the unbiased empirical median and \n\\begin{equation}\\label{eq:IQRest}\n\\widehat{\\beta}_n=\\frac{1}{2}(\\widehat{\\xi}_{\\frac{3}{4},n}-\\widehat{\\xi}_{\\frac{1}{4},n})\n\\end{equation}\nthe half-interquartile range (iqr) of the sample. Under mild regularity conditions $\\widehat{\\alpha}_n$ and  $\\widehat{\\beta}_n$ are consistent estimators of $\\alpha$ and $\\beta$. Display (3.3) of \\cite{GH:2000} then gives the so-called Bahadur representations (see Theorem 2.5.1 in \\cite{S:1980}) with\n\\[\\psi_1(x)=\\pi\\left(\\frac{1}{2}-\\mathbf{1}\\lbrace x\\leq 0\\rbrace\\right),\\quad \\mbox{and}\\quad \\psi_2(x)=\\pi\\left(\\frac{1}{2}-\\mathbf{1}\\lbrace-1\\leq x\\leq 1\\rbrace\\right),\\ x\\in\\R.\\]\nIt is easy to see that $\\E[\\psi_1(X_1)]=\\E[\\psi_2(X_1)]=0$ and $\\E[\\psi(X_1)\\psi(X_1)^\\top]=\\frac{\\pi^2}{4}I_{2}$ holds.\nWith these representations, we get the covariance kernel of $\\mathcal{Z}$ in Theorem \\ref{thm:asympVer} \n\\begin{align*}\n\\label{KMIS}\nK_{MIQ}(s,t)=&\\frac{1}{2}\\big(s^2+t^2+\\vert s-t\\vert+1\\big)e^{-\\vert s-t\\vert}\\\\\n&+\\Big[t\\big(\\vert t\\vert+1\\big)\\big(2J_1(s)-sJ_2(s)\\big)-\\big(t^2+\\vert t\\vert+1\\big)\\big(\\frac{s}{2}J_3(s)+J_4(s)\\big)\\Big]e^{-\\vert t\\vert}\\\\\n&+\\Big[s\\big(\\vert s\\vert+1\\big)\\big(2J_1(t)-tJ_2(t)\\big)-\\big(s^2+\\vert s\\vert+1\\big)\\big(\\frac{t}{2}J_3(t)+J_4(t)\\big)\\Big]e^{-\\vert s\\vert}\\\\\n&+\\frac{\\pi^2}{16}\\Big[\\big(s^2+\\vert s\\vert+1\\big)\\big(t^2+\\vert t\\vert+1\\big)+st\\big(\\vert s\\vert+1\\big)\\big(\\vert t\\vert+1\\big)\\Big]e^{-\\vert s\\vert-\\vert t\\vert},\\quad s,t\\in\\R,\n\\end{align*}\nwhere \n\\begin{equation*}\nJ_1(t)=\\int_0^1\\frac{x\\sin(tx)}{(1+x^2)^2}dx, \\ \nJ_2(t)=\\int_0^1\\frac{\\cos(tx)}{1+x^2}dx, \\ \nJ_3(t)=\\int_0^\\infty\\frac{\\sin(tx)}{1+x^2}dx, \\\nJ_4(t)=\\int_0^\\infty\\frac{x\\cos(tx)}{(1+x^2)^2}dx.\n\\end{equation*}\nDirect calculations of integrals using the weight function $\\omega_a(t)$ of the introduction lead to \n\\begin{eqnarray*}\n    \\E\\|\\mathcal{Z}\\|_{\\mathbb{H}}^2&=&\\int_{-\\infty}^\\infty K_{MIQ}(t,t)\\omega_a(t)\\,\\mbox{d}t\\\\\n    &=&(8 \\left( a+2 \\right) ^{5} \\left( 1+a\n \\right) ^{3}{a}^{3} \\left( {a}^{2}+2\\,a+2 \\right) ^{3})^{-1}\\left[\\left( {\\pi}^{2}-8 \\right) {a}^{16}+ \\left( 19\\,{\n\\pi}^{2}-152 \\right) {a}^{15}\\right.\\\\&&+ \\left( 173\\,{\\pi}^{2}-1368 \\right) {a}^\n{14}+ \\left( 1003\\,{\\pi}^{2}-7720 \\right) {a}^{13}+ \\left( 4126\\,{\\pi}\n^{2}-30192 \\right) {a}^{12}\\\\&&+ \\left( 12594\\,{\\pi}^{2}-84304 \\right) {a}\n^{11}+ \\left( 29128\\,{\\pi}^{2}-163520 \\right) {a}^{10}+ \\left( 51460\\,\n{\\pi}^{2}-188832 \\right) {a}^{9}\\\\&&+ \\left( 69320\\,{\\pi}^{2}-8256\n \\right) {a}^{8}+ \\left( 70296\\,{\\pi}^{2}+457664 \\right) {a}^{7}+\n \\left( 52176\\,{\\pi}^{2}+1025920 \\right) {a}^{6}\\\\&&+ \\left( 26848\\,{\\pi}^\n{2}+1323264 \\right) {a}^{5}+ \\left( 8576\\,{\\pi}^{2}+1151488 \\right) {a\n}^{4}+ \\left( 1280\\,{\\pi}^{2}+693248 \\right) {a}^{3}\\\\&&\\left.+280576\\,{a}^{2}+\n69632\\,a+8192\\right].\n\\end{eqnarray*}\n\n\\item \\textbf{Maximum likelihood estimators:} \\cite{MT:2005} show in Lemma A.1 that for the maximum-likelihood estimator $\\widehat{\\alpha}_n$ and $\\widehat{\\beta}_n$ in the Cauchy family the linear representations are given by\n\\[\\psi_1(x)=\\frac{4x}{1+x^2},\\quad \\mbox{and}\\quad\\psi_2(x)=\\frac{2(x^2-1)}{1+x^2},\\;x\\in\\R.\\]\nAgain, straightforward calculations show $\\E[\\psi_1(X_1)]=\\E[\\psi_2(X_1)]=0$ and $\\E[\\psi(X_1)\\psi(X_1)^\\top]=2 I_{2}$. Note that there are no closed form expressions for the estimators, such that the log-likelihood equations have to be solved numerically. This leads to the covariance kernel of $\\mathcal{Z}$ in Theorem \\ref{thm:asympVer} given by\n\\begin{align*}\n\\label{KMLS}\nK_{ML}(s,t) = & \\frac{1}{2}\\big(s^2+t^2+\\vert s-t\\vert+1\\big)e^{-\\vert s-t\\vert}-\\frac{1}{2}(t^2+\\vert t\\vert+1)(s^2+\\vert s\\vert +1)e^{-\\vert t\\vert-\\vert s\\vert}\\\\&-\\frac{1}{2}t(\\vert t\\vert+1)s(\\vert s\\vert+1)e^{-\\vert t\\vert-\\vert s\\vert},\\quad s,t\\in\\R.\n\\end{align*}\nWith this explicit formula for the covariance kernel, we compute for the weight function $w_a(t)$ in the introduction\n\\begin{equation*}\n    \\E\\|\\mathcal{Z}\\|_{\\mathbb{H}}^2=\\int_{-\\infty}^\\infty K_{ML}(t,t)\\omega_a(t)\\,\\mbox{d}t={\\frac {8\\,{a}^{4}+80\\,{a}^{3}+352\\,{a}^{2}+320\\,a+128}{{a}^{3}\n \\left( a+2 \\right) ^{5}}}\n\\end{equation*}\nand\n\\begin{eqnarray*}\n\\mbox{Var}\\|\\mathcal{Z}\\|_{\\mathbb{H}}^2&=&2\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty K_{ML}^2(s,t)\\omega_a(s)\\omega_a(t)\\,\\mbox{d}t\\mbox{d}s\\\\&=&\n\\left(\\frac12{a}^{5}\n \\left( a+1 \\right) ^{7} \\left( a+2 \\right) ^{10}\\right)^{-1}\\left[10\\,{a}^{14}+270\\,{a}^{13}+3521\\,{a}^{12}+27987\\,{a}^{11}+\n146819\\,{a}^{10}\\right.\\\\&&+510582\\,{a}^{9}+1194078\\,{a}^{8}+1914216\\,{a}^{7}+\n2134432\\,{a}^{6}+1671456\\,{a}^{5}+928192\\,{a}^{4}\\\\&&\\left.+369792\\,{a}^{3}+\n104192\\,{a}^{2}+18432\\,a+1536\\right].\n\\end{eqnarray*}\n\n\\item \\textbf{Equivariant integrated squared error estimator:} In \\cite{MT:2005} propose an equivariant version of the minimal integrated squared error estimator introduced by \\cite{BM:2001} for the Cauchy distribution. The estimators are derived by minimization of the weighted $L^2$-distance \n\\begin{equation*}\nI(\\alpha,\\beta)=\\int_{-\\infty}^{\\infty}\\vert\\varphi_n(t;\\alpha,\\beta)-e^{-\\vert t\\vert}\\vert^2\\omega(t)dt,\n\\end{equation*}\nwhere $\\varphi_n(t;\\alpha,\\beta)=\\frac{1}{n}\\sum_{j=1}^n \\exp\\Big(it\\frac{X_j-\\alpha}{\\beta}\\Big)$, $t\\in\\R$, is the empirical characteristic function of a distribution from the location scale family in dependence of the parameters. As weight function the authors chose $\\omega_\\nu(t)=\\exp(-\\nu|t|)$, $t\\in\\R$, $\\nu>0$, and hence get families of estimators $\\widehat{\\alpha}_{n,\\nu}$ and $\\widehat{\\beta}_{n,\\nu}$ in dependence of $\\nu$. Note that the optimization problem has to be solved numerically and no closed-form formula is known for the estimators. We denote this class of estimators hereinafter EISE. Lemma A.1 in \\cite{MT:2005} provides the linear asymptotic representations\n\\begin{equation*}\n\\psi_1(x,\\nu)=(\\nu+1)(\\nu+2)^3\\frac{x}{((\\nu+1)^2+x^2)^2}  \\ \\ \\mbox{and} \\ \\ \\psi_2(x,\\nu)=\\frac{1}{2}(\\nu+2)-\\frac{1}{2}(\\nu+2)^3\\frac{(\\nu+1)^2-x^2}{((\\nu+1)^2+x^2)^2}\n\\end{equation*}\nfor $x\\in\\R$, which lead for every $\\nu>0$ to the covariance kernel of $\\mathcal{Z}$ in Theorem \\ref{thm:asympVer}\n\\begin{align*}\n\\label{KEISE}\nK_{EISE}(s,t;\\nu)&= \\frac{1}{2}\\big(s^2+t^2+\\vert s-t\\vert+1\\big)e^{-\\vert s-t\\vert}\\\\\n&-\\frac{1}{2}(t^2+\\vert t\\vert+1)(\\nu+1)\\bigg(\\frac{2\\vert s\\vert(\\nu+2)\\nu^3+(\\nu+1)(1-\\vert s\\vert)+\\vert s\\vert+3}{\\nu^3}\\bigg)e^{-\\vert s\\vert-\\vert t\\vert}\\\\\n&+\\frac{1}{2}(t^2+\\vert t\\vert+1)\\bigg(\\frac{(\\nu+1)(\\vert s\\vert(\\nu+1)^2+3(\\nu+1)-\\vert s\\vert)-1}{\\nu^3}\\\\\n&\\ \\ \\ \\ \\ \\ +(\\nu+1)(s^2(\\nu+2)(\\nu+1)+2\\vert s\\vert(\\nu+2)+s^2)\\bigg)e^{-(\\nu+1)\\vert s\\vert-\\vert t\\vert}\\\\\n&-\\frac{1}{2}t(\\vert t\\vert+1)(\\nu+1)\\bigg(\\frac{s(\\nu+2)^3(\\nu+1)}{2\\nu^2}+s(\\nu+1)^2+s-4\\sgn(s)\\bigg)e^{-\\vert s\\vert-\\vert t\\vert}\\\\\n&+\\frac{1}{2}t(\\vert t\\vert+1)\\bigg(\\frac{s(\\nu+2)^3(\\vert s\\vert(\\nu+1)^3-3(\\nu+1)^2+\\vert s\\vert(\\nu+1)+1)}{4(\\nu+1)\\nu^2}\\\\\n&\\ \\ \\ \\ \\ \\ -s(\\nu+1)^2+s-4\\sgn(s)(\\nu+1)\\bigg)e^{-\\vert s\\vert-\\vert t\\vert}\\\\\n&+\\frac{1}{2\\pi}t(\\vert t\\vert+1)(\\nu+2)^3\\big(sJ_1(s)-2J_2(s)\\big)e^{-\\vert t\\vert}\\\\\n&-\\frac{1}{2}(s^2+\\vert s\\vert+1)(\\nu+1)\\bigg(\\frac{2\\vert t\\vert(\\nu+2)\\nu^3+(\\nu+1)(1-\\vert t\\vert)+\\vert t\\vert+3}{\\nu^3}\\bigg)e^{-\\vert s\\vert-\\vert t\\vert}\\\\\n&+\\frac{1}{2}(s^2+\\vert s\\vert+1)\\bigg(\\frac{(\\nu+1)(\\vert t\\vert(\\nu+1)^2+3(\\nu+1)-\\vert t\\vert)-1}{\\nu^3}\\\\\n&\\ \\ \\ \\ \\ \\ +(\\nu+1)(t^2(\\nu+2)(\\nu+1)+2\\vert t\\vert(\\nu+2)+t^2)\\bigg)e^{-(\\nu+1)\\vert s\\vert-\\vert t\\vert}\\\\\n&-\\frac{1}{2}s(\\vert s\\vert+1)(\\nu+1)\\bigg(\\frac{t(\\nu+2)^3(\\nu+1)}{2\\nu^2}+t(\\nu+1)^2+t-4\\sgn(t)\\bigg)e^{-\\vert s\\vert-\\vert t\\vert}\\\\\n&+\\frac{1}{2}s(\\vert s\\vert+1)\\bigg(\\frac{t(\\nu+2)^3(\\vert t\\vert(\\nu+1)^3-3(\\nu+1)^2+\\vert t\\vert(\\nu+1)+1)}{4(\\nu+1)\\nu^2}\\\\\n&\\ \\ \\ \\ \\ \\ -t(\\nu+1)^2+t-4\\sgn(t)(\\nu+1)\\bigg)e^{-\\vert s\\vert-\\vert t\\vert}\\\\\n&+\\frac{1}{2\\pi}s(\\vert s\\vert+1)(\\nu+2)^3\\big(tJ_1(t)-2J_2(t)\\big)e^{-\\vert s\\vert}\\\\\n& \\hspace*{-5mm} +\\big((s^2+\\vert s\\vert+1)(t^2+\\vert t\\vert+1)+s(\\vert s\\vert+1)t(\\vert t\\vert+1)\\big)\\frac{(\\nu+2)^2(5\\nu^2+14\\nu+10)}{64(\\nu+1)^3}\\cdot e^{-\\vert s\\vert-\\vert t\\vert},\n\\end{align*}\nwhere $\\sgn(\\cdot)$ is the sign function, and \n\\begin{equation*}\nJ_1(t;\\nu)=\\int_0^{\\infty}\\frac{x^2\\cos(tx)}{(1+x^2)((\\nu+1)^2+x^2)^2}\\mbox{d}x\\quad \\mbox{and}\\quad J_2(t;\\nu)=\\int_0^{\\infty}\\frac{x^3\\sin(tx)}{(1+x^2)^2((\\nu+1)^2+x^2)^2}\\mbox{d}x.\n\\end{equation*}\n\n\\end{enumerate}\n\n\\end{remark}\n\nAs demonstrated in Remark \\ref{rem:est}, the estimation procedure has some influence on the limit null distribution of $T_n$. We refer to \\cite{C:2011,F:2013} for alternative estimators of the parameters of the Cauchy distribution and to \\cite{CF:2006} for an Pitman estimator of the location parameter $\\alpha$ when $\\beta$ is known.\n\n\n\\subsection{Asymptotic normality of the limit statistic under the null hypothesis}\nThe next result gives the limiting distribution of the scaled limiting statistic $aT_{n,a}$ for $a\\to 0$ given in (\\ref{stat:lim1}).\n\n\\begin{theorem}\\label{thmlimit2} \n\\begin{enumerate}\n\\item[a)] \nLet $\\arcsine(a,b)$ denote the arcsine distribution on $[a,b]$ with distribution function \n\\[\nF(x)=\\frac{2}{\\pi}\\arcsin\\sqrt{\\frac{x-a}{b-a}},a\\leq x\\leq b.\n\\] \nIf $X\\sim C(0,1)$, then\n\\begin{align*}\n\\frac{4X^2}{(1+X^2)^2} &\\sim \\arcsine(0,1).\n\\end{align*}\n\\item[b)]\nLet $X_1,...,X_n$ be i.i.d. $\\mbox{C}(0,1)$ distributed random variables, and let $Y_{n,j}$ be the corresponding scaled residuals, using  any estimator $(\\hat\\alpha,\\hat\\beta)$ with linear representation. Then,\n\\begin{align} \\label{stat:lim3}\nT_{n,0} &= \\sqrt{2n} \\left( \\frac{8}{n} \\sum_{j=1}^n \\frac{Y_{n,j}^2}{1+Y_{n,j}^2} - 1\\right) \\ \\stackrel{\\mathcal{D}}{\\longrightarrow} \\ N(0,1),\n\\end{align}\nwhere $N(0,1)$ denotes the standard normal distribution.\n\\end{enumerate}\n\\end{theorem}\n\n\\begin{proof}\n\\begin{enumerate}\n\\item[a)]\nLet $U$ have a uniform distribution on $(-\\pi/2,\\pi/2)$, and put $X=\\tan U$. Then, $X\\sim C(0,1)$. Further, $\\sin U, \\, X^2/(1+X^2)=\\sin^2 U, \\, 1/(1+X^2)=\\cos^2 U$ as well as $2\\sin U \\cos U$ have an $\\arcsine(-1,1)$ distribution (see, e.g., \\cite{N:1983}). If $Z \\sim \\arcsine(-1,1)$, then $Z^2 \\sim \\arcsine(0,1)$. Hence,\n\\begin{align*}\n\\frac{4X^2}{(1+X^2)^2} &\\sim 4 \\sin^2 U\\cos^2 U \\sim \\arcsine(0,1).\n\\end{align*}\n\\item[b)]\nA second order Taylor expansion around $(\\alpha_0,\\beta_0)=(0,1)$ yields\n\\begin{align*} \n\\frac{1}{\\sqrt{n}} & \\sum_{j=1}^n \\frac{(X_j-\\alpha)^2/\\beta^2}{(1+(X_j-\\alpha)^2/\\beta^2)^2}\n= \\frac{1}{\\sqrt{n}} \\sum_{j=1}^n \\left\\{ \n  \\frac{X_j^2}{(1+X_j^2)^2)} \n+ \\frac{2X_j(X_j^2-1)\\alpha}{(1+X_j^2)^3} \n+ \\frac{2X_j^2(X_j^2-1)(\\beta-1)}{(1+X_j^2)^3}  \\right. \\\\\n& \\left. + \\frac{3X_j^4-8X_j^2+1)\\alpha^2}{(1+X_j^2)^4} \n+ \\frac{4X_j(X_j^4-4X_j^2+1)\\alpha(\\beta-1)}{(1+X_j^2)^4} \n+ \\frac{X_j^2(X_j^4-8X_j^2+3)(\\beta-1)^2}{(1+X_j^2)^4} \\right\\} + R_n,\n\\end{align*}\nwhere $R_n\\stackrel{\\mathcal{\\PP}}{\\longrightarrow} 0$ for $n\\to\\infty.$ If we replace $(\\alpha,\\beta)$ by any estimator $(\\hat\\alpha,\\hat\\beta)$ with linear representation, the second and third term converge to zero in probability, since, by the results given in the proof of part a),\n\\begin{align*} \n\\frac{1}{n} \\sum_{j=1}^n \\frac{X_j(X_j^2-1)}{(1+X_j^2)^3} &\\overset{a.s.}{\\longrightarrow} \\E\\frac{X_1(X_1^2-1)}{(1+X_1^2)^3} \\ = \\ 0, \\\\\n\\frac{1}{n} \\sum_{j=1}^n \\frac{X_j^2 (X_j^2-1)}{(1+X_j^2)^3} &\\overset{a.s.}{\\longrightarrow} \\E\\frac{X_1^2(X_1^2-1)}{(1+X_1^2)^3} \\ = \\ 0,\n\\end{align*}\nand $\\sqrt{n}\\hat\\alpha$ and $\\sqrt{n}(\\hat\\beta-1)$ have limiting normal distributions. Since the corresponding arithmetic means of the second order terms converge almost surely to finite values, the 4th, 5th and 6th term also converge to zero in probability. Therefore,\n\\begin{align*} \n\\frac{8}{\\sqrt{n}} \\sum_{j=1}^n  \\frac{Y_{n,j}^2}{(1+Y_{n,j}^2)^2}\n&= \\frac{8}{\\sqrt{n}} \\sum_{j=1}^n \\frac{X_j^2}{(1+X_j^2)^2} + \\tilde{R}_n,\n\\end{align*}\nwhere $\\tilde{R}_n\\stackrel{\\mathcal{\\PP}}{\\longrightarrow} 0$ for $n\\to\\infty.$ By a), $8X_1^2/(1+X_1^2)^2$ has expected value 1 and variance 1/2, and the assertion follows by the central limit theorem.\n\n\\end{enumerate}\n\\end{proof}\n\n\\begin{remark} \\label{remark:Tinfty}\nNote that the sum appearing on the right hand side of (\\ref{stat:lim2}) involves essentially the first component of the score function of the Cauchy distribution (see the second part of Remark \\ref{rem:est}). Hence, when using the maximum-likelihood estimator for $(\\alpha,\\beta)$, $\\lim_{a\\to\\infty} aT_{n,a}\\equiv 0$. For other estimators, the limit does not vanish, but a goodness-of-fit test based on it has very low power. Hence, we don't give further results for this statistic.\n\\end{remark}\n\n\n\\section{Consistency and behaviour under fixed alternatives}\nIn this section we show that the new tests are consistent against all alternatives satisfying a weak moment condition. Let $X,X_1,X_2,\\ldots$ be i.i.d. random variables with cumulative distribution function $F$ having a unique median as well as unique upper and lower quartiles and $\\E |X|^3/(1+X^2)^2<\\infty$. Since the tests $T_n$ are affine invariant, we assume w.l.o.g. that the true median $\\alpha=0$ and the half interquartile range $\\beta=1$. Under these assumptions we clearly have that \n\\begin{equation*}\n    (\\widehat{\\alpha}_n,\\widehat{\\beta}_n)\\overset{\\PP}{\\longrightarrow}(0,1),\\quad\\mbox{as}\\;n\\rightarrow\\infty,\n\\end{equation*}\nfor the MIQ estimation method in Remark \\ref{rem:est}, for details see \\cite{GH:2000}, and assume the convergence in all other cases.\n\n\\begin{theorem}\\label{thm:consistency}\nUnder the standing assumptions, we have\n\\[\\frac{T_n}{n}\\overset{\\PP}{\\longrightarrow}\\int_{-\\infty}^{\\infty}\\bigg\\vert\\E\\bigg[\\bigg(it-\\frac{2X}{1+X^2}\\bigg)e^{itX}\\bigg]\\bigg\\vert^2\\omega(t)dt=\\Delta_F,\\quad\\mbox{as}\\;n\\longrightarrow\\infty.\\]\n\\end{theorem}\nA proof of Theorem \\ref{thm:consistency} is deferred to Appendix \\ref{app:cons}. In view of the characterization in Theorem \\ref{thm:Char} $\\Delta_F=0$ if and only if $F=\\mbox{C}(0,1)$. Hence, we conclude that the tests $T_n$ are able to detect all alternatives satisfying the assumptions of this section.\n\\begin{remark}\nIn this remark we fix the weight function $\\omega_1(t)=\\exp(-|t|)$, $t\\in\\R$. By numerical integration we calculate that if $X\\sim\\mathcal{U}(-\\sqrt{3},\\sqrt{3})$, then $\\Delta_{\\mathcal{U}}\\approx 2.332019$, if $X\\sim\\mbox{N}(0,1)$ then  $\\Delta_{N}\\approx0.021839$ and if $X\\sim\\mbox{Log}(0,1)$, then $\\Delta_{L}\\approx0.041495$. \n\\end{remark}\n\nIn the following lines we use the theory presented in \\cite{BEH:2017} to derive the limit distribution under fixed alternatives. Let $Z_n^{\\bullet}(t)=n^{-1/2}Z_n(t)$, and\n\\[z(t)=\\E\\big[\\big(\\frac{2X}{1+X^2}+t\\big)\\cos(tX)+\\big(t-\\frac{2X}{1+X^2}\\big)\\sin(tX)\\big],\\quad t\\in\\R,\\]\nas well as $W_n(t)=\\sqrt{n}(Z_n^{\\bullet}(t)-z(t))$, $t\\in\\R.$ The process $W_n=(W_n(t),t\\in\\R)$ is a random element of $\\mathbb{H}$ and we have\n\\begin{align}\n\\label{Tn-Delta}\n\\sqrt{n}\\Big(\\frac{T_n}{n}-\\Delta\\Big)&=\\sqrt{n}(\\Vert Z_n^{\\bullet}\\Vert_{\\mathbb{H}}^2-\\Vert z\\Vert_{\\mathbb{H}}^2)\\nonumber=\\sqrt{n}\\langle Z_n^{\\bullet}-z,Z_n^{\\bullet}+z\\rangle_{\\mathbb{H}}\\nonumber=\\sqrt{n}\\langle Z_n^{\\bullet}-z,2z+Z_n^{\\bullet}-z\\rangle_{\\mathbb{H}}\\nonumber\\\\\n&=2\\langle\\sqrt{n}(Z_n^{\\bullet}-z),z\\rangle_{\\mathbb{H}}+\\frac{1}{\\sqrt{n}}\\Vert\\sqrt{n}(Z_n^{\\bullet}-z)\\Vert_{\\mathbb{H}}^2=2\\langle W_n,z\\rangle_{\\mathbb{H}}+\\frac{1}{\\sqrt{n}}\\Vert W_n\\Vert_\\mathbb{H}^2.\n\\end{align}\nThe next lemma is needed to prove the subsequent statements. For a proof see Appendix \\ref{app:PL}.\n\\begin{lemma}\\label{lem:Hilfslemma}\nUnder the standing assumptions, we have for $n\\longrightarrow\\infty$\n\\[W_n=\\sqrt{n}(Z_n^{\\bullet}-z)\\overset{\\mathcal{D}}{\\longrightarrow}\\mathcal{W}\\]\nin $\\mathbb{H}$, where $\\mathcal{W}$ is a centred Gaussian process in $\\mathbb{H}$ with covariance kernel\n\\begin{align*}\nK(s,t)=&\\E\\bigg[\\bigg(st+\\frac{4X^2}{(1+X^2)^2}\\bigg)\\cos(X(s-t))+\\bigg(st-\\frac{4X^2}{(1+X^2)^2}\\bigg)\\sin(X(s+t))\\bigg]\\\\\n&+\\E\\bigg[(s+t)\\frac{2X}{1+X^2}\\cos(X(s+t))+(s-t)\\frac{2X}{1+X^2}\\sin(X(s-t))\\bigg]\\\\\n&+\\E\\bigg[\\bigg(\\frac{2X}{1+X^2}+s\\bigg)(a(t)\\psi_1(X)+b(t)\\psi_2(X))\\cos(sX)\\bigg]\\\\\n&+\\E\\bigg[\\bigg(\\frac{2X}{1+X^2}+t\\bigg)(a(s)\\psi_1(X)+b(s)\\psi_2(X))\\cos(tX)\\bigg]\\\\\n&+\\E\\bigg[\\bigg(s-\\frac{2X}{1+X^2}\\bigg)(a(t)\\psi_1(X)+b(t)\\psi_2(X))\\sin(sX)\\bigg]\\\\\n&+\\E\\bigg[\\bigg(t-\\frac{2X}{1+X^2}\\bigg)(a(s)\\psi_1(X)+b(s)\\psi_2(X))\\sin(tX)\\bigg]\\\\\n&+\\E[\\psi_1(X)^2]a(s)a(t)+\\E[\\psi_2(X)^2]b(s)b(t)-3z(s)z(t).\n\\end{align*}\nHere, $\\psi_i(\\cdot),\\ i=1,2,$ are defined as in (\\ref{eq:psi_11}), $a(t)=\\E[g(t,X)],$ $b(t)=E[Xg(t,X)]$, and $g(\\cdot,\\cdot)$ is given in \\eqref{g}.\n\\end{lemma}\n By Lemma \\ref{lem:Hilfslemma} we conclude that $\\sqrt{n}(Z_n^{\\bullet}-z)$ is a tight sequence in $\\mathbb{H}$, hence $\\frac{1}{\\sqrt{n}}\\Vert\\sqrt{n}(Z_n^{\\bullet}-z)\\Vert_{\\mathbb{H}}^2=o_\\PP(1)$, and we have, by Slutzky's theorem, that the weak limit of $\\sqrt{n}(\\frac{Z_n}{n}-\\Delta)$ in (\\ref{Tn-Delta}) only depends on $2\\langle\\sqrt{n}(Z_n^{\\bullet}-z),z\\rangle_{\\mathbb{H}}$. Theorem 1 in \\cite{BEH:2017} and Lemma \\ref{lem:Hilfslemma} then directly prove the following theorem.\n\n\\begin{theorem}\\label{thm:fA}\nUnder the standing assumptions, we have\n\\[\\sqrt{n}\\bigg(\\frac{T_n}{n}-\\Delta_F\\bigg)\\overset{\\mathcal{D}}{\\longrightarrow}\\mbox{N}(0,\\sigma^2),\\quad\\mbox{as}\\;n\\rightarrow\\infty,\\]\nwhere\n\\begin{equation}\n\\label{sigma}\n\\sigma^2=4\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}K(s,t)z(s)z(t)\\omega(s)\\omega(t)\\,\\mbox{d}s\\,\\mbox{d}t.\n\\end{equation}\n\\end{theorem}\n\nThe results of Theorem \\ref{thm:fA} can be used to derive confidence intervals for $\\Delta_F$ or approximations of the power function. Note that since for most fixed alternatives and estimation procedures it is hard to explicitly derive the formulae for $K$, $z$ and $\\sigma^2$, we propose to estimate $\\sigma^2$ by a consistent estimator $\\widehat{\\sigma}_n^2$, for a construction of such an estimator and for further details on the applications of Theorem \\ref{thm:fA}, we refer to \\cite{BEH:2017}. \n\n\n\n\\section{Simulation results}\n\nIn this section, we describe an extensive simulation study to compare the power of the new tests with established ones.\nAll simulations have been done for nominal level $\\alpha=0.05$ and for sample sizes $n=20$ and $n=50$. \nThe unknown parameters are estimated either by median and IQR estimators or by the method of maximum likelihood, see Remark \\ref{rem:est}. In a first step, critical values are determined for each test statistic by a Monte Carlo simulation with $10^5$ replications. In a second step, empirical power of the different tests is calculated based on $10^4$ replications.\n\nBesides the new tests based on $T_{n,a}$ in (\\ref{eq:EF}) with weights $a=1,2,3,4,5,6$, we use the limiting test $T_{n,0}$ in (\\ref{stat:lim3}).  Power decreased for larger weights for all distributions used in the study; hence, we omit the results for weights larger than 6 as well as for the test based on the limiting statistic $T_{n,\\infty}$ in (\\ref{stat:lim2}), see also Remark \\ref{remark:Tinfty}. Moreover, we use the test based on the Kullback-Leibler distance ($KL$) described in \\cite{MZ:2017}. For the nonparametric estimation of the entropy, the authors recommend the window size $m=4$ for $n=20$ and $m=20$ for $n=50$ (\\cite{MZ:2017}, p. 1109), and we followed this suggestion. \nFrom the classical tests that utilize the empirical distribution function (so-called edf tests), we choose the following: Kolmogorov-Smirnov test ($KS$), Cram\\'{e}r-von Mises test ($CM$, Anderson Darling test ($AD$ and Watson test ($W$). Finally, we apply the tests based on the statistics $D_{n,\\lambda}$ considered in \\cite{GH:2000} and \\cite{MT:2005}, with weights $\\lambda=1,2,3,4,5,6$.\n\nBesides the standard Cauchy and normal distribution (C$(0,1)$ and N$(0,1)$, for short), we use Cauchy-normal mixtures CN$(p)=(1-p)$C$(0,1)+p$N$(0,1)$ with $p=0.5$ and $p=0.8$. Further, we employ Student's $t$-distribution (Student$(k)$) with $k=2,3,5,10$ degrees of freedom, the (symmetric) Tukey g-h distribution  with $g=0$  and $h=0.2, 0.1, 0.05$, denoted by Tukey($h$), and the Tukey lambda distribution (Tukey-L$(\\lambda)$) with $\\lambda=-3,-2,-0.5,0.5$. Note that the  Tukey lambda distribution approximates the Cauchy distribution for $\\lambda=-1$, it coincides with the logistic distribution for $\\lambda=0$, it is close to the normal for $\\lambda=0.14$, and coincides with the uniform distribution for $\\lambda=1$.\nIn the family of stable distributions S$(\\alpha,\\beta)$, we choose symmetric distributions S$(\\alpha,0)$ with $\\alpha=0.4,0.7,1.2,1.5,1.8$, and skewed distributions S$(\\alpha,1)$ with $\\alpha=0.5,1,1.5,2$. Moreover, we use the uniform, logistic, Laplace, Gumbel and exponential distribution, and the Mittag-Leffler distribution ML$(\\alpha)$ with $\\alpha=0.25,0.5,0.75$. The ML$(\\alpha)$ distribution has Laplace transform $(1+t^\\alpha)^{-1}$, and coincides with the exponential distribution for $\\alpha=1$.\n\nPower estimates of the tests under discussion are given in Tables \\ref{Table1}-\\ref{Table4}. All entries are the percentage of rejection of $H_0$, rounded to the nearest integer. In Tables \\ref{Table1} and \\ref{Table2}, the results using the median and interquantile-distance estimators are given, with sample size $n=20$ and $n=50$, respectively. Tables \\ref{Table3} and \\ref{Table4} show the corresponding results for the maximum likelihood estimator.\n\n\\begin{sidewaystable}\n\\begin{center}\n\\begin{tabular}{rrrrrrrrrrrrrrrrrrr} \n\\hline\n & $T_{n,1}$ & $T_{n,2}$ & $T_{n,3}$ & $T_{n,4}$ & $T_{n,5}$ & $T_{n,6}$ & $T_{n,0}$ \n & $KL$ & $KS$ & $CM$ & $AD$ & $W$ & $D_{n,1}$ & $D_{n,2}$ & $D_{n,3}$ & $D_{n,4}$ & $D_{n,5}$ & $D_{n,6}$ \\\\ \n\\hline\nC(0,1) & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 \\\\\n  N(0,1) & 8 & 4 & 1 & 0 & 0 & 0 & 29 & 73 & 5 & 6 & 6 & 15 & 15 & 21 & 26 & 26 & 23 & 19 \\\\\n  CN(0.5) & 4 & 3 & 3 & 3 & 3 & 3 & 9 & 14 & 4 & 3 & 3 & 5 & 5 & 5 & 5 & 5 & 4 & 3 \\\\\n  CN(0.8) & 6 & 3 & 1 & 1 & 1 & 1 & 19 & 36 & 4 & 4 & 4 & 8 & 8 & 11 & 13 & 12 & 10 & 8 \\\\\n  Student(2) & 4 & 2 & 2 & 2 & 2 & 2 & 9 & 22 & 3 & 4 & 3 & 5 & 4 & 5 & 5 & 5 & 5 & 3 \\\\\n  Student(3) & 5 & 2 & 1 & 1 & 1 & 1 & 14 & 34 & 3 & 4 & 3 & 7 & 6 & 7 & 9 & 9 & 8 & 6 \\\\\n  Student(5) & 6 & 3 & 1 & 1 & 1 & 1 & 20 & 49 & 4 & 4 & 4 & 9 & 8 & 11 & 14 & 14 & 12 & 9 \\\\\n  Student(10) & 7 & 3 & 1 & 1 & 1 & 1 & 24 & 61 & 4 & 5 & 5 & 11 & 11 & 15 & 18 & 19 & 16 & 13 \\\\\n  Stable(0.4,0) & 55 & 52 & 39 & 26 & 18 & 13 & 79 & 2 & 39 & 45 & 83 & 58 & 62 & 64 & 66 & 69 & 71 & 74 \\\\\n  Stable(0.7,0) & 16 & 15 & 12 & 9 & 8 & 7 & 21 & 2 & 12 & 13 & 23 & 14 & 16 & 18 & 20 & 22 & 24 & 25 \\\\\n  Stable(1.2,0) & 4 & 3 & 3 & 3 & 3 & 3 & 6 & 10 & 4 & 4 & 3 & 4 & 4 & 4 & 4 & 4 & 3 & 3 \\\\\n  Stable(1.5,0) & 5 & 2 & 1 & 1 & 1 & 2 & 14 & 28 & 4 & 4 & 3 & 6 & 6 & 7 & 8 & 8 & 6 & 5 \\\\\n  Stable(1.8,0) & 7 & 3 & 1 & 1 & 1 & 1 & 22 & 52 & 5 & 5 & 5 & 11 & 10 & 14 & 16 & 16 & 14 & 11 \\\\\n  Stable(0.5,1) & 91 & 97 & 90 & 76 & 61 & 50 & 40 & 65 & 94 & 90 & 97 & 88 & 95 & 76 & 57 & 46 & 44 & 46 \\\\\n  Stable(1.5,1) & 10 & 10 & 7 & 6 & 6 & 6 & 19 & 49 & 9 & 9 & 9 & 11 & 13 & 11 & 12 & 11 & 10 & 8 \\\\\n  Stable(2,1) & 8 & 4 & 0 & 0 & 0 & 0 & 28 & 73 & 5 & 6 & 6 & 14 & 14 & 20 & 24 & 24 & 22 & 17 \\\\\n  Tukey(0.2) & 5 & 2 & 1 & 1 & 1 & 1 & 14 & 36 & 3 & 4 & 3 & 7 & 6 & 8 & 10 & 9 & 8 & 7 \\\\\n  Tukey(0.1) & 5 & 3 & 1 & 1 & 1 & 1 & 21 & 52 & 4 & 4 & 4 & 9 & 8 & 12 & 15 & 15 & 13 & 10 \\\\\n  Tukey(0.05) & 7 & 3 & 1 & 0 & 0 & 0 & 24 & 62 & 4 & 5 & 5 & 12 & 11 & 15 & 19 & 19 & 17 & 13 \\\\\n  Tukey-L(-3) & 43 & 42 & 33 & 24 & 17 & 12 & 64 & 2 & 31 & 35 & 79 & 44 & 52 & 58 & 62 & 65 & 68 & 71 \\\\\n  Tukey-L(-2) & 21 & 22 & 18 & 14 & 11 & 10 & 33 & 1 & 16 & 17 & 41 & 19 & 24 & 28 & 32 & 35 & 38 & 41 \\\\\n  Tukey-L(-0.5) & 4 & 3 & 2 & 2 & 2 & 2 & 7 & 15 & 3 & 4 & 3 & 5 & 4 & 4 & 4 & 4 & 3 & 3 \\\\\n  Tukey-L(0.5) & 17 & 11 & 1 & 0 & 0 & 0 & 45 & 94 & 10 & 12 & 15 & 31 & 36 & 45 & 50 & 50 & 47 & 40 \\\\\n  Uniform & 32 & 21 & 1 & 1 & 0 & 0 & 56 & 99 & 24 & 22 & 28 & 51 & 60 & 67 & 69 & 68 & 65 & 59 \\\\\n  Logistic & 6 & 3 & 1 & 1 & 1 & 1 & 21 & 56 & 4 & 5 & 4 & 11 & 9 & 13 & 16 & 17 & 15 & 11 \\\\\n  Laplace & 4 & 2 & 1 & 1 & 1 & 1 & 10 & 32 & 3 & 3 & 3 & 6 & 5 & 6 & 8 & 8 & 7 & 5 \\\\\n  Gumbel & 10 & 9 & 3 & 3 & 2 & 2 & 26 & 70 & 9 & 8 & 8 & 15 & 17 & 18 & 20 & 20 & 18 & 15 \\\\\n  ML(0.25) & 100 & 100 & 100 & 97 & 92 & 83 & 93 & 89 & 100 & 100 & 100 & 100 & 100 & 99 & 98 & 96 & 96 & 96 \\\\\n  ML(0.5) & 98 & 99 & 93 & 79 & 63 & 49 & 55 & 81 & 99 & 96 & 99 & 97 & 99 & 86 & 69 & 58 & 55 & 57 \\\\\n  ML(0.75) & 78 & 87 & 69 & 52 & 43 & 39 & 20 & 75 & 85 & 71 & 79 & 72 & 83 & 52 & 33 & 25 & 23 & 23 \\\\\n  Exponential & 39 & 44 & 23 & 15 & 13 & 11 & 27 & 92 & 48 & 31 & 32 & 41 & 49 & 34 & 30 & 28 & 27 & 25 \\\\  \n \\hline\n\\end{tabular}\n\\caption{\\label{Table1} Percentage of 10 000 MC samples declared significant by various\ntests for the Cauchy distribution using median and IQR estimator ($\\alpha=0.05, n=20$)}\n\\end{center}\n\\end{sidewaystable}\n\n\n\\begin{sidewaystable}\n\\begin{center}\n\\begin{tabular}{rrrrrrrrrrrrrrrrrrr}\n  \\hline\n & $T_{n,1}$ & $T_{n,2}$ & $T_{n,3}$ & $T_{n,4}$ & $T_{n,5}$ & $T_{n,6}$ & $T_{n,0}$ \n & $KL$ & $KS$ & $CM$ & $AD$ & $W$ & $D_{n,1}$ & $D_{n,2}$ & $D_{n,3}$ & $D_{n,4}$ & $D_{n,5}$ & $D_{n,6}$ \\\\ \n  \\hline\nC(0,1) & 5 & 5 & 4 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 \\\\\n  N(0,1) & 24 & 24 & 4 & 1 & 0 & 0 & 72 & 100 & 26 & 29 & 55 & 67 & 65 & 83 & 89 & 92 & 93 & 94 \\\\\n  CN(0.5) & 8 & 5 & 3 & 3 & 3 & 3 & 23 & 14 & 7 & 6 & 7 & 13 & 13 & 15 & 16 & 16 & 15 & 13 \\\\\n  CN(0.8) & 16 & 13 & 2 & 1 & 1 & 1 & 51 & 46 & 14 & 14 & 23 & 38 & 37 & 49 & 54 & 55 & 55 & 54 \\\\\n  Student(2) & 6 & 4 & 2 & 2 & 2 & 2 & 23 & 52 & 6 & 6 & 8 & 14 & 10 & 15 & 19 & 21 & 23 & 23 \\\\\n  Student(3) & 10 & 7 & 2 & 2 & 1 & 1 & 38 & 82 & 9 & 10 & 16 & 27 & 21 & 32 & 40 & 44 & 47 & 49 \\\\\n  Student(5) & 14 & 12 & 2 & 1 & 1 & 1 & 52 & 96 & 13 & 15 & 28 & 41 & 36 & 52 & 62 & 67 & 70 & 72 \\\\\n  Student(10) & 18 & 18 & 3 & 1 & 1 & 0 & 64 & 100 & 18 & 20 & 41 & 54 & 49 & 70 & 78 & 83 & 85 & 86 \\\\\n  Stable(0.4,0) & 91 & 90 & 76 & 47 & 25 & 15 & 99 & 0 & 80 & 91 & 99 & 98 & 97 & 96 & 97 & 97 & 98 & 98 \\\\\n  Stable(0.7,0) & 23 & 23 & 14 & 10 & 8 & 7 & 46 & 0 & 16 & 17 & 39 & 26 & 29 & 34 & 37 & 41 & 45 & 48 \\\\\n  Stable(1.2,0) & 6 & 4 & 3 & 3 & 3 & 3 & 12 & 19 & 6 & 5 & 5 & 8 & 6 & 7 & 8 & 8 & 8 & 8 \\\\\n  Stable(1.5,0) & 11 & 8 & 3 & 2 & 2 & 2 & 38 & 53 & 9 & 9 & 13 & 24 & 20 & 29 & 34 & 36 & 37 & 37 \\\\\n  Stable(1.8,0) & 18 & 16 & 3 & 1 & 1 & 1 & 62 & 88 & 18 & 19 & 35 & 50 & 46 & 63 & 71 & 74 & 77 & 77 \\\\\n  Stable(0.5,1) & 100 & 100 & 100 & 100 & 97 & 87 & 71 & 13 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 99 & 99 \\\\\n  Stable(1.5,1) & 28 & 41 & 26 & 18 & 15 & 13 & 52 & 81 & 56 & 38 & 53 & 60 & 61 & 71 & 72 & 70 & 69 & 68 \\\\\n  Stable(2,1) & 23 & 23 & 4 & 1 & 1 & 0 & 72 & 100 & 26 & 28 & 55 & 67 & 65 & 84 & 89 & 92 & 94 & 94 \\\\\n  Tukey(0.2) & 10 & 7 & 2 & 1 & 1 & 1 & 40 & 87 & 9 & 9 & 16 & 27 & 22 & 34 & 42 & 47 & 50 & 52 \\\\\n  Tukey(0.1) & 15 & 13 & 2 & 1 & 1 & 1 & 56 & 98 & 15 & 16 & 32 & 44 & 39 & 57 & 67 & 72 & 75 & 77 \\\\\n  Tukey(0.05) & 19 & 17 & 3 & 1 & 1 & 1 & 64 & 100 & 18 & 21 & 41 & 54 & 50 & 70 & 78 & 83 & 85 & 87 \\\\\n  Tukey-L(-3) & 70 & 74 & 57 & 35 & 20 & 13 & 96 & 0 & 64 & 73 & 99 & 91 & 90 & 94 & 95 & 96 & 97 & 98 \\\\\n  Tukey-L(-2) & 33 & 35 & 24 & 16 & 12 & 10 & 64 & 0 & 26 & 29 & 72 & 47 & 48 & 58 & 64 & 68 & 72 & 75 \\\\\n  Tukey-L(-0.5) & 5 & 4 & 3 & 3 & 3 & 3 & 13 & 37 & 5 & 5 & 5 & 8 & 7 & 8 & 10 & 11 & 12 & 12 \\\\\n  Tukey-L(0.5) & 56 & 61 & 11 & 1 & 0 & 0 & 90 & 100 & 68 & 63 & 89 & 92 & 96 & 99 & 100 & 100 & 100 & 100 \\\\\n  Uniform & 87 & 87 & 18 & 1 & 0 & 0 & 97 & 100 & 95 & 86 & 98 & 99 & 100 & 100 & 100 & 100 & 100 & 100 \\\\\n  Logistic & 15 & 14 & 3 & 1 & 1 & 1 & 59 & 100 & 15 & 17 & 35 & 48 & 43 & 63 & 72 & 77 & 80 & 82 \\\\\n  Laplace & 6 & 4 & 2 & 1 & 1 & 1 & 21 & 94 & 6 & 7 & 12 & 18 & 13 & 24 & 32 & 37 & 40 & 42 \\\\\n  Gumbel & 33 & 43 & 18 & 8 & 6 & 5 & 68 & 100 & 58 & 42 & 64 & 73 & 74 & 86 & 89 & 90 & 91 & 91 \\\\\n  ML(0.25) & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 4 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 \\\\\n  ML(0.5) & 100 & 100 & 100 & 100 & 98 & 91 & 86 & 15 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 \\\\\n  ML(0.75) & 100 & 100 & 100 & 93 & 78 & 62 & 30 & 45 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 98 & 94 & 91 \\\\\n  Exponential & 94 & 96 & 75 & 44 & 32 & 26 & 64 & 100 & 100 & 92 & 98 & 99 & 100 & 100 & 99 & 98 & 97 & 97 \\\\\n   \\hline\n\\end{tabular}\n\\caption{\\label{Table2} Percentage of 10 000 MC samples declared significant by various\ntests for the Cauchy distribution using median and IQR estimator ($\\alpha=0.05, n=50$)}\n\\end{center}\n\\end{sidewaystable}\n\n\n\\begin{sidewaystable}\n\\begin{center}\n\\begin{tabular}{rrrrrrrrrrrrrrrrrrr}\n  \\hline\n & $T_{n,1}$ & $T_{n,2}$ & $T_{n,3}$ & $T_{n,4}$ & $T_{n,5}$ & $T_{n,6}$ & $T_{n,0}$ \n & $KL$ & $KS$ & $CM$ & $AD$ & $W$ & $D_{n,1}$ & $D_{n,2}$ & $D_{n,3}$ & $D_{n,4}$ & $D_{n,5}$ & $D_{n,6}$ \\\\ \n  \\hline\nC(0,1) & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 \\\\\n  N(0,1) & 16 & 30 & 34 & 28 & 10 & 3 & 26 & 76 & 5 & 3 & 2 & 29 & 23 & 17 & 10 & 7 & 4 & 2 \\\\\n  CN(0.5) & 8 & 10 & 8 & 5 & 2 & 2 & 9 & 14 & 4 & 4 & 3 & 9 & 7 & 3 & 2 & 2 & 2 & 1 \\\\\n  CN(0.8) & 13 & 19 & 18 & 12 & 5 & 2 & 18 & 37 & 5 & 3 & 2 & 18 & 14 & 8 & 5 & 3 & 2 & 1 \\\\\n  Student(2) & 8 & 9 & 8 & 6 & 3 & 2 & 10 & 22 & 4 & 3 & 2 & 10 & 7 & 4 & 2 & 2 & 1 & 1 \\\\\n  Student(3) & 10 & 14 & 14 & 10 & 4 & 2 & 15 & 37 & 4 & 2 & 1 & 14 & 10 & 6 & 3 & 2 & 2 & 1 \\\\\n  Student(5) & 12 & 19 & 20 & 14 & 6 & 2 & 19 & 51 & 5 & 3 & 2 & 19 & 14 & 9 & 5 & 4 & 3 & 2 \\\\\n  Student(10) & 15 & 25 & 27 & 20 & 8 & 3 & 22 & 64 & 5 & 3 & 2 & 24 & 19 & 13 & 8 & 5 & 3 & 2 \\\\\n  Stable(0.4,0) & 53 & 64 & 72 & 77 & 80 & 81 & 76 & 0 & 42 & 33 & 77 & 77 & 71 & 79 & 83 & 86 & 87 & 88 \\\\\n  Stable(0.7,0) & 10 & 14 & 19 & 22 & 25 & 26 & 19 & 1 & 12 & 11 & 18 & 15 & 18 & 24 & 26 & 28 & 29 & 30 \\\\\n  Stable(1.2,0) & 7 & 6 & 5 & 4 & 2 & 2 & 7 & 11 & 4 & 4 & 3 & 7 & 5 & 3 & 2 & 2 & 2 & 2 \\\\\n  Stable(1.5,0) & 10 & 13 & 12 & 8 & 3 & 2 & 14 & 29 & 4 & 3 & 2 & 14 & 9 & 5 & 3 & 2 & 2 & 1 \\\\\n  Stable(1.8,0) & 14 & 23 & 25 & 18 & 7 & 3 & 21 & 55 & 5 & 3 & 2 & 23 & 17 & 11 & 7 & 5 & 3 & 2 \\\\\n  Stable(0.5,1) & 55 & 70 & 78 & 81 & 83 & 84 & 26 & 54 & 99 & 99 & 99 & 82 & 79 & 82 & 85 & 87 & 88 & 89 \\\\\n  Stable(1.5,1) & 14 & 21 & 22 & 17 & 11 & 7 & 16 & 50 & 14 & 12 & 9 & 22 & 17 & 11 & 9 & 7 & 6 & 5 \\\\\n  Stable(2,1) & 17 & 29 & 34 & 28 & 10 & 4 & 26 & 76 & 5 & 3 & 2 & 28 & 23 & 17 & 10 & 7 & 4 & 3 \\\\\n  Tukey(0.2) & 10 & 14 & 14 & 10 & 4 & 2 & 14 & 38 & 4 & 3 & 2 & 14 & 10 & 6 & 4 & 3 & 2 & 1 \\\\\n  Tukey(0.1) & 13 & 20 & 21 & 16 & 6 & 2 & 19 & 55 & 5 & 3 & 2 & 20 & 15 & 10 & 5 & 4 & 2 & 2 \\\\\n  Tukey(0.05) & 15 & 25 & 28 & 21 & 7 & 3 & 23 & 65 & 5 & 3 & 2 & 24 & 19 & 13 & 8 & 5 & 3 & 2 \\\\\n  Tukey-L(-3) & 34 & 49 & 60 & 67 & 72 & 75 & 60 & 0 & 35 & 27 & 72 & 60 & 58 & 71 & 76 & 79 & 81 & 82 \\\\\n  Tukey-L(-2) & 13 & 21 & 29 & 35 & 40 & 42 & 29 & 1 & 17 & 15 & 34 & 26 & 27 & 38 & 42 & 45 & 48 & 49 \\\\\n  Tukey-L(-0.5) & 6 & 7 & 6 & 4 & 3 & 2 & 8 & 16 & 4 & 3 & 2 & 7 & 5 & 3 & 2 & 2 & 1 & 1 \\\\\n  Tukey-L(0.5) & 31 & 55 & 64 & 59 & 24 & 8 & 37 & 96 & 11 & 6 & 5 & 50 & 50 & 43 & 28 & 19 & 11 & 7 \\\\\n  Uniform & 48 & 74 & 81 & 78 & 43 & 14 & 45 & 99 & 22 & 13 & 10 & 66 & 72 & 65 & 48 & 35 & 24 & 14 \\\\\n  Logistic & 14 & 22 & 24 & 18 & 7 & 3 & 21 & 60 & 5 & 3 & 2 & 22 & 17 & 11 & 6 & 4 & 3 & 2 \\\\\n  Laplace & 8 & 10 & 10 & 8 & 4 & 2 & 10 & 35 & 3 & 2 & 1 & 10 & 8 & 5 & 3 & 2 & 2 & 1 \\\\\n  Gumbel & 18 & 31 & 35 & 30 & 16 & 9 & 24 & 74 & 13 & 10 & 7 & 31 & 25 & 19 & 14 & 12 & 9 & 7 \\\\\n  ML(0.25) & 99 & 99 & 100 & 100 & 100 & 100 & 92 & 77 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 \\\\\n  ML(0.5) & 80 & 86 & 90 & 91 & 92 & 92 & 43 & 72 & 100 & 100 & 100 & 96 & 93 & 92 & 94 & 95 & 95 & 96 \\\\\n  ML(0.75) & 52 & 63 & 66 & 66 & 65 & 64 & 12 & 70 & 96 & 93 & 91 & 72 & 66 & 62 & 64 & 66 & 67 & 68 \\\\\n  Exponential & 38 & 53 & 57 & 54 & 45 & 36 & 20 & 92 & 64 & 50 & 43 & 55 & 50 & 41 & 39 & 38 & 37 & 35 \\\\\n   \\hline\n\\end{tabular}\n\\caption{\\label{Table3} Percentage of 10 000 MC samples declared significant by various\ntests for the Cauchy distribution using ML estimation ($\\alpha=0.05, n=20$)}\n\\end{center}\n\\end{sidewaystable}\n\n\n\\begin{sidewaystable}\n\\begin{center}\n\\begin{tabular}{rrrrrrrrrrrrrrrrrrr}\n  \\hline\n & $T_{n,1}$ & $T_{n,2}$ & $T_{n,3}$ & $T_{n,4}$ & $T_{n,5}$ & $T_{n,6}$ & $T_{n,0}$ \n & $KL$ & $KS$ & $CM$ & $AD$ & $W$ & $D_{n,1}$ & $D_{n,2}$ & $D_{n,3}$ & $D_{n,4}$ & $D_{n,5}$ & $D_{n,6}$ \\\\ \n  \\hline\nC(0,1) & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 \\\\\n  N(0,1) & 40 & 76 & 90 & 95 & 96 & 96 & 64 & 100 & 16 & 9 & 15 & 77 & 77 & 87 & 90 & 91 & 91 & 91 \\\\\n  CN(0.5) & 14 & 20 & 20 & 15 & 10 & 6 & 22 & 14 & 6 & 4 & 3 & 19 & 16 & 13 & 10 & 9 & 8 & 7 \\\\\n  CN(0.8) & 28 & 53 & 61 & 60 & 52 & 39 & 48 & 46 & 11 & 6 & 6 & 50 & 48 & 50 & 48 & 46 & 43 & 39 \\\\\n  Student(2) & 12 & 20 & 24 & 23 & 20 & 15 & 23 & 52 & 5 & 3 & 3 & 21 & 16 & 15 & 14 & 15 & 14 & 13 \\\\\n  Student(3) & 19 & 36 & 45 & 48 & 46 & 38 & 38 & 82 & 7 & 4 & 4 & 38 & 30 & 33 & 34 & 36 & 35 & 35 \\\\\n  Student(5) & 26 & 51 & 66 & 71 & 71 & 66 & 48 & 96 & 9 & 5 & 6 & 54 & 47 & 55 & 58 & 60 & 61 & 60 \\\\\n  Student(10) & 32 & 65 & 81 & 86 & 88 & 86 & 57 & 100 & 13 & 6 & 10 & 67 & 63 & 74 & 78 & 80 & 80 & 80 \\\\\n  Stable(0.4,0) & 95 & 98 & 99 & 99 & 99 & 99 & 99 & 0 & 82 & 81 & 99 & 100 & 99 & 99 & 100 & 100 & 100 & 100 \\\\\n  Stable(0.7,0) & 23 & 34 & 40 & 45 & 47 & 49 & 44 & 0 & 18 & 15 & 31 & 38 & 36 & 44 & 49 & 52 & 53 & 55 \\\\\n  Stable(1.2,0) & 8 & 10 & 10 & 8 & 6 & 4 & 12 & 18 & 5 & 4 & 3 & 10 & 8 & 6 & 5 & 5 & 4 & 4 \\\\\n  Stable(1.5,0) & 19 & 34 & 40 & 39 & 33 & 25 & 35 & 53 & 7 & 4 & 4 & 34 & 28 & 28 & 27 & 27 & 25 & 24 \\\\\n  Stable(1.8,0) & 34 & 62 & 76 & 80 & 78 & 71 & 56 & 88 & 12 & 6 & 9 & 64 & 60 & 67 & 68 & 69 & 68 & 67 \\\\\n  Stable(0.5,1) & 98 & 100 & 100 & 100 & 100 & 100 & 46 & 6 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 \\\\\n  Stable(1.5,1) & 32 & 61 & 74 & 78 & 76 & 68 & 44 & 80 & 57 & 43 & 46 & 62 & 60 & 66 & 66 & 67 & 67 & 66 \\\\\n  Stable(2,1) & 41 & 77 & 90 & 95 & 97 & 96 & 65 & 100 & 17 & 9 & 16 & 78 & 77 & 88 & 90 & 92 & 92 & 91 \\\\\n  Tukey(0.2) & 20 & 36 & 46 & 49 & 48 & 42 & 37 & 87 & 7 & 3 & 4 & 39 & 31 & 35 & 36 & 38 & 38 & 38 \\\\\n  Tukey(0.1) & 27 & 55 & 70 & 76 & 77 & 73 & 51 & 98 & 9 & 5 & 7 & 57 & 52 & 61 & 65 & 67 & 67 & 67 \\\\\n  Tukey(0.05) & 33 & 66 & 81 & 88 & 89 & 87 & 57 & 100 & 13 & 6 & 10 & 68 & 65 & 75 & 79 & 81 & 82 & 82 \\\\\n  Tukey-L(-3) & 79 & 92 & 96 & 97 & 98 & 98 & 96 & 0 & 68 & 60 & 98 & 96 & 94 & 97 & 98 & 99 & 99 & 99 \\\\\n  Tukey-L(-2) & 33 & 51 & 63 & 69 & 73 & 76 & 63 & 0 & 29 & 23 & 62 & 60 & 57 & 69 & 75 & 78 & 80 & 81 \\\\\n  Tukey-L(-0.5) & 8 & 11 & 13 & 12 & 11 & 8 & 13 & 37 & 4 & 3 & 3 & 12 & 9 & 8 & 8 & 7 & 7 & 7 \\\\\n  Tukey-L(0.5) & 75 & 98 & 100 & 100 & 100 & 100 & 83 & 100 & 54 & 27 & 50 & 97 & 99 & 100 & 100 & 100 & 100 & 100 \\\\\n  Uniform & 95 & 100 & 100 & 100 & 100 & 100 & 91 & 100 & 91 & 53 & 80 & 100 & 100 & 100 & 100 & 100 & 100 & 100 \\\\\n  Logistic & 28 & 58 & 74 & 81 & 82 & 80 & 52 & 100 & 10 & 5 & 8 & 60 & 56 & 66 & 70 & 72 & 73 & 72 \\\\\n  Laplace & 10 & 21 & 30 & 36 & 38 & 36 & 19 & 94 & 4 & 2 & 2 & 23 & 20 & 25 & 25 & 26 & 26 & 26 \\\\\n  Gumbel & 44 & 79 & 91 & 94 & 96 & 94 & 59 & 100 & 57 & 36 & 43 & 80 & 80 & 88 & 90 & 91 & 91 & 91 \\\\\n  ML(0.25) & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 0 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 \\\\\n  ML(0.5) & 100 & 100 & 100 & 100 & 100 & 100 & 76 & 6 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 \\\\\n  ML(0.75) & 97 & 99 & 100 & 100 & 100 & 99 & 14 & 38 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 \\\\\n  Exponential & 87 & 98 & 99 & 100 & 100 & 99 & 48 & 100 & 100 & 96 & 97 & 98 & 99 & 99 & 99 & 99 & 99 & 99 \\\\\n   \\hline\n\\end{tabular}\n\\caption{\\label{Table4} Percentage of 10 000 MC samples declared significant by various\ntests for the Cauchy distribution using ML estimation ($\\alpha=0.05, n=50$)}\n\\end{center}\n\\end{sidewaystable}\n\n\nThe main conclusions that can be drawn from the simulation results are the following:\n\\begin{itemize}\n\\item\nAs always in similar situations, there exists no uniformly most powerful test, which is in accordance to the results in \\cite{J:2000}.\n\\item\nAs expected, the power of nearly all test statistics increases for increasing sample size for all alternative distributions. An exception is the Kullback-Leibler distance based test. Its power decreases for certain alternatives, in particular for the Mittag-Leffler distribution. \n\\item\nThe new tests $T_{n,a}, a=1,\\ldots,6$, perform better using the maximum likelihood estimator than with median and half-IQR. Hence, the latter estimators should not be used. For all other test statistics, including $T_{n,0}$, performance is comparable, or even better when using median and half-IQR. In any case, the choice of the estimation method can have a pronounced influence on the performance of the tests.\n\\item\nFor alternatives with finite first and second moments, the Kullback-Leibler distance based test has the highest power among all competitors. On the other hand, its power breaks down completely for some alternatives without existing first moment, and it is low for some  alternatives with infinite second moment. Hence, the test can not really be seen as an omnibus test.\n\\item\nAmong the new tests, values of the tuning parameter around $a=3$ result in a quite homogeneous power against all alternatives. For the tests based on $D_{n,\\lambda}$, $\\lambda=5$ seems to be a good choice. Both classes of tests perform similarly for the ML estimator; for the median and IQR estimator, the latter is preferable. \n\\item \nAmong the group of edf tests, $W$ outperform the other tests in most cases.\n\\item \nFor symmetric alternatives without existing first moment as Stable(0.4,0), Stable(0.4,0), Tukey-L(-3) and Tukey-L(-2), the tests based on $T_{n,0}, AD$ and $D_{n,6}$ perform best.\n\\end{itemize}\n\n\n\\section{Real data example: log-returns of cryptocurrencies}\n\nIn this section, we apply the tests for the Cauchy distribution to log-returns of various cryptocurrencies, namely\nBitcoin (BTC), Ethereum (ETH), Ripple (XRP), Litecoin (LTC), BitcoinCash (BCH), EOS (EOS), BinanceCoin (BNB) and Tron(TRX). \nThe Cauchy distribution is found to be a comparably good model for such data sets in \\cite{S:2020}. There, 58 hypothetical distributions have been fitted to 15 major cryptocurrencies, and the Cauchy model turned out to be the best fitting distribution for 10 cryptocurrencies (including all currencies considered here). In \\cite{S:2020}, the number of observations of the various data sets varied widely from 638 to 2255. Further, with very large data sets, each model will be rejected in the end. Hence, we decided to consider shorter time series: a series with daily observations from January 01, 2020 to June 10, 2021, with sample size 527 (526 for ETH), and an even shorter series from January 01, 2021 to June 10, 2021, with sample size 161 (160 for ETH). All prices are closing values in U.S. dollars obtained from cryptodatadownload.com. Returns are estimated by taking logarithmic differences. Days with zero trading volume are omitted. Figure \\ref{fig:crypto-hist} shows histogramms of the datasets with larger time span, together with the densities of fitted Cauchy distributions. Visually, the Cauchy model seems to be a reasonable approximation.\n\n\\begin{figure}\n\\centering\n\\includegraphics[scale=0.7]{crypto-hist}\n\\caption{Histograms of cryptocurrency log-returns from January\\,01,\\,2020 to June\\,10,\\,2021, together with fitted Cauchy densities}    \n\\label{fig:crypto-hist}\n\\end{figure}\n\nTables \\ref{Table5} and \\ref{Table6} report the results of the different tests for the Cauchy model for the two time series. The $p$-values are based on Monte Carlo simulation under the Cauchy model with $10^4$ replications. For the test based on the Kullback-Leibler distance, we choose the window length $m=100$ for the longer time series, and $m=50$ for the shorter one. \nThe results show that even if the Cauchy distribution fits better than many other distributional models, it is still not an acceptable fit in any of the cases. The tests based on $T_{n,5}, KL$ and $D_{n,5}$ result in $p$-values of 0.000 for all currencies for the longer time series, and $p$-values smaller than 0.01 for all currencies for the shorter one. The edf based tests have generally larger $p$-values, with the Watson test having smallest, and Cram\\'{e}r-von Mises test having largest $p$-values.\n\nOverall, the EOS data seem to be most compatible with the hypothesis of a Cauchy distribution. For the shorter time series, the $p$-values of all edf based tests are larger than 0.1, and the Kolmogorov-Smirnov and Cram\\'{e}r-von Mises tests don't reject $H_0$ even for the larger data set. These tests also show relatively large values for Ripple (XRP) and BitcoinCash (BCH). On the whole, the results confirm the findings of the simulation study concerning the comparative power of the test.\n\n\n\n\\begin{table}\n\\begin{center}\n\\begin{tabular}{rrrrrrrrrrrrr}\n  \\hline\n& $T_{n,1}$ & $T_{n,3}$ & $T_{n,5}$ & $T_{n,0}$ & $KL$ & $KS$ & $CM$ & $AD$ & $W$ & $D_{n,1}$ & $D_{n,3}$ & $D_{n,5}$ \\\\ \n \\hline\nBTC &\n0.122&      0.000&      0.000&      0.001&0.000&0.005&0.066&0.003&0.000&0.000&      0.000&      0.000 \\\\\nETH &\n0.000&      0.000&      0.000&      0.000&0.000&0.000&0.012&0.000&0.000&0.000&      0.000&      0.000 \\\\      \nXRP &\n0.357&      0.000&      0.000&      0.016&0.000&0.064&0.191&0.019&0.004&0.003&      0.000&      0.000 \\\\    \nLTC &\n0.008&      0.000&      0.000&      0.000&0.000&0.000&0.023&0.001&0.000&0.000&      0.000&      0.000 \\\\      \nBCH &\n0.036&      0.000&      0.000&      0.000&0.000&0.035&0.063&0.004&0.000&0.000&      0.000&      0.000\\\\    \nEOS &\n0.308&      0.009&      0.000&      0.559&0.000&0.119&0.277&0.044&0.024&0.006&      0.000&      0.000 \\\\  \nBNB &\n0.001&      0.000&      0.000&      0.000&0.000&0.002&0.029&0.002&0.000&0.000&      0.000&      0.000 \\\\      \nTRX &\n0.008&      0.000&      0.000&      0.001&0.000&0.015&0.042&0.002&0.000&0.000&      0.000&      0.000 \\\\    \n\\hline\n\\end{tabular}\n\\caption{\\label{Table5} $p$-values of cryptocurrency log-returns (Jan.\\,01,\\,2020-June\\,10,\\,2021) for various tests for the Cauchy distribution}\n\\end{center}\n\\end{table}\n\n\\begin{table}\n\\begin{center}\n\\begin{tabular}{rrrrrrrrrrrrr}\n  \\hline\n& $T_{n,1}$ & $T_{n,3}$ & $T_{n,5}$ & $T_{n,0}$ & $KL$ & $KS$ & $CM$ & $AD$ & $W$ & $D_{n,1}$ & $D_{n,3}$ & $D_{n,5}$ \\\\ \n \\hline\nBTC &\n0.134 &     0.000 &     0.000 &     0.019&0.000&0.062&0.151&0.041&0.005&0.002&      0.000&      0.000 \\\\\nETH &\n0.000&      0.000&      0.000&      0.000&0.000&0.004&0.022&0.006&0.000&0.000&      0.000&      0.000 \\\\      \nXRP &\n0.216&      0.003&      0.001&      0.014&0.000&0.147&0.247&0.090&0.010&0.016&      0.002&      0.002   \\\\    \nLTC &\n0.132&      0.000&      0.000&      0.015&0.000&0.073&0.163&0.043&0.002&0.001&      0.000&      0.000 \\\\      \nBCH &\n0.049&      0.001&      0.001&      0.001&0.001&0.316&0.156&0.068&0.004&0.011&      0.002&      0.001   \\\\    \nEOS &\n0.387&      0.011&      0.001&      0.339&0.001&0.222&0.458&0.153&0.108&0.021&      0.005&      0.006     \\\\  \nBNB &\n0.023&      0.000&      0.000&      0.032&0.001&0.020&0.074&0.026&0.000&0.000&      0.000&      0.000 \\\\      \nTRX &\n0.010&      0.000&      0.000&      0.000&0.000&0.070&0.074&0.027&0.000&0.001&      0.000&      0.000   \\\\    \n\\hline\n\\end{tabular}\n\\caption{\\label{Table6} $p$-values of cryptocurrency log-returns (Jan.\\,01,\\,2021-June\\,10,\\,2021), for various tests for the Cauchy distribution}\n\\end{center}\n\\end{table}\n\n\n\n\\section{Outlook}\n\nWe conclude the paper by pointing out some open problems for further research. The explicit formula of the covariance kernels in Remark \\ref{rem:est} open ground to numerical approximation of the eigenvalues of the integral operator $\\mathcal{K}$. Especially an approximation of the largest eigenvalue would offer more theoretical insights, since, as is well known, it is useful for efficiency statements in the sense of Bahadur, see \\cite{B:1960} and \\cite{N:1995}. A profound knowledge of the eigenvalues leads to explicit values of cumulants of the null distribution, such that a fit of the null distribution to the Pearson distribution system is possible. This usually gives a very accurate approximation of the quantiles of the distribution and hence of the critical values of the test, see i.e. in the context of normality testing \\cite{E:2020,H:1990}. Another desirable extension for the family of tests is a data dependent choice of the tuning parameter $a$, see \\cite{T:2019} for a procedure which might be applicable. \n\n\\bibliographystyle{abbrv}\n", "meta": {"timestamp": "2021-06-25T02:20:53", "yymm": "2106", "arxiv_id": "2106.13073", "language": "en", "url": "https://arxiv.org/abs/2106.13073"}}
{"text": "\\section{Introduction} \\label{sec:intro}\nThis open-source package is created for the analysis of turbulent velocity time series (one-dimensional). Although, it can be used for any other type of data containing a time series. \nWithin this document, a detailed procedure to use this package will be discussed. \nThe user is expected to have all the theoretical background associated with the turbulent cascade process, Fokker-Planck Equation (FPE), Integral fluctuation theorem (IFT) and its interpretation. \nSeveral research articles deal with this method for analyzing turbulent data. \nHowever, with the implementation in this open source package, for the first time the application in practice becomes easy and fast for everyone. \nFor a quick and simple overview, we recommend the following annual review article \\cite{Peinke2018}. \nWith this open source package one should be able to perform all analysis steps of this paper.\n\n\n\nThis package has been used continuously within our lab since 2018. \nIt also has been successfully used by a large number of students (practical exercises which were part of the fluid dynamics lecture at the University of Oldenburg) to ensure stability across different machines and operating systems. \nThis package has proved its value in a number of publications like:\n\\begin{itemize}\n\t\\item Reinke, Nico, et al. \"On universal features of the turbulent cascade in terms of non-equilibrium thermodynamics.\" Journal of Fluid Mechanics 848 (2018): 117-153.\n\t\\item Ali, Naseem, et al. \"Multi-scale/fractal processes in the wake of a wind turbine array boundary layer.\" Journal of Turbulence 20.2 (2019): 93-120.\n\t\\item Peinke, Joachim, MR Rahimi Tabar, and Matthias W\u00e4chter. \"The Fokker\u2013Planck approach to complex spatiotemporal disordered systems.\" Annual Review of Condensed Matter Physics 10 (2019): 107-132.\n\t\\item Fuchs, Andr\u00e9, et al. \"A Rigorous Entropy Law for the Turbulent Cascade.\" Turbulent Cascades II. Springer, Cham, 2019. 17-25.\n\t\\item Fuchs, Andr\u00e9, et al. \"Small scale structures of turbulence in terms of entropy and fluctuation theorems.\" Physical Review Fluids 5.3 (2020): 034602.\n\t\\item Fuchs, Andr\u00e9, et al. \"The entropy and fluctuation theorems of inertial particles in turbulence.\" arXiv preprint arXiv:2104.03136 (2021).\n\t\\item Kharche, Swapnil, et al. \"Energy dissipation and total entropy production in SHREK experiment.\" Progress in Turbulence VIIII. 2021. (in review process )\t\n\\end{itemize}\n\n\n\nThe package itself or parts of it, as well as results obtained by using the package, have also been presented at several international conferences, such as:\n\\begin{itemize}\n\t\t\\item Fuchs, Andr\u00e9, et al. \"A Rigorous Entropy Law for the Turbulent Cascade.\" \t\t71rd Annual Meeting of the APS Division of Fluid Dynamics. 2018.\n\t\t\\item Fuchs, Andr\u00e9, et al. \"Fine structure of turbulence determined by entropy variation.\" Workshop on different states of turbulence and transitions from one state to the other: small and large-scale aspects and their interrelations. Grenoble. 2019\n\t\t\\item Fuchs, Andr\u00e9, et al. \"Fine structure of turbulence determined by particle image velocimetry  \\& entropy variation.\" Workshop on turbulence in the context of Eulerian and Lagrangian views, with a particular focus on cascade features connected to stochastic processes, large deviation theory and instantons. Grenoble. 2019\t \n\t\t\\item Fuchs, Andr\u00e9, et al. \"An open source Matlab package for solving Fokker-Planck Equation and validation of Integral Fluctuation Theorem.\" 73rd Annual Meeting of the APS Division of Fluid Dynamics. 2020.\n\t\t\\item Kharche, Swapnil, et al. \"Energy dissipation and total entropy production in SHREK experiment.\" Interdisciplinary Turbulence initiative Conference in Turbulence. 2021\t\n\t\t\\item Fuchs, Andr\u00e9, et al. \"An open source Matlab package for solving Fokker-Planck Equation and validation of Integral Fluctuation Theorem.\" Interdisciplinary Turbulence initiative Conference in Turbulence. 2021\n\t\t\\item Fuchs, Andr\u00e9, et al. An open source Matlab package for stochastic data analysis in the context of Fokker-Planck Equation and Integral Fluctuation Theorem. No. EGU21-9608. European Geosciences Union General Assembly. 2021.\n\\end{itemize}\nNote, this list is not complete, only a selection is listed here.\n\n\n\\section{Implementation and architecture}\nThe software is implemented in \\proglang{MATLAB\\textsuperscript{\\textregistered}} (2020a). \nThe results presented in this paper were obtained using the same version. \nBefore using this script the following toolboxes should be included in your \\proglang{MATLAB} license.\n\\begin{itemize}\n\t\\itemsep0em \n\t\\item \\pkg{Curve Fitting Toolbox}\n\t\\item \\pkg{Optimization Toolbox}\n\t\\item \\pkg{Parallel Computing Toolbox}\n\t\\item \\pkg{Signal Processing Toolbox}\n\t\\item \\pkg{Statistics and Machine Learning Toolbox}  \n\\end{itemize}\n\nAs these MATLAB toolboxes are essential for the application of the package, the compatibility to Octave can not be provided.  \nBut to enhance the accessibility, standalone applications (64-bit) for \\proglang{Windows}, \\proglang{macOS} and \\proglang{Linux} are also created to run the \\proglang{MATLAB} code on target machines that do not have a \\proglang{MATLAB} license.\n\nTo apply our method, two important features must be met. \nFirst, the set of data from which one extracts the sequence of velocity increments in scale must be a \\textbf{stationary process} (performing multipoint analysis this condition is not always required). \nSecond, the process in scale must be \\textbf{Markovian}. \nThese preconditions are tested during the execution of the program.\nOverall, the analysis consists of three main steps: pre-processing, estimation of Kramers-Moyal coefficients and estimation of the total entropy variation. \nThe central part of this paper is the description of the main program, subroutines and selection of the relevant parameters.\nThe following section will give an overview of all subroutines that are accessible in this package. \nNote that all the abbreviations used in this document are listed at the end in the list of nomenclature/abbreviations. \n\n\\code{main} is the primary script in which all the different functions to carry out various analyses are included. All subroutines, which will be briefly introduced in the next chapter, are called in a logical order. These subroutines are also accessible from the command line and can be included by the user in other applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Part I: Pre-processing}\n\\subsection{Exemplary dataset and system requirements}\nTo demonstrate the application of this program an exemplary dataset obtained in a turbulent air jet experiment \\cite{renner2001} is used within this document. \nThe local velocity was sampled by a time-resolved hot-wire measurement. \nThe data acquisition comprises \\mbox{$1.25 \\times 10^7$ samples} at a sampling frequency of 8\\,kHz. \nThe package additionally includes this exemplary dataset. \nWe also provide the generated plots/results allowing the user to verify if the software is operating correctly.\nTable~\\ref{tab:overview} lists all parameters that the user must enter during the analysis to reconstruct the results shown in this paper.\n\\begin{table}[t!]\n\t\\centering\n\t\\begin{tabular}{c|c|c|c|c}\n\t\t$Fs$  & $L$ &  $\\lambda$ &  $inc\\_bin$ & $\\Delta_{EM}$\\\\ \n\t\t\\hline \n\t\t8000\\,Hz & 0.067\\,m\t& 0.0066\\,m  & 93  & 22 samples\\\\ \n\t\\end{tabular} \n\t\\caption{\\label{tab:overview} Information that the user must enter during the analysis to reconstruct the results shown in this paper. With the sampling frequency $Fs$, integral length scale $L$, Taylor length scale $\\lambda$, number of bins to be used to divide the velocity increment series $inc\\_bin$ and Einstein-Markov length $\\Delta_{EM}$.}\n\\end{table}\n\nThe system requirements (memory, disk space, processor, total run time) demanded by the script depend very much on the size of the data set to be examined, the resolution in relation to the number of bins as well as the available number of CPUs (required memory will be allocated to each worker during parallel computations).\n\n\n\n\n\n\n\n\\subsection{Loading data and variables}\n\\code{uiimport} is the first command in the program which asks the user to select interactively the data file which will be used for the analysis. \nThe name of the variable/time series to be analyzed must be $\\textbf{data}$. \nAt this point, it is possible to specify the percentage of the total data that will be used to perform the analysis (for example, the first 20 \\% of the data). \nNote that this parameter has a significant effect on the overall performance of the script.\n\n\n\\code{save\\_path} opens a dialog box to navigate through the local directories in order to select a folder for saving figures and files.\n\n\n\\code{Fs} generates a pop-up dialog box to enter the sampling frequency of the data in Hz.\n\n\n\\code{kin\\_vis } generates a pop-up dialog box to enter the value of kinematic viscosity $\\nu$ in $m^2/s$ of the fluid for which the experimental data has been acquired.\n\n\\newpage\n\\code{increment\\_bin} generates a pop-up dialog box to specify the number of bins to be used to divide the velocity increment series. \nA first estimation of the number of bins is made using\n\\begin{eqnarray}\n\tinc\\_bin=10\\frac{max(data)-min(data)}{u'}\n\\end{eqnarray}\nNote that this parameter has a significant effect on the overall performance of the script.\n\n\n\n\n\\subsection{Test of stationarity and filtering of the time series}\n\\code{plot\\_stationarity} This function plots the mean, standard deviation, skewness and kurtosis of each section of a length of 5\\% of the data and the data itself to check the stationarity condition in Figure~\\ref{fig:statio}. \nIn the title of the figure, the number of nans, which will be removed from the time series and the turbulence intensity is printed.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.8\\textwidth]{Images/1.eps}\n\n\t\\caption {(a) plot of the mean, standard deviation, skewness and kurtosis of each section of a length of 5\\% of the data (b) plot of the data itself as a function \\% of the data.}\n\t\\label{fig:statio} \n\\end{figure}\n\n\n\\code{plot\\_pdf} This function plots the probability density function (PDF) of the data with the specified number of bins (specified in the function \\code{increment\\_bin}) in Figure~\\ref{fig:pdf}. \nIt also plots the Gaussian distribution which has the same standard deviation and mean value as of the data. \nIn the title of the figure the range of the data (the difference between the maximum and minimum values of sample data), the skewness and flatness of the data are printed.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{Images/2.eps}\n\t\\caption {Probability density function (PDF) of the data. The grey dashed line corresponds to a Gaussian distribution with the same standard deviation and mean value (vertical black dashed line) as of the data.}\n\t\\label{fig:pdf} \n\\end{figure}\n\n\n\n\\code{spectrum} This function calculates the energy spectral density (ESD) of the time series using \\code{fft} function of \\proglang{MATLAB}. \nAlso, the ESD with and without averaging (moving average with equally spaced frequency interval in log-space) as a function of frequency will be plotted in Figure~\\ref{fig:spec}.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{Images/3.eps}\n\t\\caption {Energy spectral density (ESD) in the frequency domain. The red solid line corresponds to the averaged ESD as a function of frequency.}\n\t\\label{fig:spec} \n\\end{figure}\n\n\n\n\n\\code{low\\_freq} generates a pop-up dialog box to select whether the data should be filtered (low-pass filter) or not. \nIf a filter is to be applied then in the next step the frequency in Hz at which the data will be filtered by a low-pass filter have to be specified (for example 1800 Hz). \nIf the pop-up dialog box is denied, it is set to the value \\code{low\\_freq=Fs/2}.\n\n\n\\code{frequency\\_filter} This function returns the filtered data and the filtered energy spectral density in the frequency domain (see Figure~\\ref{fig:spec_filter}) using the low-pass filter at the previously set frequency using \\code{butter} and \\code{filtfilt} function of \\proglang{MATLAB}. \n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{Images/4a.eps}\n\t\\caption {Energy spectral density (ESD) in the frequency domain. The yellow solid line corresponds to the averaged and filtered energy spectral density in the frequency domain using the low-pass filter.}\n\t\\label{fig:spec_filter} \n\\end{figure}\n\nThe filtered data named as $\\textbf{data\\_filter}$ will be used for all the further data post-processing. \nIf the filtering was negated in the previous step, $\\textbf{data\\_filter}$ and $\\textbf{data}$ are equal.\nIn addition, different representations/normalization of the energy respectively dissipation spectrum density with respect to frequency $f$, scale $r$, wave number $k$ will also be plotted in Figure~\\ref{fig:spec__diss_filter}. \nThe energy spectral density is normalized (Parseval's theorem) so that\n\\begin{eqnarray}\n\tu'^2&=&\\int_{0}^{\\infty} E(k) dk\\\\\n\tk &=& \\frac{2\\pi f }{\\left<u\\right>}\\\\\n\tE(k) &=& \\frac{E(f) \\left<u\\right>}{2\\pi}.\n\\end{eqnarray}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.8\\textwidth]{Images/4b.eps}%\n\t\\caption {Different representation/normalization of the energy spectral density with respect to frequency $f$ and wave number $k$ and dissipation spectral density with respect to scale $r$ and wave number $k$.}\n\t\\label{fig:spec__diss_filter} \n\\end{figure}\n\n\n\n\n\n\\subsection{Estimation of fundamental turbulence length scales}\nEach scale can be represented either in terms of the number of samples or in meters. \nAlthough for physical interpretation, the units of scales can be taken as meters but for the post-processing of data, the number of samples is an easier choice. \nFrom data series of the longitudinal velocities $u(t)$ (the component in direction of the mean flow) we construct the data sets of their longitudinal increments \n\\begin{eqnarray}\n\tu_{\\tau}(t)=u(t)-u(t+\\tau)\n\\end{eqnarray}\nlabeled by the time-separation $\\tau$ which, assuming the Taylor hypothesis of frozen turbulence \\cite{taylor1938spectrum} (necessary condition: turbulence intensity is less than 20\\,\\%), the temporal information corresponds to spatial velocity increments, $u_r(t)=-u_{\\tau}(t)$ with $r=-\\tau\\langle u\\rangle$. \nFor example consider a hot wire signal with a sampling frequency of 50\\,kHz which is characterized by a mean velocity of 10\\,m/sec and an integral length scale of 0.1\\,m. \nAccording to the basic equation of velocity = distance/time, the integral length scale of 0.1\\,m is equal to the turnover time of 0.01\\,sec or 500 samples.\n\n\n\\code{length\\_scales} In this function, the integral length scale $L$, Taylor length scale $\\lambda$ and Kolmogorov length scale $\\eta$ are estimated using different methods of calculation. \nWithin this function, pop-up dialog boxes will be generated to enter the values of the integral, Taylor, Kolmogorov length scale in $m$ and energy dissipation rate in $m^2/s^3$ based on which the further processing of data (solving the FPE and extracting cascade trajectories) should be referred. \nThe entered length scales will be rounded towards the nearest integer sample. \nThe proposed value in the pop-up dialog box is the median length scale for all methods.\\\\\n\nThe \\textbf{integral length scale} $L$ is estimated by using:\n\\begin{enumerate}\n\t\\item the \\textbf{energy spectrum density} which requires the range of frequency that will be used to linearly extrapolate the value of ESD at a frequency of 0\\,Hz \\cite{Hinze1975,roach1987generation} (see Figure~\\ref{fig:int_spec}). Therefore the user will be asked to enter the \\textbf{f\\_start} and \\textbf{f\\_end} in Hz (\\textbf{f\\_start}$<$\\textbf{f\\_end}). \n\t(For example: f\\_start = 0.03 Hz and f\\_end = 2.7Hz.)\n\n\t\\begin{eqnarray}\n\t\tL=\\lim\\limits_{f \\rightarrow 0}\\left[\\frac{E(f)\\left<u\\right>}{4u'^2}\\right]\n\t\\end{eqnarray}\n\t\\begin{figure}[t!]\n\t\t\\centering\n\t\t\\includegraphics[width=0.43\\textwidth]{Images/5.eps}\n\t\t\\caption {Representation of the linear extrapolation of ESD at a frequency of 0\\,Hz for estimating the integral length scale with the method according to \\cite{Hinze1975,roach1987generation}. The two vertical dashed lines correspond to the range of frequency that will be used to linearly extrapolate (solid black line).}\n\t\t\\label{fig:int_spec} \n\t\\end{figure}\n\t\\item from the \\textbf{autocorrelation coefficient} $R_{\\widetilde{u}\\widetilde{u}}$ with respect to scales \\cite{Frisch_1995,pope2001turbulent,Bourgoin_2017} plotted in Figure~\\ref{fig:autocorr_expfit}\n\t\\begin{eqnarray}\n\t\tL=\\int_{0}^{\\infty}R_{\\widetilde{u}\\widetilde{u}}(r)dr,\n\t\t\\label{eq.auto_int}\n\t\\end{eqnarray}\n\twhere $r$ is the scale in meter. \n\tThe cumulative integral gives the asymptotic value at a specific scale $r$ which is characterized by the integral length scale $L$. \n\tThis method, however, leads to large errors if the correlation does not decay exponentially. \n\tIn the case of a non-monotonic decrease of $R_{\\widetilde{u}\\widetilde{u}}$, the autocorrelation function is  \n\t\\item integrated up to the first zero-crossing of the autocorrelation function \\cite{o2004autocorrelation} \n\t\\item integrated up to the first 1/e crossing of the autocorrelation function \\cite{tritton2012physical}\n\n\t\\item extrapolated by an exponential function \\cite{Hinze1975,tritton2012physical} (fit region: r=$\\left[0 : r_e\\right]$, see Figure~\\ref{fig:autocorr_expfit}), with $R_{\\widetilde{u}\\widetilde{u}}(r_e)=1/e$. \n\tThe integral can be solved analytically\n\t\\begin{eqnarray}\n\t\tL &=& \\int_{0}^{\\infty} a e^{-br}dr= \\frac{a}{b}.\n\t\\end{eqnarray}\n\t\\begin{figure}[t!]\n\t\t\\centering\n\t\t\\includegraphics[width=0.4\\textwidth]{Images/6.eps}\n\t\t\\caption{Representation of the autocorrelation coefficient $R_{\\widetilde{u}\\widetilde{u}}$ as a function of scale in meter. The two vertical dashed lines correspond to the range of scale that will be used for the extrapolation (red solid line) by an exponential function for estimating the integral length scale with the method according to \\cite{Hinze1975,tritton2012physical}.}\n\t\t\\label{fig:autocorr_expfit} \n\t\\end{figure}\n\t\\item calculated from the second order structure function $S^{2}(r)$ which holds the relationship with autocorrelation function such as\n\t\\begin{eqnarray}\n\t\tR_{\\widetilde{u}\\widetilde{u}}(r) = 1-\\left[\\frac{S^{2}(r)}{2u'^2}\\right],\n\t\\end{eqnarray}\n\twhere $u'$ can be calculated directly from the second order structure function. \n\tAt a sufficiently large length scale compared to the energy injection scale of the experiment the second order structure function truncates to the asymptotic value of $2u'^{2}$ \\cite{Mordant_2001_Ruu_with_S2}.\n\t\n\t\n\t\n\t\\item calculated using the \\code{xcorr} inbuilt function of \\proglang{MATLAB} using the $\\lq$unbiased' normalization option followed by the estimation of cumulative integral.\n\n\tThe peaks in the cumulative integration of $R_{\\widetilde{u}\\widetilde{u}}$ with respect to $r$ show the fact that autocorrelation function is fluctuating around a constant value. \n\tAfter this peak, the signal is considered to be uncorrelated and hence the scale at which this peak occurs can be taken as a measure of the integral length scale.\n\t\\item The integral length scale can be estimated via the zero crossings proposed by \\cite{mora2020estimating}.\n\n\tTo verify that zero crossing are well resolved the signal has to be filtered with a low-pass filter with a characteristic length $\\eta_c$. \n\n\n\n\n\tThis method consists of estimating the Voronoi tessellation of the 1D zero-crossings positions data set. \n\tIt is compared with a Random Poisson Process, which has no correlations between the scales. \n\tThe method proposes that the integral length scale is equal to the characteristic length $\\eta_c$ for which\n\t\\begin{eqnarray}\n\t\t\\frac{\\sigma_{voro}}{\\sigma_{RPP}}=1\n\t\\end{eqnarray}\n\t$\\sigma_{voro}$ is the standard deviation of the Voronoi cells normalized by their mean value and $\\sigma_{RPP}$ is the equivalent value for an Random Poisson Process, that is equal to $\\sqrt{(1/2)}$.\n\tIn Figure~\\ref{fig:zero_crossing_int_L} the standard deviation (normalized) of the Voronoi cells as a function of the characteristic length $\\eta_c$ is plotted.\n\t\t\\begin{figure}[t!]\n\t\t\\centering\n\t\t\\includegraphics[width=0.4\\textwidth]{Images/zero_int_L.eps}\n\t\t\\caption {Standard deviation (normalized) of the Voronoi cells as a function of the characteristic length $\\eta_c$ of a low-pass filter for estimating the integral length scale with the method according to \\cite{mora2020estimating}.}\n\t\t\\label{fig:zero_crossing_int_L} \n\t\n\t\\end{figure}\n\tFinally, the integral length scale is defined as the value of $\\eta_c$ that correspond to $\\sigma_{voro}/\\sigma_{RPP}=1$. \n\tIf we observe that $\\sigma_{voro}/\\sigma_{RPP}>1$ for all values of  $\\eta_c$, the method cannot provide the value of the integral length scale and longer signals are needed (nevertheless the extrapolation of the value remains possible).\n\\end{enumerate}\n\n\n\nThe \\textbf{Taylor length scale} $\\lambda$ is estimated by using:\n\\begin{enumerate}\n\t\\item A parabolic fit $R_{\\widetilde{u}\\widetilde{u}}(r)=1-\\frac{r^2}{\\lambda^2}$ to the autocorrelation function is used to estimate $\\lambda$ at the origin $r=0$ (see Figure~\\ref{fig:autocorr_parabolic}). \n\tThe range of the positive curvature is therefore used for estimation (close to $r=0$ the auto-correlation function has an inflection point). \n\tSince this method needs a well-resolved auto-correlation function it strongly depends on the resolution of the sensor.\n\t\\begin{figure}[t!]\n\t\t\\centering\n\t\t\\includegraphics[width=0.41\\textwidth]{Images/7.eps}\n\t\t\\caption {Autocorrelation function with parabolic fit as a function of the scale $r$ for estimating the Taylor length scale. The vertical dashed line correspond to the range of scale that will be used for the extrapolation (red solid line) by a parabolic fit.}\n\t\t\\label{fig:autocorr_parabolic} \n\t\\end{figure}\n\\end{enumerate}\n\nAssuming isotropy at small scales and Taylor's frozen turbulence hypothesis the \\textbf{Taylor length scale} $\\lambda$ is estimated using the relation \n\\begin{eqnarray}\n\t\\lambda^2 = \\frac{u'^2}{\\left<\\left(\\partial{\\widetilde{u}}/\\partial x\\right)^2\\right>}.\n\\end{eqnarray}\n\\newpage\nThe numerical differentiation in the denominator is approximated by:\n\\begin{enumerate}\n\t\\addtocounter{enumi}{1}\n\t\\item the simple difference quotient. \n\tDue to the finite spatial resolution of the measuring sensor, this method will yield an incorrect result. \n\tIn order to correctly compute the velocity derivatives, the spatial resolution must be of the order of the Kolmogorov microscale (see \\cite{hussein1990influence}).\n\t\\item the procedure proposed by \\cite{aronson1993plane}.\n\t\\begin{eqnarray}\n\t\t\\lambda = \\lim\\limits_{r \\rightarrow 0} \\sqrt{\\frac{u'^2 r^2}{S^{2}(r)}}.\n\t\t\\label{eq:Aronson_L\u00f6fdahl} \n\t\\end{eqnarray}\n\tIn Figure~\\ref{fig:Aronson_L\u00f6fdahl} the development of Eq. \\ref{eq:Aronson_L\u00f6fdahl} as a function scale $r$ is plotted.\n\tFor the extrapolation we use a linear fit. The fit-region is used for spatial lags that are larger than 4 times the scale that corresponds to the low-pass filter frequency \\code{low\\_freq}. The larger limit of fit-region must be set by the user. \n\n\t\\begin{figure}[t!]\n\t\t\\centering\n\t\t\\includegraphics[width=0.43\\textwidth]{Images/8.eps}\n\t\t\\caption {Development of Eq. \\ref{eq:Aronson_L\u00f6fdahl} with linear extrapolation (red solid line) for estimating the Taylor length scale with the method according to \\cite{aronson1993plane}. The two vertical dashed lines correspond to the range of scale that will be used for the extrapolation.}\n\t\t\\label{fig:Aronson_L\u00f6fdahl} \n\t\\end{figure}\n\t\\item using the dissipation spectrum. \n\tThis procedure has been proposed by \\cite{Hinze1975}. \n\tThe upper limit of the integration is set to the cutoff frequency or the full (5. method) dissipation spectrum.  \n\t\\begin{eqnarray}\n\t\t\\left<\\left(\\frac{\\partial{\\widetilde{u}}}{\\partial x}\\right)^2\\right> &=& \\int_{0}^{\\infty} k^2 E(k) dk.\n\t\n\t\\end{eqnarray}\n\n\n\n\t\\addtocounter{enumi}{1}\n\t\\item \\& 7. method: $\\lambda$ can be estimated via the zero crossings of the fluctuating velocity,\n\t\\begin{eqnarray}\n\t\t\\label{eq:zero_crossing_lambda} \n\t\t\\lambda=\\frac{l}{C \\pi},\n\t\\end{eqnarray}\n\twith $l$ the average distance between zero-crossings. \n\t$C$ is a constant in the order of unity that quantifies the level of non-Gaussianity of the derivative $\\partial{\\widetilde{u}}/\\partial x$. \n\tThis method was introduced by \\cite{mazellier2010turbulence,Sreenivasan_1983,Mora_2019}. \n\tIn Figure~\\ref{fig:zero_crossing_lambda}(a) the density of zero-crossings times the average distance between zero crossings as a function of the characteristic length $\\eta_c$ is plotted.\n\t\\begin{figure}[t!]\n\t\\centering\n\t\t\\includegraphics[width=0.4\\textwidth]{Images/zero_lambda.eps}\n\t\t\\caption {Density of zero-crossings times the average distance between zero crossings as a function of the characteristic length $\\eta_c$ of a low-pass filter for estimating the Taylor length scale with the method according to \\cite{mazellier2010turbulence,Sreenivasan_1983,Mora_2019}.}\n\t\n\t\t\\label{fig:zero_crossing_lambda} \n\t\\end{figure}\n\t\n\tFor values of $\\eta_c$ within the inertial range, a power law 2/3 is expected \\cite{mazellier2010turbulence}, and eventually for smaller filter sizes (or large $1/\\eta_c$) a plateau is reached. \n\tThe presence of this plateau, related to the dissipative range, implies that the density of zero-crossings $n_s$ are well resolved, and therefore $l$ can be deduced using the trivial relation $n_s \\cdot l =1$. \n\tIf the plateau is not reached, small scales are not resolved and the method can not estimate the Taylor length scale. \n\tOn the other hand, if after the plateau the value of $n_s \\cdot l$ increases again, it means that the cut-off frequency is too high and the analysis is affected by small scale noise.\n\tInitially, the constant $C=1$ (6. method) gives a good approximation of $\\lambda$ using Eq. \\ref{eq:zero_crossing_lambda}. \n\tA better estimation can be obtained if $\\partial{\\widetilde{u}}/\\partial x$ is resolved. \n\tIn that case, $C$ is defined as\n\n\t\\begin{eqnarray}\n\t\n\t\n\t\tC &=& \\sqrt{\\frac{2}{\\pi}} \\frac{\\sigma_{\\partial_x\\widetilde{u}}}{ \\left<|\\partial_x\\widetilde{u}| \\right>}\n\t\n\t\n\t\n\t\\end{eqnarray}\n\twith $\\left<|\\partial_x\\widetilde{u}| \\right>$ is the mean and $\\sigma_{\\partial_x\\widetilde{u}}$ is the standard deviation of $\\partial{\\widetilde{u}}/\\partial x$, where $\\widetilde{u}$ is filtered with the largest frequency within the plateau of $n_s \\cdot l$.\n\n\\end{enumerate}\n\n\nFurthermore, this function estimates and returns the \\textbf{mean energy dissipation rate $\\langle \\epsilon \\rangle$}, which is estimated by its one-dimensional surrogate using \n\\begin{enumerate}\n\t\\item \\& 2. method: either $2^{nd}$ or $3^{rd}$ order structure function. \n\tThe estimation of dissipation using this method relies on the transfer of energy within the inertial range. \n\tThis method is particularly useful when the higher frequency content present in the flow is not fully resolved by the measurement device. \n\tThis is generally the case where for example the length of the hot wire is larger than the Kolmogorov length scale $\\eta$ of the flow under consideration. \n\tThis function generates a pop-up dialog box to enter the value of Kolmogorov constant $C_{2}$ (typically within 2.0 - 2.2) used in the relation between second order structure function $S^{2}(r)$ and $\\epsilon(r)$ based on the assumption of homogeneous isotropic turbulence (HIT) \\cite{pope2001turbulent,taylor1938spectrum}. \n\tThe mean energy dissipation rate $\\langle \\epsilon \\rangle$ is calculated by finding the mean amongst 5 points closest to the peak value of $\\epsilon(r)$. \n\tIn Figure~\\ref{fig10} the development of $\\epsilon(r)$ using either $2^{nd}$ or $3^{rd}$ order structure function is plotted.\n\t\\begin{eqnarray}\n\t\t\\label{eq.epsi_2}\n\t\t\\epsilon(r) &=& \\frac{1}{r}\\left[\\frac{S^{2}(r)}{C_{2}}\\right]^{3/2}\\\\\n\t\t\\label{eq.epsi_3}\n\t\t\\epsilon(r) &=& -\\frac{5}{4}\\left[\\frac{S^{3}(r)}{r}\\right]\n\t\\end{eqnarray}\n\t\\begin{figure}[t!]\n\t\t\\centering\n\t\t\\includegraphics[width=0.4\\textwidth]{Images/10.eps}\n\t\t\\caption {Development of $\\epsilon(r)$ using either $2^{nd}$ or $3^{rd}$ order structure function (see Eq. \\ref{eq.epsi_2} and \\ref{eq.epsi_3}). The black solid line marks the peak value of $\\epsilon(r)$.}\n\t\t\\label{fig10} \n\t\\end{figure}\n\t\\addtocounter{enumi}{1}\n\t\\item by using the chosen Taylor length scale and the following relation\n\t\\begin{eqnarray}\n\t\t\\label{eq.HIT_Taylor}\n\t\t\\langle \\epsilon \\rangle = 15 \\nu \\frac{u'^2}{\\lambda^2}.\n\t\\end{eqnarray}\n\t\\item \\& 5. method: As a consequence of the K41 phenomenology, the second order structure function implies an energy spectrum of the form\n\t\\begin{eqnarray}\n\t\tE(k)=C_k\\langle \\epsilon \\rangle^{2/3}k^{-5/3}.\n\t\\end{eqnarray}\n\t$C_k$ is the so-called Kolmogorov constant that remains undetermined in Kolmogorov's theory (typically $C_k\\approx0.53\\pm0.01$ \\cite{Sreenivasan_1995,Oboukhov_1962}). \n\tFollowing Kolmogorov's $k^{-5/3}$ prediction the two following fits in the inertial range ($\\lambda<r<L$) are used to estimate the mean energy dissipation rate $\\langle \\epsilon \\rangle$ (see Figure~\\ref{fig:spec_fit})\n\t\\begin{eqnarray}\n\t\t\\label{eq.Kolfit_a}\n\t\tE(k)&=&C_k\\langle \\epsilon \\rangle^{2/3}(k-k_0)^{-5/3}\\\\\n\t\t\\label{eq.Kolfit_b}\n\t\tE(k)&=&C_k\\langle \\epsilon \\rangle^{2/3}k^{-5/3}.\n\t\\end{eqnarray}\n\t\\addtocounter{enumi}{1}\n\t\\item via the dissipation spectrum \n\t\\begin{eqnarray}\n\t\t\\label{eq.epsi_diss}\n\t\t\\langle \\epsilon \\rangle =  \\int_{0}^{\\infty} 15 \\nu k^2 E(k) dk.\n\t\\end{eqnarray}\n\tAs proposed in \\cite{Mora_2019} the dissipation spectrum is modeled for large wave number $k$ using a second order polynomial (see Figure~\\ref{fig:model_dissipation}). \n\t\\begin{figure}[t!]\n\t\t\\centering\n\t\t\\includegraphics[width=0.4\\textwidth]{Images/11.eps}\n\t\t\\caption {Energy spectral density (ESD) in the frequency domain. The red and yellow solid line corresponds to the fit according to equation \\ref{eq.Kolfit_a} and \\ref{eq.Kolfit_b} for estimating the mean energy dissipation rate $\\langle \\epsilon \\rangle$.}\n\t\t\\label{fig:spec_fit} \n\t\\end{figure}\n\t\\begin{figure}[t!]\n\t\t\\centering\n\t\t\\includegraphics[width=0.4\\textwidth]{Images/11_b.eps}\n\t\t\\caption {Dissipation spectral density in the wave number domain. The black dashed line corresponds to the fit (second order polynomial) to model the dissipation spectrum for large wave number $k$.}\n\t\t\\label{fig:model_dissipation} \n\t\\end{figure}\n\n\n\n\n\n\n\\end{enumerate}\n\n\n\\newpage\nThe \\textbf{Kolmogorov length scale} $\\eta$ (the smallest size of the eddy in a given turbulent flow) is estimated by using the classical relation given by \\cite{Frisch_1995} \n\\begin{eqnarray}\n\t\\eta = \\left(\\frac{\\nu^{3}}{\\langle \\epsilon \\rangle }\\right)^{1/4}.\n\\end{eqnarray} \n\n\nIn addition, the normalized energy dissipation rate will be returned \\cite{batchelor1948decay,Tennekes_1972}\n\\begin{eqnarray}\n\tC_{\\epsilon_1} &=& \\frac{\t\\langle \\epsilon \\rangle  L}{u'^3}\\\\\n\tC_{\\epsilon_2} &=& 15 \\frac{L}{\\lambda} \\frac{1}{Re_{\\lambda}},\n\\end{eqnarray}\nwith the local \\textbf{Taylor-Reynolds number}\n\\begin{eqnarray}\n\tRe_{\\lambda} = \\frac{u'\\lambda}{\\nu}.\n\n\\end{eqnarray}\n$Re_{\\lambda}$ allows for reasonable comparisons of experiments with different boundary conditions or energy injection mechanisms ($Re_{\\lambda}$ is independent of the integral length scale $L$).\\\\\n\n\nAt the end of this function, in Figure~\\ref{fig:spec_length_scale} a vertical dashed line at the integral $L$, Taylor $\\lambda$ and Kolmogorov length scale $\\eta$ will be added to different representations of the spectrum.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.47\\textwidth]{Images/9.eps}\n\n\t\\caption {ESD and compensated ESD in the frequency domain. Black dashed vertical lines correspond to the respective frequency of the fundamental length scales.}\n\t\\label{fig:spec_length_scale} \n\\end{figure}\n\n\n\n\n\n\n\n\\subsection{Normalization of the data}\n\\code{struc\\_flip\\_test} This function tests whether the data have to be flipped or not. \nThe decision of flipping of data depends on a simple relation of $3^{rd}$ order structure function $S^{3}(r)$ with the dissipation based on the assumption of homogeneous isotropic turbulence (HIT). \nThe thumb rule is that the quantity $S^{3}(r)$ must be negative. \nIn the literature, the keyword that goes with this picture is vortex stretching. \nTo verify this,\n$S^{3}(r)$ as a function of the scale $r$ is plotted, from which it is possible to decide whether it is essential to flip the data or not.\n\n\n\n\n\n\n\\code{normalization} This function is mainly to perform the normalization of the data. \nBefore doing so, it generates the pop-up dialog box which asks the user whether to flip the data or not (based on the previous investigation). \nAfter that, this function normalizes the entire data with the quantity of \\mbox{$\\sigma_{\\infty} = \\sqrt{2}\\sigma$}, where $\\sigma$ is the standard deviation of the \\textbf{data\\_filter} (this method is proposed by \\cite{renner2001}). \nThis function returns the filtered and normalized data as \\textbf{data\\_filter}, \\mbox{\\textbf{siginf} = $\\sigma_{\\infty}$} and \\mbox{\\textbf{m\\_data} = mean of the data} before normalization. \nIn addition the scale $r$ is given in units of Taylor length scale $\\lambda$. \nWe use this normalization to compare the results of different data sets. \n\\begin{eqnarray}\n\tu_r&=&\\frac{u_{r_i}}{\\sigma_\\infty}\\\\\n\tr&=&\\frac{r_i}{\\lambda}\n\\end{eqnarray}\nThis normalization affects also the Kramers-Moyal coefficients (KMCs), which will be introduced in Chapter: Estimation of Kramers-Moyal coefficients (KMCs).\nThe indices $i$ describes the initial parameter of the Kramers-Moyal coefficients without normalization:\n\\begin{eqnarray}\n\tD^{(1)} &=& \\frac{D^{(1)}_i \\lambda}{\\sigma_{\\infty}}\\\\\n\tD^{(2)} &=& \\frac{D^{(2)}_i \\lambda}{\\sigma^2_{\\infty}}\\\\\n\td_{11} &=& d_{11_i}\\lambda\\\\\n\td_{20} &=& \\frac{d_{20_i}\\lambda}{\\sigma^2_{\\infty}}\\\\\n\td_{21} &=& \\frac{d_{21_i}\\lambda}{\\sigma_{\\infty}}\\\\\n\td_{22} &=& d_{22_i}\\lambda.\n\\end{eqnarray}\n\n\n\\code{plot\\_increment\\_pdf} This function plots in Figure~\\ref{incr_pdf} the probability density function of the velocity increments at the scale $r=L$, $r=\\lambda$ and  $r=\\eta$.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{Images/incr_pdf_b_2.eps}\n\t\\caption {PDF of the velocity increments $u_r$ at the scale $r={L,\\lambda,\\eta}$. The colored dashed line correspond to Castaing fits (form factor $\\lambda^2$ \\cite{Castaing_1990}) and grey dashed line to Gaussian fits.}\n\t\\label{incr_pdf}\n\\end{figure}\n\n\\code{plot\\_struc\\_function} This function plots in Figure~\\ref{fig:struc_flip} the $k$-th order structure function \n\\begin{eqnarray}\n\tS^{k}(r)&=&\\left\\langle u_{r}^k\\right\\rangle\\\\\n\tT^{k}(r)&=&\\left\\langle |u_{r}|^k\\right\\rangle\n\\end{eqnarray}\nwith $k={2-7}$ for scales $\\lambda \\leq r \\leq L$. In addition this function plots in Figure~\\ref{fig:struc_ESS} the scaling exponent $\\zeta_k$  \n\\begin{eqnarray}\n\tS^{k}(r) \\propto r^{\\zeta_k}\n\\end{eqnarray}\nestimated using the extended-self similarity (ESS) method. The scaling of a selected set of known intermittency models is also included for comparison. For this, the user is asked to specify the intermittency coefficient $\\mu$ (experiments suggest a value of $\\mu\\approx0.227$ \\cite{Frisch_1995} and $\\mu\\approx0.26$ \\cite{Arneodo_1996}) and the coefficient $D$ of $\\beta$-Model \\cite{Anselmet_1984}.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.8\\textwidth]{Images/struc.eps}\n\t\\caption {Course of the $k$-th order structure function $S^{k}$ and $T^{k}$ with $k={2-7}$ as a function of scale. Dashed red line represents -4/5 law.}\n\t\\label{fig:struc_flip} \n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{Images/struc_ESS.eps}\n\t\\caption {Course of scaling exponent $\\zeta_k$ as a function of order $k$ of structure function. Dashed lines represent the scaling of a selected set of known intermittency models.}\n\t\\label{fig:struc_ESS} \n\\end{figure}\n\n\n\\newpage\n\\section{Part II: Estimation of Kramers-Moyal coefficients}\n\\subsection{Examination of the Markov Property/Determination of the Markov-Einstein Length}\n\\code{wilcoxon\\_test} This function determines the Einstein-Markov length $\\Delta_{EM}$ \\cite{renner2001}. \nAbove this length scale, the Markov properties hold and below this length scale, the Markov properties cease to hold. \nThe Wilcoxon test (see Figure~\\ref{wilcoxon}) is a parameter-free procedure to compare two empirically determined probability distributions (two data sets of velocity increments) (see \\cite{Lueck2006markov} for details). \nIt is a quantitative test that determines the $\\Delta_{EM}$. \nA sufficient resolution in measurement below Taylor's length scale is expected to perform this test. \n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.8\\textwidth]{Images/wilcoxon.eps}\n\t\\caption {Development of  $\\left<t(r,\\Delta r)\\right>/\\sqrt{2/\\pi}$ (see \\cite{Lueck2006markov} for details) as as function $\\Delta r$ in terms of samples (a) on log-log scale and (b) lin-log scale. The vertical dashed line corresponds to the Taylor length scale $\\lambda$ in terms of samples.}\n\t\\label{wilcoxon} \n\\end{figure}\n\n\\code{markov} generates a pop-up dialog box to enter the number of samples which corresponds to Einstein-Markov length.\nExperimental evidence shows that the Markov property can be assumed to hold for the cascade process coarse-grained by the Einstein-Markov length \\cite{Lueck2006markov,renner2001} (proposed value in the pop-up dialog box)\n\\begin{eqnarray}\n\t\\Delta_{EM} \\approx 0.9 \\lambda.\n\\end{eqnarray}\nNote, if the resolution of the used sensor ceases at $\\Delta_{EM}$, it is possible to enter the number of samples which correspond to a larger scale than $\\Delta_{EM}$ at which the data might be resolved in scale (for example samples corresponding to $\\lambda$ or $5\\lambda$). \nIn addition, a red vertical dashed line at the Einstein-Markov length will be added to the spectrum in the frequency domain.\n\n\n\n\\code{min\\_events} generates a pop-up dialog box to enter the minimum number of events/counts to occur in a single bin, which will make that specific bin valid for further processing. \nIf the minimum number of events is equal to 400 all the information in those specific bins in which the number of events/counts is less than 400 will be excluded for further post-processing of data. \nThe provision of the minimum number of events/counts is for avoiding the appearance of noise and hence for better fitting of parameters. \nBased on the experience, we have fixed the minimum value of the \\textbf{min\\_events} to 400. \nBased on the length of the data and the statistics, it is possible to increase/decrease this number.\n\n\n\n\\code{conditional\\_PDF\\_markov} This function performs a qualitative/visual check for the validation of Markov property based on the alignment or misalignment of the single conditioned and double conditioned PDFs of velocity increments for three different scales $r_0>r_1>r_2$ \neach of which is separated by $\\Delta_{EM}$. \nTo do this, a pop-up dialog box is generated to enter the conditioned value for large scale increment $u_{r_{0}}$, for example $u_{r_{0}}=\\pm 1$. Note, the condition $u_{r_{0}}=0$ corresponds to the maximum number of statistics.\nThis function also plots various representations of the single and double conditioned PDFs (shown in Figure~\\ref{fig:con_pdf_a} are only two). \nIf there is not a good agreement between the single conditioned and double conditioned PDF of velocity increments, it is possible to modify the Einstein-Markov length and/or the minimum number of events and repeat this qualitative/visual check for the validation of Markov property.\n\n\\textit{Note, the results presented hereafter are related to a modified number of bins (changed from 93 to 201).\n\n\tThe number of bins has been adjusted to get a more detailed view of the following figures (conditional PDFs and the Kramers-Moyal coefficients). These detailed illustrations will be described in this readme file in an exemplary manner. A smaller number of bins leads to slightly different results, but the general trend remains the same. Furthermore, the results presented hereafter relating to the Fokker-Planck analysis using the multiscaling approach (not multipoint analysis).}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{Images/con_pdf_a.eps} \\hspace{1cm}\n\t\\includegraphics[width=0.33\\textwidth]{Images/con_pdf_d.eps}\\\\\n\t\\includegraphics[width=0.4\\textwidth]{Images/con_pdf_b.eps}\n\t\\includegraphics[width=0.4\\textwidth]{Images/con_pdf_c.eps}\n\t\\caption {Visualization of Markov properties. Top: Contour plots showing single (black solid lines) and double conditioned PDFs (red solid lines, $u_{r_{0}}=0$) of velocity increments \n\t\n\t\tfor three different scales $r_0>r_1>r_2$ each of which is separated by $\\Delta_{EM}$. The right figure is a three-dimensional view. The dashed black lines in the top figure correspond to cut through the single and double conditioned PDFs at the marked points of $u_{r_2}$. Bottom: Cut through the single (black) and double (red) conditioned PDFs at the marked points of $u_{r_2}$.}\n\t\\label{fig:con_pdf_a} \n\\end{figure}\n\n\n\n\\subsection{Estimation of conditional moments}\n\\code{scale\\_steps} This pop-up dialog box calculates the possible number of steps between the integral length scale and Taylor length scale which are separated by the Markov length. \nFor these steps, the Kramers-Moyal coefficients (KMCs) will be estimated.\n\n\n\\code{multi\\_point} In this pop-up dialog box, it must be selected whether to perform the multipoint analysis or not (see annual review article \\cite{Peinke2018}). \nTo do the Fokker-Planck analysis using the multiscaling approach, the question in this pop-up dialog box must be denied. \nIf multipoint analysis should be performed, an additional condition on the increment must be specified in the next pop-up dialog box.\n\n\n\\code{conditional\\_moment} This function estimates the $k$-th order conditional moment\n\\begin{equation}\n\tM^{(k)}\\left(u_r,r,\\Delta r\\right) =\\int_{-\\infty}^{\\infty} \\left(u_{r'}-u_r\\right)^k p\\left(u_{r'} | u_r \\right) du_{r'},\n\\end{equation}\n$k={1-4}$ for all scales $2\\Delta_{EM} <r \\leq L$ and for each bin (specified in the function \\code{scale\\_steps} and \\code{increment\\_bin}) for all values of longitudinal velocity increments $u_r$. \nFor a fixed scale $r$ the conditional moments are calculated for 5 different scales separations (colored circles in Figure \\ref{fig:con_mom}) $\\Delta r=r-r'$ within the range of $\\Delta_{EM}\\leq \\Delta r\\leq 2\\Delta_{EM}$. The condition $r'<r$ is fulfilled.\n\n\n\n\\code{plot\\_conditional\\_moment} This function plots in Figure~\\ref{fig:con_mom} the first and second conditional moments $M^{(1,2)}\\left(u_r,r,\\Delta r\\right)$ as a function of the scale separation $\\Delta r$. \nFor this purpose, a scale $r$ and the number of a bin (value of the velocity increment $u_r$) condition must be specified. \nThe proposed value in the pop-up dialog box is $r=L$ and $u_r \\approx 0$. \nA possible deviation from a linear law for small values of $\\Delta r$ is due to the Einstein-Markov length, as the Markov properties cease to hold for very small scale separations.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{Images/con_mom_new.eps}\n\t\\caption {First and second conditional moments $M^{(1,2)}\\left(u_r,r,\\Delta r\\right)$ as a function of the scale separation $\\Delta r$. In addition, a linear extrapolation in $\\Delta r$ (solid black line) of the first and second order conditional moments is plotted (see Chapter: Estimation of Kramers-Moyal coefficients). The vertical dashed lines and the colored circles limit the range used for the linear fit ($\\Delta_{EM}\\leq \\Delta r\\leq 2\\Delta_{EM}$).}\n\t\\label{fig:con_mom} \n\\end{figure}\n\n\n\n\n\n\n\\subsection{Estimation of Kramers-Moyal coefficients (KMCs)}\n\\label{KMC}\n\\code{KM\\_Calculation} This function calculates the Kramers-Moyal coefficients $D^{(k)}\\left(u_r,r\\right)$ with $k={1-4}$ for all scales (specified in \\code{scale\\_steps}) and for each bin (specified in the function \\code{increment\\_bin}) for all values of velocity increments by a linear extrapolation in $\\Delta r$ of the $k$-th order conditional moments $M^{k}\\left(u_r,r\\right)$ (see Figure~\\ref{fig:con_mom}) and the function \\code{KM\\_plot\\_raw} plots them accordingly. With $r'<r$:\n\\begin{equation}\n\tD^{(k)}\\left(u_r,r\\right) = \\lim\\limits_{r'\\rightarrow r}\\frac{M^{(k)}\\left(u_r,r,\\Delta r\\right)}{k!\\, \\left(r'-r\\right)}.\n\t\\label{eq.KMcoeff}\n\\end{equation}\n\nThis limit approximation leads to uncertainties in the absolute values of the Kramers\u2013Moyal coefficients, whereas the functional forms of $D^{(k)}\\left(u_r,r\\right)$ are commonly well estimated. \nIn order to overcome this problem, the optimization algorithm described below is performed.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{Images/KM_raw_1.eps}\n\t\\includegraphics[width=0.4\\textwidth]{Images/KM_raw_2.eps}\n\n\t\\caption {Non-optimized Kramers-Moyal coefficients $D^{(1,2)}\\left(u_r,r\\right)$ with respect to scale $r$ (with $L\\approx 10 \\, r/\\lambda$) and velocity increment obtained by the linear extrapolation method.}\n\t\\label{fig:raw_KM} \n\\end{figure}\n\n\n\n\n\n\n\\subsection{Pointwise optimization of Kramers-Moyal coefficients: conditional PDF}\n\\label{KMC_opti}\n\\code{KM\\_STP\\_optimization} This function performs the pointwise optimization of Kramers-Moyal coefficients $D^{(1,2)}\\left(u_r,r\\right)$ at each scale and value of velocity increment to minimize possible uncertainties in the absolute values of the Kramers\u2013Moyal coefficients. \nThe purpose of this optimization is to find the best Fokker-Planck equation to reproduce the conditional PDFs as these are the essential part of the Markov process. \nThis optimization procedure is proposed in \\cite{kleinhans2005iterative,Nawroth2007,Reinke_2018} and it includes the reconstruction of the conditional probability density functions $p\\left(u_{r'} | u_r \\right)$ via the short time propagator \\cite{Risken} with $r'<r$\t\n\\begin{eqnarray}\n\t\\label{eq.P_STP}\n\tp_{stp}\\left(u_{r'} | u_r \\right) = & &\\frac{1}{\\sqrt{4\\pi D^{(2)}(u_{r},r) \\Delta r}}\\\\\n\t& & exp\\left( -\\frac{\\left( u_{r'}-u_r-D^{(1)}(u_{r},r)\\Delta r \\right)^2}{4 D^{(2)}(u_{r},r) \\Delta r}\\right). \\nonumber\n\\end{eqnarray}\nThe scale step size $\\Delta r=\\Delta_{EM}$ leads to consistent results. Smaller steps than $\\Delta_{EM}$ do not significantly improve the results. \nThe aim of this optimization is to minimize a weighted mean square error function in logarithmic space \\cite{feller1968} (analogous to Kullback\u2013Leibler entropy)\n\\begin{equation}\n\t\\xi =\\frac{\\sum_{-\\infty}^{\\infty}\\sum_{-\\infty}^{\\infty}\\left(p_{exp} + p_{stp}\\right)\\left(\n\t\tln\\left(p_{exp}\t\\right) - ln\\left(p_{stp}\\right)\\right)^2}\n\t{\\sum_{-\\infty}^{\\infty}\\sum_{-\\infty}^{\\infty}\\left(p_{exp} + p_{stp}\\right)\\left(\n\t\tln^2\\left(p_{exp}\t\\right) + ln^2\\left(p_{stp}\\right)\\right)}.\n\\end{equation}\nThis error function is a logarithmic measure of the difference between the experimental $p_{exp}$ and reconstructed $p_{stp}$ conditional probability density function. \nThe optimization procedure systematically changes $D^{(1,2)}\\left(u_r,r\\right)$ until the error function is minimized. \nThis optimization use the function \\code{fmincon} implemented in MATLAB. \nThe constraints were set in a physically and mathematically meaningful way: $d_{11}\\leq 0$, $d_{20}\\geq 0$ and $d_{22}\\geq 0$ (see for more details \\code{FIT\\_KM}).\n\nIn addition, this function generates a pop-up dialog box whether an example optimization is required. \nIf the pop-up dialog box is denied, then this function straightaway performs the optimization for all scales and all velocity increments without plotting anything. \nIf this is confirmed, then the conditional PDFs will be plotted using different representations (shown here are only two see Figure~\\ref{fig:optimi_a}) to see the differences between optimized and non-optimized and experimental conditional PDFs. \nNote, if the variable \\code{scale\\_steps} is equal to 9 then it is possible to enter any scale number from 1 up to scale number 9 (smallest respectively the largest scale).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.39\\textwidth]{Images/opti_KM_c.eps}\\hspace{0.3cm}\n\t\\includegraphics[width=0.37\\textwidth]{Images/opti_KM_d.eps}\\\\\n\t\\includegraphics[width=0.4\\textwidth]{Images/opti_KM_a.eps}\n\t\\includegraphics[width=0.4\\textwidth]{Images/opti_KM_b.eps}\n\t\\caption {Top: Contour plots showing experimental $p_{exp}$, non-optimized $p_{stp}$ and optimized conditonal PDFs $p_{stp,opti}$ using the short time propagator of velocity increments for a pair of two scales with $r'<r$ each of which is separated by $\\Delta_{EM}$. The right figure is a three-dimensional view (only $p_{exp}$ and $p_{stp,opti}$ is shown). Bottom: Non-optimized and optimized Kramers-Moyal coefficients $D^{(1,2)}\\left(u_r,r\\right)$ with respect to velocity increment $u_r$ for a fixed scale $r=3 \\lambda = 3.3 \\Delta_{EM}$ obtained by the optimization algorithm described above.}\n\t\\label{fig:optimi_a} \n\\end{figure}\n\n\n\n\\code{FIT\\_KM} This function performs the surface fit with a linear function for $D^{(1)}\\left(u_r,r\\right)$ and a parabolic function for $D^{(2)}\\left(u_r,r\\right)$ to the optimized and non-optimized KMCs interpreted in the It\u00f4 convention~\\cite{GardinerBook}\n\\begin{eqnarray} \n\t\\label{eq:D1andD2_1}\n\tD^{(1)}(u_{r},r) &=& d_{11}(r)u_{r},\\\\\n\tD^{(2)}(u_{r},r) &=& d_{22}(r)u_{r}^2+d_{21}(r)u_{r}+d_{20}(r).\n\t\\label{eq:D1andD2_2}\n\\end{eqnarray}\nCoefficients $d_{ij}(r)$ in the fits are functions of scale~$r$ of the form $\\alpha (r/\\lambda)^{\\beta}+\\gamma$. \nAfter fitting, this function plots the parameters $d_{11}$, $d_{20}$, $d_{21}$ and $d_{22}$ as a function of $\\frac{r}{\\lambda}$ for optimized and non-optimized $D^{(1,2)}\\left(u_r,r\\right)$. \n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.8\\textwidth]{Images/Fit_KM_3.eps}\n\t\\caption {Coefficients $d_{ij}(r)$ of the optimized Kramers-Moyal coefficients using the surface fits with a linear function for $D^{(1)}\\left(u_r,r\\right)$ and a parabolic function for $D^{(2)}\\left(u_r,r\\right)$ (see Eq. \\ref{eq:D1andD2_1} - \\ref{eq:D1andD2_2}) with respect to scale.}\n\t\\label{Fit_KM} \n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{Images/Fit_KM_1.eps}\n\t\\includegraphics[width=0.4\\textwidth]{Images/Fit_KM_2.eps}\n\t\\caption {Optimized Kramers-Moyal coefficients $D^{(1,2)}\\left(u_r,r\\right)$ and the surface fits with a linear function for $D^{(1)}\\left(u_r,r\\right)$ and a parabolic function for $D^{(2)}\\left(u_r,r\\right)$ (see Eq. \\ref{eq:D1andD2_1} - \\ref{eq:D1andD2_2}) with respect to scale $r$ and velocity increment $u_r$.}\n\t\\label{Fit_KM_a} \n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\\section{Part III: Estimation of total entropy variation}\nIn the remaining part of the script, the calculation leading towards the integral fluctuation theorem will be done. In the spirit of non-equilibrium stochastic thermodynamics \\cite{Seifert_2012} it is possible to associate with every individual cascade trajectory $\\left[u(\\cdot) \\right]$ a total entropy variation $\\Delta S_{tot}$. In this investigation it is assumed that a single cascade trajectory represents one realization of the turbulent cascade process and a large number of these trajectories reflect the statistics caused by the process.\n\n\\newpage\n\\subsection{Validation of Integral Fluctuation Theorem}\n\\code{trajec} Based on velocity increments $u_r$ the cascade trajectories $\\left[u(\\cdot) \\right]=\\{u_L,\\dots,u_{\\lambda}\\}$ for different scales from the integral length $L$ to the Taylor length $\\lambda$ can be extracted from the data series of velocities $v(t)$. \nA pop-up dialog box is generated to select if the start and end of the cascade trajectory should be adjusted. \nIf the pop-up dialog box is denied, then $\\left[u(\\cdot) \\right]$ start at the integral length $L$ and end  at the Taylor length $\\lambda$. \nIf this is confirmed, at the beginning of function \\code{checkFT} a pop-up dialog box is generated to specify whether the start and/or the end of the cascade trajectory should be adjusted in multiples of integral respectively Taylor length scale.\n\n\\newpage\nIn addition a pop-up dialog box is generated to select whether the total entropy variation should be calculated for overlapping (\\textbf{z=1}) or independent cascade trajectories (\\textbf{z=3}).\n\n\\code{dr\\_ind}\n A pop-up dialog box is generated to define the separation of scales/step increment (in samples) referred to the sequence from large to small scales in the cascade trajectory. The proposed value in the pop-up dialog box is equal \nto the Einstein-Markov length $\\Delta_{EM}$.\n\n\\code{data\\_length} A pop-up dialog box is generated to select the percentage of the data length that should be used to perform the calculation of the total entropy variation (for example, the first 20 \\% of the data).\n\n\n\\code{checkFT} The set of measured cascade trajectories results in a set of total entropy variation values $\\Delta S_{tot}$ (the same number of entropy values as the number of trajectories) given by \\cite{Seifert_2005,Seifert_2012,Sekimoto_2010,Nickelsen_2013,Reinke_2018}. \nThis function calculates the system entropy\n\\begin{eqnarray}\n\t\\Delta S_{sys}\\left[u(\\cdot) \\right] =  - \\ln{\\left( \\frac{p(u_\\lambda, \\lambda)}{p(u_L, L)} \\right)},\n\\end{eqnarray}\nmedium entropy \n\\begin{eqnarray}\n\t\\Delta S_{med}\\left[u(\\cdot) \\right] &=&  - \\int_{L}^{\\lambda} \\partial_r u_r \\partial_{u_r}\\varphi(u_r) dr\\\\\n\n\t&=& + \\int_{L}^{\\lambda} \\partial_r u_r \\frac{D^{(1)}(u_{r},r)-\\partial_{u_r}D^{(2)}(u_{r},r)/2}{D^{(2)}(u_{r},r)}dr\\nonumber\n\n\\end{eqnarray}\nand the total entropy variation \n\\begin{eqnarray}\n\t\\Delta S_{tot}\\left[u(\\cdot)\\right] = \\Delta S_{sys} + \\Delta S_{med}\n\\end{eqnarray}\nfor all the independent cascade trajectories. \nThe numerical differentiation is approximated by the central difference quotient:\n\\begin{eqnarray}\n\\partial_r u_r = \\lim\\limits_{r' \\rightarrow r} \\frac{u_{r'} - u_{r}}{r'-r}\n\\end{eqnarray}\nThis numerical differentiation is performed for every individual extracted cascade trajectory $\\left[u(\\cdot) \\right]$ in a sequence from large to small scales. \nThe integration in scale is approximated by using rectangles and a mid-point rule discretization of the scale intervals, therefore the integral takes the average of beginning and end of the discretization interval.\nThe probabilities of starting and ending of the cascade trajectories, $u_L$ and $u_{\\lambda}$, can be estimated from the given data. The results depend slightly on the discretization rules and convention. However, the overall statements do not depend on it.\n\n\n\n\\code{plot\\_entropy} This function plots in Figure~\\ref{IFT_a} the empirical average $\\langle e^{\\mathrm{-}\\Delta S_{tot}} \\rangle_N$ of $\\Delta S_{tot}$ as a function of the number, $N$ (sample size), of cascade trajectories $\\left[u(\\cdot) \\right]$ with error bars. In addition, the probability density function of the system, medium and total entropy will be plotted while displaying the value of $\\langle\\Delta S_{tot}\\rangle$ which should be larger than 0. The  integral fluctuation theorem (IFT) expresses the integral balance between the entropy-consuming ($\\Delta S_{tot}<0$) and the entropy-producing ($\\Delta S_{tot}>0$) cascade trajectories and states \n\\begin{equation}\n\t\\langle e^{-\\Delta S_{tot}} \\rangle_{\\left[u(\\cdot) \\right]} = \\int e^{-\\Delta S_{tot}}p\\left(\\Delta S_{tot}\\right) d\\Delta S_{tot} = 1.\n\t\\label{eq:IFT}\n\\end{equation} \n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{Images/IFT_1.eps}\n\t\\includegraphics[width=0.4\\textwidth]{Images/IFT_2.eps}\n\t\\caption {Empirical average $\\langle e^{\\mathrm{-}\\Delta S_{tot}} \\rangle_N$ of $\\Delta S_{tot}$ as a function of the number $N$ (sample size) of cascade trajectories $\\left[u(\\cdot) \\right]$ with error bars. According to the integral fluctuation theorem (IFT), the empirical average has to converge to the horizontal dashed line. Probability density function of the system $S_{sys}$, medium $S_{med}$ and total entropy variation $S_{tot}$.}\n\t\\label{IFT_a} \n\\end{figure}\n\n\n\n\n\n\n\n\n\\subsection{Pointwise optimization of Kramers-Moyal coefficients: IFT}\nIn the remaining part of the script, the pointwise optimization of Kramers-Moyal coefficients towards the integral fluctuation theorem will be done. \nThereby the separation of scales/step increment (in samples) referred to the sequence from large to small scales in the cascade trajectory is set to a minimum of 1 sample and for the optimization, independent cascade trajectories (z=3) are used. \nNote, we use here a separation that is less than or equal to the Einstein-Markov length of $\\Delta_{EM}$.\n\n\\code{iter} this pop-up dialog box is generated to enter the maximum number of iteration which will be performed for the optimization.\n\n\\code{tol\\_D1}, \\code{tol\\_D2} this pop-up dialog box is generated to specify the constraints/tolerance in percent of the coefficients $d_{ij}(r)$ which will be used to perform the optimization.\n\n\\code{OPTI\\_IFT\\_dij} This function performs the optimization of $D^{(1,2)}\\left(u_r,r\\right)$ at each scale and at each value of velocity increment in order to satisfy the integral fluctuation theorem with minimum possible error and plots the optimized $d_{ij}$ as a function of scale (see Figure~\\ref{IFT_opti__KM_a}). \nThe optimization procedure systematically changes $D^{(1,2)}\\left(u_r,r\\right)$ until the error function \n\\begin{eqnarray}\n\t\\xi =|1-\\langle e^{\\mathrm{-}\\Delta S_{tot}} \\rangle_{max(N)}|\n\\end{eqnarray}\nis minimized. \nWithin the optimization process, the user is asked which $d_{ij}(r)$ should be optimized.\nThis optimization use the function \\code{fmincon} implemented in \\proglang{MATLAB}. \n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.8\\textwidth]{Images/IFT_opti__KM_1.eps}\n\t\\caption {Coefficients $d_{ij}(r)$ of the optimized Kramers-Moyal coefficients using the surface fits with a linear function for $D^{(1)}\\left(u_r,r\\right)$ and a parabolic function for $D^{(2)}\\left(u_r,r\\right)$ (see Eq. \\ref{eq:D1andD2_1} - \\ref{eq:D1andD2_2}) with respect to scale.}\n\t\\label{IFT_opti__KM_a} \n\\end{figure}\n\n\n\n\n\nUsing the function \\code{checkFT} and \\code{plot\\_entropy} with \\code{dr\\_ind=1} and overlapping cascade trajectories (\\code{z=1}) and the optimized Kramers-Moyal coefficients the results presented in Figure~\\ref{IFT_opti_IFT} are obtained for the calculation of $\\Delta S_{tot}$.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.4\\textwidth]{Images/IFT_opti_1.eps}\n\t\\includegraphics[width=0.4\\textwidth]{Images/IFT_opti_2.eps}\n\t\\caption {Empirical average $\\langle e^{\\mathrm{-}\\Delta S_{tot}} \\rangle_N$ of $\\Delta S_{tot}$ as a function of the number $N$ (sample size) of cascade trajectories $\\left[u(\\cdot) \\right]$ with error bars. According to the integral fluctuation theorem (IFT), the empirical average has to converge to the horizontal dashed line. Probability density function of the system $S_{sys}$, medium $S_{med}$ and total entropy variation $S_{tot}$.}\n\t\\label{IFT_opti_IFT} \n\\end{figure}\n\n\\newpage\nAs it can be seen in Figure \\ref{IFT_opti__KM_a}, this optimization is a fine-tuning of the coefficients, but its impact on the IFT is clearly evident. \nIn this comparison, it must be taken into account that the separation of scales from large to small scales in the cascade trajectory is different and overlapping cascade trajectories (\\code{z=1}) are investigated here.\n\n\n\n\n\n\n\n\n\n\n\\section{Summary and discussion} \\label{sec:summary}\n\n\\subsection{Code Repository}\nThis open source MATLAB package is available as free software, under the GNU General Public License (GPL) version 3, and can be downloaded with all the supplementary material (data, source code and standalone applications (64-bit) for \\proglang{Windows}, \\proglang{macOS} and \\proglang{Linux}) to replicate all the results presented in this paper from the repository on GitHub or Matlab File Exchange Server.\\\\\n\n{\\bf Name:} OPEN\\_FPE\\_IFT\\\\\n{\\bf Persistent identifier:} GitHub\\\\ \\url{https://github.com/andre-fuchs-uni-oldenburg/OPEN\\_FPE\\_IFT}\\\\\n{\\bf Persistent identifier:} Matlab File Exchange Server\\\\\n\\url{https://www.mathworks.com/matlabcentral/fileexchange/80551-open_fpe_ift}\\\\\n{\\bf Publisher:} Andr{\\'e} Fuchs\\\\\n{\\bf Version published:} 4.0\\\\\n{\\bf Date published:} 15/05/21\\\\\n{\\bf Operating system:} \\proglang{Windows}, \\proglang{macOS} and \\proglang{Linux}\\\\\n{\\bf Programming language:} \\proglang{MATLAB}\\\\\n\n\n\n\\subsection{Reuse potential}\nThe development of this user-friendly package greatly enhances practicability and availability of this new method, which allows a comprehensive statistical description in terms of the complexity of turbulent velocity time series (one-dimensional). \nIt can also be used by researchers outside of the field of turbulence for the analysis of data with turbulent like complexity. \n\nSupport is available at  \\url{https://github.com/andre-fuchs-uni-oldenburg/OPEN\\_FPE\\_IFT}, where questions can be posted and generally receive quick responses from the authors.\n\n\n\n\n\n\\section*{Acknowledgments}\nThe software resulted from funded research. We acknowledge financial support by VolkswagenStiftung.\nWe acknowledge the following people for helpful discussions and testing pre-version of the package  \\mbox{A. Abdulrazek}, J. Ehrich, A. Engel, J. Friedrich, A. Girard, G. G\\\"ulker, P. G. Lind, D. Nickelsen, N. Reinke, \\mbox{T. Wester}, M. Obligado. \n\n\n\n\\newpage\n", "meta": {"timestamp": "2021-06-25T02:19:49", "yymm": "2106", "arxiv_id": "2106.13042", "language": "en", "url": "https://arxiv.org/abs/2106.13042"}}
{"text": "\\section{Introduction}\n\nTime series forecasting has been widely used in energy consumption, traffic and economics planning, weather and disease propagation forecasting. In these real-world applications, one pressing demand is to extend the forecast time into the far future.\nThus, in this paper, we study the \\textit{long-term forecasting} problem of time series, characterizing itself by the large length of predicted series, which can be severalfold of the length of the input series.\nRecent deep forecasting models \\cite{haoyietal-informer-2021,kitaev2020reformer,2019Enhancing,NIPS2017_3f5ee243,Flunkert2017DeepARPF,2018Modeling} have achieved great progress, especially the Transformer-based models. Benefiting from the self-attention mechanism, Transformers obtain great advantage in modeling long-term dependencies for sequential data, which enables more powerful big models \\cite{NEURIPS2020_1457c0d6,Devlin2019BERTPO}. \n\nHowever, the forecasting task is extremely challenging under the long-term setting. First, it is unreliable to discover the temporal dependencies directly from the long-term time series because the dependencies can be obscured by entangled temporal patterns. Second, canonical Transformers with self-attention mechanisms are computationally prohibitive for long-term forecasting because of the quadratic complexity of sequence length. Previous Transformer-based forecasting models \\cite{haoyietal-informer-2021,kitaev2020reformer,2019Enhancing} mainly focus on improving self-attention to a \\emph{sparse} version. While performance is significantly improved, these models still utilize the point-wise representation aggregation. Thus, in the process of efficiency improvement, they will sacrifice the information utilization because of the sparse point-wise connections, resulting in a bottleneck for long-term forecasting of time series.\n\nTo reason about the intricate temporal patterns, we try to take the idea of decomposition, which is a standard method in time series analysis \\cite{Anderson1976TimeSeries2E,Cleveland1990STLAS}. It can be used to process the complex time series and extract more predictable components. However, under the forecasting context, it can only be used as the \\emph{pre-processing} of past series because the future is unknown \\cite{Hyndman2013ForecastingPA}. This common usage limits the capabilities of decomposition and overlooks the potential future interactions among decomposed components. Thus, we attempt to go beyond pre-processing usage of decomposition and propose a generic architecture to empower the deep forecasting models with immanent capacity of progressive decomposition.\nFurther, decomposition can ravel out the entangled temporal patterns and highlight the inherent properties of time series \\cite{Hyndman2013ForecastingPA}. Benefiting from this, we try to take advantage of the series periodicity to renovate the point-wise connection in self-attention. We observe that the sub-series at the same phase position among periods often present similar temporal processes. Thus, we try to construct a series-level connection based on the process similarity derived by series periodicity.\n\n\nBased on the above motivations, we go beyond Transformer and propose a novel \\textbf{Autoformer} for long-term time series forecasting.\nAutoformer still follows residual and encoder-decoder structure but renovates Transformer into a decomposition forecasting architecture. By embedding our proposed decomposition blocks as the inner operators, Autoformer can progressively separate the long-term trend information from predicted hidden variables. This design allows our model to alternately decompose and refine the intermediate results during the forecasting procedure. \nInspired by the stochastic process theory \\cite{Chatfield1981TheAO,Papoulis1965ProbabilityRV}, Autoformer introduces an \\textbf{Auto-Correlation} mechanism in place of self-attention, which discovers the sub-series similarity based on the series periodicity and aggregates similar sub-series from underlying periods. This series-wise mechanism achieves $\\mathcal{O}(L\\log L)$ complexity for length-$L$ series and breaks the information utilization bottleneck by expanding the point-wise representation aggregation to sub-series level. \nAutoformer achieves the state-of-the-art accuracy on six benchmarks.\nThe contributions are summarized as follows:\n\\begin{itemize}\n  \\item To tackle the intricate temporal patterns of the long-term future, we present \\textit{Autoformer} as a decomposition architecture and design the inner decomposition block to empower the deep forecasting model with immanent progressive decomposition capacity.\n  \\item We propose an \\textit{Auto-Correlation} mechanism with dependencies discovery and information aggregation at the series level. Our mechanism is beyond previous self-attention family and can simultaneously benefit the computation efficiency and information utilization.\n  \\item Autoformer achieves a 38\\% relative improvement under the long-term setting on six benchmarks, covering five real-world applications: energy, traffic, economics, weather and disease.\n\\end{itemize}\n\n\\vspace{-5pt}\n\\section{Related Work}\n\\vspace{-5pt}\n\\subsection{Models for Time Series Forecasting}\nDue to the immense importance of time series forecasting, various models have been well developed.\nMany time series forecasting methods start from the classic tools \\cite{Sorjamaa2007MethodologyFL}. ARIMA \\cite{Box1968SomeRA,1976Time} tackles the forecasting problem by transforming the non-stationary process to stationary through differencing. The filtering method is also introduced for series forecasting \\cite{Kurle2020DeepRP,Bzenac2020NormalizingKF}.\nBesides, recurrent neural networks (RNNs) models are used to model the temporal dependencies for time series \\cite{Wen2017AMQ,Rangapuram2018DeepSS,2017Long,Maddix2018DeepFW}. DeepAR \\cite{Flunkert2017DeepARPF} combines autoregressive methods and RNNs to model the probabilistic distribution of future series. \nLSTNet \\cite{2018Modeling} introduces convolutional neural networks (CNNs) with recurrent-skip connections to capture the short-term and long-term temporal patterns. Attention-based RNNs \\cite{2017A,Shih2019TemporalPA,Song2018AttendAD} introduce the temporal attention to explore the long-range dependencies for prediction.\nAlso, many works based on temporal convolution networks (TCN) \\cite{Oord2016WaveNetAG,2017Conditional,Bai2018AnEE,Sen2019ThinkGA,2019Think} attempt to model the temporal causality with the causal convolution. \nThese deep forecasting models mainly focus on the temporal relation modeling by recurrent connections, temporal attention or causal convolution. \n\nRecently, Transformers \\cite{NIPS2017_3f5ee243,Wu2020AdversarialST} based on the self-attention mechanism shows great power in sequential data, such as natural language processing \\cite{Devlin2019BERTPO,NEURIPS2020_1457c0d6}, audio processing \\cite{huang2018music} and even computer vision \\cite{dosovitskiy2021an,liu2021Swin}. However, applying self-attention to long-term time series forecasting is computationally prohibitive because of the quadratic complexity of sequence length $L$ in both memory and time. LogTrans \\cite{2019Enhancing} introduces the local convolution to Transformer and proposes the LogSparse attention to select time steps following the exponentially increasing intervals, which reduces the complexity to $\\mathcal{O}(L(\\log L)^2)$. Reformer \\cite{kitaev2020reformer} presents the local-sensitive hashing (LSH) attention and reduces the complexity to $\\mathcal{O}(L\\log L)$. Informer \\cite{haoyietal-informer-2021} extends Transformer with KL-divergence based ProbSparse attention and also achieves $\\mathcal{O}(L\\log L)$ complexity. Note that these methods are based on the vanilla Transformer and try to improve the self-attention mechanism to a \\emph{sparse} version, which still follows the point-wise dependency and aggregation. In this paper, our proposed Auto-Correlation mechanism is based on the inherent periodicity of time series and can provide series-wise connections.\n\n\\subsection{Decomposition of Time Series}\nAs a standard method in time series analysis, time series decomposition \\cite{Anderson1976TimeSeries2E,Cleveland1990STLAS} deconstructs a time series into several components, each representing one of the underlying categories of patterns that are more predictable. It is primarily useful for exploring historical changes over time. For the forecasting tasks, decomposition is always used as the \\emph{pre-processing} of historical series before predicting future series \\cite{Hyndman2013ForecastingPA}, such as Prophet \\cite{Taylor2017ForecastingAS} and others \\cite{Asadi2020ASD}. However, such pre-processing is limited by the plain decomposition effect of historical series and overlooks the hierarchical interaction between the underlying patterns of series in the long-term future.\nThis paper takes the decomposition idea from a new progressive dimension. Our Autoformer harnesses the decomposition as an inner block of deep models, which progressively decomposes the hidden series throughout the whole forecasting process. \n\n\\section{Autoformer}\nThe time series forecasting problem is to predict the most probable length-$O$ series in the future given the past length-$I$ series, denoting as \\textit{input-$I$-predict-$O$}. The \\textit{long-term forecasting} setting is to predict the long-term future given the short-term history, i.e. $O \\gg I$.\nAs aforementioned, we have highlighted the difficulties of long-term series forecasting: handling intricate temporal patterns and breaking the bottleneck of computation efficiency and information utilization. To tackle these two challenges, we introduce the decomposition as a builtin block to the deep forecasting model and propose \\textit{Autoformer} as a decomposition architecture. Besides, we design the \\textit{Auto-Correlation} mechanism to discover the period-based dependencies and aggregate similar sub-series from underlying periods.\n\n\\subsection{Decomposition Architecture}\nWe renovate Transformer \\cite{NIPS2017_3f5ee243} to a deep decomposition architecture (Figure \\ref{fig:framework}), including the inner series decomposition block, Auto-Correlation mechanism, and corresponding Encoder and Decoder.\n\n\n\\paragraph{Series decomposition block}\nTo learn with the complex temporal patterns in long-term forecasting context, we take the idea of decomposition \\cite{Anderson1976TimeSeries2E,Cleveland1990STLAS}, which can separate the series into trend-cyclical and seasonal parts. These two parts reflect the long-term progression and the seasonality of the series respectively. However, directly decomposing is unrealizable for future series because the future is just unknown. To tackle this dilemma, we present a \\textit{series decomposition block} as an inner operation of Autoformer (Figure \\ref{fig:framework}), which can extract the long-term stationary trend from predicted intermediate hidden variables progressively. Concretely, we adapt the moving average to smooth out periodic fluctuations and highlight the long-term trends. For length-$L$ input series $\\mathcal{X}\\in\\mathbb{R}^{L\\times d}$, the process is:\n\\begin{equation}\\label{equ:moving_avg}\n  \\begin{split}\n  \\mathcal{X}_{\\mathrm{t}} & = \\mathrm{AvgPool}(\\mathrm{Padding}(\\mathcal{X})) \\\\\n  \\mathcal{X}_{\\mathrm{s}} & = \\mathcal{X} - \\mathcal{X}_{\\mathrm{t}}, \\\\\n  \\end{split}\n\\end{equation}\nwhere $\\mathcal{X}_{\\mathrm{s}},\\mathcal{X}_{\\mathrm{t}}\\in\\mathbb{R}^{L\\times d}$ denote the seasonal and the extracted trend-cyclical part respectively. We use $\\mathcal{X}_{\\mathrm{s}},\\mathcal{X}_{\\mathrm{t}}=\\mathrm{SeriesDecomp}(\\mathcal{X})$ to summarize above equations, which is the inner block of Autoformer. \n\n\\paragraph{Model inputs} The inputs of encoder part are the past $I$ time steps $\\mathcal{X}_{\\mathrm{en}}\\in\\mathbb{R}^{I\\times d}$.\nAs a decomposition architecture (Figure \\ref{fig:framework}), the input of Autoformer decoder contains both the seasonal part $\\mathcal{X}_{\\mathrm{des}} \\in\\mathbb{R}^{(\\frac{I}{2}+O)\\times d}$ and trend-cyclical part $\\mathcal{X}_{\\mathrm{det}} \\in\\mathbb{R}^{(\\frac{I}{2}+O)\\times d}$ to be refined. Each initialization consists of two parts: the component decomposed from the latter half of encoder's input $\\mathcal{X}_{\\mathrm{en}}$ with length $\\frac{I}{2}$ to provide recent information, placeholders with length $O$ filled by scalars. It's formulized as follows:\n\\begin{equation}\\label{equ:model_inputs}\n  \\begin{split}\n  \\mathcal{X}_{\\mathrm{ens}}, \\mathcal{X}_{\\mathrm{ent}} &= \\mathrm{SeriesDecomp}({\\mathcal{X}_{\\mathrm{en}}}_{\\frac{I}{2}:I}) \\\\\n  \\mathcal{X}_{\\mathrm{des}} & = \\mathrm{Concat}(\\mathcal{X}_{\\mathrm{ens}}, \\mathcal{X}_{0}) \\\\\n  \\mathcal{X}_{\\mathrm{det}} & = \\mathrm{Concat}(\\mathcal{X}_{\\mathrm{ent}}, \\mathcal{X}_{\\mathrm{Mean}}), \\\\\n  \\end{split}\n\\end{equation}\nwhere $\\mathcal{X}_{\\mathrm{ens}},\\mathcal{X}_{\\mathrm{ent}}\\in\\mathbb{R}^{\\frac{I}{2}\\times d}$ denote the seasonal and trend-cyclical parts of $\\mathcal{X}_{\\mathrm{en}}$ respectively, and $\\mathcal{X}_{0},\\mathcal{X}_{\\mathrm{Mean}}\\in\\mathbb{R}^{O\\times d}$ denote the placeholders filled with zero and mean of $\\mathcal{X}_{\\mathrm{en}}$ respectively.\n\n\\begin{figure*}\n\\begin{center}\n\t\\centerline{\\includegraphics[width=\\columnwidth]{framework_1_1.pdf}}\n\t\\caption{Autoformer architecture. The encoder eliminates the long-term trend-cyclical part by series decomposition blocks (\\textcolor[rgb]{0.15,0.15,0.7}{blue} blocks) and focuses on seasonal patterns modeling. The decoder accumulates the trend part extracted from hidden variables progressively. The past seasonal information from encoder is utilized by the encoder-decoder Auto-Correlation (center \\textcolor[rgb]{0.15,0.7,0.15}{green} block in decoder). }\n\t\\label{fig:framework}\n\t\\vspace{-20pt}\n\\end{center}\n\\end{figure*}\n\n\\paragraph{Encoder} \nAs shown in Figure \\ref{fig:framework}, the encoder focuses on the seasonal part modeling. The output of the encoder contains the past seasonal information and will be used as the cross information to help the decoder refine prediction results. \nSuppose we have $N$ encoder layers. The overall equations for $l$-th encoder layer are summarized as $\\mathcal{X}_{\\mathrm{en}}^{l}=\\mathrm{Encoder}(\\mathcal{X}_{\\mathrm{en}}^{l-1})$. Details are shown as follows:\n\\begin{equation}\\label{equ:overall_encoder}\n  \\begin{split}\n  \\mathcal{S}_{\\mathrm{en}}^{l,1},\\underline{~~} & = \\mathrm{SeriesDecomp}\\Big(\\mathrm{AutoCorrelation}(\\mathcal{X}_{\\mathrm{en}}^{l-1})+\\mathcal{X}_{\\mathrm{en}}^{l-1}\\Big) \\\\\n  \\mathcal{S}_{\\mathrm{en}}^{l,2},\\underline{~~} & = \\mathrm{SeriesDecomp}\\Big(\\mathrm{FeedForward}(\\mathcal{S}_{\\mathrm{en}}^{l,1})+\\mathcal{S}_{\\mathrm{en}}^{l,1}\\Big), \\\\\n  \\end{split}\n\\end{equation}\nwhere ``$\\underline{~~}$'' is the eliminated trend part. $\\mathcal{X}_{\\mathrm{en}}^{l}=\\mathcal{S}_{\\mathrm{en}}^{l,2},l\\in\\{1,\\cdots,N\\}$ denotes the output of $l$-th encoder layer and $\\mathcal{X}_{\\mathrm{en}}^{0}$ is the embedded $\\mathcal{X}_{\\mathrm{en}}$. $\\mathcal{S}_{\\mathrm{en}}^{l,i},i\\in\\{1,2\\}$ represents the seasonal component after the $i$-th series decomposition block in the $l$-th layer respectively. We will give detailed description of $\\mathrm{AutoCorrelation}(\\cdot)$ in the next section, which can seamlessly replace the self-attention.\n\n\\paragraph{Decoder}\nThe decoder contains two parts: the accumulation structure for trend-cyclical components and the stacked Auto-Correlation mechanism for seasonal components (Figure \\ref{fig:framework}). Each decoder layer contains the inner Auto-Correlation and encoder-decoder Auto-Correlation, which can refine the prediction and utilize the past seasonal information respectively. Note that the model extracts the potential trend from the intermediate hidden variables during the decoder, allowing Autoformer to progressively refine the trend prediction and eliminate interference information for period-based dependencies discovery in Auto-Correlation. \nSuppose there are $M$ decoder layers. With the latent variable $\\mathcal{X}_{\\mathrm{en}}^{N}$ from the encoder, the equations of $l$-th decoder layer can be summarized as $\\mathcal{X}_{\\mathrm{de}}^{l}=\\mathrm{Decoder}(\\mathcal{X}_{\\mathrm{de}}^{l-1},\\mathcal{X}_{\\mathrm{en}}^{N})$. The decoder can be formalized as follows:\n\\begin{equation}\\label{equ:overall_decoder}\n  \\begin{split}\n  \\mathcal{S}_{\\mathrm{de}}^{l,1},\\mathcal{T}_{\\mathrm{de}}^{l,1} & = \\mathrm{SeriesDecomp}\\Big(\\mathrm{AutoCorrelation}(\\mathcal{X}_{\\mathrm{de}}^{l-1})+\\mathcal{X}_{\\mathrm{de}}^{l-1}\\Big) \\\\\n  \\mathcal{S}_{\\mathrm{de}}^{l,2},\\mathcal{T}_{\\mathrm{de}}^{l,2} & = \\mathrm{SeriesDecomp}\\Big(\\mathrm{AutoCorrelation}(\\mathcal{S}_{\\mathrm{de}}^{l,1}, \\mathcal{X}_{\\mathrm{en}}^{N})+\\mathcal{S}_{\\mathrm{de}}^{l,1}\\Big) \\\\\n  \\mathcal{S}_{\\mathrm{de}}^{l,3},\\mathcal{T}_{\\mathrm{de}}^{l,3} & = \\mathrm{SeriesDecomp}\\Big(\\mathrm{FeedForward}(\\mathcal{S}_{\\mathrm{de}}^{l,2})+\\mathcal{S}_{\\mathrm{de}}^{l,2}\\Big) \\\\\n  \\mathcal{T}_{\\mathrm{de}}^{l} & = \\mathcal{T}_{\\mathrm{de}}^{l-1} + \\mathcal{W}_{l,1}\\ast\\mathcal{T}_{\\mathrm{de}}^{l,1}+\\mathcal{W}_{l,2}\\ast\\mathcal{T}_{\\mathrm{de}}^{l,2}+\\mathcal{W}_{l,3}\\ast\\mathcal{T}_{\\mathrm{de}}^{l,3}, \\\\\n  \\end{split}\n\\end{equation}\nwhere $\\mathcal{X}_{\\mathrm{de}}^{l}= \\mathcal{S}_{\\mathrm{de}}^{l,3},l\\in\\{1,\\cdots,M\\}$ denotes the output of $l$-th decoder layer. $\\mathcal{X}_{\\mathrm{de}}^{0},\\mathcal{T}_{\\mathrm{de}}^{0}$ denote the embedded $\\mathcal{X}_{\\mathrm{des}},\\mathcal{X}_{\\mathrm{det}}$ respectively. And $\\mathcal{S}_{\\mathrm{de}}^{l,i},\\mathcal{T}_{\\mathrm{de}}^{l,i},i\\in\\{1,2,3\\}$ represent the seasonal component and trend-cyclical component after the $i$-th series decomposition block in the $l$-th layer respectively. $\\mathcal{W}_{l,i},i\\in\\{1,2,3\\}$ represents the linear projector for the $i$-th extracted trend $\\mathcal{T}_{\\mathrm{de}}^{l,i}$ as the adaptor. \n\nThe final prediction is the sum of the refined decomposed components: $\\mathcal{W}_{\\mathcal{S}}\\ast\\mathcal{X}_{\\mathrm{de}}^{M}+\\mathcal{W}_{\\mathcal{T}}\\ast\\mathcal{T}_{\\mathrm{de}}^{M}$, where $\\mathcal{W}_{\\mathcal{S}}$ and $\\mathcal{W}_{\\mathcal{T}}$ denote the projector for seasonal and trend-cyclical components respectively.\n\n\n\\begin{figure*}\n\\begin{center}\n\t\\centerline{\\includegraphics[width=0.99\\columnwidth]{attention_arxiv.pdf}}\n\t\\vspace{-5pt}\n\t\\caption{Auto-Correlation (left) and Time Delay Aggregation (right). We utilize the Fast Fourier Transform to calculate the autocorrelation $\\mathcal{R}(\\tau)$, which reflects the time-delay similarities. Then the similar sub-processes are rolled to the same index based on selected delay $\\tau$ and aggregated by $\\mathcal{R}(\\tau)$.}\n\t\\label{fig:autocorrelation}\n\t\\vspace{-20pt}\n\\end{center}\n\\end{figure*}\n\\vspace{-5pt}\n\\subsection{Auto-Correlation Mechanism}\\label{autocorrelation_calculation}\n\\vspace{-5pt}\nAs shown in Figure \\ref{fig:autocorrelation}, we propose the Auto-Correlation mechanism with series-wise connections to expand the information utilization. Auto-Correlation discovers the {period-based dependencies} by calculating the series autocorrelation and aggregates similar sub-series by {time delay aggregation}.\n\n\\paragraph{Period-based dependencies} It is observed that the same phase position among periods naturally provides similar sub-processes.\nInspired by the stochastic process theory \\cite{Chatfield1981TheAO,Papoulis1965ProbabilityRV}, for a real discrete-time process $\\{\\mathcal{X}_t\\}$, we can obtain the autocorrelation $\\mathcal{R}_{\\mathcal{X}\\mathcal{X}}(\\tau)$ by the following equations:\\begin{equation}\\label{equ:autocorrelation}\n  \\begin{split}\n    \\mathcal{R}_{\\mathcal{X}\\mathcal{X}}(\\tau)=\\lim_{L\\to \\infty}\\frac{1}{L}\\sum_{t=0}^{L-1}\\mathcal{X}_t\\mathcal{X}_{t-\\tau}.\n  \\end{split}\n\\end{equation}\n$\\mathcal{R}_{\\mathcal{X}\\mathcal{X}}(\\tau)$ reflects the time-delay similarity between $\\{\\mathcal{X}_{t}\\}$ and its $\\tau$ lag series $\\{\\mathcal{X}_{t-\\tau}\\}$. As shown in Figure \\ref{fig:autocorrelation}, we use the autocorrelation $\\mathcal{R}(\\tau)$ as the unnormalized confidence of estimated period length $\\tau$. Then, we choose the most possible $k$ period lengths $\\tau_{1},\\cdots,\\tau_{k}$. The period-based dependencies are derived by the above estimated periods and can be weighted by the corresponding autocorrelation. \n\n\\paragraph{Time delay aggregation} The period-based dependencies connect the sub-series among estimated periods. Thus, we present the \\textit{time delay aggregation} block (Figure \\ref{fig:autocorrelation}), which can roll the series based on selected time delay $\\tau_{1},\\cdots,\\tau_{k}$. This operation can align similar sub-series that are at the same phase position of estimated periods, which is different from the point-wise dot-product aggregation in self-attention family. Finally, we aggregate the sub-series by softmax normalized confidences.\n\nFor the single head situation and time series $\\mathcal{X}$ with length-$L$, after the projector, we get query $\\mathcal{Q}$, key $\\mathcal{K}$ and value $\\mathcal{V}$. Thus, it can replace self-attention seamlessly. The Auto-Correlation mechanism is:\n\\begin{equation}\\label{equ:autocorrelation_fusion}\n  \\begin{split}\n  \\tau_{1},\\cdots,\\tau_{k} & = \\mathop{\\arg\\mathrm{Topk}}_{\\tau\\in\\{1,\\cdots,L\\}}\\left(\\mathcal{R}_{\\mathcal{Q},\\mathcal{K}}(\\tau)\\right)  \\\\\n  \\widehat{\\mathcal{R}}_{\\mathcal{Q},\\mathcal{K}}(\\tau_{1}),\\cdots,\\widehat{\\mathcal{R}}_{\\mathcal{Q},\\mathcal{K}}(\\tau_{k}) & = \\mathrm{SoftMax}\\left(\\mathcal{R}_{\\mathcal{Q},\\mathcal{K}}(\\tau_{1}),\\cdots,\\mathcal{R}_{\\mathcal{Q},\\mathcal{K}}(\\tau_{k})\\right) \\\\\n  \\mathrm{AutoCorrelation}(\\mathcal{Q},\\mathcal{K},\\mathcal{V})&= \\sum_{i=1}^{k}\\mathrm{Roll}(\\mathcal{V},\\tau_{k})\\widehat{\\mathcal{R}}_{\\mathcal{Q},\\mathcal{K}}(\\tau_{k}),\n  \\end{split}\n\\end{equation}\nwhere $\\arg\\mathrm{Topk}(\\cdot)$ is to get the arguments of the $\\mathrm{Topk}$ autocorrelations and let $k=\\lfloor c\\times\\log L\\rfloor$, $c$ is a hyper-parameter. $\\mathcal{R}_{\\mathcal{Q},\\mathcal{K}}$ is autocorrelation between series $\\mathcal{Q}$ and $\\mathcal{K}$. $\\mathrm{Roll}(\\mathcal{X},\\tau)$ represents the operation to $\\mathcal{X}$ with time delay $\\tau$, during which elements that are shifted beyond the first position are re-introduced at the last position. For the encoder-decoder Auto-Correlation (Figure \\ref{fig:framework}), $\\mathcal{K}, \\mathcal{V}$ are from the encoder $\\mathcal{X}_{\\mathrm{en}}^{N}$ and will be resized to length-$O$, $\\mathcal{Q}$ is from the previous block of the decoder.\n\nFor the multi-head version used in Autoformer, with hidden variables of $d_{\\mathrm{model}}$ channels, $h$ heads, the query, key and value for $i$-th head are $\\mathcal{Q}_{i},\\mathcal{K}_{i},\\mathcal{V}_{i}\\in\\mathbb{R}^{L\\times \\frac{d_{\\mathrm{model}}}{h}}, i\\in\\{1,\\cdots,h\\}$. The process is:\n\\begin{equation}\\label{equ:autocorrelation_multihead}\n  \\begin{split}\n  \\mathrm{MultiHead}(\\mathcal{Q},\\mathcal{K},\\mathcal{V}) &= \\mathcal{W_\\mathrm{output}}\\ast\\mathrm{Concat}(\\mathrm{head}_{1},\\cdots,\\mathrm{head}_{h}) \\\\\n  \\mathrm{where}\\ \\mathrm{head}_{i} &= \\mathrm{AutoCorrelation}(\\mathcal{Q}_{i},\\mathcal{K}_{i},\\mathcal{V}_{i}).\n  \\end{split}\n\\end{equation}\n\n\\paragraph{Efficient computation} For period-based dependencies, these dependencies point to processes at the same phase position of underlying periods and are inherently sparse. Here, we select the most possible $\\tau$ to avoid picking the opposite phases. Thus, the complexity of Equation \\ref{equ:autocorrelation_fusion},\\ref{equ:autocorrelation_multihead} is $\\mathcal{O}(L\\log L)$. For the autocorrelation computation (Equation \\ref{equ:autocorrelation}), given time series $\\{\\mathcal{X}_t\\}$, $\\mathcal{R}_{\\mathcal{X}\\mathcal{X}}(\\tau)$ can be calculated by Fast Fourier Transforms (FFT) based on the Wiener\u2013Khinchin theorem \\cite{MR1555316}:\n\\begin{equation}\\label{equ:autocorrelation_algo}\n  \\begin{split}\n  \\mathcal{S}_{\\mathcal{X}\\mathcal{X}}(f) & = \\mathcal{F}\\left(\\mathcal{X}_{t}\\right)\\mathcal{F}^\\ast\\left(\\mathcal{X}_{t}\\right) = \\int_{-\\infty}^{\\infty}\\mathcal{X}_{t}e^{-i2\\pi tf}{\\rm d}t\\overline{\\int_{-\\infty}^{\\infty}\\mathcal{X}_{t}e^{-i2\\pi tf}{\\rm d}t} \\\\\n  \\mathcal{R}_{\\mathcal{X}\\mathcal{X}}(\\tau) &= \\mathcal{F}^{-1}\\left(\\mathcal{S}_{\\mathcal{X}\\mathcal{X}}(f)\\right) = \\int_{-\\infty}^{\\infty}\\mathcal{S}_{\\mathcal{X}\\mathcal{X}}(f)e^{i2\\pi f\\tau}{\\rm d}f, \\\\\n  \\end{split}\n\\end{equation}\nwhere $\\tau\\in\\{1,\\cdots,L\\}$, $\\mathcal{F}$ denotes the FFT and $\\mathcal{F}^{-1}$ is its inverse. $\\ast$ denotes the conjugate operation and $\\mathcal{S}_{\\mathcal{X}\\mathcal{X}}(f)$ is in the frequency domain. Note that the series autocorrelation of all lags in $\\{1,\\cdots,L\\}$ can be calculated at once by FFT. Thus, Auto-Correlation achieves the $\\mathcal{O}(L\\log L)$ complexity.\n\\begin{figure*}\n\\begin{center}\n\t\\centerline{\\includegraphics[width=\\columnwidth]{compare_1.2.pdf}}\n\t\\vspace{-5pt}\n\t\\caption{Auto-Correlation vs. self-attention family. Full Attention \\cite{NIPS2017_3f5ee243} (a) adapts the fully connection among all time points. Sparse Attention \\cite{kitaev2020reformer,haoyietal-informer-2021} (b) selects points based on the proposed similarity metrics. LogSparse Attention \\cite{2019Enhancing} (c) chooses points following the exponentially increasing intervals. Auto-Correlation (d) focuses on the connections of sub-series among underlying periods.}\n\t\\label{fig:compare}\n\t\\vspace{-20pt}\n\\end{center}\n\\end{figure*}\n\n\\paragraph{Auto-Correlation vs. self-attention family} Different from the point-wise self-attention family, Auto-Correlation presents the series-wise connections (Figure \\ref{fig:compare}). Concretely, for the temporal dependencies, we find the dependencies among sub-series based on the periodicity. In contrast, the self-attention family only calculates the relation between scattered points. Though some self-attentions \\cite{2019Enhancing,haoyietal-informer-2021} consider the local information, they only utilize this to help point-wise dependencies discovery. For the information aggregation, we adopt the time delay block to aggregate the similar sub-series from underlying periods. In contrast, self-attentions aggregate the selected points by dot-product. \nBenefiting from the inherent sparsity and sub-series-level representation aggregation, Auto-Correlation can simultaneously benefit the computation efficiency and information utilization.\n\n\\vspace{-5pt}\n\\section{Experiments}\nWe extensively evaluate the proposed Autoformer on six real-world benchmarks, covering five mainstream time series forecasting applications: energy, traffic, economics, weather and disease.\n\\vspace{-5pt}\n\\paragraph{Datasets} Here is a description of the six experiment datasets: \n(1) \\textit{ETT} \\cite{haoyietal-informer-2021} dataset contains the data collected from electricity transformers, including load and oil temperature that are recorded every 15 minutes between July 2016 and July 2018. \n(2) \\textit{Electricity}\\footnote{\\url{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}} dataset contains the hourly electricity consumption of 321 customers from 2012 to 2014.\n(3) \\textit{Exchange} \\cite{2018Modeling} records the daily exchange rates of eight different countries, including Australia, British, Canada, Switzerland, China, Japan, New Zealand and Singapore, ranging from 1990 to 2016.\n(4) \\textit{Traffic}\\footnote{\\url{http://pems.dot.ca.gov}} is a collection of hourly data from California Department of Transportation, which describes the road occupancy rates measured by different sensors on San Francisco Bay area freeways. \n(5) \\textit{Weather}\\footnote{\\url{https://www.bgc-jena.mpg.de/wetter/}} is recorded every 10 minutes for 2020 whole year, which contains 21 meteorological indicators, such as air temperature, humidity, etc.\n(6) \\textit{ILI}\\footnote{\\url{https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html}} includes the weekly recorded influenza-like illness (ILI) patients data from Centers for Disease Control and Prevention of the United States between 2002 and 2021, which describes the ratio of patients seen with ILI and the total number of the patients. We follow standard protocol and split all datasets into training, validation and test set in chronological order by the ratio of 7:1:2.\n\\vspace{-7pt}\n\n\\paragraph{Implementation details}\\label{Implementation}\nOur method is trained with the L2 loss, using the ADAM \\cite{DBLP:journals/corr/KingmaB14} optimizer with an initial learning rate of $10^{-4}$. And batch size is set to 32. The training process is stopped after 5 epochs. All experiments are repeated three times, implemented in PyTorch \\cite{Paszke2019PyTorchAI} and conducted on a single NVIDIA TITAN RTX 24GB GPUs.\nThe hyper-parameter $c$ of time delay aggregation is set to 3 for all experiments.\nAutoformer contains 2 encoder layers and 1 decoder layer. We select three latest state-of-the-art transformer-based models: Informer \\cite{haoyietal-informer-2021}, Reformer \\cite{kitaev2020reformer}, LogTrans \\cite{2019Enhancing}, two RNN-based models: LSTNet \\cite{2018Modeling}, LSTM \\cite{Hochreiter1997LongSM} and CNN-based TCN \\cite{Bai2018AnEE} as our baselines. \nAll the experiments are multivariate forecasting. \n\n\\begin{table}[tbp]\n  \\caption{Quantitative results with different prediction lengths $O \\in \\{96,192,336,720\\}$. We set the input length $I$ as 24 for ILI and 96 for the others. A lower MSE or MAE indicates a better prediction.}\\label{tab:Results}\n  \\centering\n  \\begin{small}\n  \\renewcommand{\\multirowsetup}{\\centering}\n  \\setlength{\\tabcolsep}{2.6pt}\n  \\begin{tabular}{c|c|cccccccccccccc}\n    \\toprule\n    \\multicolumn{2}{c}{Models} & \\multicolumn{2}{c}{\\textbf{Autoformer}}  & \\multicolumn{2}{c}{Informer\\cite{haoyietal-informer-2021}} & \\multicolumn{2}{c}{LogTrans\\cite{2019Enhancing}}  & \\multicolumn{2}{c}{Reformer\\cite{kitaev2020reformer}} & \\multicolumn{2}{c}{LSTNet\\cite{2018Modeling}} & \\multicolumn{2}{c}{LSTM\\cite{Hochreiter1997LongSM}} & \\multicolumn{2}{c}{TCN\\cite{Bai2018AnEE}}  \\\\\n    \\cmidrule(lr){3-4} \\cmidrule(lr){5-6}\\cmidrule(lr){7-8} \\cmidrule(lr){9-10}\\cmidrule(lr){11-12}\\cmidrule(lr){13-14}\\cmidrule(lr){15-16}\n    \\multicolumn{2}{c}{Metric} & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE  \\\\\n    \\toprule\n    \\multirow{4}{*}{\\rotatebox{90}{ETT}} & 96 & \\textbf{0.194} & \\textbf{0.284} & 0.343 & 0.444 & 0.768 & 0.642 & 0.658 & 0.619 & 3.142 & 1.365 & 2.041 & 1.073 & 3.041 & 1.330  \\\\\n    & 192 & \\textbf{0.261} & \\textbf{0.323} & 0.533 & 0.563 & 0.989 & 0.757 & 1.078 & 0.827 & 3.154 & 1.369 & 2.249 & 1.112 & 3.072 & 1.339  \\\\\n    & 336 & \\textbf{0.351} & \\textbf{0.384} & 1.363 & 0.887 & 1.334 & 0.872 & 1.549 & 0.972 & 3.160 & 1.369 & 2.568 & 1.238 & 3.105 & 1.348  \\\\\n    & 720 & \\textbf{0.491} & \\textbf{0.470} & 3.379 & 1.388 & 3.048 & 1.328 & 2.631 & 1.242 & 3.171 & 1.368 & 2.720 & 1.287 & 3.135 & 1.354  \\\\\n    \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Electricity}} & 96  & \\textbf{0.197} & \\textbf{0.312} & 0.274 & 0.368 & 0.258 & 0.357 & 0.312 & 0.402 & 0.680 & 0.645 & 0.375 & 0.437 & 0.985 & 0.813  \\\\\n    & 192  & \\textbf{0.208} & \\textbf{0.321} & 0.296 & 0.386 & 0.266 & 0.368 & 0.348 & 0.433 & 0.725 & 0.676 & 0.442 & 0.473 & 0.996 & 0.821  \\\\\n    & 336  & \\textbf{0.213} & \\textbf{0.328} & 0.300 & 0.394 & 0.280 & 0.380 & 0.350 & 0.433 & 0.828 & 0.727 & 0.439 & 0.473 & 1.000 & 0.824  \\\\\n    & 720  & \\textbf{0.245} & \\textbf{0.352} & 0.373 & 0.439 & 0.283 & 0.376 & 0.340 & 0.420 & 0.957 & 0.811 & 0.980 & 0.814 & 1.438 & 0.784  \\\\\n    \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Exchange}} & 96 & \\textbf{0.134} & \\textbf{0.270} & 0.847 & 0.752 & 0.968 & 0.812 & 1.065 & 0.829 & 1.551 & 1.058 & 1.453 & 1.049 & 3.004 & 1.432  \\\\\n    & 192 & \\textbf{0.272} & \\textbf{0.374} & 1.204 & 0.895 & 1.040 & 0.851 & 1.188 & 0.906 & 1.477 & 1.028 & 1.846 & 1.179 & 3.048 & 1.444  \\\\\n    & 336 & \\textbf{0.488} & \\textbf{0.510} & 1.672 & 1.036 & 1.659 & 1.081 & 1.357 & 0.976 & 1.507 & 1.031 & 2.136 & 1.231 & 3.113 & 1.459  \\\\\n    & 720 & \\textbf{1.367} & \\textbf{0.901} & 2.478 & 1.310 & 1.941 & 1.127 & 1.510 & 1.016 & 2.285 & 1.243 & 2.984 & 1.427 & 3.150 & 1.458  \\\\\n    \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Traffic}}  & 96 & \\textbf{0.609} & \\textbf{0.376} & 0.719 & 0.391 & 0.684 & 0.384 & 0.732 & 0.423 & 1.107 & 0.685 & 0.843 & 0.453 & 1.438 & 0.784  \\\\\n    & 192 & \\textbf{0.637} & \\textbf{0.377} & 0.696 & 0.379 & 0.685 & 0.390 & 0.733 & 0.420 & 1.157 & 0.706 & 0.847 & 0.453 & 1.463 & 0.794  \\\\\n    & 336 & \\textbf{0.634} & \\textbf{0.394} & 0.777 & 0.420 & 0.733 & 0.408 & 0.742 & 0.420 & 1.216 & 0.730 & 0.853 & 0.455 & 1.479 & 0.799  \\\\\n    & 720 & \\textbf{0.646} & \\textbf{0.390} & 0.864 & 0.472 & 0.717 & 0.396 & 0.755 & 0.423 & 1.481 & 0.805 & 1.500 & 0.805 & 1.499 & 0.804  \\\\\n    \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Weather}}  & 96 & \\textbf{0.231} & \\textbf{0.312} & 0.300 & 0.384 & 0.458 & 0.490 & 0.689 & 0.596 & 0.594 & 0.587 & 0.369 & 0.406 & 0.615 & 0.589  \\\\\n    & 192 & \\textbf{0.278} & \\textbf{0.343} & 0.598 & 0.544 & 0.658 & 0.589 & 0.752 & 0.638 & 0.560 & 0.565 & 0.416 & 0.435 & 0.629 & 0.600  \\\\\n    & 336 & \\textbf{0.335} & \\textbf{0.378} & 0.578 & 0.523 & 0.797 & 0.652 & 0.639 & 0.596 & 0.597 & 0.587 & 0.455 & 0.454 & 0.639 & 0.608  \\\\\n    & 720 & \\textbf{0.429} & \\textbf{0.436} & 1.059 & 0.741 & 0.869 & 0.675 & 1.130 & 0.792 & 0.618 & 0.599 & 0.535 & 0.520 & 0.639 & 0.610  \\\\\n    \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{ILI}}  & 24   & \\textbf{3.825} & \\textbf{1.345} & 4.388 & 1.360 & 4.322 & 1.381 & 4.366 & 1.382 & 6.026 & 1.770 & 5.914 & 1.734 & 6.624 & 1.830  \\\\\n    & 36   & \\textbf{3.319} & \\textbf{1.216} & 4.651 & 1.391 & 4.186 & 1.332 & 4.446 & 1.389 & 5.340 & 1.668 & 6.631 & 1.845 & 6.858 & 1.879  \\\\\n    & 48   & \\textbf{2.854} & \\textbf{1.122} & 4.581 & 1.419 & 4.476 & 1.411 & 4.572 & 1.436 & 6.080 & 1.787 & 6.736 & 1.857 & 6.968 & 1.892  \\\\\n    & 60   & \\textbf{3.227} & \\textbf{1.232} & 4.583 & 1.432 & 4.766 & 1.477 & 4.743 & 1.478 & 5.548 & 1.720 & 6.870 & 1.879 & 7.127 & 1.918  \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\end{small}\n  \\vspace{-13pt}\n\\end{table}\n\n\\subsection{Main Results}\n\\paragraph{Quantitative results}\nWe evaluate our Autoformer with different prediction lengths: 96, 192, 336 and 720 and the corresponding ratio of future and past $\\frac{O}{I}$ are 1, 2, 3.5 and 7.5. It precisely meets the definition of long-term forecasting. \nAs shown in Table \\ref{tab:Results}, Autoformer achieves the consistent state-of-the-art performance in all benchmarks and all prediction length settings. Especially, under the input-96-predict-336 setting, compared to previous state-of-the-art results, Autoformer gives \\textbf{74\\%} (1.334$\\to$0.351) MSE improvement in ETT, \\textbf{24\\%} (0.280$\\to$0.213) in Electricity, \\textbf{64\\%} (1.357$\\to$0.488) in Exchange, \\textbf{14\\%} (0.733$\\to$0.634) in Traffic and \\textbf{26\\%} (0.455$\\to$0.335) in Weather. For the input-24-predict-60 setting of ILI, Autoformer makes \\textbf{30\\%} (4.583$\\to$3.227) MSE improvement. Overall, Autoformer yields a \\textbf{38\\%} averaged MSE improvement among above settings.\nNote that Autoformer still provides remarkable improvements in the \\textit{Exchange} dataset that is \\textbf{without obvious periodicity}. \nBesides, we can also find that the performance of Autoformer changes quite steadily as the prediction length $O$ increases. It means that Autoformer retains better \\textbf{long-term robustness}, which is meaningful for real-world practical applications, such as weather early warning and long-term energy consumption planning.\n\n\\paragraph{Decomposition architecture}\nWith our proposed progressive decomposition architecture, other models can gain consistent promotion, especially as the prediction length $O$ increases (Table \\ref{tab:ablation_of_decomposition}). This verifies that our method can generalize to other models and release the capacity of other dependencies learning mechanisms, alleviate the distraction caused by intricate patterns. Besides, our architecture outperforms the pre-processing, although the latter employs a bigger model and more parameters. Especially, pre-decomposing may even bring negative effect because it neglects the interaction of \ncomponents during long-term future, such as Transformer \\cite{NIPS2017_3f5ee243} predict-720, Informer \\cite{haoyietal-informer-2021} predict-336.\n\n\\begin{table}[hbp]\n    \\caption{Ablation of decomposition in ETT dataset with MSE metric. \\textbf{Ours} adopts our progressive architecture into other models. \\textbf{Sep} employs two models to forecast pre-decomposed seasonal and trend-cyclical components separately. Promotion is the MSE improvement compared to \\textbf{Origin}. }\\label{tab:ablation_of_decomposition}\n    \\centering\n    \\begin{small}\n    \\renewcommand{\\multirowsetup}{\\centering}\n    \\setlength{\\tabcolsep}{2.2pt}\n    \\begin{tabular}{c|cccccccccccc|cc}\n    \\toprule\n    Input-96 & \\multicolumn{3}{c}{Transformer\\cite{NIPS2017_3f5ee243}} & \\multicolumn{3}{c}{Informer\\cite{haoyietal-informer-2021}} & \\multicolumn{3}{c}{LogTrans\\cite{kitaev2020reformer}} & \\multicolumn{3}{c}{Reformer\\cite{2019Enhancing}} & \\multicolumn{2}{c}{Promotion} \\\\\n    \\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\\cmidrule(lr){8-10}\\cmidrule(lr){11-13}\\cmidrule(lr){14-15}\n    Predict-$O$ & Origin & Sep & Ours & Origin & Sep & Ours & Origin & Sep & Ours & Origin & Sep & Ours & Sep & Ours \\\\\n    \\toprule\n    96 & 0.604 & 0.311 & \\textbf{0.204} & 0.343 &0.490 & \\textbf{0.354} & 0.768 & 0.862 & \\textbf{0.231} & 0.658 &0.445 &  \\textbf{0.218} & 0.066 & 0.342 \\\\\n    192 & 1.060 & 0.760 & \\textbf{0.266} & 0.533 &0.658 & \\textbf{0.432} & 0.989 & 0.533 & \\textbf{0.378} & 1.078 &0.510 & \\textbf{0.336} & 0.300 & 0.562 \\\\\n    336 & 1.413 & 0.665 & \\textbf{0.375} & 1.363 &1.469 & \\textbf{0.847} & 1.334 & 0.762 & \\textbf{0.362} & 1.549 &1.028 & \\textbf{0.366} & 0.434 & 0.927 \\\\\n    720 & 2.672 & 3.200 & \\textbf{0.537} & 3.379 &2.766 & \\textbf{2.482} & 3.048 &2.601 & \\textbf{0.539} & 2.631 & 2.845& \\textbf{0.502} & 0.079 & 1.918 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\end{small}\n    \\vspace{-10pt}\n\\end{table}\n\n\\paragraph{Auto-Correlation vs. self-attention family} As shown in Table \\ref{tab:ablation_of_attention}, our proposed Auto-Correlation achieves the best performance under various input-$I$-predict-$O$ settings, which verifies the effectiveness of series-wise connections comparing to point-wise self-attentions (Figure \\ref{fig:compare}). Furthermore, we can also observe that Auto-Correlation is memory efficiency from the last column of Table \\ref{tab:ablation_of_attention}, which can be used in long sequence forecasting, such as input-336-predict-1440.\n\n\\begin{table}[hbp]\n    \\caption{Comparison of Auto-Correlation and self-attention in ETT dataset. We \\textbf{replace} the Auto-Correlation in Autoformer with different self-attention modules. The ``-'' indicates the out-of-memory.}\\label{tab:ablation_of_attention}\n    \\centering\n    \\begin{small}\n    \\renewcommand{\\multirowsetup}{\\centering}\n    \\setlength{\\tabcolsep}{6.6pt}\n    \\begin{tabular}{c|c|ccccccccc}\n    \\toprule\n    \\multicolumn{2}{c}{Input Length $I$} & \\multicolumn{3}{c}{96}  & \\multicolumn{3}{c}{192} & \\multicolumn{3}{c}{336}   \\\\\n    \\cmidrule(lr){3-5} \\cmidrule(lr){6-8}\\cmidrule(lr){9-11} \n    \\multicolumn{2}{c}{Prediction Length $O$} & 336 & 720 & 1440 & 336 & 720 & 1440& 336 & 720 & 1440  \\\\\n    \\toprule\n    Auto- & MSE & \\textbf{0.351}&\t\\textbf{0.491}&\t\\textbf{0.575}&\t\\textbf{0.387}&\t\\textbf{0.462}&\t\\textbf{0.747}&\t\\textbf{0.418} &\t\\textbf{0.544} &\t\\textbf{0.890}\\\\\n    Correlation & MAE & \\textbf{0.384} & \\textbf{0.470} & \\textbf{0.527} & \t\\textbf{0.414} & \\textbf{0.461} & \\textbf{0.603} & \\textbf{0.453} & \t\\textbf{0.522} & \\textbf{0.691} \\\\\n    \\midrule\n    Full & MSE & 0.375\t&0.537&\t0.667&\t0.450&\t0.702&\t0.951&\t0.462&\t0.647&\t- \\\\\n    Attention\\cite{NIPS2017_3f5ee243} & MAE &0.425&\t0.502&\t0.589&\t0.470&\t0.570&\t0.704&\t0.485&\t0.578&\t-  \\\\\n    \\midrule\n    LogSparse & MSE & 0.362 & \t0.539 & \t0.582 & \t0.420 & \t0.552 & \t0.958 & \t0.474 & \t0.601 & - \\\\\n    Attention\\cite{2019Enhancing} & MAE & 0.413\t & 0.522 & \t0.529 & \t0.450 & \t0.513 & \t0.736 & \t0.474 & \t0.524 & - \\\\\n    \\midrule\n    LSH & MSE & 0.366 & \t0.502 & \t0.663 & \t0.407 & \t0.636 & \t1.069 & \t0.442 & \t0.615 & - \\\\\n    Attention\\cite{kitaev2020reformer} & MAE & 0.404\t & 0.475 & \t0.567 & \t0.421 & \t0.571 & \t0.756 & \t0.476 & \t0.532 & - \\\\\n    \\midrule\n    ProbSparse & MSE & 0.847 & \t2.482 & \t1.501 & \t0.482 & \t0.996 & \t1.756 & \t0.479 & \t0.684 & \t1.168\\\\\n    Attention\\cite{haoyietal-informer-2021} & MAE &0.654&\t1.065\t&0.894&\t0.482\t&0.716&\t0.938&\t0.495\t&0.597&\t0.773 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\end{small}\n    \\vspace{-5pt}\n\\end{table}\n\n\\subsection{Model Analysis}\n\\paragraph{Time series decomposition} As shown in Figure \\ref{fig:visual_decomp}, without our series decomposition block, the forecasting model cannot capture the increasing trend and peaks of the seasonal part. By adding the series decomposition blocks, Autoformer can aggregate and refine the trend-cyclical part from series progressively. This design also facilitates the learning of the seasonal part, especially the peaks and troughs. This verifies the necessity of our proposed progressive decomposition architecture.\n\n\\begin{figure*}[htbp]\n\\begin{center}\n\t\\centerline{\\includegraphics[width=\\columnwidth]{decomp.pdf}}\n\t\\vspace{-5pt}\n\t\\caption{Visualization of learned seasonal $\\mathcal{X}_{\\mathrm{de}}^{M}$ and trend-cyclical $\\mathcal{T}_{\\mathrm{de}}^{M}$ of the last decoder layer. We gradually add the decomposition blocks in decoder from left to right. This case is from ETT dataset under input-96-predict-720 setting. For clearness, we add the linear growth to raw data additionally.}\n\t\\label{fig:visual_decomp}\n\t\\vspace{-20pt}\n\\end{center}\n\\end{figure*}\n\n\\paragraph{Dependencies learning} The marked time delay sizes in Figure \\ref{fig:visual_periodicity}(a) indicate the most likely periods. Our learned periodicity can guide the model to aggregate the sub-series from the same or neighbor phase of periods by $\\mathrm{Roll}(\\mathcal{X},\\tau_{i}),\\ i\\in\\{1,\\cdots,6\\}$. For the last time step (declining stage), Auto-Correlation fully utilizes all similar sub-series without omissions or errors compared to self-attentions.\n\n\\begin{figure*}[htbp]\n  \\begin{center}\n    \\centerline{\\includegraphics[width=\\columnwidth]{attention_visual.pdf}}\n    \\vspace{-5pt}\n    \\caption{Visualization of learned dependencies. For clearness, we select the top-6 time delay sizes $\\tau_{1},\\cdots,\\tau_{6}$ of Auto-Correlation and mark them in raw series (\\textcolor{red}{red} lines). For self-attentions, top-6 similar points with respect to the last time step (\\textcolor{red}{red} stars) are also marked by \\textcolor[rgb]{0.8,0.6,0.4}{orange} points.}\n    \\label{fig:visual_periodicity}\n    \\vspace{-20pt}\n  \\end{center}\n  \\end{figure*}\n\n\\paragraph{Efficiency analysis} We compare the running memory and time among Auto-Correlation-based and self-attention-based models (Figure \\ref{fig:model_analysis}). The proposed Autoformer shows $\\mathcal{O}(L\\log L)$ complexity in both memory and time and achieves better long-term sequences efficiency.\n\n\\begin{figure*}[thbp]\n\\begin{center}\n\t\\centerline{\\includegraphics[width=0.9\\columnwidth]{analysis_final.pdf}}\n\t\\caption{Efficiency Analysis. For memory, we replace Auto-Correlation with self-attention family in Autoformer and record the memory with input 96. For running time, we run the Auto-Correlation or self-attentions $10^3$ times to get the execution time per step. The output length increases exponentially.}\n\t\\label{fig:model_analysis}\n\t\\vspace{-20pt}\n\\end{center}\n\\end{figure*}\n\n\n\\section{Conclusions}\n\\vspace{-5pt}\n\nThis paper studies the long-term forecasting problem of time series, which is a pressing demand for real-world applications. However, the intricate temporal patterns prevent the model from learning reliable dependencies. We propose the Autoformer as a decomposition architecture by embedding the series decomposition block as an inner operator, which can progressively aggregate the long-term trend part from intermediate prediction. Besides, we design an efficient Auto-Correlation mechanism to conduct dependencies discovery and information aggregation at the series level, which contrasts clearly from the previous self-attention family. Autoformer can naturally achieve $\\mathcal{O}(L\\log L)$ complexity and yield consistent state-of-the-art performance in extensive real-world datasets.\n\n\n\\small\n\\bibliographystyle{plain}\n", "meta": {"timestamp": "2021-06-25T02:18:51", "yymm": "2106", "arxiv_id": "2106.13008", "language": "en", "url": "https://arxiv.org/abs/2106.13008"}}
{"text": "\\section{Introduction}\nIn the past few decades, nanotechnology has attracted a growing interest in various scientific and technological areas.\nIn particular, the development of novel nanomaterials is increasingly investigated.\nDue to their size-related effects leading to unique physical, chemical, optical, electrical, and magnetic properties, these materials can be widely used to develop novel sensors \\cite{NanomatSensors}. Among the different types of sensing technologies, optical based sensors offer important advantages: they enable small sizes, offer sensitivity to multiple environmental parameters, require small cable sizes and weights.\n\nIn contrast to electrical schemes, optical sensing offers potentials of high sensitivity, immunity to electromagnetic interference, and safe operation in explosive or combustive atmospheres, as well as more options for signal retrieval from optical intensity, spectrum, phase, and polarization \\cite{OpticalSensors}. Nanomaterial-based optical sensors can also be expected to exhibit further advantages over conventional sensors.\nIn particular, the integration of nanoparticles in polymer hosts and their use in combination with optical fibers or planar platforms have attracted interest. \nSuch arrangements are a key step towards the development of advanced analytical instrumentation, aiming small scale and multiparameter capability \\cite{QDsInHost}.\n\nThe unique features of quantum dots (QDs) are highly attractive for fiber sensing platforms.\nPolymer fibers are excellent candidates to host QDs because of the good processability of their organic materials, which facilitate the nanoparticle integration \\cite{PolymerFibers}.\nPolymer optical fibers and waveguides have received continuous attention in recent years due to their excellent properties, such as simple preparation, biocompatibility, low cost, high optical transparency and flexibility.\nAlthough organic dyes were traditionally used for doping, they have been gradually superseded in several fields by quantum dots, because of their unique controlled size-dependent electronic and optical properties \\cite{QDs_Dyes}.\nIn addition, the QDs brightness is enhanced by 20 times and the stability against photobleaching is improved by 100 times compared to organic dyes \\cite{QDs_Dyes2}.\\\\\nAt present, some of the most attractive features of QDs are being explored for the detection of specific molecules in biomedical researches and clinical diagnosis \\cite{Biosensors1,Biosensors2,Biosensors3,Biosensors4}.\nPolymer embedded nanocrystals are also widely employed as luminescent indicators for detection of chemical species in sensing probes \\cite{PolChemSensors1,PolChemSensors2}.\nThe unique features of QDs are also highly attractive for fiber sensing platforms. \nIn spite of this, to date, the types of solutions where QDs are simultaneously immobilized and allowed to interact with the environment for sensing purposes are still very limited and the application of QDs in optical fiber technology remains largely unexplored. \\cite{QDsInHost}.\nOne of the most investigated fields involves the use of QDs integrated in polymer optical fibers (POF) or planar waveguides for thermometry applications \\cite{TempPOF1,TempPOF2,TempPOF3,TempPOF4,TempPOF5,WaveguideTempSensing1,POFsTempSensing}.\n\\begin{comment}\nAlso Photonic Crystal Fibers (PCFs), also called holey fibers because of tiny air holes along their structure, were doped with QDs and used as temperature sensors \\cite{PCFsTempSensing1}.\n\\end{comment}\nAmong other relevant application fields, fluorescent fiber probes based on nanomaterials were employed to detect metal ions for monitoring of heavy metal pollution \\cite{MetalIonsSensing1}, as well as for intracellular sensing and medical diagnostics \\cite{MetalIonsSensing2}.\\\\\nNanomaterials provide a wide range of applications in nanopiezotronics, which utilizes the coupled piezoelectric and semiconducting property of nanomaterials.\nPiezoelectricity is a coupling between mechanical and electrical behavior of a material.\nWhen a piezoelectric material is squeezed, twisted, or bent, the transport behavior of electric charges is largely influenced.\nConversely, when a piezoelectric material is subjected to a voltage drop, it mechanically deforms.\nElectronics based on this effect are coined as piezotronics \\cite{Piezophototronics0}.\nIn recent years, a large variety of piezotronic devices have been invented, such as strain sensors, transistors, logic circuits, and electromechanical memories \\cite{Piezophototronics1}.\nBy introducing light illumination, piezoelectric properties affect the photoluminescence emission of nanomaterials. \nThis effect leads to the development of a new research area named piezophototronics \\cite{Piezophototronics2}.\nAt present, the piezophototronic effect is extensively applied to enhance the performance of a variety of optoelectronic devices, such as light-emitting diodes, solar cells, and photodetectors \\cite{Piezophototronics1}.\nThe study of the piezophototronic effect of QDs integrated in polymer waveguide and optical fibers can open to interesting possibilities and lead to the development of competitive devices.\n\nIn this paper, the core/shell CdSe/ZnS QDs were integrated in a flexible millimeter-scale polymer waveguide.\nWhen a force is applied on the waveguide, the light emission detected at the output changes as a result of the piezophototronic effect of the QDs contained into the waveguide core.\nA calibration experiment of intensity modulation and wavelength shift as a function of the force applied on the waveguide demonstrates the feasibility of the proposed force sensor design.\nTheoretical model and simulation results further validate the presented sensing principle.\n\n\\section{Results and discussion}\n\n\\subsection{Materials selection and manufacturing process}\n\nOptical fibres are made by an inner cylindrical core and an outer covering layer called cladding.\nThey are able to transmit light because of the phenomenon of \\textit{total internal reflection} (\\textit{TIR}).\nSnell's law well describes the behaviour of the light at the interface between two media. \nFrom Snell's law, the equation of the critical angle can be deduced as follows:\n$$\n\\theta_{c} = \\arcsin(\\dfrac{n_{2}}{n_{1}})\n$$\n\\noindent\nAssuming that the refractive index of the first medium $n_{1}$ is higher than the refractive index of the second medium $n_{2}$ and calling $\\theta$ the angle between the propagation direction of a light ray and the axis perpendicular to the interface between the two media, all the rays having $\\theta > \\theta_{c}$ are totally reflected into the first medium.\nIn the case of an optical fibre, if $n_{1} > n_{2}$ with $n_{1}$ refractive index of the core and $n_{2}$ refractive index of the cladding, all the rays satisfying the above angular condition are transmitted through the core.\\\\\nSome PTFE sleeves (clear Polytetrafluoroethylene, 3.05mm bore diameter, RS components) were employed as cladding.\nThe main material being used to create the core was PMMA powder (Polymethyl methacrylate, average Mw ~120,000 by GPC, Sigma-Aldrich).\nThe typical refractive index value is around 1.50 at 500 nm for PMMA and 1.38 at 500 nm for PTFE. Since a low concentration of quantum dots was embedded into the core of the optical fibre (as detailed below), it can be assumed that the nanoparticle integration does not affect the refractive index of the core, therefore $\\theta_{1} > \\theta_{2}$ as required for TIR to occur.\n\nThe PMMA powder was diluted in the organic solvent DMF (N,N-Dimethylformamide, anhydrous, 99.8\\%, Sigma-Aldrich) at 22wt\\%. The solution was mixed with a stirrer at 1100 rpm for about 5 hours.\nThe core-shell CdSe/ZnS quantum dots with emission wavelength of 560 nm (powder form, PlasmaChem GmbH) was diluted in toluene (Anhydrous, 99.8\\%, Sigma-Aldrich) with a mass concentration of 5mg ml$^{-1}$.\nQuantum dots were added to the polymer solution in the concentration of 0.06wt\\% and mixed for 30min using the stirrer. \nThe final solution was placed in the ultrasonic bath for 15 minutes to remove air bubbles.\nThen the polymeric composite material was injected into the PTFE tubes using a syringe.\nThe filled tubes were left to dry for 24 hours at room temperature and then for 72 hours at 60\u00b0C in the oven.\nAfter the curing process, the polymeric composite material containing quantum dots has a gel-form and this allows the optical fibre to be flexible.\\\\\nThe fabrication process of the plain optical fibre (not containing quantum dots) follows the same procedure as described above, except the phase of quantum dots integration.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/ExpSetup.png}\n    \\caption{\\textbf{Quantum dots piezoelectric effect for sensing in composite polymer waveguides.}\n    a) Experimental setup: a collimated laser diode with emission peak around 532nm injects light into the polymer waveguide. The output signal is collected by an optical system and sent to the spectrometer which is connected to a PC. A pressure is applied by a probe mounted on a force sensor.\n    b) The output spectrum was acquired using a plain polymer waveguide (without QDs) and a polymer waveguide with QDs incorporated in its volume. \n    The zoomed plot highlights the range [540-620]nm: considering the plain waveguide, the spectrum appears flat, while the signal corresponding to the QDs piezoelectric emission stretches on the range [550-600]nm in the case of the doped waveguide.\n    c) The main materials for the polymer waveguide fabrication are Teflon (PTFE) for the cladding and Plexiglass (PMMA) in the form of a gel for the core. \n    The whole waveguide is flexible and compressible.\n    }\n    \\label{fig:setup}\n\\end{figure}\n\n\\subsection{Experimental setup}\n\nThe experimental setup is shown in Fig.\\ref{fig:setup}a.\nA collimated laser diode ($\\lambda_{em} \\approx$ 532nm, 4.5mW, Thorlabs) injects light into a cylindrical polymer waveguide of 8.5cm length and diameter of 2mm.\nA long-pass filter ($\\lambda_{cut-on} \\approx$ 550nm, Thorlabs) was placed at the output of the waveguide to filter out the light coming from the laser and isolate the signal emitted by the quantum dots.\nThe output signal is collected by an optical system and sent to the spectrometer (range [200-1000]nm, Thorlabs). \nThe acquired spectrum is displayed through a software on a PC.\nFig.\\ref{fig:setup}b shows a comparison between the spectrum acquired using a plain waveguide (not containing quantum dots) and the one employing a waveguide with embedded quantum dots.\nFor both the acquisitions, the same integration time was set on the spectrometer.\nIn the case of the plain waveguide some laser light can still reaches the spectrometer although the presence of the long-pass filter and its peak saturates around 532nm, while the rest of the spectral profile is flat (blue spectrum in fig. \\ref{fig:setup}b).\nIn the doped waveguide instead, nanoparticles absorb part of the laser light and eventually re-emit it at longer wavelength.\nSo, a spectral distribution stretches on the range [550-600]nm (red spectrum in fig. \\ref{fig:setup}b).\nThis emission corresponds to the quantum dots piezoelectric emission and it can be detected also when the waveguide is not compressed.\nIn fact, even under resting conditions, the quantum dots are affected by an inner strain for being integrated in the gel-like matrix of the waveguide core and because of the injection process undergone by the soft material during manufacturing.\n\nThe proposed waveguide is shown in Fig.\\ref{fig:setup}c.\nAn optical fibre of 1 cm length was inserted at the input and output of the PTFE tube corresponding to the waveguide cladding.\nA fibre with core diameter similar to the waveguide core diameter (3mm) was employed to reduce the light loss at the interface between the optical fiber and the soft material of the waveguide core. \nThe optical fibres were firmly fixed inside the PTFE tube using some thread sealing tape.\nWhen a force is applied on the waveguide, the two optical fibres assume the role of stoppers and keep the soft material of the waveguide under compression.\nThe pressure is then sensed by the quantum dots embedded into the gel-like matrix and their light emission changes as a result of the piezophototronic effect.\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/Calibration.png}\n    \\caption{\n    \\textbf{Amplitude modulation and wavelength shift calibration.}\n    The waveguide was compressed on two different locations: 3 cm and 4 cm from the input surface of the waveguide.\n    For each force value, the amplitude of the piezoelectric peak of quantum dots a) and its wavelength shift b) were measured.\n    }\n    \\label{fig:Calibration}\n\\end{figure*}\n\n\\subsection{Experimental evaluation}\n\nA calibration experiment was carried out to determine the characteristic trend of intensity and wavelength modulation as a function of the force applied on the waveguide.\\\\\nA 6-axes force/torque sensor (ATI Nano-17, resolution 0.003 N) was mounted on a manual translation stage (Thorlabs, resolution 10 $\\mu$m) which was moved forward to progressively compress the waveguide (fig.\\ref{fig:setup}a).\nA circular probe of 8 mm diameter mounted on the calibration force sensor was employed to apply the compression on two different locations: 3cm and 4cm from the input surface of the waveguide.\nThe force applied covers the range [0-12]N, with a step of 1N. \nThe maximum value of 12N is the mechanical limit imposed by the experimental setup in use and the amplitude corresponding to this force value was used as normalisation factor.\nThe integration time of the spectrometer was set to 5s to get a good signal-to-noise ratio. \nFor each force value, 10 repeated spectra were acquired.\nA Gaussian fit was applied on each one of the 10 spectra, then the mean of the coefficients was calculated to get the final Gaussian fit for each force value.\nThe amplitude coefficient of the final Gaussian trend was considered to get the modulation of the peak amplitude of the signal (fig.\\ref{fig:Calibration}a), while the centroid location coefficient was used to get the wavelength shift as a function of force (fig.\\ref{fig:Calibration}b).\nThe error bars were calculated as standard deviation on the 10 repeated measurements for each force value. \nThe black error bars take into account the stress relaxation occurring during the acquisition time for each force value. \nThis effect will be discussed in the next paragraph.\n\n\\subsection{Stress relaxation}\n\nPolymer materials show a combination of elastic and viscous behaviour known as viscoelasticity.\nAn immediate consequence of viscoelasticity is that deformations under stress are time dependent.\nIf a constant deformation is imposed then the induced stress will relax with time (stress relaxation).\nSeveral polymeric materials show a linear viscoelastic behaviour, such that the stress (at a constant strain) decreases in time as a simple exponential Debye function:\n\\begin{equation}\n\\sigma(t)=\\sigma_0 exp^{-t/\\tau}\n\\end{equation}\nThe time constant $\\tau$ is the relaxation time at which $\\sigma(t)$ decays to the value $1/e$ of\nits initial value $\\sigma_0$\\cite{StressRelax}.\\\\\nTo study the time dependency of amplitude and wavelength of the output signal, a constant pressure was applied on the waveguide and the spectrum was monitored for 90 minutes.\nResults are shown in Fig.\\ref{fig:StressRelax}.\nThe exponential decay of the amplitude over the time suggests that the polymeric soft material of the waveguide core is subject to a stress relaxation effect.\nBoth amplitude and wavelength move towards a plateau after about one hour and an half of acquisition. \n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/StressRelax.png}\n    \\caption{\n    \\textbf{Time-dependent relaxation of the polymer waveguide.}\n    The waveguide was kept under constant pressure for 90 minutes and the output spectum was acquired every 5 minutes.\n    a) The amplitude modulation as a function of time $A(t)$ shows a negative exponential behaviour, typical of stress-relaxation in viscoelastic materials, especially polymeric materials.\n    The amplitude $A(t)$ reaches its equilibrium value $A_{eq}$ after 90 minutes of acquisition. \n    b) The wavelength shift presents a linear fashion.\n    Both the trends move towards a plateau after about one hour and an half of acquisition.\n    }\n    \\label{fig:StressRelax}\n\\end{figure*}\n\n\\subsection{Analysis of simulation results}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/single_qd_v5.png}\n    \\caption{\\textbf{Single quantum dot calculated photoemission spectra.}\n    a) Normalised simulated photonic emission luminosity spectra of a single quantum dot under compression, shown here for a range of compression force values. We note that the trend of the feature at the 540 to 580 $nm$ range blue shifts and increases in amplitude under compression.\n    b) Heat map of the single quantum dot photoemission effect. The range of \n    compression is evaluated from the force of 0 to 0.04 $\\mu N$.\n    The lower wavelength band is associated with the QD effective energy, whereas the upper wavelength band corresponds to the\n    piezoelectric dipole emission (see text). Upon the application of stress, the excitonic band is slightly\n    red shifted with pressure, whereas the piezo-electric driven emissions are blue shifted. Dotted-line box indicates the region shown in c). We note that\n    the emission is simulated for a single QD in vacuum. \n    c) Blown up scales to highlight the regime of forces\n    which is comparable to experiments. Dotted line as a guide to the eye for the blue shift trend of the piezo-electric feature under compression. The colour scheme for b) is logarithmic whereas c) is linear.}\n    \\label{single}\n\\end{figure}\n\nThe normalised single quantum dot emission spectra are illustrated in Fig.\\ref{single}.a) where the increase in emission amplitude and the blue shift in wavelength, as the compressive force increases, for the piezoelectric band is shown.\n\nThe full single quantum dot emission spectrum under compressive forces of range $0-0.04 nN$ is shown as a heat map in Fig.\\ref{single}.b), here we observe two distinct features: the quantum dot effective energy peak at shorter wavelengths and the piezoelectric dipole emissions for longer wavelengths (as clarified in the Theory section). The two features undergo contrasting wavelength shifts under an increase in force; the shorter wavelength band redshifts, whereas the longer wavelength band blueshifts. This presents an upper limit of the operational force range of this technique due to the adjoining of the two features at $\\sim 0.03 nN$, at least at the single quantum dot emission spectra level. \n\nIn Fig.\\ref{single}.c) we showcase the single quantum dot operational force range\n(the same region is shown in Fig.\\ref{single}.b) in a dotted box). The blue shifting trend is explicitly depicted as a dotted line.\n\nTo further justify our experimental observations, the single quantum dot emission spectra were utilised in the ray tracing simulation; the waveguide core was split into three separate simulation segments of refractive index $n_{PMMA}=1.49$, dimensions $50\\times50\\times50 nm$ each, surrounded by a cladding along the length of the waveguide of refractive index $n_{PTFE}=1.38$, thickness $10 nm$. The rays are generated in a spherical cone distribution of $30 rad$ at the beginning of the first segment, which also contains spherical structures of refractive index\\cite{cdse_n} $n_{CdSe}=2.64$ (radius = $1.5nm$, consistent with the manufacturer values) distributed in a cubic mesh manner with inter-quantum dot distance of $10 nm$\n. These spherical structures play the role of the quantum dots suspended in the waveguide, and contain absorption and emission spectra corresponding to the ambient force (as calibrated through matching the simulation results with experimental observations) within the PMMA. The second and third segments contains simulated quantum dots with different spectra, consistent with a further externally applied force\n\nThe ray tracing simulation takes several types of events into account as a ray approaches any interface between objects; the ray could be reflected and adopt a new direction of propagation, or transmitted at angle due to the difference of refractive indices at the interface. The ray could also be absorbed, corresponding to the absorption spectrum of a quantum dot. Finally after being absorbed into a quantum dot, the ray has a likelihood be being emitted from a quantum dot with a new wavelength and propagation direction.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/events_stacked.png}\n    \\caption{\\textbf{Ray-tracing outcomes.}\n    a) Stacked bar chart illustrating the breakdown of ray tracing events, where: i) 'Detector' indicates that a ray reaches the detector without being absorbed by and/or emitted from a QD, ii) 'Emission' denotes the subset of rays that is absorbed and emitted by a QD at least once, iii) 'QD loss' depicts rays that are absorbed by a quantum dot, and due to the quantum efficiency of the quantum dot, are not emitted again, iv) 'WG loss' (waveguide loss) accounts for rays that escapes the waveguide through the outer coating of the waveguide. b) Stacked histograms resolved in wavelengths showing the further split of the 'Emission' events in a). c) Wireframe structure diagram of a scaled down example of the ray tracing simulation.}\n    \\label{events}\n\\end{figure}\n\nThe percentages of final outcome of the rays for a sample ray tracing simulation of five million rays, at ambient waveguide conditions, are shown in Fig.\\ref{events}.a). The outcomes are split into four categories; all rays that have not been absorbed or emitted by a quantum dot, and are transmitted through the detector-end interface between the simulated waveguide core and vacuum are shown as a part of the 'Detector' category in blue. Rays that have not been absorbed or emitted by a quantum dot, and are not transmitted through the waveguide core/ detector interface are depicted in the 'WG loss' category in red, indicating that the rays have been lost due to transmission through the waveguide cladding into simulated vacuum. The rays that have been absorbed by a quantum dot, and are not re-emitted due to the quantum yield of the quantum dots (estimated at $60\\%$) are shown in green as 'QD loss'. Lastly, all rays which have been emitted by a quantum dot are classified in the 'Emission' category in orange.  \n\nThe 'Emission' rays are further split into three wavelength resolved outcome categories in Fig.\\ref{events}.b), where the classification of ray categories follow those of Fig.\\ref{events}.a), but all rays have been emitted by a quantum dot.\n\nA comparison between the ratio of 'Detector' rays in Fig.\\ref{events}.a) and 'Detector' rays in Fig.\\ref{events}.b) informs us that we are in the regime where the detector senses an order of magnitude more light-source wavelength rays than quantum dot emitted (or signal) rays, consistent with the experimentally observed ratios where the light-source wavelength is detected as a saturated peak. However, due to the lack of information on the thickness of quantum dot shells, we were unable to calculate the relative densities of PMMA and quantum dots. Therefore, we could not use the simulation results to guide the manufacturing process, in order to experimentally obtain the optimal signal to light-source ratio.\n\nMoreover, a significant amount of simulated rays are lost through the 'WG loss' categories. This is due to ray being scattered or emitted by quantum dots at angles with large perpendicular components with respect to the length of the waveguide. This allows for the possibility where rays propagate to the cladding at an angle greater than $\\theta_c$, whereby the ray is transmitted through the cladding and are lost. This phenomena also explains the significantly larger ratio of 'WG loss' in Fig.\\ref{events}.b), where the rays have been emitted by quantum dots, compared to 'WG loss' in Fig.\\ref{events}.a), where the rays have not been emitted by quantum dots and are likely to propagate with a large parallel component with respect tot the length of the waveguide. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/stacked_x_histogram.png}\n    \\caption{\\textbf{Statistical analysis of simulated detected photons. }\n    a) Spectral analysis of the detected photons after transmission through the waveguide with a simulated compressive force of $6 nN$.\n    The subset of measured photons are colour coded into three groups: $\\lambda <= 650nm$, $650nm < \\lambda <= 800nm$ and $800nm < \\lambda$. b) Spatially resolved cross-section of the measured photons. c) We also report the \n    angular distributions in terms of normalised momentum vector with respect to the c) perpendicular and d) parallel components.}\n    \\label{hist}\n\\end{figure}\n\nThe 'Detector' category in Fig.\\ref{events}.b) can be further analysed to obtain the distribution of ray location and propagation directions. For a simulated compressive force of $6 nN$ on the quantum dots of the middle waveguide segment, all rays (of a five million ray simulation) that have been emitted by a quantum dot and is transmitted through the detector end of the waveguide is shown in Fig.\\ref{hist}.a). Here, the rays are colour coded into three groups ($\\lambda <= 650nm$ for green, $650nm < \\lambda <= 800nm$ for orange and $800nm < \\lambda$ for blue). The position of the ray incident on the simulated waveguide-detector interface is illustrated in Fig.\\ref{hist}.b), where r is the normalised uni-dimensional distance of the ray position from the centre of the cross-section of the interface. The normalised component of ray propagation which is perpendicular with respect to the length of the waveguide is shown in Fig.\\ref{hist}.c), and the parallel component is shown in Fig.\\ref{hist}.d).\n\nWe observe in Fig.\\ref{hist}.b) that there is a slight preference for rays to propagate incident to the interface with r close to unity. In our view, this is due to rays that are consistently reflected between the waveguide core-cladding interface and the outermost quantum dots from the centre of the waveguide. \n\nIn fig.\\ref{hist}.c) there is a tendency for rays to transmit through the interface with a nonzero perpendicular component. We explain this bias by considering rays with purely zero perpendicular component, since Fig.\\ref{hist} show only the subset of rays that have been emitted by a quantum dot, rays that have been emitted with zero perpendicular component (and hence travel parallel to the length of the waveguide), will likely interact with the next neighbouring quantum dot in the cubic mesh. Therefore, this decreases the likelihood that the detector senses a ray with a zero perpendicular component. \\textcolor{black}{Furthermore, we observe a disinclination of rays with a normalised perpendicular component of $\\sim 0.35$. We attribute this to the scattering of rays in the quantum dot mesh}. Lastly, rays with unity perpendicular components will not propagate to the interface, and hence we observe zero counts of such rays.\n\nIn fig.\\ref{hist}.d) a propensity of rays with $0.7 - 1$ normalised parallel components are observed. This can be explained through the geometry of the simulated waveguide; rays that transmits through the waveguide-detector interface likely has a large parallel component, such that no obstructing quantum dots are in the path of propagation. Furthermore, no counts of rays with a parallel component $< \\sim0.35$ was detected. This is due to the critical angle of the interface; the angle of incidence of such rays are less than $\\theta_c$ between the PMMA waveguide core and vacuum.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/trend.png}\n    \\caption{\\textbf{Simulated trend of the intensity and wavelength.}\n    a) Theoretical amplitude of the measured photonic distribution measured at the detector as a function of\n    the applied force, where the latter is applied near the detector (source) and labelled in blue (green). \n    The distribution amplitude is obtained by a skewed Gaussian fit of the waveguide spectrum, evaluated as the percentage increase of the amplitude with respect to ambiant conditions ($\\Delta A$) as a function of waveguide compression. The applied forces ($F$) is normalised by the saturation limit ($F_{sat}$). b) We report the wavelength shift of the maxima of the distribution's peak, as a function of waveguide compression.}\n    \\label{trend}\n\\end{figure}\n\nWe present the skewed Gaussian-fitted trend of simulated signal intensity as a function of compressive force applied onto the middle waveguide segment (green) and last waveguide segment (blue) in Fig.\\ref{trend}.a), and the wavelength shift trend in Fig.\\ref{trend}.b). The range of forces considered are normalised to a saturation force $F_{sat}$, where the amplitude increase of the signal saturates. This increase in amplitude and subsequent saturation is due to the growth and assimilation of the two features shown in Fig.\\ref{single}.b); as the compressive force increases, the two features increase in amplitude and become closer in wavelength, whereby the fit interpret the two features as a single entity. That is, until the two features diverge in wavelength, and the fit only analyses the initially longer (now shorter) wavelength peak.\n\nBoth the intensity and wavelength trends of the 'Detector' and 'Source' simulations agree with the experimental results. Moreover, the trends are more pronounced for the 'Detector' simulations. We explain this due to the likelihood of a quantum dot emitted ray to become scattered out of the waveguide (WG loss), or be absorbed (QD loss) and re-emitted by a subsequent quantum dot; the closer to the detector this compressive force is applied, the less likely that the signal information is overwritten by these effects from a subsequent quantum dot.\n\n\\section{Theory}\n\n\nIn this section, we discuss the evaluation of the set of piezoelectric affected emission spectra from a single quantum dot under a range of compressive forces. A few difficulties exist in simulating these spectra; the crystalline structure and thickness of the ZnS shell are not specified by the provider. \n\nZnS exist in both sphalerite (cubic) and wurtzite (hexagonal) forms, the piezoelectric tensor of the sphalerite form only contain a single unique nonzero element: $d_{14}$ (in Voigt notation), which does not correspond to a uniaxial compressive stress, and therefore can be safely neglected for our calculations. The relevant piezoelectric tensor elements of the wurtzite form are roughly a factor of three smaller than the corresponding CdSe tensor elements ($d^{ZnS}_{31}=-1.1$, $d^{ZnS}_{33}=3.2$, $d^{CdSe}_{31}=-3.92$, $d^{CdSe}_{33}=+7.8$, all values are in $pC/N$ and are cited from \\cite{cdse_coeff}), and thus do not play a significant role in affecting the piezoelectric trend that we wish to showcase as a proof of concept. Moreover, since the ZnS shell of the quantum dots do not drastically alter the spectral line shape\\cite{qd-shell}, we modelled our quantum dots as pure CdSe entities following the quantum mechanical analysis of piezoelectric effects from Zhang et al\\cite{Zhang2018}.\n\nIn the cited work, rate of emission is calculated via the Lorentzian expression of the zero dimensional density of states and a product of Fermi-Dirac distributions:\n\\begin{equation}\n    R(\\omega)=\\frac{\\Gamma/2}{(\\hbar\\omega-E_{QD})^2+(\\Gamma/2)^2}f_c(1-f_v),\n\\end{equation}\nwhere $\\Gamma$ is the line width (or full width at half maximum) of the distribution, which describes the coupling between a photon and an electron in the conduction band of the quantum dot. $f_c$ and $f_v$ are the Fermi-Dirac distributions of electrons in the conduction and valence bands of the quantum dot, and $E_{QD}$ is the effective energy of the quantum dot, described as:\n\\begin{equation}\n    E_{QD}(r,F,T)=E_g+E_e(r)+E_h(r)-E_{ex}-E_p(r,F)+\\alpha_1T\n\\end{equation}\nwith $E_g$ as the band gap of bulk CdSe, $E_{e,h}$ are the electron and hole confinement energies, and are inverse-squarely proportional to the radius of the quantum dot. $E_{ex}$ is the excitonic binding energy, for CdSe the electronic thermal energy (at room temperature and in vivo) exceeds the binding energy of excitons. Therefore, we neglect the excitonic binding energy. $E_{p}$ is the energy change induced by the piezoelectric potential of the material, it is dependent on both the radius of the quantum dot and the force applied onto the quantum dot. In addition, we have also included the shift in $E_{QD}$ due to temperature; the Debye temperature of CdSe ($\\Theta_D=181K$\\cite{cdse_coeff}) is significantly lower than room temperature, we therefore implemented a linear shift in energy with the trend $\\alpha_1=0.32MeV/K$\\cite{temperature}.\n\nThe calculation of the Fermi-Dirac distributions $f_{c,v}$ require the notion of chemical potential $\\mu_{c,v}$ in both the conduction and valence bands, which are evaluated by considering the radius and force dependent, piezoelectrically generated charge carriers in the respective bands:\n\\begin{equation}\n    n_{c,v}(r,F)=\\frac{dF}{V}=\\frac{1}{2\\pi^2}(\\frac{2m_r}{\\hbar^2})^{3/2}\\int\\frac{\\sqrt{E}}{e^{(E-\\mu_{c,v})/k_BT}+1}dE\n\\end{equation}\nwhere the charge carrier density is determined from the piezoelectric strain coefficient of CdSe $d$, the applied force $F$ and the volume of the quantum dot $V$, this can be further expressed through the notion of reduced mass $m_r$, with an integral over energy $E$.\n\nThe resultant spectra contains two identifiable features: the Lorentzian peak corresponding to the effective quantum dot energy, and the overlap between the piezoelectric charge Fermi-Dirac distributions, as illustrated in Fig.\\ref{single}. The two structures are dependent on both the radius of the quantum dot and the force applied. However, the effective quantum dot energy peak red-shifts under increasing force, whereas the piezoelectric band blue-shifts. Moreover, the pair of structures depend differently on the radius of quantum dots. \n\nThis therefore permits a rough calibration of the unique parameters which best describe the experimental setup, assuming that the experimental spectra of quantum dots suspended in a waveguide are indicative of the simulated single quantum dot spectra. The parameters we used to match the experimental setup are $r=2.36 nm$ (which is consistent with the manufacturer's estimation of quantum dot size of $r\\sim1.5nm$), and a calculationally calibrated ambient force  of $F=1 nN$, corresponding to the an estimation of the force experienced by quantum dots suspend in PMMA.\n\n\\section{Conclusions}\n\nA millimeter-scale waveguide made by polymeric materials was manufactured and the core/shell CdSe/ZnS QDs were integrated in the volume of its core.\nSince the light source peak (532 nm) is within the wavelength absorption range of the selected QDs, during the transmission through the waveguide the light can be absorbed and eventually re-emitted by the quantum dots at a longer wavelength.\n\nA preliminary experiment was carried out comparing the output spectrum of a non-doped waveguide and the one of a QDs-doped waveguide.\nThe results showed that a clear distribution stretching on the range [550-600]nm is visible just in the doped waveguide measurement, and it is identifiable as the QDs piezoelectric emission.\nAmplitude and wavelength of the light detected at the output of the waveguide are both affected by the pressure applied on the waveguide.\nIn particular, the calibration experiment showed that the output signal increases in amplitude and blue shifts under compression.\nThese trends are confirmed by the simulated emission spectra of a single quantum dot under compression shown in Fig.\\ref{single}.\n\nAlthough the QDs optical waveguide can be considered a coupled system of multiple quantum dots (see for example \\cite{coupling}), the theoretical model was implemented for a single QD. \nThe accordance between experimental and simulation results suggests that the overall phenomenon leads to the same modulation effect of the analysed light properties.\nConsidering the wavelength range experimentally obtained, it can also be deduced by the simulation analysis that the regime of force sensed by each single QD contained in the waveguide is between [0-0.02]$\\mu$N.\n\nA ray-tracing simulation was also carried out to study the behaviour of the multiple quantum dots system of the doped waveguide.\nThe analysis of the possible events happening within the simulated waveguide core were categorized and the corresponding probability of occurrence were calculated, as shown in Fig.\\ref{events}.\nA statistical analysis of wavelength distribution and directional information of the simulated detected photons was also presented in Fig.\\ref{hist}.\n\nFinally, theoretical amplitude and wavelength shift of the measured photonic distribution measured at the detector as a function of the applied force was obtained in the case of force applied on two different location of the waveguide.\nBoth in the experimental and simulation results, the trends are more pronounced when the force is applied closer to the detector.\nThis can be explained by the fact that the closer to the detector this compressive force is applied, the less likely for a QD emitted ray to be scattered out of the waveguide, or absorbed and not re-emitted (these two events would reduce the amplitude variation), or to be absorbed and re-emitted by a subsequent QD (this event would reduce the wavelength variation).\nThis observation suggests that not only the pressure can be detected, but also the location of the applied force can be obtained, as both light amplitude and wavelength are affected differently through the scattering process.\n\nThe accordance between theoretical model and simulation results validates the presented sensing principle.\nThe calibration experiment demonstrates the feasibility of the proposed force sensor design.\nThe proposed prototype presents promising capabilities of measuring pressures and detecting their location.\nIn addition, all the main advantages inherited from the optical sensors potentially make it as a good candidate for several application fields such as nanobiotechnology and robotic sensing.\n\nThe main limitation of the prototype is the significant amount of light lost through the waveguide, mainly due to rays being scattered or emitted by quantum dots at angles which do not satisfy the total internal reflection requirements and thereby are transmitted through the cladding and lost.\nThe efficiency of the QDs waveguide should be further investigated and optimised. \nA study of the QDs concentration or a sensor design improvement (maybe involving additional layers of high refractive index materials which may collect and lead more light to the waveguide output) could help with reducing this drawback.\n\n\\begin{acknowledgement}\n\nCW was supported by grants EP/R02992X/1 and EP/R013977/1 from the UK Engineering and Physical Sciences Research Council (EPSRC).\n\n\\end{acknowledgement}\n\n\\begin{comment}\n\\begin{suppinfo}\n\nSUPPORTING MATERIALS\n\n\\end{suppinfo}\n\\end{comment}\n\n", "meta": {"timestamp": "2021-06-25T02:18:38", "yymm": "2106", "arxiv_id": "2106.13004", "language": "en", "url": "https://arxiv.org/abs/2106.13004"}}
{"text": "\\section*{To Do}\n\n\n\n\\section{Introduction and Motivation} \\label{sec:intro}\n\\subsection{Introduction and Related Work}\nSingular Value Decomposition (SVD)\n and its close relative, Principal Component Analysis (PCA), \nare well-known linear matrix factorisation  techniques that are widely used in applications as varied as dimension reduction \nand clustering, \nmatrix completion~\\citep{Davenport_2016} (e.g. for recommender systems),  dictionary learning~\\citep{tovsic2011dictionary} and time series analysis~\\citep{khoshrou2018data}.\nIn a surprising turn of events, (deep) matrix factorisation \nalso plays a role in the implicit regularisation that \nenables \nacceptable generalisation \nin deep learning~\\citep{gunasekar2017implicit}.\n\n\nIn their abstract version, SVD and PCA  amount to two different \nbut related types of matrix factorisation.  \nMore precisely, given a general (data) matrix $A$, the aim is \nto approximate it as a product of simpler \n(i.e. lower-rank) matrices. \nSpecifically: \n\\begin{itemize}\n    \\item PCA-type decomposition: $A \\approx P Q^T $  \n    where  the columns of $Q$ are orthonormal, i.e. $Q^T Q = I$;\n    \\item SVD-type decomposition: $ A \\approx  PBQ^T $\n    where $B$ is diagonal, while $P^TP = I, \\, \\,Q^TQ =I$.\n\\end{itemize}\nThe approximation in the above equations is \nmeasured in terms of the Frobenius (matrix) norm which \nfor an arbitrary matrix $X \\in \\mathbb{R}^{n\\times m}$ is \ndefined as: \n\\begin{equation}\n|\\!| X  |\\!|_F^2 = \\sum_{i=1}^n \\sum_{j = 1}^m  x_{ij}^2 = \\mbox{Tr}(XX^T) = \\mbox{Tr}(X^TX) = |\\!| X^T  |\\!|_F^2 .\n\\label{eq:Frobenius}\n\\end{equation}\n(In the remainder of the paper, we will drop the subscript $F$). \n\n\n\nAlthough these factorisation techniques are both conceptually \nsimple and effective, it is well-known that they are \nsensitive to noise and outliers in the input data. \nAs a consequence,  some modifications of the original \nalgorithms have been proposed to alleviate the effect of \nthese disturbances~\\citep{brooks2013pure,kwak2013principal}. \nCandes et al.~\\citep{candes2011robust} introduce  {\\it Robust PCA (RPCA)} which aims to \nseparate signal from outliers by decomposing  any given matrix into the sum of a low-rank approximation and a sparse matrix of outliers.\nAn extension of this work for inexact recovery of the data is presented in~\\citep{zhou2010stable}. \nAnother example of sparse PCA using low rank approximation is proposed in~\\citep{shen2008sparse}. \n\nAdding a regularisation term is another \nversatile  \nway to tackle the problem of noisy input.  For instance,  \nDumitrescu et al.~\\citep{dumitrescu2017regularized} \nshow how a regularized version of K-SVD algorithm can be adapted to the Dictionary Learning (DL) problem. \nHowever, the presence of noise in the input is not the only \nreason to invoke regularisation.\nRecent research~\\citep{jin2015low} shows that \nin many real data sets, it is  \nnot only the observed data that lie on a (non-)linear low dimensional manifold, but this \nalso applies to the features.\nHe et al.~\\citep{he2019graph} \npoint out that if the columns of the matrix $A$ \nare interpreted as data points, then the rows are features. \nThe neighbourhood structure of both the data points \nand the features give rise to distinct graphs (the \nso-called data \nand the feature graph) and hence,  to corresponding \ngraph \nLaplacians ($L_d$ and $L_f$ respectively).  \nThe resulting regularised PCA is referred to as \nthe {\\it  graph-dual Laplacian PCA }\n(gDLPCA) and for a given \ndata matrix $A$, is obtained \nby minimising the functional: \n\n\\begin{equation}\nJ(V,Y) = |\\!| A - VY|\\!|^2 \n     + \\alpha \\, \\mbox{Tr}(V^T L_d V) \n     + \\beta \\, \\mbox{Tr}(YL_f Y^T) \n     \\quad \\quad \\mbox{ subject to \\,} V^TV = I\n     \\label{eq:J_He_1}\n\\end{equation}\nThe ability of the \ngraph dual regularization technique\nto incorporate both data and feature structure\nhas deservedly attracted considerable attention in \ndimensionality \nreduction applications~\\citep{yin2015dual,shahid2016fast, he2019graph}.\n\nIn the present paper, we \ntake the functional.~\\eqref{eq:J_He_1} as a starting \npoint and investigate the two factorisation approaches \nmentioned above (invoking eq.~\\eqref{eq:Frobenius} \nto recast the trace as a norm): \n\n\\begin{itemize}\n    \\item PCA-type decomposition ($A \\approx PQ^T$) by minimising the regularisation  functional:\n    \n    \\begin{equation}\n     |\\!| A - PQ^T|\\!|^2 + \\lambda\\, |\\!| DP |\\!|^2  + \\mu\\,  |\\!| GQ |\\!|^2 \n   \\label{eq:pca_fac}\n   \\end{equation}\n    \n    \\item SVD-type decomposition ($ A \\approx  PBQ^T $) \n    by minimising the regularisation functional:\n     \\begin{equation}\n      |\\!| A - PBQ^T|\\!|^2 + \\lambda\\, |\\!| DP |\\!|^2  + \n   \\mu\\, |\\!| GQ |\\!|^2 \n   \\label{eq:svd_fac}\n   \\end{equation}\n  \n\\end{itemize}\nThe minimisation of the  functional~\\eqref{eq:pca_fac} \nwas discussed in \\citep{he2019graph}, but the proposed \nsolution contains an error which we correct in this paper. \nIn addition, we also provide an algorithm to solve functional~\\eqref{eq:svd_fac}, which somewhat surprisingly \nis quite different from the one for~\\eqref{eq:pca_fac}.\n\n\n\nThe remainder of this paper is organised as follows: \nWe finalise this section by recapitulating some important \nfacts facts about SVD. \nIn section~\\ref{sct:pca_type_reg} \nand \\ref{sct:svd_type_reg} we derive an algorithm \nfor minimisation of the regularised version of PCA-type \nand SVD-type\nfactorisation, respectively. \nIn section~\\ref{sct:comp_aspects} how gradient descent \ncan be implemented by drawing on some elementary facts \nfrom Lie-group theory.  Finally, we conclude \nby giving  some pointers to potential extensions.  \n\n\n\n\\subsection{Brief recap of Singular Value Decomposition (SVD)}\nFor the sake of completeness, we first recall the well-known  SVD result; for more details we refer to \nstandard textbooks such as~\\citep{strang1993introduction}\\citep{Horn1985}.\n\\begin{theorem}[{\\bf Singular Value Decomposition, SVD}] \n\\label{thm:svd_1}\nAny real-valued $n\\times m$  matrix $A$ can be factorized into the product of three matrices: \n\\begin{equation}\n   A = U S V^T   \\quad \\quad \\mbox{where}   \\quad \n   U \\in {\\cal O }(n) \\quad  \\mbox{and} \\quad  V  \\in {\\cal O }(m)  \\quad \n   \\mbox{are orthonormal, }\n   \\label{eq:svd_def}\n\\end{equation}\nand $S$ is an $n\\times m$ diagonal matrix where the elements on the main\n``diagonal'' (so-called {\\it singular values} ) are non-negative (i.e. $\\sigma_i:=S_{ii} \\geq 0$ for $1 \\leq i \\leq \n\\min(n,m)$). \\\\\nAssuming that the rank  $rk(A) = r \\leq \\min(n,m)$,  we can sort the \nsingular values such that\n$$\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r > 0 = \n\\sigma_{r+1} = \\ldots = \\sigma_{\\min(n,m)}$$\nand recast eq.~\\eqref{eq:svd_def} as \n\\begin{equation}\n     A  = \\sum\\limits_{i = 1}^r \\sigma_i U_i V_i^T    \\quad \\quad \n     \\mbox{where $U_i, V_i$ are the $i$-th columns of $U$ and $V$, \n     respectively.}\n     \\label{eq:svd_def_2}\n\\end{equation}\nFor the singular values sorted as above, \nwe introduce the short-hand \nnotation $U_{(1:k)}$  and $V_{(1:k)}$ to denote the matrix \ncomprising the first $k$ {\\it columns} of $U$ and $V$, \nrespectively: \n$$\nU_{(1:k)}  :=  [U_1, U_2, \\ldots, U_k] \\quad\\quad \n\\mbox{and} \\quad \\quad \nV_{(1:k)}  :=  [V_1, V_2, \\ldots, V_k] .\n$$\nIn this notation, eq.~\\eqref{eq:svd_def_2} can be \nexpressed concisely as: \n\\begin{equation}\n    A = U_{(1:r)} \\> diag(\\sigma_1, \\ldots , \\sigma_r) \n    \\> V_{(1:r)}^T. \n    \\label{eq:svd_def_3}\n\\end{equation}\n\n\\qed\n\\end{theorem}\nTo appreciate the significance of Theorem~\\ref{thm:svd_1}, it \nis helpful to   \nhighlight its geometric interpretation. \n Recall that any  $n\\times m$ matrix $A$ gives rise to a \n corresponding linear transformation \n  $ A:  \\mathbb{R}^m \\longrightarrow \\mathbb{R}^n  $\n   \n    that maps the standard basis in $\\mathbb{R}^m$ into the columns of $A$:\n    %\n    $$   A \\mathbf{e}_k  = A_k  \\quad \\quad \n    \\mbox{where} \\quad \\mathbf{e}_k = {(0, 0, \\ldots,0, 1,0,  \\ldots, 0)^T}.\n    $$\n    %\n    Roughly speaking, the SVD theorem therefore tells us that it is always possible to select  \n    an {\\it orthonormal } basis in $\\mathbb{R}^m$ (columns of $V$) that is \n    mapped (up to non-negative scaling factors, i.e. \n    the singular values) into an \n    {\\it orthonormal} basis in $\\mathbb{R}^n$ (columns of $U$). This is immediately \n    obvious from eq.~\\eqref{eq:svd_def_2}:\n    \n    $$ A V_{\\ell}  =  \\sum\\limits_{k = 1}^r \\sigma_kU_k V_k^T   V_{\\ell}   \n    = \\sum\\limits_{k = 1}^r \\sigma_k U_k \\delta_{k\\ell}  \n    =  \\sigma_{\\ell} U_{\\ell}. \n    $$\nwhere $\\delta_{k \\ell }$ is a Kronecker delta function.  \nIt is worth noting that insisting on the orthogonality of $V$ ($V^TV = I$)\nis not restrictive. \nIndeed, a linear transformation is completely and uniquely determined by specifying its \neffect on any basis, and there is no loss of generality \nby insisting on the orthonormality of this basis. \nHowever, the non-trivial message of this theorem is  \nthis  orthonormal basis ($V$) can be chosen \nin such a way that its image $U$ under $A$ is also orthonormal \n(again, up to non-negative scalings). \nFurthermore, \nin a generic case (where all singular values are different)   \nthe singular value decomposition is unique, up to an arbitrary \nrelabeling of the {\\it basis-vectors} and a simultaneous sign-flip \nof corresponding columns in $U$ and $V$, i.e. \n$(U_\\ell,V_\\ell) \\rightarrow (-U_\\ell,-V_\\ell) $  for any number of columns. \n\nThe importance of the SVD result, and the starting point for this paper, is the following well-known minimisation result (more details can be found in ~\\citep{golub2013matrix,eckart1936approximation}).\n\n\n\\begin{theorem}[{\\bf Eckart-Young-Mirsky Theorem:  Optimal low rank approximation}]\n\\label{thm:rank_approx}\nLet us consider an $n \\times m$ matrix $A$ with rank $rk(A) = r \\leq \\min(n,m)$. \nFor $k < r$,  finding the rank-$k$ matrix $A_k$ that is closest to $A$ in (Frobenius) norm gives rise to the following constrained minimisation problem: \n\\begin{equation*}\n    \\min_{A_k} |\\!| A - A_k|\\!|^2    \\quad \\quad \n\\mbox{subject to}  \\quad rk(A_k) \\leq  k.\n\\end{equation*}\nThe solution to this problem \nis obtained by truncating the SVD expansion eq.~\\eqref{eq:svd_def_2} \nafter the $k$-th largest singular value: \n\\begin{equation}\n     A_k  = \\sum\\limits_{i= 1}^k \\sigma_i U_i V_i^T  \n      =  U_{(1:k)} \\> diag(\\sigma_1, \\ldots , \\sigma_k) \n    \\>  V_{(1:k)}^T. \n\\end{equation}\n\\qed\n\\end{theorem}\nRecall that a rank-$k$ matrix of size $n\\times m$ can \nalways be written as a product $A_k = PQ^T$ where \n$P \\in \\mathbb{R}^{n \\times k}$ and \n$Q \\in \\mathbb{R}^{m \\times k}$ are matrices of full rank $k$. \nAgain, in this factorisation, there is no loss of \ngenerality in requiring $Q^T Q = I_k$.  In fact, \nit is necessary to remove indeterminacy due to arbitrary but trivial rescalings such as  $P \\longmapsto rP $ \nwhile $Q \\longmapsto (1/r)Q$ (with $r\\neq 0$), and the like.\nHence, one can reformulate Theorem~\\ref{thm:rank_approx} as the  factorisation result in Theorem~\\ref{thm:low_rank_factors}.\n\\begin{theorem}[{\\bf PCA-type factorisation}]\n\\label{thm:low_rank_factors}\nAssume that the $n \\times m$ matrix $A$ has rank $rk(A) = r \\leq \\min(n,m)$. \nWe now define the functional $G(P,Q)$ as follow:\n\\begin{equation} \nG(P,Q) = \n|\\!| A - PQ^T |\\!|^2\n\\label{eq:low_rank_factor_0}\n\\end{equation}\nand the corresponding constrained optimisation problem:\n\\begin{equation} \n\\min_{P, Q} G(P,Q)   \\quad \\quad \n\\mbox{subject to}  \\quad rk(P) = rk(Q) = k \\quad \n\\mbox{and} \\quad  Q^TQ  = I_k\n\\label{eq:low_rank_factor_1}\n\\end{equation}\nwhere $k < r$. A solution to the above constrained minimisation problem \n(in $P \\in \\mathbb{R}^{n \\times k}$ and \n$Q \\in \\mathbb{R}^{m \\times k}$)\nis given by (using the SVD notation given above): \n\\begin{equation} \nQ = V_{(1:k)}  \\quad\\quad \n\\mbox{and } \\quad \\quad \nP = U_{(1:k)}\\> diag(\\sigma_1, \\ldots , \\sigma_k)\n\\label{eq:low_rank_factor_2}\n\\end{equation}\nhence: \n\\begin{equation}\nPQ^T = \\sum\\limits_{i=1}^k \\sigma_i U_i V_i^T.\n\\label{eq:low_rank_factor_3}\n\\end{equation}\nFrom \\eqref{eq:low_rank_factor_2} \nthis it also follows that  $P^TP$ is diagonal, but not \nnecessarily equal to the identity. \n\\qed\n\\end{theorem}\nNote that If we drop the insistence on the diagonal form \n    for $P^T P$  (i.e. $P$ need no longer be an orthogonal frame), then \nthe solution is no longer unique.  Indeed, by taking any $ k \\times k$ orthogonal matrix $R$ with \n$R^T R  = I_k = RR^T$, it is clear that $P' = PR$  \nand $Q' = QR$ are also solutions. In this case: \n$Q'^T Q' = R^TQ^TQ R = I_k$ but $P'^T P' = R^T P^T P R \n= R^T (SS^T) R$ is in general a positive definite symmetric matrix. \n\n\n\n\\section{Regularisation for PCA-type \nfactorisation}\n\\label{sct:pca_type_reg}\n\n\\begin{comment}\n\\subsection{Main result}\n\nAs stated above, {\\it robust} low-rank matrix approximation techniques have received tremendous attention in the literature.\nThe family of such methods propose adding some regularization term(s) to the original problem in Theorems~\\ref{thm:rank_approx} and \\ref{thm:low_rank_factors}. \n\nAs mentioned before, the present paper is inspired by~\\citep{he2019graph}.\nThe reported work has formulated the optimal low rank approximation problem as follows:\n\\begin{equation}\n\\label{eq_10_He_paper}\n    \\min_{V,Y} \\quad |\\!| A - VY|\\!|^2 + \\alpha \\mbox{Tr}(V^TL^{(f)}V) + \\beta \\mbox{Tr}(YL^{(d)}Y^T) \\quad \\mbox{subject to } \\quad V^TV=I\n\\end{equation}\nwhere $(.)^{f}$ and $(.)^{d}$ denote the \\emph{feature} and \\emph{data} manifolds, respectively~\\citep{he2019graph}. However, it seems that section $4.2$ of the aforementioned paper contains erroneous results as the solution of conventional PCA ($Y=V^TX$) has been used to find the solution for eq.~\\eqref{eq_10_He_paper}.\nIn below, we address this problem from two perspectives of PCA-type and SVD-type solutions in full details.\n\\end{comment}\n\n\\subsection{Regularised PCA}\nThe following theorem outlines an obvious \ngeneralisation to the regularised version of the minimisation problem.\n\\begin{theorem}[{\\bf Regularised PCA}]\n\\label{thm:rank_approx_reg}\nLet $A$ be an $n\\times m$ matrix of rank $r \\leq \\min(n,m)$.  For $k \\leq r$,  let $P \\in \\mathbb{R}^{n\\times k}$ and $Q \\in \\mathbb{R}^{m\\times k} $ full rank matrices (i.e. of rank $k$).  Furthermore, \nfor arbitrary (non-zero) integers $d$ and $g$\nwe introduce \nregularisation matrices $D \\in \\mathbb{R}^{d\\times n}$ \nand $G \\in \\mathbb{R}^{g\\times m}$,   \nas well as  weights $\\lambda,\\mu \\geq 0$. \n\nWe now define the following functional $F$ in the \nvariables $P$ and $Q$:\n\\begin{equation}\nF(P,Q) =  |\\!| A - PQ^T|\\!|^2 + \\lambda\\, |\\!| DP |\\!|^2  \n  + \\mu\\, |\\!| GQ |\\!|^2 \n\\label{eq:functional_factor_full} \n\\end{equation}\nand pose the corresponding {constrained} optimisation problem: \n\\begin{equation}\n\\min_{P,Q}F(P,Q) \\quad \\quad \n\\mbox{subject to} \\quad \\quad Q^TQ = I_k.\n\\label{eq:functional_2_lambda_mu} \n\\end{equation}\nIntroducing short-hand notation \n$ L := D^T D \\in \\mathbb{R}^{n\\times n}$ and \n$M := G^T G  \\in \\mathbb{R}^{m\\times m}$ (both symmetric and positive semi-definite), the solution of the constrained \noptimisation problem \\eqref{eq:functional_2_lambda_mu} is constructed as follows: \n\\begin{itemize}\n    \\item The $k$ columns of the $m\\times k$ matrix $Q$ are the eigenvectors of the $m\\times m$ matrix:\n    $$ K:= A^T(I_n + \\lambda L)^{-1}  A  -  \\mu \\,M$$\n   \n    corresponding to the $k$ largest eigenvalues;\n    \\item Furthermore:  $P = (I_n + \\lambda L)^{-1}  AQ$\n\\end{itemize}\n\\end{theorem}\nFor the sake of completeness, we reiterate that the condition \n$Q^T Q = I_k$ is not restrictive but necessary to eliminate \narbitrary rescalings. \nIn passing, we point out that result above corrects an error \nin \\citep{he2019graph} where it is incorrectly stated that $P = AQ$.\n\\begin{comment}\n\\begin{verbatim}  \nNotes on this can be found in rSVD2 . \nSoftware experiments in it s in rsvd_supplement_low_rank_factor_1.m\n\\end{verbatim}\n\\end{comment}\n\\begin{proof}\n\\label{sct:main_proof}\n\nSince the variable $P$ in the \nfunctional~\\eqref{eq:functional_factor_full} \nin unconstrained, we can identify the optimum \nin $P$ (for fixed $Q$) by computing \nthe gradient: \n\\begin{eqnarray}\n{\\displaystyle \\frac{1}{2} \\nabla_P  F} &=& (PQ^T -A)Q + \\lambda D^TD P\n\\end{eqnarray}\nand solving for $P$:\n\\begin{equation}\n\\nabla_P F =0 \\quad \\Rightarrow \\quad  P \\underbrace{Q^TQ}_{I_k} - AQ + \\lambda L P=0  \\quad \\Rightarrow \\quad\n    (I_k + \\lambda L)P = AQ  .\n    \\label{eq_solution_P}\n\\end{equation}\nThis condition needs to hold at the solution point. \nBy first re-writing $F(P,Q)$ formula as the trace of matrices and then plugging in~\\eqref{eq_solution_P}, we have:\n\\begin{eqnarray*}\nF(P,Q) &=& \\mbox{Tr}\\left[ (A-PQ^T)(A^T-QP^T) \\right] + \\lambda \\> \\mbox{Tr}(P^T{L}P) +  \\mu \\,\\mbox{Tr}(Q^T M Q)\\\\\n&=&\\mbox{Tr}\\, \\left[ \nAA^T- AQP^T -P Q^TA^T +P Q^TQ P^T \\right] + \\lambda \\> \\mbox{Tr} (P^TLP)\n+  \\mu \\,\\mbox{Tr}(Q^T M Q)\n\\end{eqnarray*}\nConsidering the fact that the trace operator is invariant under transposition \nas well as cyclic permutation, \nand plugging in eq.~\\eqref{eq_solution_P} we arrive at: \n\\begin{eqnarray}\nF(P,Q) &=& \\mbox{Tr}\\left[ AA^T -2(I_n+\\lambda L)PP^T+PP^T \\right] + \\lambda \\> \\mbox{Tr}(P^TLP) +  \\mu \\,\\mbox{Tr}(Q^T M Q) \\nonumber \\\\\n&=& \\mbox{Tr} \\left( AA^T - PP^T -2 \\lambda L PP^T \\right) + \\lambda \\> \\mbox{Tr}(P^TLP) +  \\mu \\, \\mbox{Tr}(Q^T M Q) \\nonumber\\\\\n&=& \\mbox{Tr}(AA^T) - \\mbox{Tr}(PP^T) -2 \\lambda \\> \\mbox{Tr}(LPP^T) + \\lambda \\> \\mbox{Tr}(P^TLP) +  \\mu \\,\\mbox{Tr}(Q^T M Q) \\nonumber \\\\\n &=& \\mbox{Tr}(AA^T) - \\mbox{Tr}(P^TP) -\\lambda \\> \\mbox{Tr}(P^TLP)\n    +  \\mu \\,\\mbox{Tr}(Q^T M Q) \\nonumber\\\\\n    &=& \\mbox{Tr}(AA^T) - \\mbox{Tr}\\left[ P^T \\underbrace{(I_n+\\lambda L)P}_{AQ}\\right] +  \\mu \\,\\mbox{Tr}(Q^T M Q).\n\\end{eqnarray}\nExtracting $P$ and its transpose from eq.~\\eqref{eq_solution_P}:  \n\\begin{equation}\n    P=(I_n+\\lambda L)^{-1}AQ \\quad \\Rightarrow  \\quad \n    P^T = Q^TA^T (I_n+\\lambda L)^{-1}\\quad \\quad \\text{ as $L$ is symmetric}\n    \\label{eq_P}\n\\end{equation}\nwe arrive at:\n\\begin{equation}\n    F(P,Q) = \\mbox{Tr}(AA^T)-\\mbox{Tr}\\left[ Q^T \\left(A^T(I_n+\\lambda L)^{-1}A - \\mu M \\right)Q\\right].\n\\end{equation}\nTherefore, in order to minimize $F$, one must maximize the right-most term \nas $\\mbox{Tr}(AA^T)$ is a constant.  \nThis is achieved by selecting for $Q$, eigenvectors corresponding to the \n$k$ largest eigenvalues of \n$(A^T(I_n+\\lambda L)^{-1}A -\\mu M)$. \nOnce $Q$ is determined, $P$ is obtained via eq.~\\eqref{eq_P}. \n\nAs a concluding remark, we  point out that the matrix \n$I_n + \\lambda L$ is always  invertible.\nIndeed, since $L=D^TD$ is positive semi-definite and symmetric, it has a complete set of eigenvectors \nwith corresponding non-negative eigenvalues, i.e., $L=W\\Lambda W^T$, where $W$ is orthogonal \n(i.e. $W^T W = WW^T = I_n$) \nand $\\Lambda \\geq 0$. \nHence, the matrix\n$(I_n + \\lambda L)$\nhas strictly positive diagonal elements, \nand is indeed invertible. \n\\end{proof}\nSome illustrative numerical experiments can be found~\\citep{code_theorem_4}.\n\n\n\\subsection{Some special cases}\n\\label{sct:some_special_cases}\n\n\\begin{itemize}\n    \\item \\framebox{$\\lambda = 0$ and $\\mu =0$}:\\quad In that case, $Q$ comprises the first $k$ eigenvectors of \n    $  K = A^TA $ and  $P = AQ$, which means that \n    we end up with the standard SVD, as expected. Some \n    numerical experiments can be found~\\citep{code_special_case_mu_0_lambda_0}. \n    \n   \n    \n    \\item \\framebox{$D = I_n$ and $\\mu = 0 $}:\\quad The following section provides an overview of the results in~\\citep{dumitrescu2017regularized} where a regularized K-SVD problem is addressed. \n    In the aforementioned paper, the authors consider a special case, where  $\\mu = 0 $ and $D = I_n$.  Since this \nimplies that $L = D^T D = I_n$ and $\\mu M = 0$, the matrix $K$ simplifies \nto \n$$ K = \\frac{1}{1+\\lambda}\\,A^TA $$\nThe eigenvectors of $K$ \nare therefore the right singular \nvectors of $A$ (i.e. the eigenvectors of $A^TA$). \nHence $Q = V_{(1:k)}$, and as a result: \n$$  P = \\frac{1}{1 + \\lambda}AQ\\quad and\\quad AQ = U_{(1:k)}\\, diag(\\sigma_1, \\ldots , \\sigma_k) . $$  \n\n\nIn particular, for $k=1$ (the rank-$1$ reconstruction), we obtain: \n$$ Q = \\mathbf{v}_1  \\quad \\quad \\mbox{and} \\quad\\quad \nP = \\frac{\\sigma_1}{1+\\lambda} \\,\\mathbf{u}_1$$\nwhich is the result that can be found in \\citep{dumitrescu2017regularized}. The experiments are available in~\\citep{code_special_case_mu_0_D_In}.\n\n\n\\end{itemize}\n\n\n\n\\section{Regularisation for SVD-type \nfactorisation}\n\\label{sct:svd_type_reg}\nWe now turn our attention to the SVD-type factorisation  \nwhich looks for an approximation of the form: \n\\begin{equation*}\n   A \\approx PBQ^T  \\quad\\quad \n\\mbox{subject to:}  \\quad \n \\quad   Q^TQ = I_k, \\quad  \\,|\\!| P_i|\\!|=1 \\,\\,\n \\mbox{$\\forall i\\in \\{1,2,\\ldots, k\\}$, \\, and  $B$ diagonal.} \n\\end{equation*}\nLoosely speaking, since the columns of $P$ and $Q$ are \nof unit length, they only pins down the structure of \n$A$, whereas  the diagonal matrix $B=diag(\\beta_1, \\beta_2, \\ldots, \\beta_k)$ captures the \\emph{amplitude} \nof the corresponding structures. \nSimilar to before, the columns of $Q$ are orthonormal, i.e., we again insist on $Q^TQ = I_k$.\nHowever, unlike before, the columns of $P$ are now \nonly required to have unit length. \n\nIn light of the aforementioned SVD-type matrix factorisation technique, Theorems~\\ref{thm_rsvd_mu_0} and~\\ref{thm_rsvd_mu_not_0} provide an alternative solution to the lower-rank matrix approximation problem.\nFor notational convenience,  \nTheorem~\\ref{thm_rsvd_mu_0} first addresses the \nsimplified case for $\\mu=0$. \nFinally, in \nTheorem~\\ref{thm_rsvd_mu_not_0} \nwe return to the general case. \n\\begin{theorem}[{\\bf Regularised SVD}]\n\\label{thm_rsvd_mu_0}\nLet $A$ be an $n\\times m$ matrix of rank $r \\leq \\min(n,m)$.  For $k \\leq r$,  let $P \\in \\mathbb{R}^{n\\times k}$ and $Q \\in \\mathbb{R}^{m\\times k} $ of rank $k$, \nwhile $B \\in \\mathbb{R}^{k\\times k} $ diagonal \n(i.e.  $ B = diag(\\beta_1, \\beta_2, \\ldots, \\beta_k)  $).  Furthermore, \nfor arbitrary non-zero integer $d$ \nwe introduce \nregularisation matrix $D \\in \\mathbb{R}^{d\\times n}$, \nas well as  weight $\\lambda \\geq 0$. \nFinally, we introduce the short-hand notation \n$ L := D^T D \\in \\mathbb{R}^{n\\times n}$ \n(symmetric and positive-definite).\nWe are now in a position to define the following functional $F$ in the \nvariables $P, Q$ and $B$:\n\\begin{equation}\nF(P,Q, B) =  |\\!| A - P B Q^T|\\!|^2 + \\lambda\\, |\\!| DP |\\!|^2,   \n\n\\label{eq:functional_1} \n\\end{equation}\nand the corresponding constrained optimisation problem: \n\\begin{equation}\n\\min_{P,Q, B}F(P,Q, B) \\quad \\quad \n\\mbox{subject to:}  \\quad \n \\quad Q^TQ = I_k, \\quad  \\,|\\!| P_i|\\!|=1 \\,\\,\n\\mbox{$\\forall i\\in \\{1,2, \\ldots,k\\}$, \\, and  $B$ diagonal.} \n\\label{eq:functional_rsvd} \n\\end{equation}\nThis problem is solved by the solution Algorithm~\\ref{algorithm_mu_0}\nspecified below.  \n\\\\\n\\begin{algorithm}[H]\n\\SetAlgoLined\n\\KwIn{$A,\\> k,\\> \\lambda, \\> D $}\n\\KwOut{$P,\\> B,\\> Q $}\n Initialization\\\\\n \\While{no convergence}{\n\\begin{enumerate}\n    \\item Determine the $m\\times k$ matrix $Q = [\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_k]$ \n    (with orthonormal columns: $Q^TQ = I_k$) \n    such that the sum of the \n    smallest eigenvalue of each of the $k$ symmetric matrices \n    $S(\\mathbf{q}_i) $ is minimal, i.e.:\n    %\n    $$ \\min_{Q} \\psi(Q) =  \\min_{Q} \\sum_{i=1}^k  \\lambda_1(\\mathbf{q}_i) \\quad\\quad \\mbox{such that } \n    Q^TQ=I_k$$\n    \n    where $\\lambda_1(\\mathbf{q}_i) = \\min (eig(S(\\mathbf{q}_i))$. To this end we use gradient descent (see Section~\\ref{sct:comp_aspects}). \n    \\item  For each $\\mathbf{q}_i$ as determined above, take $\\mathbf{p}_i$  \n    to be the eigenvector $W_1(\\mathbf{q}_i)$ corresponding \n    to the smallest eigenvector $\\lambda_1(\\mathbf{q}_i)$. \n    Construct the $n\\times k$ matrix \n     $P = [\\mathbf{p}_1, \\mathbf{p}_2, \\ldots, \\mathbf{p}_k]$. \n    \\item Finally, set $B = diag(\\beta_1, \\ldots, \\beta_n)$ where  $\\beta_i = (P^TAQ)_{ii}$.\n\\end{enumerate}\n }\n \\caption{Proposed RSVD method ($\\mu = 0$)}\n \\label{algorithm_mu_0}\n\\end{algorithm}\n\n\n    \n    \n    \n    \n    \n\n\\end{theorem}\n\\begin{proof}\nSince $B$ is unconstrained, we can determine its optimal value \nby computing the derivative with respect to $B$ and \nequating it to zero:\n\\begin{equation}\n\\nabla_B F(P,Q,B) = \\nabla_B  |\\!| A - P B Q^T|\\!|^2 . \n\\label{eq:nabla_B_1}\n\\end{equation}\nExpanding the norm in terms of a trace \n(cf. eq.~\\eqref{eq:Frobenius}), \nand using the invariance of a trace under transposition, \nwe arrive at (recall $Q^TQ = I_k$ ):\n\\begin{eqnarray} \n|\\!| A - P B Q^T|\\!|^2 & = & \n\\mbox{Tr}\\left[ (A - P B Q^T)(A^T - Q B P^T) \\right]   \\nonumber \\\\\n&=& \\mbox{Tr} (AA^T) - 2 \\mbox{Tr}(AQBP^T)  + \\mbox{Tr}(P B^2P^T)  \\nonumber \\\\[1ex]\n&=& |\\!| A |\\!|^2   - 2\\, \\mbox{\\mbox{Tr}}(P^TAQB)  \n+ \\mbox{\\mbox{Tr}}(B^2 P^TP)\n\\nonumber \\\\\n& = & |\\!| A |\\!|^2   - 2\\,\\sum_{i=1}^k (P^TAQ)_{ii}\\, \\beta_i  \n+ \\sum_{i=1}^k (P^TP)_{ii}\\,\\beta_i^2 \\quad \\quad \\text{($B$ is diagonal)} \\\\\n& = & |\\!| A |\\!|^2   - 2\\,\\sum_{i=1}^k (P^TAQ)_{ii}\\, \\beta_i  \n+ \\sum_{i=1}^k \\beta_i^2 \\quad \\quad \\text{($|\\!| P_i |\\!|=1 \\Rightarrow  (P^TP)_{ii} = 1$).}\n\\label{eq:functional_beta}\n\\end{eqnarray}\nWe therefore calculate the gradient of the functional $F$ with respect to $B$ as follow:\n$$\n\\frac{\\partial}{\\partial \\beta_i}\\,  |\\!| A - P B Q^T|\\!|^2  = \n2\\, ( \\beta_i  -  (P^TAQ)_{ii}).  \n$$\nFor given $P$ and $Q$, we find the optimal $B$ by insisting that \nthe resulting gradient vanishes, which yields: \n\\begin{equation}\n    \\beta_i = (P^TAQ)_{ii}  \\quad \\quad \\forall i \\in \\{1,2, \\ldots, k\\}.\n    \\label{eq:beta_optimal}\n\\end{equation}\nPlugging this optimal choice  back into eq.~\\eqref{eq:functional_beta}\nthe functional~\\eqref{eq:functional_1} simplifies to \n\\begin{equation}\n|\\!| A - P B Q^T|\\!|^2  =  |\\!| A|\\!|^2 - \\sum_{i = 1}^k \\beta_i^2 \n\\label{eq:beta_2}\n\\end{equation}\nTo recast eq.~\\eqref{eq:beta_2} in terms of $P$ and $Q$ \n(in order to eliminate $B$), we observe that for an arbitrary matrix $H$ \nwe have \n$ H_{ij} = \\mathbf{e}_i^T H  \\mathbf{e}_j $,   where \n$\\mathbf{e}_i = (0,0,\\ldots, 1, \\ldots, 0)^T$ are the standard basis \nvectors. \nHence,  using \nthe fact \nthat the diagonal of a matrix is unchanged under \ntransposition,  we conclude that \n$$ \\beta_i = \n\\left\\{  \\begin{array}{lcl}\n(P^TAQ)_{ii} &=&  \\mathbf{e}_i^T P^T A Q\\,\\mathbf{e}_i = \n\\mathbf{p}_i^T  A \\mathbf{q}_i\\\\[2ex]\n(Q^TA^TP)_{ii} &=&  \\mathbf{e}_i^T Q^T A^T P \\,\\mathbf{e}_i = \n\\mathbf{q}_i^T  A^T \\mathbf{p}_i\n\\end{array}\n\\right.\n$$\nwhere $\\mathbf{p}_i, \\mathbf{q}_i$ are the $i$-th columns of $P$ and \n$Q$, respectively.  \ni.e. \n $P = [\\mathbf{p}_1, \\mathbf{p}_2, \\ldots, \\mathbf{p}_k]$  and \n$Q = [\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_k]$. \nAs a consequence, \n\\begin{equation}\n\\sum_{i = 1}^k \\beta_i^2 =    \n\\sum_{i = 1}^k  \\mathbf{p}_i^T  A \\mathbf{q}_i \\, \\mathbf{q}_i^T  A^T \\mathbf{p}_i. \n\\label{eq:beta_3}\n\\end{equation}\nAs a final step, we introduce the notation  $L = D^T D$ to recast the \nregularisation term as: \n\\begin{equation}\n    |\\!| DP |\\!|^2 = \\mbox{Tr}(P^T L P) \n    = \\sum_{i=1}^k  \\mathbf{e}_i^T P^T L P\\, \\mathbf{e}_i \n    = \\sum_{i=1}^k \\mathbf{p}_i^T  L \\, \\mathbf{p}_i.\n\\label{eq:reg_1}    \n\\end{equation}\nPlugging eqs.~(\\ref{eq:beta_3}) and (\\ref{eq:reg_1}) \ninto eq.~(\\ref{eq:functional_1}), we obtain the following simplified \nform for the functional $F$ (assuming that we eliminate $B$ \nby using its optimal value): \n\n\\begin{equation}\nF(P,Q) = |\\!| A|\\!|^2 + F_1(P,Q), \\quad\\quad \n\\mbox{where} \\quad \\quad \n F_1(P,Q) =\n\\sum_{i=1}^k  \\mathbf{p}_i^T(\\lambda L - A\\mathbf{q}_i\\mathbf{q}_i^TA^T)\\mathbf{p}_i. \n    \\label{eq:functional_2}\n\\end{equation}\nIntroducing the notation $ S(\\mathbf{q}) := \\lambda L  - A\\mathbf{q}\\mathbf{q}^TA^T  $, \nwe conclude that  \n$$ F_1(P,Q) = \\sum_{i=1}^k  \\mathbf{p}_i^T \nS(\\mathbf{q}_i)\\mathbf{p}_i. $$\nSince each $S(\\mathbf{q})$ is a symmetric matrix, it \ncan be diagonalised with respect to  \nan orthonormal basis, i.e. \nthere is an orthogonal $n \\times n $ \nmatrix $W$ (with $W^T W = WW^T = I_n$) \nand a diagonal matrix \n$\\Lambda = diag(\\lambda_1, \\ldots, \\lambda_n)$  (ordered \n$\\lambda_1 \\leq \\lambda_2 \\leq \\ldots \\leq \\lambda_n$), \nboth depending on $\\mathbf{q}$\nsuch that \n$$ S(\\mathbf{q}) = W(\\mathbf{q}) \\Lambda (\\mathbf{q}) W(\\mathbf{q})^T ,  $$\ni.e. the columns of $W$ \nare the eigenvectors of $S(\\mathbf{q})$, \nwith the corresponding eigenvalues on the diagonal of $\\Lambda$. \nBy introducing the notation $\\lambda_1(S(\\mathbf{q}))$ to denote \nthe smallest eigenvalue of $\\Lambda(\\mathbf{q})$,\nwe obtain the minimal value $\\mathbf{p}_i^T S(\\mathbf{q}_i) \\mathbf{p}_i= \\lambda_1(\\mathbf{q}_i)$ \nwhen choosing $\\mathbf{p}_i$ to be the (unit) eigenvector ($W_1(\\mathbf{\\mathbf{q}}_i)$)\ncorresponding to the smallest eigenvalue. \nAs a consequence, the solution strategy boils down to steps in Algorithm~\\ref{algorithm_mu_0}.\n\n\nThis choice of $P, Q$ and $B$ solves the constrained \nminimisation problem~\\eqref{eq:functional_rsvd}.  \nNotice that due to the fact that $P$ and $B$ matrices are determined after finding $Q$, this optimisation problem can essentially be translated into a search in the space of $Q$ matrices. \nSome illustrative numerical experiments are available at~\\citep{code_theorem_5_6}.\n\n\\end{proof}\nWe conclude this section by giving a slightly more general \nversion ($\\mu \\neq 0$) of the previous theorem, thus re-establishing the \nsymmetry between $P$ and $Q$.  \n\\begin{theorem}[{\\bf Regularised SVD, symmetric version}]\n\\label{thm_rsvd_mu_not_0}\nLet $A$ be an $n\\times m$ matrix of rank $r \\leq \\min(n,m)$.  For $k \\leq r$,  let $P \\in \\mathbb{R}^{n\\times k}$ and $Q \\in \\mathbb{R}^{m\\times k} $ of rank $k$, \nwhile $B \\in \\mathbb{R}^{k\\times k} $ diagonal \n(i.e.  $ B = diag(\\beta_1, \\beta_2, \\ldots, \\beta_k)  $).  Furthermore, \nfor arbitrary non-zero integers $d$ and $g$\nwe introduce \nregularisation matrices $D \\in \\mathbb{R}^{d\\times n}$, \nand $G \\in \\mathbb{R}^{g\\times m}$,   \nas well as  weights $\\lambda, \\mu \\geq 0$. \nFinally, we introduce the short-hand notation \n$ L := D^T D \\in \\mathbb{R}^{n\\times n}$ \nand $M := G^T G  \\in \\mathbb{R}^{m\\times m}$ \nsymmetric and positive-definite).\nWe are now in a position to define the following functional $F$ in the \nvariables $P, Q$ and $B$:\n\\begin{equation}\nF(P,Q, B) =  |\\!| A - P B Q^T|\\!|^2 + \\lambda\\, |\\!| DP |\\!|^2 \n + \\mu\\, |\\!| GQ |\\!|^2 \n\\label{eq:functional_1_full} \n\\end{equation}\nand the corresponding constrained optimisation problem: \n\\begin{equation}\n\\min_{P,Q, B}F(P,Q, B) \\quad \\quad \n\\mbox{subject to:}  \\quad \n \\quad   Q^TQ = I_k, \\quad  \\,|\\!| P_i|\\!|=1, \\,\\,\n \\mbox{$\\forall i\\in \\{1,2,\\ldots, k\\}$ \\, and  $B$ diagonal.}\n\\label{eq:functional_rsvd_1} \n\\end{equation}\nThis problem is solved by the solution\nspecified in Algorithm~\\ref{lag_mu_not_zero}.  \n\\end{theorem}\n\n\\begin{proof}\nUsing the notation introduced above and in Theorem~\\ref{thm_rsvd_mu_0}, we see that \n$$ |\\!| GQ |\\!|^2 = \\mbox{Tr}(Q^T M Q) \n= \\sum_{i = 1}^k \\mathbf{q}_i^T M  \\mathbf{q}_i . $$\nHence, the functional (\\ref{eq:functional_1_full}) can be recast as: \n\\begin{equation}\nF(P,Q) = |\\!| A|\\!|^2 + F_2(P,Q), \\quad\\quad \n\\mbox{where} \\quad \\quad \n F_2(P,Q) =\n\\sum_{i=1}^k  \\mathbf{p}_i^T(\\lambda L - A\\mathbf{q}_i\\mathbf{q}_i^TA^T)\\mathbf{p}_i \n+ \\mu\\,\\sum_{i = 1}^k \\mathbf{q}_i^T M  \\mathbf{q}_i  .\n    \\label{eq:functional_2_full}\n\\end{equation}\nThe minimum of each term in the first summation \nin $F_2$ \nis equal to the smallest \neigenvalue $\\lambda_1(S(\\mathbf{q}_i))$. \nFinding the minimum for the constrained optimisation problem (\\ref{eq:functional_rsvd_1}) therefore amounts to \nfinding the minimum of the functional:\n\\begin{equation}\n    \\psi(Q) := \\sum_{i = 1}^k  \\left(\\lambda_1(S(\\mathbf{q}_i)) \n    + \\mu \\,\\mathbf{q}_i^T M  \\mathbf{q}_i) \\right) \n   \n\\label{eq:min_psi}\n\\end{equation}\nsubject to the constraint $Q^T Q = I_k$. \nTherefore, the minimisation problem again calls for \na minimisation in $Q$ space, as the optimal choice for $P$ (corresponding \neigen-vectors) follows automatically. We therefore arrive \nat the following  Algorithm~\\ref{lag_mu_not_zero}. Some illustrative numerical examples are available in~\\citep{code_theorem_5_6}.\n \n\\end{proof}\n\n\\begin{algorithm}[H]\n\\SetAlgoLined\n\\KwIn{$A,\\> k,\\> \\mu,\\> \\lambda, \\> D, \\> G$}\n\\KwOut{$P,\\> B,\\> Q $}\n Initialization\\\\\n \\While{no convergence}{\n  \\begin{enumerate}\n    \\item Recall that for \n    any unit vector $\\mathbf{q} \\in \\mathbb{R}^m$ we define $S(\\mathbf{q}) = \\lambda L - Aqq^TA^T$.  Since this is a symmetric \n    $n\\times n$\n    matrix, it has a complete set of eigenvectors and corresponding eigenvalues. \n    Denote the smallest eigenvalue of each $S(\\mathbf{q}_i)$  as \n    $\\lambda_1(S(\\mathbf{q}_i))$.\n    \n    \\item For a given  $m\\times k$ matrix $Q = [\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_k]$ \n    (with orthonormal columns: $Q^TQ = I_k$) \n    compute the functional: \n    %\n    $$ \\psi(Q) := \\sum_{i = 1}^k  \\left(\\lambda_1(S(\\mathbf{q}_i)) \n    + \\mu \\,\\mathbf{q}_i^T M  \\mathbf{q}_i) \\right)\n    $$\n    and use gradient descent (on the compact \\textit{torus domain}, \n    see section~\\ref{sct:comp_aspects}) to \n    find the minimum. \n   \n    \\item  For each $\\mathbf{q}_i$ as determined above, take $\\mathbf{p}_i$  \n    to be the eigenvector $W_1(\\mathbf{q}_i)$ corresponding \n    to the smallest eigenvector $\\lambda_1(S(\\mathbf{q}_i))$. \n    Construct the $n\\times k$ matrix \n     $P = [\\mathbf{p}_1, \\mathbf{p}_2, \\ldots, \\mathbf{p}_k]$. \n     \n    \\item Finally, set $B = diag(\\beta_1, \\ldots, \\beta_n)$ where  $\\beta_i = (P^TAQ)_{ii}$.\n\\end{enumerate}\n\n }\n \\caption{Proposed RSVD method ($\\mu \\neq  0$)}\n \\label{lag_mu_not_zero}\n\\end{algorithm}\n\n\n\n\n\\section{Computational Aspects}\n\\label{sct:comp_aspects}\n\\subsection{Gradient Descent on the Unitary Domain}\nFrom Algorithm~\\ref{lag_mu_not_zero} it becomes clear that \nthe full regularisation problem \ncan be reduced to the simpler \nconstrained minimisation problem \ndetailed in eq.~(\\ref{eq:min_psi}).  Since \nthe $\\psi$-functional is smooth \non a compact domain,  \nthis minimum is guaranteed to \nexist and one can use gradient descent to \nlocate it. However, gradient descent \nneeds to respect the constraint $Q^T Q = I_k$. This can \nbe achieved by applying orthogonal transformations \nto the current $Q$ matrix, as this will \npreserve orthonormality. Specifically, recall all \northogonal $m\\times m$ matrices with determinant equal to 1 \n(rather than $-1$),  \nconstitute a multiplicative group denoted as\n$\\mathcal{SO}(m)$ and formally  defined as: \n\n\\begin{equation*}\n    \\mathcal{SO}(m) = \\left\\{  R \\in \\mathbb{R}^{m\\times m} \\,\\,|\\,\\, \nR R^T = I_m = R^T R, \\quad \\mbox{and} \n\\quad \\det (R) = 1\n\\right\\}\n\\end{equation*}\nIt is then straightforward to check that \nfor any  $R \\in \\mathcal{SO}(m)$, it holds that if \n$\\bar{Q} = RQ$, \nthe condition $Q^T Q = I_k$ implies that \n $\\bar{Q}^T\\bar{Q} = I_k$.  It therefore follows \n that we can generate the ``\\textit{infinitesimal \n variations}\" needed to compute the gradient \n $\\nabla_Q \\> \\psi(Q)$ by applying  ``sufficiently \n small'' orthogonal matrices to the \n current value of $Q$. \n %\n More precisely, we draw on the fact that \n $\\mathcal{SO}(m)$ is actually a Lie-group \\citep{iserles2000lie}\n and that therefore each $R \\in \\mathcal{SO}(m)$ \n can be generated by exponentiating \n an element from its Lie-algebra \n $  so(m) = \\left\\{ K \\in \\mathbb{R}^{m \\times m} \\,\\,|\\,\\, K^T = -K\\right\\}$ (the skew-symmetric matrices):   \n %\n\\begin{equation*}\n    R = \\exp(tK) \\equiv I_m + tK + \\frac{1}{2!}t^2K^2 +\\ldots \n+  \\frac{1}{n!}t^n K^n + \\ldots  \\quad\\quad \n\\mbox{(with $K^T = -K$)}\n\\end{equation*}\nBy choosing $t$ sufficiently small, one obtains an orthogonal \ntransformation that is close to the identity $I_m$. \nFurthermore, it suffices to restrict the variations to \northogonal transformations that result from exponentiating \na basis \nfor the space of skew-symmetric matrices. \nSuch a basis is provided by  \nthe $m(m-1)/2$\nskew-symmetric matrices $K_{ij}$ \n(where $1 \\leq i < j \\leq m$) for which the \nmatrix element $k,\\ell$ is given by: \n\\begin{equation*}\n    K_{ij}(k,\\ell) = \\left\\{\n\\begin{array}{rcl}\n    1 & \\mbox{if} & k = i, \\, \\ell = j  \\\\\n    -1 & \\mbox{if} & k = j,\\, \\ell = i  \\\\\n    0 & & \\mbox{otherwise}\n\\end{array}\n\\right.\n\\end{equation*}\nGiven the current value $Q_0$, \nwe construct nearby \nvalues for $Q$  by looping over $K_{12}, \nK_{13}, K_{23}, \\ldots etc$ \nand constructing the corresponding orthogonal matrices  \n$R_{12}(t) = \\exp(tK_{12}),\\ldots, etc$. \nDenoting these ``\\emph{infinitesimal}'' rotation \nmatrices as $R_{\\alpha}$ (where \n$\\alpha = 1,\\ldots, m(m-1)/2$),  \nwe see that the partial derivatives with \nrespect to these rotations can be estimated \nas: \n\\begin{equation*}\n    \\frac{ \\partial \\psi(Q)}{\\partial R_\\alpha} \n\\approx \\frac{\\psi(R_\\alpha(t) Q_0) - \\psi(Q_0)}{t} \n\\quad \\quad \\quad \\quad \\mbox{(for $t$ sufficiently small). }\n\\end{equation*}\nFrom these results we can select the \ninfinitesimal rotation that results in the \nsteepest descent.  \n\nSince computing  $\\psi$ is computationally expensive \n(it requires determining eigenvalues) a viable alternative \nto computing the gradient, is random descent: \ngenerate random rotations \n(by exponentiating random skew matrices) and check\nwhether they result in a lower $\\psi$-value.  As \nsoon as one is found, proceed in that direction, and \nrepeat the process. \n\n\n\n\n\\subsection{Illustrative example:  Smoothing a noisy matrix}\n\nAs common in the literature e.g.,~\\citep{jin2015low, he2019graph, gavish2014optimal}, we start from the assumption that \nthe $n\\times m$ data matrix $A$ has a relatively smooth underlying structure \nthat is corrupted by noise: \n$$  A = \\mathbf{u} \\mathbf{v}^T + \\tau Z,  $$\nwhere the $n\\times m$ matrix $Z $ has independent standard \nnormal entries, and $\\tau$ controls the size of the noise. \n\nTo recover the underlying \"signals\"  $ \\mathbf{u}$ \nand  $\\mathbf{v}$,  \nwe minimise the SVD-type regularisation \nfunctional~\\eqref{eq:functional_1_full} \nwhere the smoothness of the result is enforced by \nusing regularisation matrices $D$ and $F$ that \nextract the second derivative, i.e. \n$$D = F=\n \\begin{bmatrix}\n  -1 & 1 & 0 & & \\cdots& & 0 \\\\\n  1 & -2 & 1 & 0& \\cdots&  & 0 \\\\\n  0 & 1 & -2 & 1& 0 &\\cdots & 0 \\\\\n  0 & 0 & 1 & -2& 1 &\\cdots & 0 \\\\\n \n \n \n \n  0 & & \\ddots&\\ddots &\\ddots&\\ddots&0  \\\\\n   \\vdots& &\\cdots &0&1&-2&1 \\\\\n  0 &  &\\cdots&    &0&1 & -1\n \\end{bmatrix}\n$$\nA typical result \nfor a rank-1 ($k=1$) approximation \nis depicted in Figure~\\ref{fig:rsvd_exp_2}, and \ncompared to the standard SVD solution. This illustrative example is available in~\\citep{num_exp_sec}.\n\n\n\n\n\\begin{figure}[]\n    \\centering\n   \n      \\includegraphics[width=0.45\\textwidth]{figures/rsvd_A2_noiseless.png}\n    \\includegraphics[width=0.45\\textwidth]{figures/rsvd_A2_noisy}\n     \\includegraphics[width=0.45\\textwidth]{figures/svd_A2_k_1.png}\n      \\includegraphics[width=0.45\\textwidth]{figures/rsvd_A2_grad.png}\n       \\includegraphics[width=0.45\\textwidth]{figures/P2_grad_vs_svd.png}\n    \\includegraphics[width=0.45\\textwidth]{figures/Q2_grad_vs_svd.png}\n    \\caption{ \n    Reconstruction of noisy matrix based on RSVD. \n    Top left: noise-less rank-1 matrix $\\mathbf{u} \\mathbf{v}^T ,$ (image) ,  top right: noisy input image \n     $\\mathbf{u} \\mathbf{v}^T + \\tau Z$\n    (high noise level), Middle left: \n    standard rank-1 SVD reconstruction, middle right: RSVD \n    reconstruction (D and F are 2nd deriv matrices.  \n    weight parameters $\\lambda = \\mu = 1.5$).\n    Bottom: comparison of standard SVD $U(:,1)$ (red) versus $P$ (blue), \n    and $V(:,1)$ (red) (left) versus $Q$ (blue) (right). The actual  $\\mathbf{u}$  and   $\\mathbf{v}$ for the noiseless \n    input signal are drawn in green. \n    }\n    \\label{fig:rsvd_exp_2}\n\\end{figure}\n\n \n\n\\begin{comment}\n\\section{Related Work}\n\nPCA has been widely used in two main applications: 1) Dimensionality reduction or low-rank matrix recovery; and 2) Data clustering.\nAs mentioned before, PCA underperforms in the presence of outliers. It also does not scale well with respect to the number of data samples. \nIn the literature, different varieties of Robust PCA have been proposed to solve the former issue by adding a sparse penalty term~\\citep{candes2011robust}.\nRobust PCA (RPCA) attempts to decompose a given data matrix \ninto a sum of a  low-rank  and a sparse matrix.  \n\nDifferent matrix factorization models have also been suggested to handle the scalability issue~\\citep{yu2009fast}.\nIn order to consider the geometrical information of both data and feature manifolds simultaneously, graph dual regularization\ntechnique has attracted many attentions in dimensionality reduction.\nBesides, for some applications such as clustering, a graph of data similarity can be used to improve the performance of the PCA-based clustering models.\nShahid et al.~\\citep{shahid2015robust} introduce a new Robust\nPCA on graphs model which incorporates spectral graph regularization into the Robust PCA framework.\n\nHe et al.~\\citep{he2019graph} \npoint out that the if the columns of the matrix $X$ \nare interpreted as data points, then the rows are features. \nThe neighbourhood structure of both the data points \nand the features give rise to distinct graphs (the \nso-called data \nand the feature graph) and hence,  to corresponding \ngraph \nLaplacians ($L_d$ and $L_f$ respectively).  \nIn order to preserve the geometric structure present in both the \ndata and the feature space, they \ninclude both these Laplacians \nin the regularisation term.  \nThe resulting regularised PCA is therefore called \nthe {\\it  graph-dual Laplacian regularized PCA }\n(gDLPCA) and for a given \ndata matrix $X$ is obtained \nby minimising the functional: \n\n\\begin{equation}\nJ(V,Y) = |\\!| X - VY|\\!|^2 \n     + \\alpha \\, \\mbox{Tr}(V^T L_d V) \n     + \\beta \\, \\mbox{Tr}(YL_f Y^T) \n     \\quad \\quad \\mbox{ s.t. \\,} V^TV = I\n     \\label{eq:J_He}\n\\end{equation}\nTo see this functional is equivalent to \nthe one in eq.~\\ref{eq:functional_1}, we use \nthe definition of the \nFrobenius norm (eq.~\\ref{eq:Frobenius}) \nand the fact that it is \ninvariant under transposition: \n    \\begin{eqnarray*}\n    F(P,Q)  &=&  |\\!| A - PQ^T|\\!|^2 + \\lambda\\, |\\!| DP |\\!|^2  \n  + \\mu\\, |\\!| GQ |\\!|^2 \\\\\n  &=&  |\\!| A^T - QP^T|\\!|^2 + \\lambda\\, \\mbox{Tr}(P^TD^T DP) \n  + \\mu\\, \\mbox{Tr}(Q^TG^T GQ) \\\\\n   &=&  |\\!| A^T - QP^T|\\!|^2 + \\lambda\\, \\mbox{Tr}(P^T L_1 P) \n  + \\mu\\, \\mbox{Tr}(Q^T L_2 Q) \\quad \\quad \\mbox{where } \\, \n  L_1 = D^TD, \\, L_2 = G^T G. \\\\\n & & \\mbox{ subject to $Q^T Q = I$.} \n    \\end{eqnarray*}\n    %\n %\nFrom this, the equivalence becomes obvious, \nby making the following identification: \n\n $$ X = A^T, \\quad V = Q, \\quad Y = P^T, \n \\quad L_2 = L_f, \\quad L_1 = L_d, \\quad \\alpha  = \\mu, \\quad \\beta = \\lambda.  $$\n\n\n\\end{comment}\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusions and Future Research}\n\\label{sec:conclusion}\n\nSingular Value Decomposition (SVD) and Principal Component \nAnalysis (PCA) are important matrix factorisation techniques that underpin numerous applications. However, it is well-known that disturbances in the input (noise, outliers or  missing values) have a significant effect on the outcome. \nFor that reason we investigate regularisation in two \ndifferent but related versions of the factorisation, \nand detail the solution algorithms. \n\nAn important topic for further research \nwould be to find ways in which the gradient descent procedure \nin Algorithms~\\ref{algorithm_mu_0} and \n \\ref{lag_mu_not_zero}\ncan be accelerated by taking advantage of the fact that \nthe functional is very smooth and locally approximately \nquadratic. It would also be useful to derive some estimates \nfor appropriate values for the weights $\\lambda$ and \n$\\mu$ in terms of noise characteristics corrupting the \nunderlying signal. Finally, although the $P$ matrix in \nalgorithm  \\ref{lag_mu_not_zero} has unit-length columns, \nwe were not able to prove that these columns are \nalso orthogonal ($P^TP = I$) as is the case in standard SVD. \nIn fact, numerical experiments seem to indicate that \nsuch a constraint is not compatible with minimisation \nof the functional.  This requires further theoretical \nelucidation. \n\n\n\n\n\n\n\\section*{Acknowledgment}\nThe authors gratefully acknowledge partial support by the Dutch NWO \nESI-Bida project NEAT (647.003.002). \n\n\\bibliographystyle{elsarticle-num}\n", "meta": {"timestamp": "2021-06-25T02:17:02", "yymm": "2106", "arxiv_id": "2106.12955", "language": "en", "url": "https://arxiv.org/abs/2106.12955"}}
{"text": "\\section{Dynamic weight sharing}\n\\label{app:dynamic_w_sh}\n\n\\subsection{Noiseless case}\nEach neuron receives the same $k$-dimensional input $\\bb x$, and its response $z_i$ is given by\n\\begin{align}\n    z_i = \\textbf{w}_i\\trans \\textbf{x} = \\sum_{j=1}^k w_{ij} x_j \\, .\n\\end{align}\n\nTo equalize the weights $\\textbf{w}_i$ among all neurons, the network minimizes the following objective,\n\\begin{align}\n    \\mathcal{L}_{\\mathrm{w.\\ sh.}}(\\textbf{w}_1,\\dots, \\textbf{w}_N) \\,&= \\frac{1}{4MN}\\sum_{m=1}^M\\sum_{i=1}^N\\sum_{j=1}^N\\brackets{z_i - z_j}^2 + \\frac{\\gamma}{2}\\sum_{i=1}^N\\|\\textbf{w}_i - \\textbf{w}^{\\mathrm{init}}_i\\|^2\\\\\n    &= \\frac{1}{4MN}\\sum_{m=1}^M\\sum_{i=1}^N\\sum_{j=1}^N\\brackets{\\textbf{w}_i\\trans \\textbf{x}_m - \\textbf{w}_j\\trans \\textbf{x}_m}^2 + \\frac{\\gamma}{2}\\sum_{i=1}^N\\|\\textbf{w}_i - \\textbf{w}^{\\mathrm{init}}_i\\|^2\\,,\n    \\label{app:eq:dynamic_weight_sharing_objective}\n\\end{align}\nwhere $\\textbf{w}^{\\mathrm{init}}_i$ is the weight at the start of dynamic weight sharing. This is a strongly convex function, and therefore it has a unique minimum.\n\nThe SGD update for one $\\textbf{x}_m$ is\n\\begin{equation}\n    \\Delta \\bb w_i \\propto -\\brackets{z_i - \\frac{1}{N}\\sum_{j=1}^N z_j} \\bb x_m - \\gamma \\brackets{\\bb w_i  - \\bb w_i^{\\mathrm{init}}}\\,.\n    \\label{app:eq:dynamic_weight_sharing_update}\n\\end{equation}\n\nTo find the fixed point of the dynamics, we first set the sum over the gradients to zero,\n\\begin{align}\n    \\sum_i \\deriv{\\mathcal{L}_{\\mathrm{w.\\ sh.}}(\\textbf{w}_1,\\dots, \\textbf{w}_N)}{\\textbf{w}_i} \\,&=  \\frac{1}{M}\\sum_{i, m}\\brackets{z_i - \\frac{1}{N}\\sum_{j=1}^N z_j} \\bb x_m + \\gamma \\sum_i\\brackets{\\bb w_i  - \\bb w_i^{\\mathrm{init}}}\\\\\n    &=\\gamma \\sum_i\\brackets{\\bb w_i  - \\bb w_i^{\\mathrm{init}}} = 0\\,.\n\\end{align}\nTherefore, at the fixed point the mean weight $\\boldsymbol{\\mu}^* = \\sum_i \\textbf{w}_i^*/N$ is equal to $\\boldsymbol{\\mu}^{\\mathrm{init}}=\\sum_i \\textbf{w}_i^{\\mathrm{init}}/N$, and\n\\begin{align}\n    \\frac{1}{N}\\sum_{i=1}^N z_i=\\frac{1}{N}\\sum_{i=1}^N \\textbf{w}_i^{*\\top} \\textbf{x}_m = (\\boldsymbol{\\mu}^{\\mathrm{init}})\\trans\\,\\textbf{x}_m\\,.\n\\end{align}\n\nWe can now find the individual weights,\n\\begin{align}\n    \\deriv{\\mathcal{L}_{\\mathrm{w.\\ sh.}}(\\textbf{w}_1,\\dots, \\textbf{w}_N)}{\\textbf{w}_i} \\,&= \\frac{1}{M}\\sum_{m}\\brackets{z_i - \\frac{1}{N}\\sum_{j=1}^N z_j} \\bb x_m + \\gamma \\brackets{\\bb w_i  - \\bb w_i^{\\mathrm{init}}}\\\\\n    &= \\frac{1}{M}\\sum_m \\textbf{x}_m \\textbf{x}_m\\trans \\brackets{\\textbf{w}_i - \\boldsymbol{\\mu}^{\\mathrm{init}}} + \\gamma \\brackets{\\bb w_i  - \\bb w_i^{\\mathrm{init}}}=0\\,.\n\\end{align}\n\nDenoting the covariance matrix $\\textbf{C} \\equiv \\frac{1}{M}\\sum_m \\bb x_m \\bb x_m\\trans$, we see that\n\\begin{align}\n    \\bb w_i^* \\,&= \\brackets{\\textbf{C} + \\gamma\\, \\textbf{I}}^{-1} \\brackets{\\textbf{C} \\, \\boldsymbol{\\mu}^{\\mathrm{init}} + \\gamma\\, \\bb w_i^{\\mathrm{init}}} =  \\brackets{\\textbf{C} + \\gamma\\, \\textbf{I}}^{-1}  \\brackets{\\textbf{C} \\, \\frac{1}{N} \\sum_{i=1}^N \\bb w_i^{\\mathrm{init}} + \\gamma\\, \\bb w_i^{\\mathrm{init}}}\\,,\n    \\label{app:eq:dynamic_w_sh_solution}\n\\end{align}\nwhere $\\textbf{I}$ is the identity matrix. From \\cref{app:eq:dynamic_w_sh_solution} it is clear that $\\textbf{w}_i^*\\approx \\boldsymbol{\\mu}^{\\mathrm{init}}$ for small $\\gamma$ and full rank $\\bb C$. For instance, for $\\bb C = \\bb I$,\n\\begin{equation}\n    \\bb w_i^* = \\frac{1}{1+\\gamma}\\boldsymbol{\\mu}^{\\mathrm{init}} + \\frac{\\gamma}{1+\\gamma}\\textbf{w}_i^{\\mathrm{init}}\\,.\n\\end{equation}\n\n\\subsection{Biased noiseless case, and its correspondence to the realistic implementation}\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\textwidth]{diagrams/snr_biased.pdf}\n    \\caption{Logarithm of inverse signal-to-noise ratio (mean weight squared over weight variance, see \\cref{eq:snr}) for weight sharing objectives in a layer with 100 neurons. \\textbf{A.} Dynamics of \\cref{app:eq:biased_update} for different kernel sizes $k$ (meaning $k^2$ inputs) and $\\gamma$. \\textbf{B.} Dynamics of weight update that uses \\cref{eq:realistic_dynamics} for $\\alpha=10$, different kernel sizes $k$ and $\\gamma$. In each iteration, the input is presented for 150 ms.}\n    \\label{app:fig:snr_plots_biased}\n\\end{figure}\n\nThe realistic implementation of dynamic weight sharing with an inhibitory neuron (\\cref{sec:realistic_update}) introduces a bias in the update rule: \\cref{app:eq:dynamic_weight_sharing_update} becomes\n\\begin{equation}\n    \\Delta \\bb w_i \\propto -\\brackets{z_i - \\frac{\\alpha}{N(1+\\alpha)}\\sum_{j=1}^N z_j} \\bb x_m - \\gamma \\brackets{\\bb w_i  - \\bb w_i^{\\mathrm{init}}}\n    \\label{app:eq:biased_update}\n\\end{equation}\nfor inhibition strength $\\alpha$.\n\nFollowing the same derivation as for the unbiased case, we can show that the weight dynamics converges to \n\\begin{align}\n    \\sum_i \\deriv{\\mathcal{L}_{\\mathrm{w.\\ sh.}}(\\textbf{w}_1,\\dots, \\textbf{w}_N)}{\\textbf{w}_i} \\,&=  \\frac{1}{M}\\sum_{i, m}\\brackets{z_i - \\frac{\\alpha}{1+\\alpha}\\frac{1}{N}\\sum_{j=1}^N z_j} \\bb x_m + \\gamma \\sum_i\\brackets{\\bb w_i  - \\bb w_i^{\\mathrm{init}}}\\\\\n    &=\\frac{1}{1+\\alpha} \\bb C \\sum_i \\textbf{w}_i + \\gamma \\sum_i\\brackets{\\bb w_i  - \\bb w_i^{\\mathrm{init}}} = 0\\,.\n\\end{align}\nTherefore $\\boldsymbol{\\mu}^* = \\gamma\\brackets{\\frac{1}{1+\\alpha} \\bb C + \\gamma \\bb I}^{-1} \\boldsymbol{\\mu}^{\\mathrm{init}}$, and\n\n\\begin{align}\n    \\bb w_i^* \\,&= \\brackets{\\textbf{C} + \\gamma\\, \\textbf{I}}^{-1} \\brackets{\\frac{\\gamma\\alpha}{1+\\alpha}\\,\\textbf{C}\\brackets{\\frac{1}{1+\\alpha} \\bb C + \\gamma \\bb I}^{-1}   \\boldsymbol{\\mu}^{\\mathrm{init}} + \\gamma\\, \\bb w_i^{\\mathrm{init}}}\\,.\n\\end{align}\n\nFor $\\textbf{C} = \\bb I$, this becomes\n\\begin{align}\n    \\bb w_i^* \\,&= \\frac{\\gamma}{1 + \\gamma} \\brackets{\\frac{ \\alpha}{1 + \\gamma (1 + \\alpha)} \\boldsymbol{\\mu}^{\\mathrm{init}} + \\bb w_i^{\\mathrm{init}}}\\,.\n\\end{align}\nAs a result, the final weights are approximately the same among neurons, but have a small norm due to the $\\gamma$ scaling. \n\nThe dynamics in \\cref{app:eq:biased_update} correctly captures the bias influence in \\cref{eq:realistic_dynamics}, producing similar SNR plots; compare \\cref{app:fig:snr_plots_biased}A (\\cref{app:eq:biased_update} dynamics) to \\cref{app:fig:snr_plots_biased}B (\\cref{eq:realistic_dynamics} dynamics). The curves are slightly different due to different learning rates, but both follow the same trend of first finding a very good solution, and then slowly incorporating the bias term (leading to smaller SNR). \n\n\n\\subsection{Noisy case}\n\nRealistically, all neurons can't see the same $\\textbf{x}_m$. However, due to the properties of our loss, we can work even with noisy updates. To see this, we write the objective function as\n\\begin{align}\n    \\mathcal{L}_{\\mathrm{w.\\ sh.}}(\\textbf{w}_1,\\dots, \\textbf{w}_N) \\,&= \\frac{1}{M}\\sum_{m=1}^M f(\\bb W, \\bb X_m)\n\\end{align}\nwhere matrices $\\bb W$ and $\\bb X$ satisfy $(\\bb W)_i=\\textbf{w}_i$ and $(\\bb X_m)_i=\\textbf{x}_m$, and\n\\begin{align}\n     f(\\bb W, \\bb X_m) \\,&= \\frac{1}{4N}\\sum_{i=1}^N\\sum_{j=1}^N\\brackets{\\textbf{w}_i\\trans \\textbf{x}_m - \\textbf{w}_i\\trans \\textbf{x}_m}^2+  \\frac{\\gamma}{2}\\sum_{i=1}^N\\|\\textbf{w}_i - \\textbf{w}^{\\mathrm{init}}_i\\|^2\\, .\n\\end{align}\n\nWe'll update the weights with SGD according to\n\\begin{equation}\n    \\Delta \\bb W^{k+1} = -\\eta_k \\left.\\deriv{}{\\bb W} f(\\bb W, \\bb X_{m(k)} + \\bb E^k) \\right|_{\\bb W^k}\\,,\n\\end{equation}\nwhere $(\\bb E^k)_i = \\boldsymbol{\\eps}_i$ is zero-mean input noise and $m(k)$ is chosen uniformly.\n\nLet's also bound the input mean and noise as\n\\begin{equation}\n    \\expect_{\\bb E} \\norm{\\textbf{x}_{m(k)} + \\boldsymbol{\\eps}_i}^2 \\leq \\sqrt{c_{x\\eps}},\\quad  \\expect_{\\bb E} \\norm{\\textbf{x}_{m(k)} + \\boldsymbol{\\eps}_i}^4 \\leq c_{x\\eps}\\,.\n    \\label{app:eq:noise_condition}\n\\end{equation}\n\nWith this setup, we can show that SGD with noise can quickly converge to the correct solution, apart from a constant noise-induced bias. Our analysis is standard and follows \\cite{gower2019sgd}, but had to be adapted for our objective and noise model.\n\\begin{theorem} For zero-mean isotropic noise $\\bb E$ with variance $\\sigma^2$, uniform SGD sampling $m(k)$ and inputs $\\textbf{x}_m$ that satisfy \\cref{app:eq:noise_condition}, choosing $\\eta_k = O(1/k)$ leads to\n\\begin{align}\n    \\expect\\,\\|\\bb W^{k+1} - \\bb W^*\\|^2_F = O\\brackets{\\frac{\\norm{\\bb W^{\\mathrm{init}} - \\bb W^*}_F}{k + 1}}  + O\\brackets{\\sigma^2\\|\\bb W^*\\|^2_F}\\,,\n\\end{align}\nwhere $(\\bb W^*)_i$ is given by \\cref{app:eq:dynamic_w_sh_solution}.\n\\end{theorem}\n\\begin{proof}\nUsing the SGD update,\n\\begin{align}\n    \\|\\bb W^{k+1} - \\bb W^*\\|^2_F \\,&= \\left\\|\\bb W^{k} -\\eta_k \\left.\\deriv{}{\\bb W} f(\\bb W, \\bb X_{m(k)} + \\bb E^k) \\right|_{\\bb W^k} - \\bb W^*\\right\\|^2_F    \\\\\n    &=\\left\\|\\bb W^{k} - \\bb W^*\\right\\|^2_F - 2\\eta_k\\dotprod{\\bb W^{k} - \\bb W^*}{ \\left.\\deriv{}{\\bb W} f(\\bb W, \\bb X_{m(k)} + \\bb E^k) \\right|_{\\bb W^k}}\\\\\n    &+\\eta_k^2\\left\\| \\left.\\deriv{}{\\bb W} f(\\bb W, \\bb X_{m(k)} + \\bb E^k) \\right|_{\\bb W^k}\\right\\|^2_F\\,.\n\\end{align}\n\nWe need to bound the second and the third terms in the equation above.\n\n\\textbf{Second term.} As $f$ is $\\gamma$-strongly convex in $\\bb W$,\n\\begin{align}\n    &-\\dotprod{\\bb W^{k} - \\bb W^*}{ \\left.\\deriv{}{\\bb W} f(\\bb W, \\bb X_{m(k)} + \\bb E^k) \\right|_{\\bb W^k}}  \\\\\n    &\\qquad\\qquad \\leq f(\\bb W^*, \\bb X_{m(k)} + \\bb E^k)- f(\\bb W^k, \\bb X_{m(k)} + \\bb E^k) - \\frac{\\gamma}{2}\\|\\bb W^{k} - \\bb W^*\\|^2_F\\,.\n\\end{align}\n\nAs $f$ is convex in $\\bb X$,\n\\begin{align}\n    &f(\\bb W^*, \\bb X_{m(k)} + \\bb E^k)- f(\\bb W^k, \\bb X_{m(k)} + \\bb E^k) \\leq f(\\bb W^*, \\bb X_{m(k)})- f(\\bb W^k, \\bb X_{m(k)}) \\\\\n    &\\qquad\\qquad + \\dotprod{\\left.\\deriv{}{\\bb X} f(\\bb W^*, \\bb X) \\right|_{\\bb X_{m(k)} + \\bb E^k} - \\left.\\deriv{}{\\bb X} f(\\bb W^k, \\bb X) \\right|_{\\bb X_{m(k)}}}{\\bb E^k}\\,.\n\\end{align}\nWe only need to clarify one term here,\n\\begin{align}\n    \\brackets{\\left.\\deriv{}{\\bb X} f(\\bb W^*, \\bb X) \\right|_{\\bb X_{m(k)} + \\bb E^k}}_i = \\brackets{\\left.\\deriv{}{\\bb X} f(\\bb W^*, \\bb X) \\right|_{\\bb X_{m(k)}}}_i + \\brackets{\\bb w_i^{*\\top}\\boldsymbol{\\eps}_i - \\frac{1}{N}\\sum_j\\bb w_j^{*\\top}\\eps_j}\\bb w_i^*\\,.\n\\end{align}\n\nNow we can take the expectation over $m(k)$ and $\\bb E$. As $m(k)$ is uniform, and $\\bb W^*$ minimizes the global function,\n\\begin{equation}\n    \\expect_{m(k)}\\, \\brackets{f(\\bb W^*, \\bb X_{m(k)})- f(\\bb W^k, \\bb X_{m(k)})} = \\mathcal{L}_{\\mathrm{w.\\ sh.}}(\\textbf{w}_1^*,\\dots, \\textbf{w}_N^*) - \\mathcal{L}_{\\mathrm{w.\\ sh.}}(\\textbf{w}_1^k,\\dots, \\textbf{w}_N^k) \\leq 0\\,.\n\\end{equation}\nAs $\\bb E^k$ is zero-mean and isotropic with variance $\\sigma^2$,\n\\begin{align}\n    &\\expect_{m(k), \\bb E^k}\\dotprod{\\left.\\deriv{}{\\bb X} f(\\bb W^*, \\bb X) \\right|_{\\bb X_{m(k)} + \\bb E^k} - \\left.\\deriv{}{\\bb X} f(\\bb W^k, \\bb X) \\right|_{\\bb X_{m(k)}}}{\\bb E^k}\\\\\n    &= \\expect_{\\bb E^k} \\sum_i\\brackets{\\bb w_i^{*\\top}\\boldsymbol{\\eps}_i - \\frac{1}{N}\\sum_j\\bb w_j^{*\\top}\\eps_j}\\bb w_i^{*\\top}\\boldsymbol{\\eps}_i =  \\brackets{1 - \\frac{1}{N}}\\expect_{\\bb E^k} \\sum_i \\brackets{\\bb w_i^{*\\top}\\boldsymbol{\\eps}_i}^2\\\\\n    &=\\brackets{1 - \\frac{1}{N}}\\expect_{\\bb E^k} \\sum_i \\tr\\brackets{\\bb w_i^{*}\\bb w_i^{*\\top}\\boldsymbol{\\eps}_i\\boldsymbol{\\eps}_i\\trans}\\leq\\sigma^2 \\|\\bb W^*\\|^2_F\\,.\n\\end{align}\n\nSo the whole second term becomes\n\\begin{align}\n    &-2\\eta_k\\expect_{m(k),\\bb E}\\,\\dotprod{\\bb W^{k} - \\bb W^*}{ \\left.\\deriv{}{\\bb W} f(\\bb W, \\bb X_{m(k)} + \\bb E^k) \\right|_{\\bb W^k}} \\\\\n    &\\qquad\\qquad\\leq - \\gamma\\eta_k \\expect_{m(k),\\bb E^k}\\|\\bb W^{k} - \\bb W^*\\|^2_F + \\eta_k\\sigma^2 \\|\\bb W^*\\|^2_F\\,.\n\\end{align}\n\n\\textbf{Third term.}\nFirst, observe that\n\\begin{align}\n    \\deriv{}{\\bb w_i} f(\\bb W, \\bb X) \\,&= \\textbf{x}_i\\textbf{x}_i\\trans \\textbf{w}_i-\\textbf{x}_i\\frac{1}{N}\\sum_j\\textbf{x}_j\\trans \\textbf{w}_j + \\gamma \\textbf{w}_i - \\gamma \\textbf{w}_i^{\\mathrm{init}} \\\\\n    &=\\brackets{1 - \\frac{1}{N}}\\bb A_i \\textbf{w}_i - \\bb B_i \\bb W+\\gamma \\textbf{w}_i - \\gamma \\textbf{w}_i^{\\mathrm{init}}\\,,\n\\end{align}\nwhere $\\bb A_i=\\textbf{x}_i\\textbf{x}_i\\trans$ and $(\\bb B_i)_j = \\ind[i\\neq j]\\textbf{x}_i\\textbf{x}_j\\trans / N$.\n\nTherefore, using $\\|a+b\\|^2 \\leq 2\\|a\\|^2 + 2\\|b\\|^2$ twice, properties of the matrix 2-norm, and $(1 - 1/N)\\leq 1$,\n\\begin{align}\n    \\norm{\\deriv{}{\\bb w_i} f(\\bb W, \\bb X)}^2 \\leq 4\\norm{\\bb A_i}^2_2\\norm{\\textbf{w}_i}^2 + 4\\norm{\\bb B_i}^2_2\\norm{\\bb W}^2 + 4\\gamma^2 \\norm{\\textbf{w}_i}^2 + 4\\gamma^2\\norm{\\textbf{w}_i^{\\mathrm{init}}}^2\\,.\n\\end{align}\nIn our particular case, bounding the 2 norm with the Frobenius norm gives\n\\begin{align}\n    \\expect_{m(k), \\bb E} \\norm{\\bb A_i}^2_2 \\,&\\leq \\expect_{m(k), \\bb E} \\norm{(\\textbf{x}_{m(k)} + \\boldsymbol{\\eps}_i)(\\textbf{x}_{m(k)} + \\boldsymbol{\\eps}_i)\\trans}^2_F \\\\\n    &= \\expect_{m(k), \\bb E} \\norm{\\textbf{x}_{m(k)} + \\boldsymbol{\\eps}_i}^4 \\leq c_{x\\eps}\\,.\n\\end{align}\n\nSimilarly,\n\\begin{align}\n    \\expect_{m(k), \\bb E} \\norm{\\bb B_i}^2_2 \\,&\\leq \\expect_{m(k), \\bb E} \\norm{\\bb B_i}^2_F \n    \\leq \\frac{1}{N^2}\\expect_{m(k), \\bb E} \\sum_{j\\neq i}\\norm{\\textbf{x}_{m(k)} + \\boldsymbol{\\eps}_i}^2\\norm{\\textbf{x}_{m(k)} + \\eps_j}^2\\leq \\frac{c_{x\\eps}}{N}\\,.\n\\end{align}\n\nTherefore, we can bound the full gradient by the sum of individual bounds (as it's the Frobenius norm) and using  $\\|a+b\\|^2 \\leq 2\\|a\\|^2 + 2\\|b\\|^2$ again,\n\\begin{align}\n    &\\expect_{m(k), \\bb E}\\norm{\\left.\\deriv{}{\\bb W} f(\\bb W, \\bb X_{m(k) + \\bb E^k})\\right|_{\\bb W^k}}^2_F  \\leq 4(2 c_{x\\eps} + \\gamma^2) \\norm{\\bb W^k}^2_F + 4\\gamma^2\\norm{\\bb W^{\\mathrm{init}}}^2_F\\\\\n    &\\qquad\\qquad\\leq 8(2 c_{x\\eps} + \\gamma^2) \\norm{\\bb W^k - \\bb W^*}^2_F + 8(2 c_{x\\eps} + \\gamma^2) \\norm{\\bb W^*}^2_F + 4\\gamma^2\\norm{\\bb W^{\\mathrm{init}}}^2_F\\,.\n\\end{align}\n\nCombining all of this, and taking the expectation over all steps before $k+1$, gives us\n\\begin{align}\n    \\expect\\,\\|\\bb W^{k+1} - \\bb W^*\\|^2_F \\,&\\leq \\brackets{1 - \\gamma \\eta_k + \\eta_k^2 8(2 c_{x\\eps} + \\gamma^2)} \\expect\\, \\left\\|\\bb W^{k} - \\bb W^*\\right\\|^2_F\\\\\n    & + \\eta_k\\sigma^2 \\|\\bb W^*\\|^2_F +\\eta_k^2\\brackets{8(2 c_{x\\eps} + \\gamma^2) \\norm{\\bb W^*}^2_F + 4\\gamma^2\\norm{\\bb W^{\\mathrm{init}}}^2_F}\\,.\n\\end{align}\nIf we choose $\\eta_k$ such that $\\eta_k\\cdot\\brackets{8(2 c_{x\\eps} + \\gamma^2) \\norm{\\bb W^*}^2_F + 4\\gamma^2\\norm{\\bb W^{\\mathrm{init}}}^2_F} \\leq \\sigma^2$, we can simplify the result, \n\\begin{align}\n    \\expect\\,\\|\\bb W^{k+1} - \\bb W^*\\|^2_F \\,&\\leq \\brackets{1 - \\gamma \\eta_k + \\eta_k^2 8(2 c_{x\\eps} + \\gamma^2)} \\expect\\, \\left\\|\\bb W^{k} - \\bb W^*\\right\\|^2_F + 2\\eta_k\\sigma^2 \\|\\bb W^*\\|^2_F \\\\\n    &\\leq \\brackets{\\prod_{s=0}^{k}\\brackets{1 - \\gamma \\eta_s + \\eta_s^2 8(2 c_{x\\eps} + \\gamma^2)}} \\expect\\, \\left\\|\\bb W^{\\mathrm{init}} - \\bb W^*\\right\\|^2_F \\\\\n    & + 2\\sigma^2\\sum_{t=0}^{k}\\eta_t\\brackets{\\prod_{s=1}^{t}\\brackets{1 - \\gamma \\eta_s + \\eta_s^2 8(2 c_{x\\eps} + \\gamma^2)}} \\|\\bb W^*\\|^2_F\\,.\n\\end{align}\nIf we choose $\\eta_k = O(1/k)$, the first term will decrease as $1/k$. The second one will stay constant with time, and proportional to $\\sigma^2$.\n\n\n\\end{proof}\n\n\\subsection{Applicability to vision transformers}\n\\label{app:subsec:transformers}\nIn vision transformers (e.g. \\cite{dosovitskiy2020image}), an input image is reshaped into a matrix $\\bb Z\\in\\RR^{N\\times D}$ for $N$ non-overlapping patches of the input, each of size $D$. As the first step, $\\bb Z$ is multiplied by a matrix $\\bb U\\in\\RR^{D\\times 3D}$ as $\\bb Z' = \\bb Z\\bb U$. Therefore, an output neuron $z'_{ij} = \\sum_k z_{ik} u_{kj}$ looks at $\\bb z_i$ with the same weights as $z'_{i'j} = \\sum_k z_{i'k} u_{kj}$ uses for $\\bb z_{i'}$ for any $i'$. \n\nTo share weights with dynamic weight sharing, for each $k$ we need to connect all $z_{ik}$ across $i$ (input layer), and for each $j$ -- all $z_{ij}'$ across $i$ (output layer). After that,  weight sharing will proceed just like for locally connected networks: activate an input grid $j_1$ (one of $D$ possible ones) to create a repeating input patter, then activate a grid $j_2$ and so on. \n\n\\subsection{Details for convergence plots}\nBoth plots in \\cref{fig:snr_plots} show mean negative log SNR over 10 runs, 100 output neurons each. Initial weights were drawn from $\\mathcal{N}(1, 1)$. At every iteration, the new input $\\textbf{x}$ was drawn from $\\mathcal{N}(1, 1)$ independently for each component. Learning was performed via SGD with momentum of 0.95. The minimum SNR value was computed from \\cref{eq:dynamic_w_sh_solution}. For our data, the SNR expression in \\cref{eq:snr} has $\\brackets{\\frac{1}{N}\\sum_i (\\bb w_i)_j}^2\\approx 1$ and  $\\frac{1}{N}\\sum_i \\brackets{(\\bb w_i)_j - \\frac{1}{N}\\sum_{i'} (\\bb w_{i'})_j}^2 \\approx \\gamma ^2 / (1+\\gamma)^2$, therefore $-\\log \\mathrm{SNR}_{\\mathrm{min}} =2 \\log(\\gamma / (1 + \\gamma))$.\n\nFor \\cref{fig:snr_plots}A, we performed 2000 iterations (with a new $\\textbf{x}$ each time). Learning rate at iteration $k$ was $\\eta_k = 0.5 / (1000 + k)$. For \\cref{app:fig:snr_plots_biased}A, we did the same simulation but for $10^4$ iterations.\n\nFor \\cref{fig:snr_plots}B, network dynamics (\\cref{eq:realistic_dynamics}) was simulated with $\\tau = 30$ ms, $b=1$ using Euler method with steps size of 1 ms. We performed $10^4$ iterations (150 ms per iteration, with a new $\\textbf{x}$ each iteration). Learning rate at iteration $k$ was $\\eta_k = 0.0003 / \\sqrt{1 + k / 2} \\cdot\\ind[k \\geq 50]$.\n\nThe code for both runs is provided in the supplementary material.\n\n\\section{Experimental details}\n\\label{app:experiments}\n\n\n\nBoth convolutional and LC layers did not have the bias term, and were initialized according to Kaiming Normal initialization \\cite{he2015delving} with ReLU gain, meaning each weight was drawn from $\\mathcal{N}(0, 2 / (c_{\\mathrm{out}}k^2))$ for kernel size $k$ and $c_{\\mathrm{out}}$ output channels.\n\nAll runs were done with automatic mixed precision, meaning that inputs to each layer (but not the weights) were stored as float16, and not float32. This greatly improved performance and memory requirements of the networks.\n\n\\subsection{CIFAR10/100, TinyImageNet}\nMean performance over 5 runs is summarized in \\cref{app:tab:pad0} (padding of 0), \\cref{app:tab:pad4} (padding of 4), and \\cref{app:tab:pad8} (padding of 8). Maximum minus minimum accuracy is summarized in \\cref{app:tab:pad0_minmax}, \\cref{app:tab:pad4_minmax}, and \\cref{app:tab:pad8_minmax}. Hyperparameters for AdamW (learning rate and weight decay) are provided in \\cref{app:tab:pad0_param}, \\cref{app:tab:pad4_param}, and \\cref{app:tab:pad8_param}.\n\nHyperparameters were optimized on a train/validation split (see \\cref{sec:experiments}) over the following grids. \n\\textbf{CIFAR10/100.} Learning rate: [1e-1, 5e-2, 1e-2, 5e-3] (conv), [1e-3, 5e-4, 1e-4, 5e-5] (LC); weight decay [1e-2, 1e-4] (both). \\textbf{TinyImageNet.} Learning rate: [5e-3, 1e-3, 5e-4] (conv), [1e-3, 5e-4] (LC); weight decay [1e-2, 1e-4] (both). The learning rate range for TinyImageNet was smaller as preliminary experiments showed poor performance for slow learning rates.\n\nFor all runs, the batch size was 512. For all final runs, learning rate was divided by 4 at 100 and then at 150 epochs (out of 200). Grid search for CIFAR10/100 was done for the same 200 epochs setup. For TinyImageNet, grid search was performed over 50 epochs with learning rate decreases at 25 and 37 epochs (i.e., the same schedule but compressed) due to the larger computational cost of full runs.\n\n\\subsection{ImageNet}\nIn addition to the main results, we also tested the variant of the locally connected network with a convolutional first layer (\\cref{app:tab:imagenet_accuracy}). It improved performance for all configurations: from about 2\\% for weight sharing every 1-10 iterations, to about 5\\% for 100 iterations and for no weight sharing. This is not surprising, as the first layer has the largest resolution (224 by 224; initially, we performed these experiment due to memory constraints). Our result suggests that adding a ``good'' pre-processing layer (e.g. the retina) can also improve performance of locally connected networks.\n\n\\textbf{Final hyperparameters.} Learning rate: 1e-3 (conv, LC with w.sh. (1)), 5e-4 (all other LC; all LC with 1st layer conv), weight decay: 1e-2 (all). Hyperparameters were optimized on a train/validation split (see \\cref{sec:experiments}) over the following grids. \nConv: learning rate [1e-3, 5e-4], weight decay [1e-2, 1e-4, 1e-6]. LC: learning rate [1e-3, 5e-4, 1e-4, 5e-5], weight decay [1e-2]. LC (1st layer conv): learning rate [1e-3, 5e-4], weight decay [1e-2, 1e-4, 1e-6]. For LC, we only tried the large weight decay based on earlier experiment (LC (1st layer conv)). For LC (1st layer conv), we only tuned hyperparameters for LC and LC with weight sharing in each iteration, as they found the same values (weight sharing every 10/100 iterations interpolates between LC and LC with weight sharing in each iteration, and therefore is expected to behave similarly to both). In addition, for LC (1st layer conv) we only tested learning rate of 5e-4 for weight decay of 1e-2 as higher learning rates performed significantly worse for other runs (and in preliminary experiments).\n\nFor all runs, the batch size was 256. For all final runs, learning rate was divided by 4 at 100 and then at 150 epochs (out of 200). Grid search was performed over 20 epochs with learning rate decreases at 10 and 15 epochs (i.e., the same schedule but compressed) due to the large computational cost of full runs.\n\n\\begin{table}[]\n\\caption{Performance of convolutional (conv) and locally connected (LC) networks for padding of 0 in the input images (mean accuracy over 5 runs). For LC, two regularization strategies were applied: repeating the same image $n$ times with different translations ($n$ reps) or using dynamic weight sharing every $n$ batches (ws ($n$)). LC nets additionally show performance difference w.r.t. conv nets.}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}cccccccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Regularizer}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR10}} &\n  \\multicolumn{4}{c}{\\textbf{CIFAR100}} &\n  \\multicolumn{4}{c}{\\textbf{TinyImageNet}} \\\\ \\cmidrule(l){3-12} \n &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} \\\\ \\midrule\n\\multirow{2}{*}{-} & conv& 84.1 & - & 49.5 & - & 78.2 & - & 26.0 & - & 51.2 & -  \\\\\n& LC& 67.2 & -16.8 & 34.9 & -14.6 & 62.2 & -16.0 & 12.0 & -14.1 & 30.4 & -20.7  \\\\\n\\midrule\n\\multirow{3}{*}{Weight Sharing} & LC - ws(1)& 74.8 & -9.3 & 41.8 & -7.7 & 70.1 & -8.1 & 24.9 & -1.2 & 49.1 & -2.1  \\\\\n& LC - ws(10)& 75.9 & -8.1 & 44.4 & -5.1 & 72.0 & -6.2 & 28.1 & 2.0 & 52.5 & 1.3  \\\\\n& LC - ws(100)& 75.4 & -8.6 & 43.4 & -6.1 & 71.9 & -6.3 & 27.4 & 1.3 & 51.9 & 0.8  \\\\\n \\bottomrule\n\\end{tabular}}\n\\label{app:tab:pad0}\n\\end{table}\n\n\\begin{table}[]\n\\caption{Mean performance over 5 runs. Same as \\cref{app:tab:pad0}, but for padding of 4.}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}cccccccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Regularizer}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR10}} &\n  \\multicolumn{4}{c}{\\textbf{CIFAR100}} &\n  \\multicolumn{4}{c}{\\textbf{TinyImageNet}} \\\\ \\cmidrule(l){3-12} \n &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} \\\\ \\midrule\n\\multirow{2}{*}{-} & conv& 88.3 & - & 59.2 & - & 84.9 & - & 38.6 & - & 65.1 & -  \\\\\n& LC& 80.9 & -7.4 & 49.8 & -9.4 & 75.5 & -9.4 & 29.6 & -9.0 & 52.7 & -12.4  \\\\\n\\midrule\n\\multirow{3}{*}{Data Translation} & LC - 4 reps& 82.9 & -5.4 & 52.1 & -7.1 & 76.4 & -8.5 & 31.9 & -6.7 & 54.9 & -10.2  \\\\\n& LC - 8 reps& 83.8 & -4.5 & 54.3 & -5.0 & 77.9 & -7.0 & 33.0 & -5.6 & 55.6 & -9.5  \\\\\n& LC - 16 reps& 85.0 & -3.3 & 55.9 & -3.3 & 78.8 & -6.1 & 34.0 & -4.6 & 56.2 & -8.8  \\\\\n\\midrule\n\\multirow{3}{*}{Weight Sharing} & LC - ws(1)& 87.4 & -0.8 & 58.7 & -0.5 & 83.4 & -1.6 & 41.6 & 3.0 & 66.1 & 1.1  \\\\\n& LC - ws(10)& 85.1 & -3.2 & 55.7 & -3.6 & 80.9 & -4.0 & 37.4 & -1.2 & 61.8 & -3.2  \\\\\n& LC - ws(100)& 82.0 & -6.3 & 52.8 & -6.4 & 80.1 & -4.8 & 37.1 & -1.5 & 62.8 & -2.3 \\\\\n \\bottomrule\n\\end{tabular}}\n\\label{app:tab:pad4}\n\\end{table}\n\n\\begin{table}[]\n\\caption{Mean performance over 5 runs. Same as \\cref{app:tab:pad0}, but for padding of 8.}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}cccccccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Regularizer}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR10}} &\n  \\multicolumn{4}{c}{\\textbf{CIFAR100}} &\n  \\multicolumn{4}{c}{\\textbf{TinyImageNet}} \\\\ \\cmidrule(l){3-12} \n &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} \\\\ \\midrule\n\\multirow{2}{*}{-} & conv& 88.7 & - & 59.6 & - & 85.4 & - & 42.6 & - & 68.7 & -  \\\\\n& LC& 80.7 & -8.0 & 47.7 & -11.8 & 74.8 & -10.6 & 31.9 & -10.7 & 55.4 & -13.3  \\\\\n\\midrule\n\\multirow{3}{*}{Data Translation} & LC - 4 reps& 82.8 & -6.0 & 50.6 & -9.0 & 76.2 & -9.2 & 35.5 & -7.1 & 58.6 & -10.1  \\\\\n& LC - 8 reps& 83.6 & -5.1 & 53.0 & -6.6 & 77.4 & -8.0 & 35.8 & -6.7 & 59.0 & -9.7  \\\\\n& LC - 16 reps& 85.0 & -3.8 & 55.6 & -4.0 & 78.4 & -7.0 & 37.9 & -4.7 & 60.3 & -8.4  \\\\\n\\midrule\n\\multirow{3}{*}{Weight Sharing} & LC - ws(1)& 87.8 & -0.9 & 59.2 & -0.4 & 84.0 & -1.4 & 43.6 & 1.0 & 67.9 & -0.9  \\\\\n& LC - ws(10)& 84.3 & -4.5 & 53.7 & -5.8 & 80.4 & -5.0 & 39.6 & -2.9 & 64.5 & -4.3  \\\\\n& LC - ws(100)& 79.5 & -9.3 & 50.0 & -9.6 & 78.6 & -6.8 & 39.2 & -3.4 & 64.8 & -3.9  \\\\\n \\bottomrule\n\\end{tabular}}\n\\label{app:tab:pad8}\n\\end{table}\n\n\n\n\\begin{table}[]\n\\caption{Max minus min performance over 5 runs; padding of 0.}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}cccccccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Regularizer}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multicolumn{1}{c}{\\textbf{CIFAR10}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR100}} &\n  \\multicolumn{2}{c}{\\textbf{TinyImageNet}} \\\\ \\cmidrule(l){3-7} \n &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n  \\\\ \\midrule\n\\multirow{2}{*}{-} & conv& 0.5 & 1.0 & 1.7 & 1.0 & 0.4  \\\\\n& LC& 0.4 & 1.6 & 1.5 & 1.0 & 1.7  \\\\\n\\midrule\n\\multirow{3}{*}{Weight Sharing} & LC - ws(1)& 0.5 & 1.3 & 1.3 & 1.2 & 2.0  \\\\\n& LC - ws(10)& 0.8 & 1.0 & 0.7 & 1.8 & 2.1  \\\\\n& LC - ws(100)& 0.9 & 0.7 & 0.9 & 1.0 & 1.3  \\\\\n \\bottomrule\n\\end{tabular}}\n\\label{app:tab:pad0_minmax}\n\\end{table}\n\n\\begin{table}[]\n\\caption{Max minus min performance over 5 runs; padding of 4.}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}cccccccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Regularizer}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multicolumn{1}{c}{\\textbf{CIFAR10}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR100}} &\n  \\multicolumn{2}{c}{\\textbf{TinyImageNet}} \\\\ \\cmidrule(l){3-7} \n &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n  \\\\ \\midrule\n\\multirow{2}{*}{-} & conv& 0.7 & 1.5 & 0.2 & 1.2 & 1.1  \\\\\n& LC& 0.8 & 1.1 & 0.4 & 0.7 & 0.8  \\\\\n\\midrule\n\\multirow{3}{*}{Data Translation} & LC - 4 reps& 0.8 & 1.3 & 0.8 & 0.5 & 0.8  \\\\\n& LC - 8 reps& 0.3 & 1.4 & 1.3 & 0.7 & 1.2  \\\\\n& LC - 16 reps& 0.7 & 0.7 & 0.6 & 0.9 & 0.5  \\\\\n\\midrule\n\\multirow{3}{*}{Weight Sharing} & LC - ws(1)& 0.5 & 1.1 & 0.9 & 0.9 & 0.6  \\\\\n& LC - ws(10)& 0.6 & 1.1 & 0.3 & 0.6 & 1.2  \\\\\n& LC - ws(100)& 0.7 & 1.0 & 0.6 & 0.2 & 0.9  \\\\\n\n \\bottomrule\n\\end{tabular}}\n\\label{app:tab:pad4_minmax}\n\\end{table}\n\n\\begin{table}[]\n\\caption{Max minus min performance over 5 runs; padding of 8.}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}cccccccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Regularizer}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multicolumn{1}{c}{\\textbf{CIFAR10}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR100}} &\n  \\multicolumn{2}{c}{\\textbf{TinyImageNet}} \\\\ \\cmidrule(l){3-7} \n &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n  \\\\ \\midrule\n\\multirow{2}{*}{-} & conv& 0.9 & 1.5 & 1.2 & 1.7 & 1.0  \\\\\n& LC& 0.5 & 0.6 & 0.5 & 0.5 & 0.9  \\\\\n\\midrule\n\\multirow{3}{*}{Data Translation} & LC - 4 reps& 0.4 & 0.9 & 0.3 & 0.6 & 0.8  \\\\\n& LC - 8 reps& 0.6 & 0.9 & 0.5 & 0.5 & 0.6  \\\\\n& LC - 16 reps& 0.9 & 0.9 & 0.6 & 0.5 & 1.1  \\\\\n\\midrule\n\\multirow{3}{*}{Weight Sharing} & LC - ws(1)& 0.4 & 1.2 & 1.5 & 0.7 & 0.7  \\\\\n& LC - ws(10)& 0.2 & 1.4 & 0.9 & 1.4 & 1.2  \\\\\n& LC - ws(100)& 0.4 & 0.5 & 0.7 & 0.7 & 0.9  \\\\ \n \\bottomrule\n\\end{tabular}}\n\\label{app:tab:pad8_minmax}\n\\end{table}\n\n\n\n\\begin{table}[]\n\\caption{Hyperparameters for padding of 0.}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}ccccccccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Regularizer}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR10}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR100}} &\n  \\multicolumn{2}{c}{\\textbf{TinyImageNet}} \\\\ \\cmidrule(l){3-8} \n &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Learning \\\\ rate \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Weight \\\\ decay \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Learning \\\\ rate \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Weight \\\\ decay \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Learning \\\\ rate \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Weight \\\\ decay \\end{tabular} &\n  \\\\ \\midrule\n\\multirow{2}{*}{-} & conv& 0.01 & 0.01 & 0.01 & 0.01 & 0.005 & 0.01  \\\\\n& LC& 0.001 & 0.01 & 0.001 & 0.01 & 0.001 & 0.0001  \\\\\n\\midrule\n\\multirow{3}{*}{Weight Sharing} & LC - ws(1)& 0.001 & 0.01 & 0.001 & 0.01 & 0.001 & 0.0001  \\\\\n& LC - ws(10)& 0.0005 & 0.01 & 0.0005 & 0.0001 & 0.0005 & 0.01  \\\\\n& LC - ws(100)& 0.0001 & 0.01 & 0.0001 & 0.01 & 0.001 & 0.0001  \\\\\n \\bottomrule\n\\end{tabular}}\n\\label{app:tab:pad0_param}\n\\end{table}\n\n\n\\begin{table}[]\n\\caption{Hyperparameters for padding of 4.}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}ccccccccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Regularizer}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR10}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR100}} &\n  \\multicolumn{2}{c}{\\textbf{TinyImageNet}} \\\\ \\cmidrule(l){3-8} \n &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Learning \\\\ rate \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Weight \\\\ decay \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Learning \\\\ rate \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Weight \\\\ decay \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Learning \\\\ rate \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Weight \\\\ decay \\end{tabular} &\n  \\\\ \\midrule\n\\multirow{2}{*}{-} & conv& 0.01 & 0.0001 & 0.01 & 0.01 & 0.005 & 0.0001  \\\\\n& LC& 0.001 & 0.0001 & 0.0005 & 0.01 & 0.0005 & 0.0001  \\\\\n\\midrule\n\\multirow{3}{*}{Data Translation} & LC - 4 reps& 0.001 & 0.01 & 0.001 & 0.01 & 0.0005 & 0.01  \\\\\n& LC - 8 reps& 0.0005 & 0.01 & 0.0005 & 0.0001 & 0.0005 & 0.01  \\\\\n& LC - 16 reps& 0.0005 & 0.01 & 0.0005 & 0.01 & 0.0005 & 0.01  \\\\\n\\midrule\n\\multirow{3}{*}{Weight Sharing} & LC - ws(1)& 0.001 & 0.01 & 0.001 & 0.0001 & 0.001 & 0.01  \\\\\n& LC - ws(10)& 0.0005 & 0.01 & 0.0005 & 0.01 & 0.001 & 0.0001  \\\\\n& LC - ws(100)& 0.0005 & 0.01 & 0.0005 & 0.01 & 0.001 & 0.01  \\\\ \\bottomrule\n\\end{tabular}}\n\\label{app:tab:pad4_param}\n\\end{table}\n\n\\begin{table}[]\n\\caption{Hyperparameters for padding of 8.}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}ccccccccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Regularizer}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR10}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR100}} &\n  \\multicolumn{2}{c}{\\textbf{TinyImageNet}} \\\\ \\cmidrule(l){3-8} \n &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Learning \\\\ rate \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Weight \\\\ decay \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Learning \\\\ rate \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Weight \\\\ decay \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Learning \\\\ rate \\end{tabular} &\n  \n  \\begin{tabular}[c]{@{}c@{}}Weight \\\\ decay \\end{tabular} &\n  \\\\ \\midrule\n\\multirow{2}{*}{-} & conv& 0.01 & 0.01 & 0.01 & 0.01 & 0.005 & 0.01  \\\\\n& LC& 0.001 & 0.01 & 0.0005 & 0.0001 & 0.001 & 0.01  \\\\\n\\midrule\n\\multirow{3}{*}{Data Translation} & LC - 4 reps& 0.0005 & 0.01 & 0.001 & 0.0001 & 0.0005 & 0.01  \\\\\n& LC - 8 reps& 0.001 & 0.01 & 0.0005 & 0.0001 & 0.0005 & 0.0001  \\\\\n& LC - 16 reps& 0.0005 & 0.0001 & 0.0005 & 0.01 & 0.0005 & 0.01  \\\\\n\\midrule\n\\multirow{3}{*}{Weight Sharing} & LC - ws(1)& 0.001 &\n0.0001 & 0.001 & 0.01 & 0.001 & 0.01  \\\\\n& LC - ws(10)& 0.0005 & 0.01 & 0.0005 & 0.0001 & 0.001 & 0.0001  \\\\\n& LC - ws(100)& 0.0005 & 0.01 & 0.0005 & 0.0001 & 0.001 & 0.0001  \\\\\n\\bottomrule\n\\end{tabular}}\n\\label{app:tab:pad8_param}\n\\end{table}\n\n\\begin{table}[]\n\\caption{Performance of convolutional (conv), locally connected (LC) and locally connected with convolutional first layer  (LC + 1st layer conv) networks on ImageNet (1 run). For LC, we also used dynamic weight sharing every $n$ batches. LC nets additionally show performance difference w.r.t. the conv net.}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multirow{2}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Weight sharing \\\\ frequency\\end{tabular}}} &\n  \\multicolumn{4}{c}{\\textbf{ImageNet}} \\\\ \\cmidrule(l){4-7} \n &\n   &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular}} &\n  \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular}} &\n  \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}} Diff\\end{tabular}} \\\\\n  \\midrule\n  \\multirow{5}{*}{\\begin{tabular}[c]{@{}c@{}}0.5x ResNet18\\end{tabular}}\n    & conv & -   & 63.5   & - & 84.7 & - \\\\\n    & LC   & -   & 46.7   & -16.8  & 70.0 & -14.7 \\\\\n    & LC   & 1   & 61.7   & -1.8  & 83.1     & -1.6 \\\\\n    & LC   & 10  & 59.3  & -4.2  & 81.1 & -3.6 \\\\\n    & LC   & 100 & 54.5  & -9.0  & 77.7 & -7.0 \\\\\n    \\midrule\n   \\multirow{5}{*}{\\begin{tabular}[c]{@{}c@{}}0.5x ResNet18 \\\\ (1st layer conv)\\end{tabular}}\n    & LC   & -   & 52.2   & -11.3  & 75.1 & -9.6 \\\\\n    & LC   & 1   & 63.6   & 0.1  & 84.5 & -0.2 \\\\\n    & LC   & 10  & 61.6  & -1.9  & 83.1 & -1.6 \\\\\n    & LC   & 100 & 59.1  & -4.4  & 81.1 & -3.6 \\\\\n    \\bottomrule\n\\end{tabular}}\n\\label{app:tab:imagenet_accuracy}\n\\end{table}\n\\section{Experiments}\n\\label{sec:experiments}\nWe split our experiments into two parts: small-scale ones with CIFAR10, CIFAR100 \\cite{krizhevsky2009cifar} and TinyImageNet \\cite{le2015tiny}, and large-scale ones with ImageNet \\cite{deng2009imagenet}. The former illustrates the effects of data augmentation and dynamic weight sharing on performance of locally connected networks; While the latter concentrates on dynamic weight sharing, as extensive data augmentations are too computationally expensive for large networks and datasets. We used the AdamW \\cite{loshchilov2017decoupled} optimizer in all runs. Our code is available at \\href{https://github.com/romanpogodin/towards-bio-plausible-conv}{https://github.com/romanpogodin/towards-bio-plausible-conv} (PyTorch \\cite{NEURIPS2019_9015} implementation).\n\n\\textbf{Datasets.} CIFAR10 consists of 50k training and 10k test images of size $32\\times 32$, divided into 10 classes. CIFAR100 has the same structure, but with 100 classes. For both, we tune hyperparameters with a 45k/5k train/validation split, and train final networks on the full 50k training set. TinyImageNet consists of 100k training and 10k validation images of size $64\\times 64$, divided into 200 classes. As the test labels are not publicly available, we divided the training set into 90k/10k train/validation split, and used the 10k official validation set as test data. ImageNet consists of 1281k training images and 50k test images of different sizes, reshaped to 256 pixels in the smallest dimension. As in the case for TinyImageNet, we used the train set for a 1271k/10k train/validation split, and 50k official validation set as test data.\n\n\\textbf{Networks.} For CIFAR10/100 and TinyImageNet, we used CIFAR10-adapted ResNet20 from the original ResNet paper \\cite{he2016deep}. The network has three blocks, each consisting of 6 layers, with 16/32/64 channels within the block. We chose this network due to good performance on CIFAR10, and the ability to fit the corresponding locally connected network into the 8G VRAM of the GPU for large batch sizes on all three datasets. For ImageNet, we took the half-width ResNet18 (meaning 32/64/128/256 block widths) to be able to fit a common architecture (albeit halved in width) in the locally connected regime into 16G of GPU VRAM. For both networks, all layers had $3 \\times 3$ receptive field (apart from a few $1\\times 1$ residual downsampling layers), meaning that weight sharing worked over $9$ individual grids in each layer.\n\n\\textbf{Training details.} We ran the experiments on our local laboratory cluster, which consists mostly of NVIDIA GTX1080 and RTX5000 GPUs. The small-scale experiments took from 1-2 hours per run up to 40 hours (for TinyImageNet with 16 repetitions). The large-scale experiments took from 3 to 6 days on RTX5000 (the longest run was the locally connected network with weight sharing happening after every minibatch update).\n\n\\subsection{Data augmentations.} \nFor CIFAR10/100, we padded the images (padding size depended on the experiment) with mean values over the training set (such that after normalization the padded values were zero) and cropped to size $32\\times 32$. We did not use other augmentations to separate the influence of padding/random crops. For TinyImageNet, we first center-cropped the original images to size $(48+2\\,\\mathrm{pad}) \\times (48+2\\,\\mathrm{pad})$ for the chosen padding size $\\mathrm{pad}$. The final images were then randomly cropped to $48\\times 48$. This was done to simulate the effect of padding on the number of available translations, and to compare performance across different padding values on the images of the same size (and therefore locally connected networks of the same size). After cropping, the images were normalized using ImageNet normalization values. For all three datasets, test data was sampled without padding. For ImageNet, we used the standard augmentations. Training data was resized to 256 (smallest dimension), random cropped to $224\\times 224$, flipped horizontally with $0.5$ probability, and then normalized. Test data was resized to 256, center cropped to 224 and then normalized. In all cases, data repetitions included multiple samples of the same image within a batch, keeping the total number of images in a batch fixed (e.g. for batch size 256 and 16 repetitions, that would mean 16 original images)\n\n\\begin{table}[]\n\\caption{Performance of convolutional (conv) and locally connected (LC) networks for padding of 4 in the input images (mean accuracy over 5 runs). For LC, two regularization strategies were applied: repeating the same image $n$ times with different translations ($n$ reps) or using dynamic weight sharing every $n$ batches (ws($n$)). LC nets additionally show performance difference w.r.t. conv nets.}\n\\resizebox{\\textwidth}{!}{\\begin{tabular}{@{}cccccccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Regularizer}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multicolumn{2}{c}{\\textbf{CIFAR10}} &\n  \\multicolumn{4}{c}{\\textbf{CIFAR100}} &\n  \\multicolumn{4}{c}{\\textbf{TinyImageNet}} \\\\ \\cmidrule(l){3-12} \n &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular} &\n  \\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular} \\\\ \\midrule\n  \\multirow{2}{*}{-} & conv& 88.3 & - & 59.2 & - & 84.9 & - & 38.6 & - & 65.1 & -  \\\\\n& LC& 80.9 & -7.4 & 49.8 & -9.4 & 75.5 & -9.4 & 29.6 & -9.0 & 52.7 & -12.4  \\\\\n\\midrule\n\\multirow{3}{*}{Data Translation} & LC - 4 reps& 82.9 & -5.4 & 52.1 & -7.1 & 76.4 & -8.5 & 31.9 & -6.7 & 54.9 & -10.2  \\\\\n& LC - 8 reps& 83.8 & -4.5 & 54.3 & -5.0 & 77.9 & -7.0 & 33.0 & -5.6 & 55.6 & -9.5  \\\\\n& LC - 16 reps& 85.0 & -3.3 & 55.9 & -3.3 & 78.8 & -6.1 & 34.0 & -4.6 & 56.2 & -8.8  \\\\\n\\midrule\n\\multirow{3}{*}{Weight Sharing} & LC - ws(1)& 87.4 & -0.8 & 58.7 & -0.5 & 83.4 & -1.6 & 41.6 & 3.0 & 66.1 & 1.1  \\\\\n& LC - ws(10)& 85.1 & -3.2 & 55.7 & -3.6 & 80.9 & -4.0 & 37.4 & -1.2 & 61.8 & -3.2  \\\\\n& LC - ws(100)& 82.0 & -6.3 & 52.8 & -6.4 & 80.1 & -4.8 & 37.1 & -1.5 & 62.8 & -2.3  \\\\\\bottomrule\\end{tabular}}\n\\label{tab:small}\n\\end{table}\n\\subsection{CIFAR10/100 and TinyImageNet}\nTo study the effect of both data augmentation and weight sharing on performance, we ran experiments with non-augmented images (padding 0) and with different amounts of augmentations.\nThis included padding of 4 and 8, and repetitions of 4, 8, and 16. Without augmentations, locally connected networks performed much worse than convolutional, although weight sharing improved the result a little bit (see \\cref{app:experiments}). For padding of 4 (mean accuracy over 5 runs \\cref{tab:small}, see \\cref{app:experiments} for max-min accuracy), increasing the number of repetitions increased the performance of locally connected networks. However, even for 16 repetitions, the improvements were small comparing to weight sharing (especially for top-5 accuracy on TinyImageNet).\nFor dynamic weight sharing, doing it moderately often -- every 10 iterations, meaning every 5120 images -- did as well as 16 repetitions on CIFAR10/100. For TinyImageNet, sharing weights every 100 iterations (about every 50k images) performed much better than data augmentation. \n\nSharing weights after every batch performed almost as well as convolutions (and even a bit better on TinyImageNet, although the difference is small if we look at top-5 accuracy, which is a less volatile metric for 200 classes), but it is too frequent to be a plausible ``sleep'' phase. We include it to show that best possible performance of partial weight sharing is comparable to actual convolutions. %\n\nFor padding of 8, the performance did improve for all methods (including convolutions), but the relative differences had a similar trend as for padding of 4 (see \\cref{app:experiments}).\nWe also trained locally connected network with one repetition, but for longer and with a much smaller learning rate to simulate the effect of data repetitions. Even for 4x-8x longer runs, the networks barely matched the performance of a 1-repetition network on standard speed (not shown).\n\n\n\\subsection{ImageNet}\nOn ImageNet, we did not test image repetitions due to the computational requirements (e.g., running 16 repetitions with our resources would take almost 3 months). We used the standard data augmentation, meaning that all networks see different crops of the same image throughout training.\n\nOur results (\\cref{tab:imagenet_accuracy}) show that even infrequent dynamic weight sharing (10 iterations, which is 2560 images for the batch size of 256) achieves nearly convolutional performance. In contrast, the purely locally connected network has a large performance gap w.r.t. the convolutional one. \n\n\\begin{table}[]\n\\caption{Performance of convolutional (conv) and locally connected (LC) networks on ImageNet (1 run). For LC, we also used dynamic weight sharing every $n$ batches. LC nets additionally show performance difference w.r.t. the conv net.}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}} &\n  \\multirow{2}{*}{\\textbf{Connectivity}} &\n  \\multirow{2}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Weight sharing \\\\ frequency\\end{tabular}}} &\n  \\multicolumn{4}{c}{\\textbf{ImageNet}} \\\\ \\cmidrule(l){4-7} \n &\n   &\n   &\n  \\begin{tabular}[c]{@{}c@{}}Top-1 \\\\ accuracy (\\%)\\end{tabular} &\n  \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}Diff\\end{tabular}} &\n  \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}Top-5 \\\\ accuracy (\\%)\\end{tabular}} &\n  \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}} Diff\\end{tabular}} \\\\\n  \\midrule\n  \\multirow{5}{*}{\\begin{tabular}[c]{@{}c@{}}0.5x ResNet18\\end{tabular}}\n    & conv & -   & 63.5   & - & 84.7 & - \\\\\n    & LC   & -   & 46.7   & -16.8  & 70.0 & -14.7 \\\\\n    & LC   & 1   & 61.7   & -1.8  & 83.1     & -1.6 \\\\\n    & LC   & 10  & 59.3  & -4.2  & 81.1 & -3.6 \\\\\n    & LC   & 100 & 54.5  & -9.0  & 77.7 & -7.0 \\\\\n    \\bottomrule\n\\end{tabular}}\n\\label{tab:imagenet_accuracy}\n\\end{table}\n\\section{Introduction}\nConvolutional networks are a cornerstone of modern deep learning: they're widely used in the visual  domain \\cite{krizhevsky2012imagenet, simonyan2014very, he2016deep}, speech recognition \\cite{abdel2014convolutional}, text classification \\cite{conneau2016very}, and time series classification \\cite{karim2017lstm} -- basically, in any system that is approximately translation invariant.\nThey have also played an important role in enhancing our understanding of the visual stream \\cite{lindsay2020conv}. Indeed, simple and complex cells in the visual cortex \\cite{hubel1962receptive} inspired convolutional and pooling layers in deep networks \\cite{fukushima1982neocognitron} (with simple cells implemented with convolution and complex ones with pooling).\nMoreover, the representations found in convolutional networks are similar to those in the visual stream \\cite{yamins2014performance, khaligh2014deep, schrimpf2018brain, cadena2019deep} (see \\cite{lindsay2020conv} for an in-depth review).\n\nDespite the success of convolutional networks at reproducing activity in the visual system, as a model of the visual system they are somewhat problematic. That's because convolutional networks share weights, something biological networks, for which weight updates must be local, can't do \\cite{grossberg1987competitive}. Locally connected networks avoid this problem by using the same receptive fields as convolutional networks (thus locally connected), but without weight sharing \\cite{bartunov2018assessing}. However, they pay a price for biological plausibility: locally connected networks are known to perform worse than their convolutional counterparts on hard image classification tasks \\cite{bartunov2018assessing, d2019finding}. There is, therefore, a need for a mechanism to bridge the gap between biologically plausible locally connected networks and implausible convolutional ones.\n\nHere, we consider two such mechanisms. One is to use extensive data augmentation (primarily image translations); the other is to introduce an auxiliary objective that allows some form of weight sharing, which is implemented by lateral connections; we call this approach dynamic weight sharing.\n\nThe first approach, data augmentation, is simple, but we show that it suffers from two problems: it requires far more training data than is normally used, and even then it fails to close the performance gap between convolutional and locally connected networks. The second approach, dynamic weight sharing, implements a sleep-like phase in which neural dynamics facilitate weight sharing. This is done through lateral connections in each layer, which allows subgroups of neurons to share their activity. Through this lateral connectivity, each subgroup can first equalize its weights via anti-Hebbian learning, and then generate an input pattern for the next layer that helps it to do the same thing. Dynamic weight sharing doesn't achieve perfectly convolutional connectivity, as in each channel only subgroups of neurons share weights. However, it implements a similar inductive bias, and, as we show in experiments, it performs almost as well as convolutional networks. \n\nOur study suggests that convolutional networks may be biologically plausible as they can be approximated in realistic networks simply by adding lateral connectivity and Hebbian learning. As convolutional networks and locally connected networks with dynamic weight sharing have similar performance, convolutional networks remain a good ``model organism'' for neuroscience. This is important, as they consume much less memory than locally connected networks, and run much faster.\n\n\\section{Related work}\n\nStudying systems neuroscience through the lens of deep learning is an active area of research, especially when it comes to the visual system \\cite{richards2019deep}. As mentioned above, convolutional networks in particular have been extensively studied as a model of the visual stream (and also inspired by it) \\cite{lindsay2020conv}, and also as mentioned above, because they require weight sharing they lack biological plausibility. They also have been widely used to evaluate the performance of different  biologically plausible learning rules \\cite{nokland2016direct, bartunov2018assessing,moskovitz2018feedback, mostafa2018deep,nokland2019training, akrout2019deep, laborieux2020scaling,pogodin2020kernelized}.\n\nSeveral studies have tried to relax weight sharing in convolutions by introducing locally connected networks \\cite{bartunov2018assessing, d2019finding, neyshabur2020towards} (\\cite{neyshabur2020towards} also shows that local connectivity itself can be learned from a fully connected network with proper weight regularization). Locally connected networks perform as well as convolutional ones in shallow architectures \\cite{bartunov2018assessing, neyshabur2020towards}. However, they perform worse for large networks and hard tasks, unless they're initialized from an already well-performing convolutional solution \\cite{d2019finding} or have some degree of weight sharing \\cite{pmlr-v119-elsayed20a}. In this study, we seek biologically plausible regularizations of locally connected network to improve their performance.\n\nConvolutional networks are not the only deep learning architecture for vision: visual transformers (e.g., \\cite{dosovitskiy2020image, carion2020end, touvron2020training, zhu2020deformable}), and more recently, the transformer-like architectures without self-attention \\cite{tolstikhin2021mlpmixer, touvron2021resmlp, liu2021pay}, have shown competitive results. However, they still need weight sharing: at each block the input image is reshaped into patches, and then the same weight is used for all patches. Our Hebbian-based approach to weight sharing fits this computation as well (see \\cref{app:subsec:transformers}).\n\n\\section*{Acknowledgments}\nThis  work  was  supported  by  the  Gatsby  Charitable Foundation and the Wellcome Trust.\n\n\\bibliographystyle{unsrt}\n\n\\section{Submission of papers to NeurIPS 2021}\n\nPlease read the instructions below carefully and follow them faithfully.\n\n\\subsection{Style}\n\nPapers to be submitted to NeurIPS 2021 must be prepared according to the\ninstructions presented here. Papers may only be up to {\\bf nine} pages long,\nincluding figures. Additional pages \\emph{containing only acknowledgments and\nreferences} are allowed. Papers that exceed the page limit will not be\nreviewed, or in any other way considered for presentation at the conference.\n\nThe margins in 2021 are the same as those in 2007, which allow for $\\sim$$15\\%$\nmore words in the paper compared to earlier years.\n\nAuthors are required to use the NeurIPS \\LaTeX{} style files obtainable at the\nNeurIPS website as indicated below. Please make sure you use the current files\nand not previous versions. Tweaking the style files may be grounds for\nrejection.\n\n\\subsection{Retrieval of style files}\n\nThe style files for NeurIPS and other conference information are available on\nthe World Wide Web at\n\\begin{center}\n  \\url{http://www.neurips.cc/}\n\\end{center}\nThe file \\verb+neurips_2021.pdf+ contains these instructions and illustrates the\nvarious formatting requirements your NeurIPS paper must satisfy.\n\nThe only supported style file for NeurIPS 2021 is \\verb+neurips_2021.sty+,\nrewritten for \\LaTeXe{}.  \\textbf{Previous style files for \\LaTeX{} 2.09,\n  Microsoft Word, and RTF are no longer supported!}\n\nThe \\LaTeX{} style file contains three optional arguments: \\verb+final+, which\ncreates a camera-ready copy, \\verb+preprint+, which creates a preprint for\nsubmission to, e.g., arXiv, and \\verb+nonatbib+, which will not load the\n\\verb+natbib+ package for you in case of package clash.\n\n\\paragraph{Preprint option}\nIf you wish to post a preprint of your work online, e.g., on arXiv, using the\nNeurIPS style, please use the \\verb+preprint+ option. This will create a\nnonanonymized version of your work with the text ``Preprint. Work in progress.''\nin the footer. This version may be distributed as you see fit. Please \\textbf{do\n  not} use the \\verb+final+ option, which should \\textbf{only} be used for\npapers accepted to NeurIPS.\n\nAt submission time, please omit the \\verb+final+ and \\verb+preprint+\noptions. This will anonymize your submission and add line numbers to aid\nreview. Please do \\emph{not} refer to these line numbers in your paper as they\nwill be removed during generation of camera-ready copies.\n\nThe file \\verb+neurips_2021.tex+ may be used as a ``shell'' for writing your\npaper. All you have to do is replace the author, title, abstract, and text of\nthe paper with your own.\n\nThe formatting instructions contained in these style files are summarized in\nSections \\ref{gen_inst}, \\ref{headings}, and \\ref{others} below.\n\n\\section{General formatting instructions}\n\\label{gen_inst}\n\nThe text must be confined within a rectangle 5.5~inches (33~picas) wide and\n9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point\ntype with a vertical spacing (leading) of 11~points.  Times New Roman is the\npreferred typeface throughout, and will be selected for you by default.\nParagraphs are separated by \\nicefrac{1}{2}~line space (5.5 points), with no\nindentation.\n\nThe paper title should be 17~point, initial caps/lower case, bold, centered\nbetween two horizontal rules. The top rule should be 4~points thick and the\nbottom rule should be 1~point thick. Allow \\nicefrac{1}{4}~inch space above and\nbelow the title to rules. All pages should start at 1~inch (6~picas) from the\ntop of the page.\n\nFor the final version, authors' names are set in boldface, and each name is\ncentered above the corresponding address. The lead author's name is to be listed\nfirst (left-most), and the co-authors' names (if different address) are set to\nfollow. If there is only one co-author, list both author and co-author side by\nside.\n\nPlease pay special attention to the instructions in Section \\ref{others}\nregarding figures, tables, acknowledgments, and references.\n\n\\section{Headings: first level}\n\\label{headings}\n\nAll headings should be lower case (except for first word and proper nouns),\nflush left, and bold.\n\nFirst-level headings should be in 12-point type.\n\n\\subsection{Headings: second level}\n\nSecond-level headings should be in 10-point type.\n\n\\subsubsection{Headings: third level}\n\nThird-level headings should be in 10-point type.\n\n\\paragraph{Paragraphs}\n\nThere is also a \\verb+\\paragraph+ command available, which sets the heading in\nbold, flush left, and inline with the text, with the heading followed by 1\\,em\nof space.\n\n\\section{Citations, figures, tables, references}\n\\label{others}\n\nThese instructions apply to everyone.\n\n\\subsection{Citations within the text}\n\nThe \\verb+natbib+ package will be loaded for you by default.  Citations may be\nauthor/year or numeric, as long as you maintain internal consistency.  As to the\nformat of the references themselves, any style is acceptable as long as it is\nused consistently.\n\nThe documentation for \\verb+natbib+ may be found at\n\\begin{center}\n  \\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}\n\\end{center}\nOf note is the command \\verb+\\citet+, which produces citations appropriate for\nuse in inline text.  For example,\n\\begin{verbatim}\n   \\citet{hasselmo} investigated\\dots\n\\end{verbatim}\nproduces\n\\begin{quote}\n  Hasselmo, et al.\\ (1995) investigated\\dots\n\\end{quote}\n\nIf you wish to load the \\verb+natbib+ package with options, you may add the\nfollowing before loading the \\verb+neurips_2021+ package:\n\\begin{verbatim}\n   \\PassOptionsToPackage{options}{natbib}\n\\end{verbatim}\n\nIf \\verb+natbib+ clashes with another package you load, you can add the optional\nargument \\verb+nonatbib+ when loading the style file:\n\\begin{verbatim}\n   \\usepackage[nonatbib]{neurips_2021}\n\\end{verbatim}\n\nAs submission is double blind, refer to your own published work in the third\nperson. That is, use ``In the previous work of Jones et al.\\ [4],'' not ``In our\nprevious work [4].'' If you cite your other papers that are not widely available\n(e.g., a journal paper under review), use anonymous author names in the\ncitation, e.g., an author of the form ``A.\\ Anonymous.''\n\n\\subsection{Footnotes}\n\nFootnotes should be used sparingly.  If you do require a footnote, indicate\nfootnotes with a number\\footnote{Sample of the first footnote.} in the\ntext. Place the footnotes at the bottom of the page on which they appear.\nPrecede the footnote with a horizontal rule of 2~inches (12~picas).\n\nNote that footnotes are properly typeset \\emph{after} punctuation\nmarks.\\footnote{As in this example.}\n\n\\subsection{Figures}\n\n\\begin{figure}\n  \\centering\n  \\fbox{\\rule[-.5cm]{0cm}{4cm} \\rule[-.5cm]{4cm}{0cm}}\n  \\caption{Sample figure caption.}\n\\end{figure}\n\nAll artwork must be neat, clean, and legible. Lines should be dark enough for\npurposes of reproduction. The figure number and caption always appear after the\nfigure. Place one line space before the figure caption and one line space after\nthe figure. The figure caption should be lower case (except for first word and\nproper nouns); figures are numbered consecutively.\n\nYou may use color figures.  However, it is best for the figure captions and the\npaper body to be legible if the paper is printed in either black/white or in\ncolor.\n\n\\subsection{Tables}\n\nAll tables must be centered, neat, clean and legible.  The table number and\ntitle always appear before the table.  See Table~\\ref{sample-table}.\n\nPlace one line space before the table title, one line space after the\ntable title, and one line space after the table. The table title must\nbe lower case (except for first word and proper nouns); tables are\nnumbered consecutively.\n\nNote that publication-quality tables \\emph{do not contain vertical rules.} We\nstrongly suggest the use of the \\verb+booktabs+ package, which allows for\ntypesetting high-quality, professional tables:\n\\begin{center}\n  \\url{https://www.ctan.org/pkg/booktabs}\n\\end{center}\nThis package was used to typeset Table~\\ref{sample-table}.\n\n\\begin{table}\n  \\caption{Sample table title}\n  \\label{sample-table}\n  \\centering\n  \\begin{tabular}{lll}\n    \\toprule\n    \\multicolumn{2}{c}{Part}                   \\\\\n    \\cmidrule(r){1-2}\n    Name     & Description     & Size ($\\mu$m) \\\\\n    \\midrule\n    Dendrite & Input terminal  & $\\sim$100     \\\\\n    Axon     & Output terminal & $\\sim$10      \\\\\n    Soma     & Cell body       & up to $10^6$  \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\section{Final instructions}\n\nDo not change any aspects of the formatting parameters in the style files.  In\nparticular, do not modify the width or length of the rectangle the text should\nfit into, and do not change font sizes (except perhaps in the\n\\textbf{References} section; see below). Please note that pages should be\nnumbered.\n\n\\section{Preparing PDF files}\n\nPlease prepare submission files with paper size ``US Letter,'' and not, for\nexample, ``A4.''\n\nFonts were the main cause of problems in the past years. Your PDF file must only\ncontain Type 1 or Embedded TrueType fonts. Here are a few instructions to\nachieve this.\n\n\\begin{itemize}\n\n\\item You should directly generate PDF files using \\verb+pdflatex+.\n\n\\item You can check which fonts a PDF files uses.  In Acrobat Reader, select the\n  menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can\n  also use the program \\verb+pdffonts+ which comes with \\verb+xpdf+ and is\n  available out-of-the-box on most Linux machines.\n\n\\item The IEEE has recommendations for generating PDF files whose fonts are also\n  acceptable for NeurIPS. Please see\n  \\url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}\n\n\\item \\verb+xfig+ \"patterned\" shapes are implemented with bitmap fonts.  Use\n  \"solid\" shapes instead.\n\n\\item The \\verb+\\bbold+ package almost always uses bitmap fonts.  You should use\n  the equivalent AMS Fonts:\n\\begin{verbatim}\n   \\usepackage{amsfonts}\n\\end{verbatim}\nfollowed by, e.g., \\verb+\\mathbb{R}+, \\verb+\\mathbb{N}+, or \\verb+\\mathbb{C}+\nfor $\\mathbb{R}$, $\\mathbb{N}$ or $\\mathbb{C}$.  You can also use the following\nworkaround for reals, natural and complex:\n\\begin{verbatim}\n   \\newcommand{\\RR}{I\\!\\!R} %\n   \\newcommand{\\Nat}{I\\!\\!N} %\n   \\newcommand{\\CC}{I\\!\\!\\!\\!C} %\n\\end{verbatim}\nNote that \\verb+amsfonts+ is automatically loaded by the \\verb+amssymb+ package.\n\n\\end{itemize}\n\nIf your file contains type 3 fonts or non embedded TrueType fonts, we will ask\nyou to fix it.\n\n\\subsection{Margins in \\LaTeX{}}\n\nMost of the margin problems come from figures positioned by hand using\n\\verb+\\special+ or other commands. We suggest using the command\n\\verb+\\includegraphics+ from the \\verb+graphicx+ package. Always specify the\nfigure width as a multiple of the line width as in the example below:\n\\begin{verbatim}\n   \\usepackage[pdftex]{graphicx} ...\n   \\includegraphics[width=0.8\\linewidth]{myfile.pdf}\n\\end{verbatim}\nSee Section 4.4 in the graphics bundle documentation\n(\\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})\n\nA number of width problems arise when \\LaTeX{} cannot properly hyphenate a\nline. Please give LaTeX hyphenation hints using the \\verb+\\-+ command when\nnecessary.\n\n\\begin{ack}\nUse unnumbered first level headings for the acknowledgments. All acknowledgments\ngo at the end of the paper before the list of references. Moreover, you are required to declare\nfunding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).\nMore information about this disclosure can be found at: \\url{https://neurips.cc/Conferences/2021/PaperInformation/FundingDisclosure}.\n\nDo {\\bf not} include this section in the anonymized submission, only in the final paper. You can use the \\texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.\n\\end{ack}\n\n\\section*{References}\n\nReferences follow the acknowledgments. Use unnumbered first-level heading for\nthe references. Any choice of citation style is acceptable as long as you are\nconsistent. It is permissible to reduce the font size to \\verb+small+ (9 point)\nwhen listing the references.\nNote that the Reference section does not count towards the page limit.\n\\medskip\n\n{\n\\small\n\n[1] Alexander, J.A.\\ \\& Mozer, M.C.\\ (1995) Template-based algorithms for\nconnectionist rule extraction. In G.\\ Tesauro, D.S.\\ Touretzky and T.K.\\ Leen\n(eds.), {\\it Advances in Neural Information Processing Systems 7},\npp.\\ 609--616. Cambridge, MA: MIT Press.\n\n[2] Bower, J.M.\\ \\& Beeman, D.\\ (1995) {\\it The Book of GENESIS: Exploring\n  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:\nTELOS/Springer--Verlag.\n\n[3] Hasselmo, M.E., Schnell, E.\\ \\& Barkai, E.\\ (1995) Dynamics of learning and\nrecall at excitatory recurrent synapses and cholinergic modulation in rat\nhippocampal region CA3. {\\it Journal of Neuroscience} {\\bf 15}(7):5249-5262.\n}\n\n\\section{Discussion}\n\\label{sec:disscusion}\nWe presented two ways to circumvent the biological implausibility of weight sharing, a crucial component of convolutional networks. The first way was through data augmentation via multiple image translations. The second was dynamic weight sharing via lateral connections, which allows neurons to share weight information during a sleep-like phase; weight updates are then done using Hebbian plasticity. \nData augmentations requires a large number of repetitions in the data, and consequently longer training times, and yields only small improvements in performance. However, at least a small number of repetitions can be naturally covered by saccades.\nDynamic weight sharing needs a separate ``weight sharing'' phase, rather than more data, and yields large performance gains. In fact, it achieves near convolutional performance even on hard tasks, such as ImageNet classification, making it a much more likely candidate than data augmentation for the brain.\n\nThere are several limitations to our implementation of dynamic weight sharing. First, it relies on precise lateral connectivity. This can be genetically encoded, or learned early on using correlations in the input data (if layer $l$ can generate repeated patters, layer $l+1$ can modify its lateral connectivity based on input correlations). Lateral connections do in fact exist in the visual stream, with neurons that have similar tuning curves showing strong lateral connections \\cite{fitzpatrick1997}.\nSecond, the ``sleep'' phase works iteratively over layers. This can be implemented with neuromodulation that enables plasticity one layer at a time. Alternatively, weight sharing could work simultaneously in the whole network due to weight regularization (as it makes sure the final solution preserves the initial average weight), although this would require longer training due to additional noise in deep layers. Next, in our scheme the lateral connections are used only for dynamic weight sharing, and not for actual computation. However, they can be useful for computation too, as they can be used for centering of the neural activity (in deep networks, this is done as a part of  normalization layers). Finally, we trained networks using backpropagation, which is not biologically plausible \\cite{richards2019deep}. However, our weight sharing scheme is independent of the main training algorithm, and therefore can be applied with any biologically plausible update rule.\n\nOur approach to dynamic weight sharing is not only relevant to convolutions. First, it is applicable to non-convolutional networks, and in particular visual transformers \\cite{dosovitskiy2020image, carion2020end, touvron2020training, zhu2020deformable} (and more recent MLP-based architectures \\cite{tolstikhin2021mlpmixer, touvron2021resmlp, liu2021pay}). In such architectures, input images (and intermediate two-dimensional representations) are split into non-overlapping patches; each patch is then transformed with the \\textit{same} fully connected layer -- a computation that would require weight sharing in the brain. This can be done by connecting neurons across patches that have the same relative position, and applying our weight dynamics (see \\cref{app:subsec:transformers}).\nSecond, \\cite{akrout2019deep} faced a problem similar to weight sharing -- weight transport (i.e., neurons not knowing their output weights) -- when developing a plausible implementation of backprop. Their weight mirror algorithms used an idea similar to ours: the value of one weight was sent to another through correlations in activity.\n\nOur study shows that both performance and the computation of convolutional networks can be reproduced in more realistic architectures. This supports convolutional networks as a model of the visual stream, and also justifies them as a ``model organism'' for studying learning in the visual stream (which is important partially due to their computational efficiency). While our study does not have immediate societal impacts (positive or negative), it further strengthens the role of artificial neural networks as a model of the brain. Such models can guide medical applications such as brain machine interface and neurological rehabilitation. However, that could also lead to the design of potentially harmful adversarial attacks on the brain. \n\n\\section{Regularization in locally connected networks}\n\\subsection{Convolutional versus locally connected networks}\nConvolutional networks are implemented by letting the weights depend on the difference in indices. Considering, for simplicity, one dimensional convolutions and a linear network. Letting the input and output of a one layer in a network be $x_j$ and $z_i$, respectively, the activity in a convolutional network is\n\n\\begin{equation}\n    z_i = \\sum_{j=1}^N w_{i-j} x_j \\, ,\n\\end{equation}\nwhere $N$ is the number of neurons; for definiteness, we'll assume $N$ is the same in in each layer (right panel in \\cref{fig:conv_lc_fc}).\nAlthough the index $j$ ranges over all $N$ neurons, many, if not most, of the weights are zero: $w_{i-j}$ is nonzero only when $|i-j| \\le k / 2 < N$ for the kernel size $k$.\n\nFor networks that aren't convolutional, the weight matrix $w_{i-j}$ is replaced by $w_{ij}$,\n\\begin{equation}\n    z_i = \\sum_{j=1}^N w_{ij} x_{j} \\, .\n    \\label{zll}\n\\end{equation}\nAgain, the index $j$ ranges over all $N$ neurons. If all the weights are nonzero, the network is fully connected (left panel in \\cref{fig:conv_lc_fc}). But, as in convolutional networks, we can restrict the connectivity range by letting $w_{ij}$ be nonzero only when $|i-j| \\le k / 2 < N$, resulting in a locally connected (but still non-convolutional) network (center panel in \\cref{fig:conv_lc_fc}).\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{diagrams/network_types.pdf}\n    \\caption{Comparison between layer architectures: fully connected (left), locally connected (middle) and convolutional (right). Locally connected layers do no share weights among neurons (like fully connected ones), but have the same receptive fields as convolutional layers.}\n    \\label{fig:conv_lc_fc}\n\\end{figure}\n\n\\subsection{Developing convolutional weights: data augmentation versus dynamic weight sharing}\nHere we explore the question: is it possible for a locally connected network to develop approximately convolutional weights? That is, after training, is it possible to have $w_{ij} \\approx w_{i-j}$?\nThere is one straightforward way to do this: augment the data to provide multiple translations of the same image, so that each neuron within a channel learns to react similarly to them (\\cref{fig:regul_approaches}A). A potential problem is that a large number of translations will be needed. This makes training costly (see \\cref{sec:experiments}), and is unlikely to be consistent with animal learning, as animals see only a handful of translations of any one image.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{diagrams/weight_regul.pdf}\n    \\caption{Two regularization strategies for locally connected networks. \\textbf{A.} Data augmentation, where multiple translations of the same image are presented simultaneously. \\textbf{B.} Dynamic weight sharing, where a subset of neurons equalizes their weights through lateral connections and learning.}\n    \\label{fig:regul_approaches}\n\\end{figure}\n\nA less obvious solution is to modify the network so that during learning the weights become approximately convolutional. As we show in the next section, this can be done by adding lateral connections, and introducing a ``sleep'' phase during training (\\cref{fig:regul_approaches}B). This solution doesn't need more data, but it does need an additional training step.\n\n\\section{A Hebbian solution to dynamic weight sharing}\n\nIf we were to train a locally connected network without any weight sharing or data augmentation, the weights of different neurons would diverge (region marked ``training'' in \\cref{fig:dynamic_w_sh}A). Our strategy to make them convolutional is to introduce an occasional ``sleep'' phase, during which the weights relax to their mean over output neurons (region marked ``sleep'' in \\cref{fig:dynamic_w_sh}A). This will compensate weight divergence during learning by convergence during the sleep phase. If the latter is sufficiently strong, the weights will remain approximately convolutional.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{diagrams/dynamic_w_sh.pdf}\n    \\caption{\\textbf{A.} Dynamical weight sharing interrupts the main training loop, and equalizes the weights through internal dynamics. After that, the weights start diverging again until the next weight sharing phase. \\textbf{B.} A locally connected network, where both input and output neurons have lateral connections. The input layer uses it to generate data: at each iteration, on of the grids on the left (indicated by colors) creates its own repeated pattern. The output layer uses this pattern to share weights.}\n    \\label{fig:dynamic_w_sh}\n\\end{figure}\n\n\n\n\n\nTo implement this, we introduce lateral connectivity, chosen to equalize activity in both the input and output layer. That's shown in \\cref{fig:dynamic_w_sh}B, where every third neuron in both the input ($\\textbf{x}$) and output ($\\textbf{z}$) layers are connected.\nOnce the connected neurons in the input layer have equal activity, all output neurons receive identical input. \nSince the lateral output connections also equalize activity, all connection that correspond to a translation by three neurons see exactly the same pre and postsynaptic activity. A naive Hebbian learning rule (with weight decay) would, therefore, make the network convolutional.\nHowever, we have to take care that the initial weighs are not over-written during Hebbian learning. We now describe how that is done.\n\nTo ease notation, we'll let $\\textbf{w}_i$ be a vector containing the incoming weights to neuron $i$: $(\\textbf{w}_i)_j \\equiv w_{ij}$. Moreover, we'll let $j$ run from 1 to $k$, independent of $i$. With this convention, the response of neuron $i$, $z_i$, to a $k$-dimensional input, $\\textbf{x}$, is given by\n\\begin{align}\n    z_i = \\textbf{w}_i\\trans \\textbf{x} = \\sum_{j=1}^k w_{ij} x_j \\, .\n\\end{align}\n\n\nAssume that every neuron sees the same $\\bb x$, and consider the following update rule for the weights,\n\\begin{equation}\n    \\Delta \\bb w_i \\propto -\\brackets{z_i - \\frac{1}{N}\\sum_{j=1}^N z_j} \\bb x - \\gamma \\brackets{\\bb w_i  - \\bb w_i^{\\mathrm{init}}}\\,,\n    \\label{eq:dynamic_weight_sharing_update}\n\\end{equation}\nwhere $\\bb w_i^{\\mathrm{init}}$ are the weights at the beginning  of the dynamic weight sharing cycle, and not before the overall training.\n\nThis Hebbian update effectively implements SGD over the sum of $(z_i-z_j)^2$   , plus a regularizer (the second term) to keep the weights near $\\textbf{w}_i^{\\rm init}$. If we present the network with $M$ different input vectors, $\\bb x_m$, and denote the covariance matrix $\\textbf{C} \\equiv \\frac{1}{M}\\sum_m \\bb x_m \\bb x_m\\trans$, then the weight dynamics in \\cref{eq:dynamic_weight_sharing_update} converges to (see \\cref{app:dynamic_w_sh} for the derivation)\n\\begin{align}\n    \\bb w_i^* \\,&= \\brackets{\\textbf{C} + \\gamma\\, \\textbf{I}}^{-1}  \\brackets{\\textbf{C}  \\frac{1}{N} \\sum_{i=1}^N \\bb w_i^{\\mathrm{init}} + \\gamma\\, \\bb w_i^{\\mathrm{init}}}\\,,\n    \\label{eq:dynamic_w_sh_solution}\n\\end{align}\nwhere $\\textbf{I}$ is the identity matrix.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\textwidth]{diagrams/snr_plots.pdf}\n    \\caption{Logarithm of inverse signal-to-noise ratio (mean weight squared over weight variance, see \\cref{eq:snr}) for weight sharing objectives in a layer with 100 neurons. \\textbf{A.} Dynamics of \\cref{eq:dynamic_weight_sharing_update} for different kernel sizes $k$ (meaning $k^2$ inputs) and $\\gamma$. Dark dotted lines show the theoretical minimum. \\textbf{B.} Dynamics of weight update that uses \\cref{eq:realistic_model_w_sh} for $\\alpha=10$, different kernel sizes $k$ and $\\gamma$. In each iteration, the input is presented for 150 ms.}\n    \\label{fig:snr_plots}\n\\end{figure}\n\nThis is exactly what we want, since as long as $\\textbf{C}$ is full rank %\nand $\\gamma$ is small, we arrive at shared weights: $\\bb w_i^* \\approx \\frac{1}{N}\\sum_{i=1}^N \\bb w_i^{\\mathrm{init}}$. It might seem advantageous to set $\\gamma=0$, as non-zero $\\gamma$ only biases the equilibrium value of the weight. However, non-zero $\\gamma$ ensures that for noisy input, $\\bb x_i = \\bb x + \\xi_i$ (which is much more realistic), the weights still converge (at least approximately) to the mean of the initial weights (see \\cref{app:dynamic_w_sh}). \n\nIn practice, the dynamics in \\cref{eq:dynamic_weight_sharing_update} converges quickly. We illustrate it in \\cref{fig:snr_plots}A by plotting $-\\log\\mathrm{SNR}_w$ over time, where $\\textrm{SNR}_w$, the signal to noise ratio of the weights, is defined as \n\\begin{equation}\n    \\mathrm{SNR}_w = \\frac{1}{k^2}\\sum_j\\frac{\\brackets{\\frac{1}{N}\\sum_i (\\bb w_i)_j}^2}{\\frac{1}{N}\\sum_i \\brackets{(\\bb w_i)_j - \\frac{1}{N}\\sum_{i'} (\\bb w_{i'})_j}^2}\\,.\n    \\label{eq:snr}\n\\end{equation}\nFor all kernel sizes (we used 2d inputs, meaning $k^2$ inputs per neuron), the weight converge to a nearly convolutional solution within a few hundred iterations (note the logarithmic scale of the y axis in \\cref{fig:snr_plots}A). See \\cref{app:dynamic_w_sh} for simulation details. In order to run our experiments with deep networks in a realistic time frame, we perform weight sharing directly during the sleep phase.\n\n\\subsection{Dynamic weight sharing in convolutional layers}\n\n\n\n\nAs shown in \\cref{fig:dynamic_w_sh}B, the $k$-dimensional input, $\\textbf{x}$, repeats every $k$ neurons. Consequently, during the sleep phase, the weights are not set to the mean of their initial value averaged across all neurons; instead, they're set to the mean averaged across a set of neurons spaced by $k$. Thus, in one dimension, the sleep phase equilibrates the weights in $k$ different modules. In two dimensions (the realistic case), the sleep phase equilibrates the weights in $k^2$ different modules.\n\nWe need this spacing to span the whole $k$-dimensional (or $k^2$ for 2d) space of inputs.\nFor instance, activating the red grid on the left in \\cref{fig:dynamic_w_sh}B generates $\\textbf{x}_1$, covering one input direction for all output neurons (and within each module, every neuron receives the same input). Next, activating the blue grid generates $\\textbf{x}_2$ (a new direction), and so on.\n\nIn multiple layers, the sleep phase is implemented layer by layer. In layer $l$, lateral connectivity creates repeated input patterns and feeds them to layer $l+1$.\nAfter weight sharing in layer $l+1$, the new pattern from $l+1$ is fed to $l+2$, and so on. We assume that the first layer can receive suitable patterns from the retina. However, this pattern generation scheme needs layers to have receptive fields of the same size.\n\n\\subsection{A realistic model that implements the update rule}\n\\label{sec:realistic_update}\nOur update rule, \\cref{eq:dynamic_weight_sharing_update}, implies a linear neuron which firing rate $r_i$ that depends on the upstream input $z_i=\\textbf{w}_i\\trans \\bb x$ and lateral input as\n\\begin{equation}\n    r_i = z_i - \\frac{1}{N}\\sum_{j=1}^N z_j \\equiv \\bb w_i\\trans \\bb x - \\frac{1}{N}\\sum_{j=1}^N \\bb w_j\\trans \\bb x \\,,\n\\end{equation}\nmaking an anti-Hebbian weight change $-r_i\\, \\bb x$.\nIn a realistic circuit, this can be implemented with excitatory neurons $r_i$, and an inhibitory neuron $r_{\\mathrm{inh}}$, which obey the dynamics\n\\begin{subequations}\n\\begin{align}\n    \\tau \\dot r_i \\,&= -r_i + \\bb w_i\\trans \\bb x - \\alpha\\, r_{\\mathrm{inh}} + b\\,,\\\\\n    \\tau \\dot r_{\\mathrm{inh}} \\,&= -r_{\\mathrm{inh}} + \\frac{1}{N} \\sum_j r_{j} - b\\,,\n    \\label{eq:realistic_dynamics}\n\\end{align}\n\\end{subequations}\nwhere $b$ is the shared bias term that ensures non-negativity of firing rates (assuming $\\sum_i \\bb w_i\\trans \\bb x$ is positive, which would be the case for excitatory input neurons).\nThe only fixed point of this equations is\n\\begin{equation}\n    r_i^* = b + \\bb w_i\\trans \\bb x - \\frac{1}{N}\\sum_j \\bb w_j\\trans \\bb x + \\frac{1}{1 + \\alpha}\\frac{1}{N}\\sum_j \\bb w_j\\trans \\bb x \\underset{\\alpha \\gg  1}{\\approx} b + \\bb w_i\\trans \\bb x - \\frac{1}{N}\\sum_j \\bb w_j\\trans \\bb x\\,,\n    \\label{eq:realistic_model_w_sh}\n\\end{equation}\nwhich is stable.\nAs a result, for strong inhibition ($\\alpha \\gg 1$),  \\cref{eq:dynamic_weight_sharing_update} can be implemented with an anti-Hebbian term $-(r_i-b)\\bb x$. Note that if $\\textbf{w}_i \\trans \\textbf{x}$ is zero on average, then $b$ is the mean firing rate over time. To show that \\cref{eq:realistic_model_w_sh} provides enough signal, we simulated training in a network of 100 neurons that receives a new $\\bb x$ each 150 ms. For a variety $k$ and $\\gamma$, it converged to a nearly convolutions solution within minutes (\\cref{fig:snr_plots}B, each iteration is 150 ms). Having finite inhibition did lead to worse final signal-to-noise ration ($\\alpha=10$ in  \\cref{fig:snr_plots}B), but the variance of the weight was still very small. Moreover, the nature of the $\\alpha$-induced bias suggests that stopping training before convergence leads to better results (around 2k iterations in \\cref{fig:snr_plots}B). See \\cref{app:dynamic_w_sh} for a discussion.", "meta": {"timestamp": "2021-06-25T02:19:33", "yymm": "2106", "arxiv_id": "2106.13031", "language": "en", "url": "https://arxiv.org/abs/2106.13031"}}
{"text": "\\section{Introduction}\n\\label{sec_I}\nThe presence of energetic, suprathermal particle populations, which can modify the global plasma dynamics due to their high kinetic energies while having a density small compared to that of the thermal particles, is a common feature in astrophysical and fusion plasmas. For example, such energetic particles are part of the cosmic rays originating outside the solar system that penetrate the magnetospheric plasma. Also, charged particles are energized and accelerated during solar flares and coronal mass ejections, due to magnetic reconnection and shock formation. In addition, it is known that in the magnetospheric ring-current plasma, energetic particles coexist with particles of significantly lower energy that constitute the plasma bulk (e.g. see \\cite{Daglis1999}). Although, the density of the thermal particles is dominant, the high-energy content of the fast particles renders even small populations capable of significantly affecting the plasma dynamics. Analogous situations, where suprathermal particles are produced and interact with a thermal plasma bulk occur in fusion experiments by external plasma heating mechanisms, such as Neutral Beam Injection (NBI) and  Ion Cyclotron Resonance Heating (ICRH), which accelerate hydrogen isotopes and \\({^3}He\\) to energies of order 1 MeV \\cite{Start1999}. Also, in a burning plasma the deuterium\u2013tritium  (D\u2013T) fusion reactions produce  particularly energetic alpha particles (3.5 MeV) that are expected to heat the plasma, and consequently their impact on the dynamics of burning plasmas cannot be neglected. Energetic particles in fusion experiments are responsible for the destabilization of the Alfv\\'en Eigenmodes (AEs) (e.g. see \\cite{Wesson2011,Chen2007,Todo2019}) due to resonant wave-particle interactions occuring \nwhen there exists a particle population with velocities near the phase velocity of the Alfv\\'en wave, leading eventually to particle losses, preventing energetic particles from transferring their energy to the thermal plasma and thus deteriorating the heating efficiency.\n\nThe coexistence of a thermal or cold bulk and suprathermal populations has motivated the development of several hybrid multi-scale plasma models using fluid equations to describe the bulk plasma and Vlasov, or reduced kinetic equations (Drift kinetic and Gyrokinetic) to describe the energetic particle dynamics. Of course, a fully kinetic description using, for example, the Maxwel-Vlasov system, contains all the micro and macro-physics involved in such systems. However, the hybrid fluid-kinetic description significantly reduces the computational cost since it is not required to simulate the dynamics of all particle species using kinetic equations. This makes the hybrid models important tools for performing numerical simulations and studying nonlinear dynamics because resolving all the kinetic scales of large systems with complex geometries is practically not possible with contemporary computational resources.\n\nThe hybrid models, require a set of fluid equations and a kinetic equation that should self-consistently describe the interaction of the plasma bulk with the energetic particles and the electromagnetic fields. For the fluid equations the most popular choice is to consider an MHD description while for the kinetic component one may utilize the Vlasov or reduced kinetic theories, if the magnetic field is strong and the particle magnetic moment is an adiabatic invariant.  To couple the dynamics of the two plasma components there are two main strategies: one relies on coupling through the current density, called Current Coupling Scheme (CCS) while the second is a Pressure Coupling Scheme (PCS) where the kinetic effects are introduced through the pressure tensors appearing in the fluid momentum equation. The kinetic-MHD model in PCS was introduced in \\cite{Cheng1991} and the first CCS kinetic-MHD, considering gyrokinetic particles was introduced in \\cite{Park1992} to study the nonlinear behavior of energetic particle effects.  Since then, several hybrid drift kinetic and gyrokinetic MHD models have been employed to simulate plasma dynamics containing Alfv\\'en eigenmodes, using either the CCS or the PCS, for example \\cite{Briguglio1995,Todo1998,Park1999,Zhu2016}. \n\nThe CCS and PCS variants of drift kinetic and gyrokinetic MHD models were formulated using Hamiltonian variational principles and Euler-Poincare reduction \\cite{Burby2017,Close2018}, and before that, a Hamiltonian approach to the hybrid description of plasmas combining the noncanonical Poisson bracket \\cite{Morrison1980,Morrison2009} of MHD with the particle bracket was developed in \\cite{Tronci2010} (see also \\cite{pjmTT14}). This approach resulted in Vlasov kinetic-MHD models in CCS and PCS.  The Hamiltonian construction of these models opened up the possibility to employ the Energy-Casimir Hamiltonian variational principle \\cite{Morrison1998} to derive equilibrium and stability conditions for planar kinetic-MHD using CCS and PCS in \\cite{pjmTT14,Tronci2015}. Tronci in \\cite{Tronci2010} also provides the derivation of a noncanonical Poisson bracket that correctly describes the dynamics of a standard hybrid model that treats the electrons as a fluid with zero inertia while retaining a kinetic description for the ions (see e.g. \\cite{Winske2003}). This model resolves the ion kinetic scales but not the electron ones, thus saving computational resources while reproducing the structural details of the reconnection region, which may consist of thin current sheets with thickness on the order of the ion inertial length. These characteristics, render the model a popular choice for studying magnetic reconnection \\cite{Hesse1994,Le2016,Cerri2017}.\n\nRecently, a generalized, quasineutral hybrid model containing additionally a fluid ion component while considering arbitrary number of kinetic species, was presented in \\cite{Amano2018}. This model makes no assumptions regarding the electron mass and employs the CCS for coupling between the fluid and kinetic components in contrast with other hybrid models that take into account electron inertial effects. The generalized model of \\cite{Amano2018} lies between the quasineutral two-fluid (QNTF) description and the common hybrid approach of purely kinetic ions and fluid electrons with finite or zero electron inertia. This unifying framework enables the study of kinetic effects in a continuous manner, starting from the fluid description and proceeding to situations with more than one kinetic and thermal species.\n\nIn this paper we follow the approach of \\cite{Amano2018} to derive the corresponding model in the CCS for massless electrons accompanied, however, with its Hamiltonian formulation. Starting from Hamiltonian theories such as, the Hall MHD \\cite{Holm1987,Lingam2015} and the Vlasov equation, which has no collision operator,  one expects to find a Hamiltonian structure for the resulting hybrid model. The identification of this structure might be important for the construction of structure preserving Hamiltonian algorithms that improve the stability and the fidelity of plasma simulations \\cite{Morrison2017,Kraus2017}. Such a structure preserving code has been recently developed for the simulation of MHD waves that interact with energetic particles in the framework of hybrid kinetic-MHD \\cite{Holderied2021}. Moreover, we obtain a second model upon considering the method of Hamiltonian construction of \\cite{Tronci2010} for the PCS. This new model has a set of Casimir invariants, i.e., global constants of motion whose gradients are elements of the Poisson kernel \\cite{Morrison1998}, that provide  an interesting coupling between the fluid and the kinetic components, as was the case in the Hamiltonian Kinetic-MHD model in the PCS studied in \\cite{Tronci2015}. This coupling leads to novel equilibrium conditions upon employing the energy-Casimir variational principle \\cite{Morrison1998}. An additional feature of our study is that we consider an anisotropic electron pressure tensor. Electron pressure anisotropy is a rather ubiquitous feature in guide-field magnetic reconnection, caused by particle trapping in the parallel electric fields \\cite{Egedal2008,Le2009}. Also, we should note that in the limit of massless electrons a gyrotropic pressure tensor can be consistently considered as in \\cite{Ito2007}. An analogous treatment for Amano's QNTF model would require the introduction of finite-Larmor-radius and gyroviscous effects which is not trivial in the Hamiltonian framework. This is left for future research. We note, however, that the early approach in the context of reduced fluid modelling in \\cite{pjmHH87} and significantly generalized in \\cite{Lingam2020} shows that working with Hamiltonian structure can enable  the introduction of such effects.\n\nThe present paper is organized as follows: in section 2 we present the parent equations, the method of constructing the CCS and the associated Hamiltonian structure. In section 3 we derive the novel hybrid model in the PCS using Tronci's method and we compare it with the model that would have been obtained using a standard (non-Hamiltonian) approach. In section 4 we derive the translationally symmetric counterpart of the Hamiltonian structure of the PCS and  the associated Casimir invariants. We employ also the energy-Casimir variational principle that leads to equilibrium equations and section 5 summarizes our results.\n\n\\section{Current Coupling Scheme}\n\\label{sec_II}\n\\subsection{The model equations}\nThe starting point of our derivation is the same as in \\cite{Tronci2010} and \\cite{Amano2018}, i.e. the multifluid equations governing the dynamics of the thermal components with the Vlasov equation for the energetic particles, providing self-consistent closure to the Maxwell equations:\n\\begin{eqnarray}\n&&\\partial_t n_s+\\nabla\\cdot(n_s{\\mathbf v}_s)=0\\,,\\label{mf_contin}\\\\\n&&m_sn_s\\left(\\partial_t{\\mathbf v}_s+{\\mathbf v}_s\\cdot\\nabla{\\mathbf v}_s\\right)=e_sn_s\\left({\\mathbf E}+{\\mathbf v}_s\\times{\\mathbf B}\\right)-\\nabla\\cdot\\mathbb{P}_s\\,,\\label{mf_momentum}\\\\\n&&\\partial_t f_p=-\\boldsymbol{v}\\cdot\\nabla f_p-\\frac{e_p}{m_p}\\left({\\mathbf E}+\\boldsymbol{v}\\times{\\mathbf B}\\right)\\cdot\\nabla_{\\boldsymbol{v}} f_p\\,, \\label{Vlasov}\\\\\n&&\\partial_t{\\mathbf B}=-\\nabla\\times{\\mathbf E}\\,, \\label{Faraday}\\\\\n&&\\partial_t{\\mathbf E}=\\epsilon_0^{-1}\\mu_0^{-1}\\nabla\\times{\\mathbf B}-\\epsilon_0^{-1}{\\mathbf J}\\,,\\label{Ampere}\\\\\n&&\\nabla\\cdot{\\mathbf E}=\\sigma/\\epsilon_0\\,,\\quad \\nabla\\cdot{\\mathbf B}=0\\,, \\label{Gauss}\\\\\n&&\\sigma= \\sigma_f+\\sigma_k\\,,\\quad \\sigma_f=\\sum_s e_s n_s\\,, \\; \\sigma_k=\\sum_p e_p \\int d^3v f_p\\,,\\\\ \\label{sigma_def}\n&&{\\mathbf J}={\\mathbf J}_f+{\\mathbf J}_k\\,,\\quad {\\mathbf J}_f=\\sum_s e_s n_s {\\mathbf v}_s \\,, \\; \n{\\mathbf J}_k=\\sum_p e_p \\int d^3v f_p \\boldsymbol{v} \\,, \\label{J_def}\n\\end{eqnarray}\nwhere, $\\mathbb{P}_s$ is the pressure tensor of the thermal species $s$,  $f_p=f_p({\\mathbf x},\\boldsymbol{v},t)$ is the Vlasov distribution function, i.e., the particle density in phase space \\({\\mathbf x},\\boldsymbol{v}\\), for the particle species \\(p\\). For a two-fluid plasma bulk the subscript \\(s\\) is \\(s=i,e\\). Note that \\(\\boldsymbol{v}\\) is the microscopic particle velocity, and therefore\n\\begin{eqnarray}\n\\frac{\\partial v_i}{\\partial x_j}=0\\,,\\quad \\forall i,j\\,.\n\\end{eqnarray}\nIn the low frequency limit it is legitimate to impose quasineutrality in a strict manner, i.e. setting $\\sigma=0$. The quasineutrality assumption renders the displacement current term negligible and the Gauss law redundant. In addition, when effects stemming from the finite electron inertia can be ignored, we can think of the electrons as being massless and use the electron fluid equation to solve for \\({\\mathbf E}\\) with $m_e=0$, i.e.\n\\begin{eqnarray}\n{\\mathbf E}=-{\\mathbf v}_e\\times {\\mathbf B}-\\frac{\\nabla\\cdot\\mathbb{P}_e}{en_e}\\,. \\label{Ohm_original}\n\\end{eqnarray}\nUsing equations \\eqref{Ampere} and \\eqref{J_def}, the electron velocity can be expressed in terms of the current density:\n\\begin{eqnarray}\n{\\mathbf v}_e=\\frac{n_i}{n_e}{\\mathbf v}-\\frac{1}{en_e}\\left(\\mu_0^{-1}\\nabla\\times{\\mathbf B}-\\sum_p e_p\\int d^3v \\boldsymbol{v} f_p\\right)\\,, \\label{v_e}\n\\end{eqnarray} \nwhere ${\\mathbf v}\\equiv {\\mathbf v}_i$. Inserting Eq. \\eqref{v_e} in the Ohm's law \\eqref{Ohm_original} we obtain\n\\begin{eqnarray}\n{\\mathbf E}=-\\frac{n_i}{n_e}{\\mathbf v}\\times{\\mathbf B}+\\frac{1}{en_e}\\left(\\mu_0^{-1}\\nabla\\times {\\mathbf B}-{\\mathbf J}_k\\right)\\times{\\mathbf B}-\\frac{\\nabla\\cdot\\mathbb{P}_e}{en_e}\\,. \\label{GOL}\n\\end{eqnarray}\nNote that due to the energetic particle component, quasineutrality does not imply $n_i\\neq n_e$, but $n_i=n_e-e^{-1}\\sigma_k$. In view of \\eqref{GOL} the Faraday's law leads to the following induction equation\n\\begin{eqnarray}\n\\partial_t{\\mathbf B}=\\nabla\\times\\left[\\frac{n_i}{n_e}{\\mathbf v}\\times{\\mathbf B}-\\frac{1}{en_e}\\left({\\mathbf J}-{\\mathbf J}_k\\right)\\times{\\mathbf B}+\\frac{\\nabla\\cdot\\mathbb{P}_e}{en_e}\\right]\\,, \\label{induction}\n\\end{eqnarray}\nwhere ${\\mathbf J}=\\mu_0^{-1}\\nabla\\times{\\mathbf B}$.\nInserting also the generalized Ohm's law \\eqref{GOL} into the ion momentum equation \\eqref{mf_momentum} and the Vlasov equation \\eqref{Vlasov} we obtain, respectively \n\\begin{eqnarray}\n&\\hspace{-1cm}\nmn_i\\left(\\partial_t{\\mathbf v}+{\\mathbf v}\\cdot\\nabla{\\mathbf v}\\right)=\\left[\\frac{n_i}{n_e}\\sigma_k {\\mathbf v} +\\frac{n_i}{n_e} \\left({\\mathbf J}-{\\mathbf J}_k \\right)\\right]\\times{\\mathbf B}-\\frac{n_i}{n_e}\\nabla\\cdot\\mathbb{P}_e-\\nabla P_i\\,,\\label{momentum_CCS}\\\\\n&\\hspace{-2cm}\n\\partial_t f_p+\\boldsymbol{v}\\cdot\\nabla f_p+\\frac{e_p}{m_p}\\left\\{\\left[\\boldsymbol{v}-\\frac{n_i}{n_e}{\\mathbf v}+\\frac{1}{en_e}\\left({\\mathbf J}-{\\mathbf J}_k\\right)\\right]\\times{\\mathbf B} -\\frac{\\nabla\\cdot\\mathbb{P}_e}{en_e}\\right\\}\\cdot\\nabla_{\\boldsymbol{v}} f_p=0\\,,\\label{Vlasov_CCS}\n\\end{eqnarray}\nwhere, from now on, $m\\equiv m_i$. Here we have assumed that the thermal ions have isotropic pressure. For a magnetized plasma, a consistent consideration of thermal pressure effects for the ions within the two fluid framework, requires the inclusion of anisotropic (and in particular non-gyrotropic) pressure effects. This will be though the topic of future research. Presently, it suffices for our purpose, which is to bring out the Hamiltonian nature of the model, to consider an isotropic pressure assuming low temperature ions. Thermal ions that deviate from the isotropic pressure description can be incorporated in the kinetic component described by the Vlasov equation.\n\nThe continuity equation for the ions remains unchanged, while for the electron fluid, inserting \\eqref{v_e} into  \\eqref{mf_contin} we find\n\\begin{eqnarray}\n\\partial_t n_e=-\\nabla\\cdot(n_i{\\mathbf v})-\\frac{\\nabla\\cdot{\\mathbf J}_k}{e}\\,, \\label{dn_e/dt}\n\\end{eqnarray}\nthat is \n\\begin{eqnarray}\ne\\partial_t(n_i-n_e)=\\nabla\\cdot {\\mathbf J}_k\\,, \\label{charge_conserv}\n\\end{eqnarray}\nwhich is an equation for electric charge conservation. This can be equivalently written in the following forms:\n\\begin{eqnarray}\n\\partial_t \\sigma_f+\\nabla\\cdot{\\mathbf J}_f=0\\,, \\quad \\partial_t \\sigma_k+\\nabla\\cdot {\\mathbf J}_k=0\\,.\n\\end{eqnarray}\nThe complete system of the dynamical equations consists of the ion continuity equation, the charge conservation equation \\eqref{charge_conserv} or one of its variants, the induction equation \\eqref{induction}, the momentum equation \\eqref{momentum_CCS} and the Vlasov equation \\eqref{Vlasov_CCS}. \n\\subsection{Hamiltonian structure of the CCS}\nLet us now follow the procedure established by Tronci in \\cite{Tronci2010} to derive the Hamiltonian structure of the above model. The difference here is that we apply the procedure in a more general model than those presented in \\cite{Tronci2010}, considering a multi-component plasma and generalizing also for electron pressure anisotropy. The starting point for our derivation is the combination of the Hall MHD noncanonical bracket, derived in \\cite{Holm1987}, with the particle Poisson bracket. This can be done by directly adding the two brackets, which are written, however, in terms of the canonical momentum density $\\bar{{\\mathbf M}}:=\\rho{\\mathbf v}+\\frac{e}{m}\\rho{\\mathbf A}$, where $\\rho:=m n_i$, and the canonical particle momentum $\\bar{{\\boldsymbol \\pi}}_p=m_p\\boldsymbol{v}+e_p{\\mathbf A}$, where ${\\mathbf A}$ is the vector potential. \n\nBefore we start this derivation let us recapitulate here the basic notions of noncanonical Hamiltonian dynamics. The Lagrange-Euler map from the Lagrangian to the Eulerian description, renders the Poisson brackets explicitly dependent on the dynamical variables of the system, say $\\boldsymbol{\\xi}$, i.e. it has the form\n\\begin{eqnarray}\n\\{F,G\\}=\\langle F_{\\xi_i},  {\\mathcal{J}} _{ij}(\\boldsymbol{\\xi}) G_{\\xi_j}\\rangle\\,,\n\\end{eqnarray}\nwhere, $F_\\xi$ represents the functional derivative of $F$ with respect to $\\xi$, and $ {\\mathcal{J}} (\\boldsymbol{\\xi})$ is the so-called Poisson operator.\n\n The Poisson bracket still satisfies the antisymmetry condition and the Jacobi identity\n\\begin{eqnarray}\n&\\{F,G\\}=-\\{G,F\\}\\,,\\\\\n&\\{F,\\{G,H\\}\\}+\\{H,\\{F,G\\}\\}+\\{G,\\{H,F\\}\\}=0\\,,\n\\end{eqnarray}\nwhere $F,G,H$ are functionals that are defined on the functional phase space. Due to the explicit dependence on the dynamical variables there exist non-trivial functionals $ {\\mathcal{C}} $ satisfying\n\\begin{eqnarray}\n\\{ {\\mathcal{C}} ,F\\}=0\\,,\\quad \\forall F\\,.\n\\end{eqnarray}\nThese functionals $ {\\mathcal{C}} $ are called Casimirs. The equations of motion result from the following Hamilton equations \n\\begin{eqnarray}\n\\partial_t \\xi_i=\\{\\xi_i, {\\mathcal{H}} \\}\\,, \\label{Hamilton}\n\\end{eqnarray}\nwhere $ {\\mathcal{H}} $ is the Hamiltonian of the model. Obviously the Hamiltonian is conserved in view of the antisymmetry of the Poisson bracket and the Casimirs are constants due to their Poisson-commutative property. Such Hamiltonian structures have been derived for fluid mechanics and ordinary MHD \\cite{Morrison1998,Morrison1980,Morrison1982}, Hall-MHD \\cite{Holm1987,Lingam2015}, extended-MHD \\cite{Abdelhamid2015,Lingam2015} and Vlasov-Poisson and Vlasov-Maxwell theories \\cite{pjm80} (with a correction for Vlasov-Maxwell in  \\cite{wemo1981,Marsden1982,Morrison1982} and a limitation to the correction pointed out in \\cite{Morrison1982}, which was followed up more recently in \\cite{pjm13,pjmH20} and then in  \\cite{lasawe2019}\n).    \nFor a more comprehensive presentation of the noncanonical Hamiltonian dynamics, emerging in the Eulerian description of fluids and other continuum theories the reader is referred to \\cite{Morrison1998}. For Vlasov models the interested reader can consult \\cite{Morrison2009}. \n\nRegarding our model, the sum of the Hall MHD and the particle brackets expressed in terms of canonical momenta is\n\\begin{eqnarray}\n\\{F,G\\}=&&\\int d^3x\\Big\\{ \\bar{\\mathbf{M}}\\cdot\\left(G_{\\bar{\\mathbf{M}}}\\cdot\\nabla F_{\\bar{\\mathbf{M}}}-F_{\\bar{\\mathbf{M}}}\\cdot\\nabla G_{\\bar{\\mathbf{M}}}\\right)\\nonumber\\\\\n&&+\\rho\\left(G_{\\bar{\\mathbf{M}}}\\cdot\\nabla F_{\\rho}-F_{\\bar{\\mathbf{M}}}\\cdot\\nabla G_{\\rho}\\right)\n-e^{-1}\\left(G_{{\\mathbf A}}\\cdot\\nabla F_{n_e}-F_{{\\mathbf A}}\\cdot\\nabla G_{n_e}\\right)\\nonumber\\\\\n&&-\\frac{1}{en_e}(\\nabla\\times{\\mathbf A})\\cdot(F_{{\\mathbf A}}\\times G_{{\\mathbf A}})+\\sum_p \\int d^3{\\boldsymbol \\pi} f_p  [F_{\\bar{f}_p},G_{\\bar{f}_p}]_{\\boldsymbol \\pi}\\Big\\}\\,, \\label{Poisson_CCS_1}\n\\end{eqnarray}\nwhere \n\\begin{eqnarray}\n[g,h]_{\\boldsymbol \\pi}=\\nabla g \\cdot \\nabla_{\\boldsymbol \\pi} h-\\nabla h \\cdot \\nabla_{\\boldsymbol \\pi} g\\,, \n\\end{eqnarray}\nand $\\bar{f}_p$ is the distribution function expressed in terms of ${\\boldsymbol \\pi}$, i.e. $\\bar{f}_p=\\bar{f}_p({\\mathbf x},m_p\\boldsymbol{v}-e_p{\\mathbf A},t)$.\n\nThe above Poisson bracket, along with the appropriate Hamiltonian, which is the sum of the Hall-MHD and particle Hamiltonians expressed in terms of $\\bar{\\mathbf{M}}$, ${\\boldsymbol \\pi}$, describe correctly the dynamics of the system. However, the canonical momentum variables are not convenient because they mix up the velocity and the magnetic field potential. Ultimately, we would like to have a system of equations that treats velocity and magnetic field variables separately, i.e. like the system we derived by the standard approach of the previous subsection. To this end, following \\cite{Tronci2010} and \\cite{Marsden1982} we may perform the following change of variables  \n\\begin{eqnarray}\n\\bar{\\mathbf{M}}\\rightarrow {\\mathbf v}\\,,\\quad \\bar{f}_p({\\mathbf x},m_p\\boldsymbol{v}-e_p{\\mathbf A},t)\\rightarrow f_p({\\mathbf x},\\boldsymbol{v},t)\\,,\n\\end{eqnarray}\nwhere \n\\begin{align}\n{\\mathbf v}=\\rho^{-1}\\bar{\\mathbf{M}}-\\frac{e}{m}{\\mathbf A}\\,. \\label{v_M_trans}\n\\end{align}\n For convenience, this change of variables will be performed in two stages. First, we can write the bracket in terms of $f_p$ and then we can complete the change transforming from $\\bar{\\mathbf{M}}$ to ${\\mathbf v}$. For the first change we can directly use a result from \\cite{Marsden1982} that is, for any functional $F[{\\mathbf A},f_p]$ and functions $g({\\mathbf x},\\boldsymbol{v})$, $h({\\mathbf x},\\boldsymbol{v})$, we have $F[{\\mathbf A},f_p({\\mathbf x},\\boldsymbol{v},t)]=\\bar{F}[{\\mathbf A},\\bar{f}_p({\\mathbf x},m_p \\boldsymbol{v}+e_p{\\mathbf A},t)]$,  $g({\\mathbf x},\\boldsymbol{v})=\\bar{g}({\\mathbf x},m_p\\boldsymbol{v}+e_p{\\mathbf A})$,  $h({\\mathbf x},\\boldsymbol{v})=\\bar{h}({\\mathbf x},m_p\\boldsymbol{v}+e_p{\\mathbf A})$, and the following transformation rules must hold\n\\begin{eqnarray}\n[\\bar{g},\\bar{h}]_{{\\boldsymbol \\pi}}=\\frac{1}{m_p}[\\bar{g},\\bar{h}]_{\\boldsymbol{v}}+\\frac{e_p}{m_p^2}(\\nabla\\times{\\mathbf A})\\cdot(\\nabla_{\\boldsymbol{v}} g\\times\\nabla_{\\boldsymbol{v}} h)\\,, \\label{trans_1}\\\\\n\\bar{F}_{{\\mathbf A}}=F_{{\\mathbf A}}-\\sum_p\\frac{e_p}{m_p}\\int d^3v f_p\\nabla_{\\boldsymbol{v}} F_{f_p}\\,,\\label{trans_2}\n\\end{eqnarray}\nwhere \n\\begin{eqnarray}\n[g,h]_{\\boldsymbol{v}}=\\nabla g \\cdot \\nabla_{\\boldsymbol{v}} h-\\nabla h \\cdot \\nabla_{\\boldsymbol{v}} g\\,. \n\\end{eqnarray}\nUsing Eqs. \\eqref{trans_1} and \\eqref{trans_2}, the bracket \\eqref{Poisson_CCS_1} takes the following form\n\\begin{align}\n\\{F,G\\}=&\\int d^3x \\bigg\\{\\bar{\\mathbf{M}}\\cdot\\left(G_{\\bar{\\mathbf{M}}}\\cdot\\nabla F_{\\bar{\\mathbf{M}}}-F_{\\bar{\\mathbf{M}}}\\cdot\\nabla G_{\\bar{\\mathbf{M}}}\\right)\\nonumber\\\\\n&+\\rho\\left(G_{\\bar{\\mathbf{M}}}\\cdot\\nabla F_{\\rho}-F_{\\bar{\\mathbf{M}}}\\cdot\\nabla G_{\\rho}\\right)\n-e^{-1}\\left(G_{{\\mathbf A}}\\cdot\\nabla F_{n_e}-F_{{\\mathbf A}}\\cdot\\nabla G_{n_e}\\right)\\nonumber\\\\\n&-\\frac{1}{en_e}(\\nabla\\times{\\mathbf A})\\cdot(F_{{\\mathbf A}}\\times G_{{\\mathbf A}})-e^{-1}\\sum_p \\frac{e_p}{m_p}f_p\\left[\\nabla G_{n_e}\\cdot\\nabla_{\\boldsymbol{v}} F_{f_p}-\\nabla F_{n_e}\\cdot\\nabla_{\\boldsymbol{v}} G_{f_p} \\right]\\nonumber \\\\\n&+\\frac{1}{en_e}\\sum_p\\frac{e_p}{m_p}f_p (\\nabla\\times{\\mathbf A})\\cdot\\left(\\nabla_{\\boldsymbol{v}} F_{f_p}\\times G_{{\\mathbf A}}-\\nabla_{\\boldsymbol{v}} G_{f_p}\\times F_{{\\mathbf A}}\\right)\\nonumber\\\\\n&-\\frac{1}{e n_e}(\\nabla\\times {\\mathbf A})\\cdot\\int \\int d^3vd^3v' \\left(\\sum_p \\frac{e_p}{m_p}f_p\\nabla_{\\boldsymbol{v}} F_{f_p}\\right)\\times\\left(\\sum_{p'} \\frac{e_{p'}}{m_{p'}}f_{p'}\\nabla_{\\boldsymbol{v}'} F_{f_{p'}}\\right)\\nonumber\\\\\n&+\\sum_p \\int d^3v \\frac{f_p}{m_p} \\left[[F_{f_p},G_{f_p}]_{\\boldsymbol{v}}+\\frac{e_p}{m_p}(\\nabla\\times {\\mathbf A})\\cdot\\left(\\nabla_{\\boldsymbol{v}} F_{f_p}\\times\\nabla_{\\boldsymbol{v}} G_{f_p}\\right) \\right]\\bigg\\}\\,. \\label{Poisson_CCS_2}\n\\end{align}\nNow, we can proceed by expressing this bracket in terms of ${\\mathbf v}$ and ${\\mathbf B}=\\nabla\\times {\\mathbf A}$ instead of $\\bar{{\\mathbf M}}$ and ${\\mathbf A}$. Let us note first that from Eq. \\eqref{v_M_trans} an arbitrary variation of ${\\mathbf v}$ can be written as\n$$\\delta{\\mathbf v}=\\rho^{-1}\\delta \\bar{{\\mathbf M}}-\\rho^{-2}\\bar{{\\mathbf M}}\\delta\\rho-\\frac{e}{m}\\delta{\\mathbf A}\\,.$$\nConsidering a functional $\\bar{F}$ of $\\bar{{\\mathbf M}}$, ${\\mathbf A}$ and $\\rho$ and then upon expressing it in terms of ${\\mathbf v}$, ${\\mathbf B}$ and $\\rho$ the equality $\\bar{F}[\\bar{{\\mathbf M}},{\\mathbf A},\\rho]=F[{\\mathbf v},{\\mathbf B},\\rho]$ must hold. Upon taking the first variation of this relation and using the chain rule for the functional derivatives, the following relations can be deduced\n\\begin{align}\n\\bar{F}_{\\bar{{\\mathbf M}}}=&\\rho^{-1}F_{{\\mathbf v}}\\,,\\label{F_trans_1_1}\\\\\n\\bar{F}_\\rho=&F_\\rho-\\rho^{-1}\\left({\\mathbf v}+\\frac{e}{m}{\\mathbf A}\\right)\\cdot F_{{\\mathbf v}}\\,,\\label{F_trans_1_2}\\\\\n\\bar{F}_{{\\mathbf A}}=&\\nabla\\times F_{{\\mathbf B}}-\\frac{e}{m}F_{{\\mathbf v}}\\,. \\label{F_trans_1_3}\n\\end{align}\nSubstituting equations \\eqref{v_M_trans} and \\eqref{F_trans_1_1}-\\eqref{F_trans_1_3} to \\eqref{Poisson_CCS_2} we find the final bracket, which reads\n\\begin{align}\n\\{F,G\\}=&\\int d^3x\\bigg\\{\\left(G_{\\mathbf v}\\cdot \\nabla F_\\rho -F_{\\mathbf v}\\cdot\\nabla G_\\rho\\right)+\\rho^{-1} (\\nabla\\times{\\mathbf v})\\cdot\\left(F_{\\mathbf v}\\times G_{\\mathbf v}\\right)\\nonumber\\\\\n&+\\frac{e}{m^2}\\left(\\frac{mn_e-\\rho}{\\rho n_e}\\right){\\mathbf B}\\cdot\\left(F_{\\mathbf v}\\times G_{\\mathbf v}\\right)+m^{-1} \\left(G_{\\mathbf v}\\cdot\\nabla F_{n_e}-F_{\\mathbf v}\\cdot\\nabla G_{n_e}\\right)\\nonumber\\\\\n&+ \\frac{1}{m n_e} {\\mathbf B} \\cdot  \\left[ F_{{\\mathbf v}}\\times(\\nabla\\times G_{\\mathbf B})-G_{{\\mathbf v}}\\times(\\nabla\\times F_{\\mathbf B}) \\right]-\\frac{1}{en_e}{\\mathbf B}\\cdot\\left[(\\nabla\\times F_{\\mathbf B})\\times(\\nabla\\times G_{\\mathbf B})\\right]\\nonumber\\\\\n&+\\frac{1}{en_e}\\sum_p\\frac{e_p}{m_p}\\int d^3v\\, f_p {\\mathbf B} \\cdot\\Big[\\nabla_{\\boldsymbol{v}} F_{f_p}\\times(\\nabla\\times G_{\\mathbf B})-\\nabla_{\\boldsymbol{v}} G_{f_p}\\times(\\nabla\\times F_{\\mathbf B})\\nonumber\\\\\n&+\\frac{e}{m}\\left(\\nabla_{\\boldsymbol{v}} G_{f_p}\\times F_{{\\mathbf v}}-\\nabla_{\\boldsymbol{v}} F_{f_p}\\times G_{{\\mathbf v}}\\right)\\Big]\\nonumber\\\\\n&-\\frac{1}{e}\\sum_p \\frac{e_p}{m_p}\\int d^3v\\, f_p \\left(\\nabla G_{n_e}\\cdot \\nabla_{\\boldsymbol{v}} F_{f_p}-\\nabla F_{n_e}\\cdot \\nabla_{\\boldsymbol{v}} G_{f_p}\\right)\\nonumber\\\\ \n&-\\frac{1}{en_e}{\\mathbf B} \\cdot\\left( \\sum_{p,p'}\\frac{e_p}{m_p}\\frac{e_{p'}}{m_{p'}}\\int\\int d^3v d^3v'\\, f_p f_{p'} \\nabla_{\\boldsymbol{v}} F_{f_p}\\times \\nabla_{\\boldsymbol{v}} G_{f_{p'}}\\right) \\nonumber\\\\\n&+\\sum_p \\int d^3v\\, \\frac{f_p}{m_p}\\left[[F_{f_p},G_{f_p}]_{\\boldsymbol{v}}+\\frac{e_p}{m_p}{\\mathbf B}\\cdot (\\nabla_{\\boldsymbol{v}} F_{f_p}\\times \\nabla_{\\boldsymbol{v}} G_{f_p})\\right] \\bigg\\}\\,. \\label{Poisson_CCS_3}\n\\end{align}\nObserve, unlike the bracket of \\eqref{Poisson_CCS_1}, \\eqref{Poisson_CCS_3} is gauge invariant.\nNote that for deriving the bracket \\eqref{Poisson_CCS_3} we have made use of the vector calculus identity\n$$\\nabla {\\mathbf b}\\cdot {\\mathbf a}={\\mathbf a}\\times\\nabla\\times{\\mathbf b}+{\\mathbf a}\\cdot\\nabla {\\mathbf b}\\,.$$\nNow, having computed the bracket of our model, we write down the Hamiltonian functional, which is the direct sum of the fluid and particle Hamiltonians, i.e.,\n\\begin{align}\n     {\\mathcal{H}} =\\int d^3x \\left[\\rho \\frac{|{\\mathbf v}|^2}{2}+\\rho U(\\rho)+n_e {\\mathcal{U}} _e+\\frac{|{\\mathbf B}|^2}{2\\mu_0}\\right]+\\sum_p\\int\\int d^3x\\,d^3v\\, m_p \\frac{v^2}{2} f_p\\,. \\label{Hamiltonian_CCS}\n\\end{align}\nHere, $U(\\rho)$ is the specific internal energy of the thermal ion fluid, and $ {\\mathcal{U}} _e$ is some electron internal energy function. The dependence of this function is dictated by the form of the electron pressure tensor. In our case, a gyrotropic electron pressure tensor is the most general case that we may consider. To see this let us write the electron pressure equation as it emerges from the moment hierarchy of the Vlasov equation \\cite{Hunana2019a}\n\\begin{align}\n\\hspace{-3mm}\nm_e\\left[\\partial_t \\mathbb{P}_e+\\nabla\\cdot\\left({\\mathbf v}_e\\mathbb{P}_e + \\mathbb{Q}_e\\right)+\\mathbb{P}_e\\cdot\\nabla {\\mathbf v}_e+(\\mathbb{P}_e\\cdot\\nabla {\\mathbf v}_e)^T\\right] =e\\left[{\\mathbf B}\\times \\mathbb{P}_e+({\\mathbf B}\\times\\mathbb{P}_e)^T\\right]\\,, \\label{pressure_tensor_eq}\n\\end{align}\nWhere $\\mathbb{Q}_e$ is the electron heat flux tensor. In the limit $m_e \\rightarrow 0$ only the rhs of \\eqref{pressure_tensor_eq} survives, and thus ${\\mathbf B}\\times \\mathbb{P}_e+({\\mathbf B}\\times\\mathbb{P}_e)^T$ should be equal to zero. A general solution of this equation is given by the gyrotropic pressure tensor\n\\begin{align}\n\\mathbb{P}_e=\\frac{P_{e\\parallel}-P_{e\\perp} }{B^2}{\\mathbf B}\\bfB+P_{e\\perp}{\\mathbf I}\\,,\\label{pressure_tensor}\n\\end{align}\nwhere \n\\begin{align}\n    P_{e\\parallel}&=\\mathbb{P}_e\\boldsymbol{:}{\\mathbf b}\\bfb\\,,\\nonumber\\\\\n    P_{e\\perp}&=\\frac{1}{2}\\mathbb{P}_e\\boldsymbol{:} ({\\mathbf I}-{\\mathbf b}\\bfb)\\,.\n\\end{align}\nIt was first shown in \\cite{Morrison1982} (and subsequent work, e.g., \\cite{pjmHM13}) that such gyrotropic pressure tensors follow from an internal energy that depends on ${\\mathbf B}$.  If  an  electron internal energy has the form \n$$ {\\mathcal{U}} _e= {\\mathcal{U}} _e(n_e,|{\\mathbf B}|)\\,,$$\n then following equations give the pressure tensor:\n\\begin{align}\n    \\frac{\\partial  {\\mathcal{U}} _e}{\\partial n_e}&=\\frac{P_{e\\parallel}}{n_e^2}\\,, \\label{dUe_dne}\\\\\n    \\frac{\\partial  {\\mathcal{U}} _e}{\\partial  |{\\mathbf B}|}&=\\frac{P_{e\\perp}-P_{e\\parallel}}{n_e|{\\mathbf B}|}\\,;\\label{dUe_dB}\n\\end{align}\nhence\n\\begin{align}\n    \\frac{\\delta  {\\mathcal{H}} }{\\delta n_e}= {\\mathcal{U}} _e+\\frac{P_{e\\parallel}}{n_e}\\,,\\label{dH/dne}\\\\\n    \\frac{\\delta  {\\mathcal{H}} }{\\delta {\\mathbf B}}=(\\mu_0^{-1}-\\gamma){\\mathbf B}\\,, \\label{dH/d|B|}\n\\end{align}\nwhere\n$\\gamma:= \\frac{P_{e\\parallel}-P_{e\\perp}}{B^2}$ is the electron pressure anisotropy parameter.\n\nThe equations of motion follow from \\eqref{Hamilton} with the Poisson bracket \\eqref{Poisson_CCS_3}, the Hamiltonian \\eqref{Hamiltonian_CCS}, and also using equations \\eqref{dH/d|B|}, \\eqref{dH/dne}. \nIt can be readily seen that $\\partial_t n_e=\\{n_e, {\\mathcal{H}} \\}$ and $\\partial_t\\rho=\\{\\rho, {\\mathcal{H}} \\}$ are indeed equations, \\eqref{dn_e/dt} and the continuity equation for the fluid ions, respectively. The latter is \n$$\\partial_t\\rho=-\\nabla\\cdot(\\rho{\\mathbf v})\\,.$$\nThe remaining equations, i.e. the momentum, the induction and the Vlasov equation that stem from the Hamilton's equations are\n\\begin{eqnarray}\n&&\\hspace{-15mm}\\partial_t{\\mathbf v}={\\mathbf v}\\times\\nabla\\times{\\mathbf v} -\\nabla\\left( h+\\frac{v^2}{2} \\right)+\\frac{e}{m}\\left(1-\\frac{n_i}{n_e} \\right){\\mathbf v}\\times{\\mathbf B}\\nonumber \\\\ &&\\hspace{-10mm}+\\frac{1}{mn_e}\\left(\\mu_0^{-1}\\nabla\\times{\\mathbf B} -{\\mathbf J}_k\\right)\\times{\\mathbf B}-m^{-1}\\nabla\\left(  {\\mathcal{U}} _e+\\frac{P_{e\\parallel}}{n_e}\\right)-\\frac{\\nabla\\times(\\gamma {\\mathbf B})}{mn_e}\\times{\\mathbf B} \\,,\\label{dv/dt}\\\\\n&&\\hspace{-15mm}\\partial_t {\\mathbf B}=\\nabla\\times\\left[\\frac{n_i}{n_e}{\\mathbf v}\\times{\\mathbf B}-\\frac{1}{en_e}\\left(\\mu_0^{-1}\\nabla\\times{\\mathbf B}-{\\mathbf J}_k\\right)\\times{\\mathbf B}\\right]\\nonumber\\\\\n&&\\hspace{-10mm}+\\nabla\\times\\left[\\frac{\\gamma}{en_e}\\left({\\mathbf B}\\cdot\\nabla{\\mathbf B}-\\nabla{\\mathbf B}\\cdot{\\mathbf B}\\right)+\\frac{\\nabla\\gamma\\times{\\mathbf B}}{en_e}\\times{\\mathbf B}\\right]\\,,\\label{dB/dt}\\\\\n&&\\hspace{-15mm}\\partial_t f_p=-\\boldsymbol{v}\\cdot\\nabla f_p-\\frac{e_p}{m_p}\\Big[\\left(\\boldsymbol{v}-\\frac{n_i}{n_e}{\\mathbf v}\\right)\\times{\\mathbf B}\\nonumber\\\\\n&&\\hspace{-10mm}+\\frac{1}{en_e}\\left(\\mu_0^{-1}\\nabla\\times{\\mathbf B}-{\\mathbf J}_k\\right)\\times{\\mathbf B} -\\frac{1}{e}\\nabla\\left( {\\mathcal{U}} _e+\\frac{P_{e\\parallel}}{n_e}\\right)-\\frac{\\nabla\\times(\\gamma{\\mathbf B})}{en_e}\\times{\\mathbf B}\\Big]\\cdot\\nabla_{\\boldsymbol{v}} f_p\\,. \\label{df/dt}\n\\end{eqnarray}\nEmploying a couple of vector identities one can see that\n\\begin{eqnarray}\n&&-\\frac{1}{m}\\nabla\\left( {\\mathcal{U}} _e+\\frac{P_{e\\parallel}}{n_e}\\right)-\\frac{\\nabla\\times(\\gamma{\\mathbf B})}{mn_e}=\\nonumber\\\\\n&&-\\frac{1}{mn_e}\\left(\\nabla P_{e\\perp}+\\gamma {\\mathbf B}\\cdot\\nabla{\\mathbf B} +{\\mathbf B}\\bfB\\cdot\\nabla\\gamma \\right)=-\\frac{\\nabla\\cdot \\mathbb{P}_e}{mn_e}\\,.\\label{div_tensor}\n\\end{eqnarray}\nThe last equality can be easily proven upon taking the divergence of the gyrotropic pressure equation \\eqref{pressure_tensor}. In light of Eq. \\eqref{div_tensor}, the momentum and the Vlasov equation, \\eqref{dv/dt} and  \\eqref{df/dt}, respectively, which arise from the Hamiltonian formulation, are identical to equations \\eqref{momentum_CCS}, \\eqref{Vlasov_CCS}. In addition, after performing some calculus manipulations on the last term of the induction equation \\eqref{dB/dt} it can be seen that\n\\begin{eqnarray}\n&&\\nabla\\times\\left[\\frac{\\gamma}{en_e}\\left({\\mathbf B}\\cdot\\nabla{\\mathbf B}-\\nabla{\\mathbf B}\\cdot{\\mathbf B}\\right)+\\frac{\\nabla\\gamma\\times{\\mathbf B}}{en_e}\\times{\\mathbf B}\\right]\\nonumber\\\\\n&&=\\nabla\\times\\left[\\frac{\\nabla\\cdot\\mathbb{P}_e}{en_e}-\\frac{1}{e}\\nabla {\\mathcal{U}} _e-\\frac{1}{e}\\nabla\\left(n_e\\frac{\\partial  {\\mathcal{U}} _e}{\\partial n_e}\\right)\\right]=\\nabla\\times\\frac{\\nabla\\cdot\\mathbb{P}_e}{en_e}\\,.\n\\end{eqnarray}\nHence, Eq. \\eqref{dB/dt} is indeed the induction equation \\eqref{induction}.\n\n\\subsection{Casimir invariants}\nThe bracket \\eqref{Poisson_CCS_3} can be rearranged in the following form\n\\begin{eqnarray}\n\\{F,G\\}=\\int d^3x F_{u_i} {\\mathcal{J}} _{ij}G_{u_j}\\,,\n\\end{eqnarray}\nwhere $ {\\mathcal{J}} $ is the Poisson operator of our bracket.\nSolving the following system of equations\n\\begin{eqnarray}\n {\\mathcal{J}} _{ij}G_{u_j}=0\\,,\\quad i=1,...,5\n\\end{eqnarray}\n we were able to find a set of Casimir invariants:\n\\begin{eqnarray}\n {\\mathcal{C}} _1&=&\\int \\rho d^3x\\,,\\quad\n {\\mathcal{C}} _2 = \\int d^3x\\, n_e\\,,\\\\\n {\\mathcal{C}} _3&=&\\frac{1}{2}\\int d^3x\\, {\\mathbf A}\\cdot{\\mathbf B}\\,,\\\\\n {\\mathcal{C}} _4&=&\\frac{1}{2}\\int d^3x\\, \\left({\\mathbf v}+\\frac{e}{m}{\\mathbf A}\\right)\\cdot\\left(\\nabla\\times{\\mathbf v}+\\frac{e}{m}{\\mathbf B}\\right)\\,,\\\\\n {\\mathcal{C}} _p&=&\\int\\int d^3x d^3v\\, \\Lambda_p(f_p)\\,.\n\\end{eqnarray}\nThis set of Casimir invariants, typical of magnetofluid models (e.g.\\ \\cite{pjmLM16}), has two helicity invariants, but is  augmented by an additional kinetic Casimir for each particle species $p$, involving the corresponding distribution function, while there are no cross fluid-kinetic Casimirs. Here, $\\Lambda_p(f_p)$ are arbitrary functions of their respective distribution functions $f_p$.\n\\section{Pressure Coupling Scheme}\n\\label{sec_III}\n\\subsection{Transformation of the Hamiltonian CCS}\nThe coupling through the current density is one method to model the interaction of the fluid and the kinetic species, however, as it is well known, another method to couple the energetic and the fluid components is through the pressure tensor appearing in a center-of-mass momentum equation. This can be done upon taking the first-order fluid moment of the Vlasov equations, governing the dynamics of the energetic species, and then adding them to the momentum equation of the thermal or cold ions. The first-order fluid moment of \\eqref{Vlasov} yields\n\\begin{eqnarray}\nm_p\\partial_t (n_p{\\mathbf v}_p)=e_pn_p\\left({\\mathbf E}+{\\mathbf v}_p\\times{\\mathbf B}\\right)-\\sum_p\\nabla\\cdot \\tilde{\\mathbb{P}}_p\\,, \\label{particle_fluid_moment}\n\\end{eqnarray}\nwhere\n\\begin{eqnarray}\nn_p=\\int d^3v\\, f_p\\,,\\quad {\\mathbf v}_p=\\frac{1}{n_p}\\int d^3v\\,\\boldsymbol{v} f_p \\,,\\quad \\tilde{\\mathbb{P}}_p=m_p\\int d^3v\\, \\boldsymbol{v}\\bsv f_p\\,.\n\\end{eqnarray}\nAdding Eqs. \\eqref{particle_fluid_moment} and \\eqref{mf_momentum} for $s=i$ and using the Ohm's law \\eqref{GOL} we obtain the following momentum equation\n\\begin{eqnarray}\n\\partial_t{\\mathbf M}+m\\nabla\\cdot(n_i {\\mathbf v}\\bfv)=\\mu_0^{-1}(\\nabla\\times{\\mathbf B})\\times{\\mathbf B}\n-\\nabla P_i-\\nabla\\cdot\\mathbb{P}_e-\\sum_p\\nabla\\cdot\\tilde{\\mathbb{P}}_p\\,, \\label{gen_PCS_dM/dt}\n\\end{eqnarray}\nwhere \n\\begin{eqnarray}\n{\\mathbf M}:=mn_i{\\mathbf v}+\\sum_p m_pn_p {\\mathbf v}_p\\,.\n\\end{eqnarray}\nIf we replace Eq. \\eqref{momentum_CCS} of the CCS model by Eq. \\eqref{gen_PCS_dM/dt} we \nobtain a totally equivalent model coupling though the fluid and the particle components of the plasma through the pressure tensor $\\tilde{\\mathbb{P}}_p$. It is logical then to expect that this reformulation of the model would have a Hamiltonian structure resulting by some transformation of the dynamical variables. Tronci showed \\cite{Tronci2010} that the reformulation of the Hamiltonian structure is effected upon considering ${\\mathbf M}$ as an independent dynamical variable instead of ${\\mathbf v}$. Performing this change of variables one can show how the functional derivatives with respect to the old and the new set of dynamical variables should relate, requiring\n\\begin{eqnarray}\n\\delta F[n_i,n_e,{\\mathbf v},{\\mathbf B},f_p]=\\delta \\tilde{F}[n_i,n_e,{\\mathbf M},{\\mathbf B},f_p]\\,,\n\\end{eqnarray}\nand employing the chain rule for functional derivatives. The resulting relations are\n\\begin{eqnarray}\n F_{n_e}=\\tilde{F}_{n_e}\\,, \\quad F_{{\\mathbf B}}=\\tilde{F}_{{\\mathbf B}}\\,,\\quad\nF_{{\\mathbf v}}=m n_i \\tilde{{\\mathbf M}}\\,,\\nonumber\\\\\nF_{n_i}=\\tilde{F}_{n_i}+m {\\mathbf v}\\cdot \\tilde{F}_{{\\mathbf M}}\\,,\\quad\nF_{f_p}=\\tilde{F}_{f_p}+m_p\\boldsymbol{v}\\cdot \\tilde{F}_{{\\mathbf M}}\\,. \\label{transf_v_M}\n\\end{eqnarray}\nSubstituting Eqs. \\eqref{transf_v_M} into the Poisson bracket \\eqref{Poisson_CCS_3} and using the definition of ${\\mathbf M}$, we find, after some algebra, the following bracket\n\\begin{eqnarray}\n\\hspace{-10mm}\\{F,G\\}&=&\\int d^3x\\,\\Big\\{{\\mathbf M}\\cdot\\left(G_{{\\mathbf M}}\\cdot\\nabla F_{{\\mathbf M}}-F_{{\\mathbf M}}\\cdot\\nabla G_{{\\mathbf M}}\\right) +n_i\\left(G_{{\\mathbf M}}\\cdot\\nabla F_{n_i}-F_{{\\mathbf M}}\\cdot\\nabla G_{n_i}\\right)\\nonumber\\\\\n\\hspace{-25mm}&+&n_e\\left(G_{{\\mathbf M}}\\cdot\\nabla F_{n_e}-F_{{\\mathbf M}}\\cdot\\nabla G_{n_e}\\right)+{\\mathbf B}\\cdot\\left[F_{{\\mathbf M}}\\times(\\nabla\\times{\\mathbf B})-G_{{\\mathbf M}}\\times(\\nabla\\times F_{{\\mathbf B}})\\right]\\nonumber\\\\\n\\hspace{-25mm}&-&  \\sum_p\\frac{e_p}{em_p}\\int d^3v\\, f_p\\big(\\nabla_{\\boldsymbol{v}}F_{f_p}\\cdot\\nabla G_{n_e}-\\nabla_{\\boldsymbol{v}}G_{f_p}\\cdot\\nabla F_{n_e}\\big)\\nonumber\\\\\n\\hspace{-25mm}&+&\\frac{1}{en_e}\\sum_p \\frac{e_p}{m_p}\\int d^3v f_p{\\mathbf B}\\cdot\\left[\\nabla_{\\boldsymbol{v}}F_{f_p}\\times(\\nabla\\times G_{{\\mathbf B}})-\\nabla_{\\boldsymbol{v}}G_{f_p}\\times(\\nabla\\times F_{{\\mathbf B}})\\right]\\nonumber\\\\\n\\hspace{-25mm}&-& \\frac{1}{en_e}{\\mathbf B}\\cdot \\int \\int d^3v\\,d^3v'\\, \\sum_{p,p'}\\frac{e_pe_{p'}}{m_pm_{p'}}f_p(\\boldsymbol{v})f_{p'}(\\boldsymbol{v}')\\nabla_{\\boldsymbol{v}} F_{f_p}\\times\\nabla_{\\boldsymbol{v}'}G_{f_{p'}}\\nonumber\\\\\n\\hspace{-25mm}&+&\\sum_p m_p^{-1}\\int d^3v \\, f_p \\Big[[F_{f_p},G_{f_p}]+\\frac{e_p}{m_p}{\\mathbf B}\\cdot\\left(\\nabla_{\\boldsymbol{v}} F_{f_p}\\times \\nabla_{\\boldsymbol{v}} G_{f_p}\\right)\\nonumber\\\\\n\\hspace{-25mm}&+&m_p\\left([F_{f_p},\\boldsymbol{v}\\cdot G_{{\\mathbf M}}]-[G_{f_p},\\boldsymbol{v}\\cdot F_{{\\mathbf M}}]\\right)\\Big]-\\frac{1}{en_e}{\\mathbf B}\\cdot\\left[(\\nabla\\times F_{{\\mathbf B}})\\times(\\nabla\\times G_{{\\mathbf B}})\\right]\\Big\\} \\,. \\label{Poisson_PCS_1}\n\\end{eqnarray}\nWe should also write the Hamiltonian in terms of ${\\mathbf M}$ and then invoke Hamilton's equations to retrieve the dynamical system in this PCS formulation. The transformed Hamiltonian reads\n\\begin{eqnarray}\n {\\mathcal{H}} &=&\\int d^3x \\, \\left[\\frac{|{\\mathbf M}-{\\mathbf M}_p|^2}{2m n_i}+ n_i  {\\mathcal{U}} _i(n_i)+n_e  {\\mathcal{U}} _e(n_e,|{\\mathbf B}|)+\\frac{|{\\mathbf B}|^2}{2\\mu_0} \\right]\\nonumber \\\\\n&+& \\sum_p \\frac{m_p}{2}\\int \\int d^3x d^3v\\, f_pv^2\\,, \\label{Hamiltonian_PCS_gen}  \n\\end{eqnarray}\nwhere\n\\begin{eqnarray}\n{\\mathbf M}_p:=\\sum_p \\int d^3v\\, m_pf_p \\boldsymbol{v}\\,.\n\\end{eqnarray}\nThe functional derivative of $ {\\mathcal{H}} $ with respect to the new variable ${\\mathbf M}$ is \n\\begin{eqnarray}\n\\frac{\\delta  {\\mathcal{H}} }{\\delta {\\mathbf M}}={\\mathbf v}\\,.\n\\end{eqnarray}\nAlso, one should notice that the functional derivative with respect to $f_p$ is not the same as in the previous case of the CCS, but \n\\begin{eqnarray}\n\\frac{\\delta  {\\mathcal{H}} }{\\delta f_p}=m_p\\frac{v^2}{2}-m_p\\boldsymbol{v}\\cdot{\\mathbf v}\\,.\n\\end{eqnarray}\nOne can verify that the Hamilton's equations are indeed the dynamical equations of the CCS model with the momentum equation being replaced by Eq. \\eqref{gen_PCS_dM/dt}.\n\n\\subsection{Standard derivation of a PCS}\n The Hamiltonian system described in the previous subsection is equivalent to the CCS model of Sec. \\ref{sec_II} since it is obtained from CCS by a mere change of variables. However, it is a commonality in the hybrid fluid-kinetic models to employ pressure coupling schemes which involve a simpler coupling of the bulk plasmas with the energetic particles asuming that $n_p\\ll n_i$ and ${\\mathbf M}/\\rho \\sim {\\mathbf v}$ in the dynamical equations. These assumptions result in a momentum equation governing the ion momentum density, instead of the total momentum density ${\\mathbf M}$, and containing the divergence of a pressure tensor associated with the energetic particles e.g. \\cite{Park1992,Fu2006,Kim2004,Takahashi2009}. A similar treatment in our case where we treat the bulk component as a two-fluid plasma described by Hall MHD, results in the following set of equations\n %\n \\begin{eqnarray}\n &&\\partial_t n +\\nabla\\cdot(n{\\mathbf v})=0\\,,\\\\\n&&\\rho (\\partial_t{\\mathbf v}+{\\mathbf v}\\cdot\\nabla{\\mathbf v})={\\mathbf J}\\times{\\mathbf B} -\\sum_p\\nabla\\cdot\\mathbb{P}_p-\\nabla\\cdot\\mathbb{P}_e-\\nabla P_i\\,,\\\\\n &&\\partial_t B=\\nabla\\times\\left[ {\\mathbf v}\\times {\\mathbf B} -\\frac{1}{en}{\\mathbf J}\\times {\\mathbf B}+\\frac{1}{en}\\nabla\\cdot\\mathbb{P}_e\\right]\\,,\\\\\n &&\\partial_t f_p+\\boldsymbol{v}\\cdot\\nabla f_p+\\frac{e_p}{m_p}\\left [ (\\boldsymbol{v}-{\\mathbf v})\\times {\\mathbf B} +\\frac{1}{en}{\\mathbf J}\\times{\\mathbf B}-\\frac{1}{en}\\nabla\\cdot\\mathbb{P}_e\\right]\\cdot\\nabla_{\\boldsymbol{v}} f_p=0\\,. \n \\end{eqnarray}\n Here, $n=n_i=n_e$ which is the zeroth order quasineutrality condition for $n_p/n_i \\ll 1$. \n %\n \\subsection{Hamiltonian construction of a PCS}\n It has been stressed in several works \\cite{Tronci2010,Tronci2014,Burby2017,Close2018} that such PC models, which are widely employed in the study of energetic particle effects on plasma stability, do not conserve some energy functional as all consistent ideal plasma models do, e.g. \\cite{Morrison1980,Marsden1982,Lingam2015}. In addition, it has been shown that a Vlasov-MHD model in the pressure-coupling scheme  exhibits spurious instabilities attributed to the lack of energy conservation \\cite{Tronci2014}. In contrast, a Hamiltonian variant of the model, derived by a procedure that ensures energy conservation \\cite{Tronci2010}, does not contain these unphysical modes. Prompted by this observation we employ the method of \\cite{Tronci2010} in our case so as to derive a simplified, yet Hamiltonian, PCS model. The idea is that, instead of assuming $n_p\\ll n_i$ for simplifying the equations of motion, to replace the ion momentum density ${\\mathbf M}-{\\mathbf M}_p$ by the total momentum density ${\\mathbf M}$ in the Hamiltonian functional, i.e. by implicitly assuming ${\\mathbf M}\\approx mn_i{\\mathbf v}_i$ on the Hamiltonian level. Hence, the term  in  $|{\\mathbf M}-{\\mathbf M}_p|^2/(2m n_i)$ in \\eqref{Hamiltonian_PCS_gen} becomes $|{\\mathbf M}|^2/(2mn_i)$, leading to\n \\begin{eqnarray}\n  {\\mathcal{H}} =&&\\int d^3x\\, \\left[\\frac{|{\\mathbf M}|^2}{2mn_i}+n_i {\\mathcal{U}} _i(n_i)+n_e {\\mathcal{U}} _{e}(n_e,|{\\mathbf B}|)+\\frac{|{\\mathbf B}|^2}{2\\mu_0}\\right]\\nonumber\\\\\n &&+\\sum_p \\frac{m_p}{2}\\int \\int d^3xd^3v\\, f_p v^2\\nonumber\\\\\n &&=\\int d^3x\\, \\left[\\frac{mn_i}{2}|{\\mathbf u}|^2+n_i {\\mathcal{U}} _i(n_i)+n_e {\\mathcal{U}} _{e}(n_e,|{\\mathbf B}|)+\\frac{|{\\mathbf B}|^2}{2\\mu_0}\\right]\\nonumber\\\\\n &&+\\sum_p \\frac{m_p}{2}\\int \\int d^3xd^3v\\, f_p v^2\\,, \\label{Hamiltonian_PCS_spec}\n \\end{eqnarray}\n where ${\\mathbf u}:={\\mathbf M}/(m n_i)\\approx {\\mathbf v}_i$ is a center of mass velocity. \n Actually, a center of mass velocity would be \n $${\\mathbf u}_{cm}=\\frac{{\\mathbf M}}{m n_i +\\sum_p m_p n_p}\\,,$$\n but, in view of $n_p\\ll n_i$, we may write ${\\mathbf u}_{cm}={\\mathbf u}$. Note also that due to this assumption, if ${\\mathbf v}_p$ is of the order of ${\\mathbf v}$ or smaller, then ${\\mathbf u}={\\mathbf v}$ up to zeroth order in $n_p/n_i$.  It is convenient for us, in terms of comparing with the results of  the previous section and other studies, to write the Poisson bracket in terms of the velocity ${\\mathbf u}$. This is done in \\ref{app_A}. The Casimirs of this bracket are\n \\begin{eqnarray}\n  {\\mathcal{C}} _1 &=& \\int d^3x\\, n_i\\,, \\quad\n  {\\mathcal{C}} _2 = \\int d^3x\\, n_e\\,,\\\\\n  {\\mathcal{C}} _3 &=& \\frac{1}{2}\\int d^3x\\, {\\mathbf A}\\cdot {\\mathbf B}\\,,\\\\\n  {\\mathcal{C}} _4 &=& \\int d^3x\\,{\\mathbf u}\\cdot\\left(\\frac{1}{2}\\nabla\\times{\\mathbf u}+\\frac{e}{m}{\\mathbf B}\\right)\\nonumber\\\\\n &&-\\sum_p\\int d^3x\\frac{m_p}{m n_i}\\int d^3v f_p\\boldsymbol{v}\\cdot\\left [\\nabla\\times\\left( {\\mathbf u}-\\frac{1}{2m n_i}\\sum_{p'}m_{p'}\\int d^3v' f_{p'}\\boldsymbol{v}'\\right)+\\frac{e}{m}{\\mathbf B}\\right]\\,,\\\\\n  {\\mathcal{C}} _p &=& \\int d^3x d^3v\\, \\Lambda_p(f_p)\\,.\n \\end{eqnarray}\nThe new equations of motion that arise from the Hamilton's equations with the Poisson bracket \\eqref{Poisson_PCS_1} and the new Hamiltonian \\eqref{Hamiltonian_PCS_spec}, are\n\\begin{eqnarray}\n&&\\partial_t n_i=-\\nabla\\cdot(n_i{\\mathbf u})\\,, \\\\\n&&\\partial_t n_e=-\\nabla\\cdot(n_e {\\mathbf u})-\\frac{1}{e}\\nabla\\cdot{\\mathbf J}_k\\,,\\\\\n&&\\partial_t {\\mathbf B}=\\nabla\\times\\left[{\\mathbf u}\\times{\\mathbf B} -\\frac{1}{en_e}\\left(\\mu_0^{-1}\\nabla\\times{\\mathbf B}-{\\mathbf J}_k\\right)\\times{\\mathbf B}+\\frac{1}{en_e}\\nabla\\cdot\\mathbb{P}_e\\right]\\,,\\\\\n&&\\partial_t (\\rho {\\mathbf u})+\\nabla\\cdot(\\rho{\\mathbf u}\\bfu)=-\\rho\\nabla h_i -\\nabla\\cdot\\mathbb{P}_e-\\sum_p \\nabla\\cdot\\tilde{\\mathbb{P}}_p\\,,\\\\\n&&\\partial_t f_p=-(\\boldsymbol{v}+{\\mathbf u})\\cdot\\nabla f_p-\\frac{e_p}{m_p}\\Big[\\boldsymbol{v}\\times{\\mathbf B}+\\frac{1}{en_e}(\\mu_0^{-1}\\nabla\\times{\\mathbf B}-{\\mathbf J}_k)\\times{\\mathbf B}\\nonumber\\\\\n&&\\hspace{50mm}-\\frac{1}{en_e}\\nabla\\cdot\\mathbb{P}_e\\Big]\\cdot\\nabla_{v}f_p+\\nabla{\\mathbf u}:\\boldsymbol{v}\\nabla_{\\boldsymbol{v}} f_p\\,,\n\\end{eqnarray}\nwhere $\\rho=m n_i$.\n\n\\section{Translationally symmetric formulation and energy-Casimir equilibria}\n\\subsection{Translationally symmetric formulation}\nIn this section we consider a simpler case of dynamics with dynamical variables being invariant along a fixed straight axis. In this translationally symmetric case the magnetic and velocity field variables can be expressed in terms of five scalar Clebsch potentials. Note though, that we allow the distribution functions to depend on all three microscopic velocity coordinates. Translationally symmetric models facilitate computer simulations, since the dependency on one spatial coordinate is dropped, and can be considered good approximations whenever a strong guiding magnetic field directed in one fixed direction is present.  Using Cartesian coordinates $(x,y,z)$ in physical space, and assuming invariance along the  $z$ axis, the velocity and the magnetic field can be written as\n\\begin{eqnarray}\n{\\mathbf u} &=& u_z(x,y)\\hat{z} +\\nabla\\chi(x,y)\\times \\hat{z}+\\nabla\\Upsilon(x,y)\\,, \\label{ts_decomp_1}\\\\\n{\\mathbf B} &=& B_z(x,y)\\hat{z}+\\nabla\\psi(x,y)\\times\\hat{z}\\,.\\label{ts_decomp_2}\n\\end{eqnarray}\nTo obtain a Hamiltonian formulation in the symmetric case we need to translate this field decomposition in a decomposition of the vector functional derivatives in terms of functional derivatives with respect to the scalar potentials \\(\\chi,\\psi,\\Upsilon\\). The process of deriving these relations has been described several times before, e.g., in \\cite{Andreussi2010,Kaltsas2017,Grasso2017,Kaltsas2018}. Following the same procedure here, we find \n\\begin{eqnarray}\nF_{{\\mathbf u}}=F_{u_z}\\hat{z}+\\nabla F_\\Omega \\times\\hat{z}-\\nabla F_{w}\\,, \\label{F_v} \\\\\nF_{{\\mathbf B}}=F_{B_z}\\hat{z}-\\nabla\\left(\\Delta^{-1}F_\\psi\\right)\\times\\hat{z}\\,, \\label{F_Bz}\n\\end{eqnarray}\nwhere $\\Delta^{-1}$ is the inverse Laplacian operator and $\\omega:=-\\Delta \\chi\\hat{z}$. The translationally symmetric Hall MHD bracket is known from \\cite{Kaltsas2017,Grasso2017}, hence we should compute the translationally symmetric cross kinetic-HMHD terms and the term accounting for electron fluid thermodynamics. This is done in \\ref{app_B} upon substituting \\eqref{F_v} and \\eqref{F_Bz} in corresponding terms of the bracket \\eqref{Poisson_PCS_1}. The resulting bracket is \n\\begin{eqnarray}\n\\hspace{-10mm}\\{F,G\\}_{TS}&=&\\int d^2x \\bigg\\{ F_\\rho \\Delta G_w-G_\\rho\\Delta F_w\\nonumber\\\\\n&&\\hspace{-10mm}+\\frac{n_e}{\\rho}\\left([F_\\Omega,G_{n_e}]-[G_\\Omega,F_{n_e}]+\\nabla F_w\\cdot\\nabla G_{n_e}-\\nabla G_w\\cdot \\nabla F_{n_e}\\right)\\nonumber\\\\\n&&\\hspace{-10mm}+\\rho^{-1} \\Omega \\left([F_\\Omega,G_\\Omega]+[F_w,G_w]+\\nabla F_w \\cdot \\nabla G_\\Omega -\\nabla F_\\Omega\\cdot\\nabla G_w\\right)\\nonumber\\\\\n&&\\hspace{-10mm}+u_z\\big([F_\\Omega,\\rho^{-1}G_{u_z}]-[G_\\Omega,\\rho^{-1}F_{u_z}]+\\nabla(\\rho^{-1}G_{u_z})\\cdot\\nabla F_w\\nonumber\\\\\n&&\\hspace{-10mm}-\\nabla(\\rho^{-1}F_{u_z})\\cdot\\nabla G_w+\\rho^{-1}F_\\Upsilon G_{u_z}-\\rho^{-1}G_\\Upsilon F_{u_z}\\big)\\nonumber\\\\\n&&\\hspace{-10mm}+\\psi \\Big([F_\\Omega,\\rho^{-1}G_\\psi]-[G_\\Omega,\\rho^{-1} F_\\psi]+[F_{B_z},\\rho^{-1}G_{u_z}]-[G_{B_z},\\rho^{-1}F_{u_z}]\\nonumber\\\\\n&&\\hspace{-10mm}+\\nabla F_w \\cdot \\nabla (\\rho^{-1} G_\\psi)-\\nabla G_w \\cdot \\nabla (\\rho^{-1} F_\\psi)+\\rho^{-1}F_\\Upsilon G_\\psi -\\rho^{-1} G_\\Upsilon F_\\psi \\Big) \\nonumber\\\\\n&&\\hspace{-10mm}+\\rho^{-1}B_z\\left([F_\\Omega,G_{B_z}]-[G_\\Omega,F_{B_z}]+\\nabla F_w\\cdot \\nabla G_{B_z}-\\nabla G_w\\cdot\\nabla F_{B_z} \\right)\\nonumber\\\\\n&&\\hspace{-10mm}+\\frac{1}{e}\\psi\\left([G_{B_z},n_{e}^{-1}F_\\psi]-[F_{B_z},n_e^{-1}G_\\psi]\\right)-\\frac{1}{en_e}B_z[F_{B_z},G_{B_z}]\\nonumber\\\\\n&&\\hspace{-10mm}-\\frac{1}{e}\\sum_p\\frac{e_p}{m_p}\\int d^3v \\big[n_e^{-1}B_z\\nabla_{v_\\perp} f_p \\cdot \\left(G_{f_p} \\nabla F_{B_z}-F_{f_p}\\nabla G_{B_z}\\right)\\nonumber\\\\\n&&\\hspace{-10mm}+\\psi([n_e^{-1}F_{f_p}\\partial_{v_z}f_p,G_{B_z}]-[n_e^{-1}G_{f_p}\\partial_{v_z}f_p,F_{B_z}]\\big]\\nonumber\\\\\n&&\\hspace{-10mm}+\\nabla\\cdot(n_e^{-1}F_\\psi G_{f_p}\\nabla_{v_\\perp} f_p)-\\nabla\\cdot(n_e^{-1}G_\\psi F_{f_p}\\nabla_{v_\\perp} f_p))\\big]\\nonumber\\\\\n&&\\hspace{-10mm}+\\frac{1}{e}\\sum_p\\frac{e_p}{m_p}\\int d^3v\\, (\\nabla_{v_\\perp} f_p)\\cdot\\left(F_{f_p}\\nabla G_{n_e}-G_{f_p}\\nabla F_{n_e}\\right)\\nonumber\\\\\n&&\\hspace{-10mm}-\\frac{1}{en_e}\\sum_{p,p'}\\frac{e_p}{m_p}\\frac{e_{p'}}{m_{p'}}\\int \\int d^3v d^3v' f_p f_{p'}\\Big[B_z\\langle F_{f_p},G_{f_{p'}}\\rangle\\nonumber\\\\\n&&\\hspace{-10mm}-\\nabla\\psi\\cdot\\Big(\\partial_{v_z}F_{f_p}\\nabla_{v_{\\perp}'}G_{f_{p'}}-\\partial_{v_z'}G_{f_p'}\\nabla_{v_{\\perp}}F_{f_{p}}\\Big)\\Big]\n\\nonumber\\\\\n&&\\hspace{-10mm}+\\sum_p\\int d^3v \\frac{f_p}{m_p}\\Big[[\\![F_{f_p},G_{f_p}]\\!]+\\frac{e_p}{m_p}\\Big(B_z\\langle F_{f_p},G_{f_p} \\rangle \\nonumber\\\\\n&&\\hspace{-10mm}+\\nabla\\psi \\cdot\\left[\\partial_{v_z}G_{f_p}\\nabla_{v_\\perp} F_{f_p}-\\partial_{v_z}F_{f_p}\\nabla_{v_\\perp} G_{f_p}\\right] \\Big)\\nonumber\\\\\n&&\\hspace{-10mm}+m_p\\Big([\\![F_{f_p},\\rho^{-1}\\left(v_zG_{u_z}+\\hat{z}\\cdot \\boldsymbol{v}_\\perp \\times\\nabla G_{\\Omega}-\\boldsymbol{v}_\\perp \\cdot \\nabla G_w \\right)]\\!]\\nonumber\\\\\n&&\\hspace{-10mm} -[\\![G_{f_p},\\rho^{-1}\\left(v_zF_{u_z}+\\hat{z}\\cdot \\boldsymbol{v}_\\perp \\times\\nabla F_{\\Omega}-\\boldsymbol{v}_\\perp \\cdot \\nabla F_w \\right)]\\!]\\Big)\\Big]\\bigg\\}\\,. \\label{Poisson_ts}\n\\end{eqnarray}\nHere, $w:=\\Delta \\Upsilon$. Note that we have introduced a new bracket notation, namely \n\\begin{eqnarray}\n[a,b]&:=&(\\nabla a\\times\\nabla b)\\cdot\\hat{z}\\,,\\\\\n\\langle a,b\\rangle &:=&(\\nabla_{v_\\perp} a\\times \\nabla_{v_\\perp} b)\\cdot\\hat{z}\\,,\\\\\n\\left[\\![a,b]\\!\\right]&:=&\\nabla a\\cdot\\nabla_{v_\\perp} b-\\nabla b\\cdot \\nabla_{v_\\perp} a\\,.\n\\end{eqnarray}\nThe translationally symmetric Hamiltonian reads as follows\n\\begin{eqnarray}\n {\\mathcal{H}}  &=& \\int d^3x \\Big[\\frac{1}{2}\\rho \\left(u_z^2+|\\nabla\\chi|^2+2[\\Upsilon,\\chi]+|\\nabla \\Upsilon|^2\\right)\\nonumber\\\\\n&&\\hspace{10mm}+\\frac{B_z^2}{2\\mu_0}+\\frac{|\\nabla\\psi|^2}{2\\mu_0}+\\rho U_i(\\rho)+n_e {\\mathcal{U}} _e(n_e,B_z,|\\nabla\\psi|)\\Big]\\nonumber\\\\\n&&\\hspace{10mm}+\\sum_p\\int d^3xd^3v\\, \\frac{1}{2}m_p f_p v^2\\,. \\label{ts_Hamiltonian}\n\\end{eqnarray}\nNow, the functional derivatives with respect to the two scalars associated with the magnetic field are\n\\begin{eqnarray}\n\\frac{\\delta {\\mathcal{H}} }{\\delta B_z} &=& (\\mu_0^{-1}-\\gamma)B_z\\,,\\\\\n\\frac{\\delta  {\\mathcal{H}} }{\\delta\\psi} &=& -\\nabla\\cdot\\left[(\\mu_0^{-1}-\\gamma)\\nabla\\psi\\right]\\,.\n\\end{eqnarray}\nHaving these expressions and also computing the functional derivatives with respect to the remaining scalars one can derive the translationally symmetric dynamical equations, and in addition, equilibrium and stability conditions. For equilibrium and stability analysis one has to identify the families of Casimir invariants $ {\\mathcal{C}} $ that span the non trivial null space of the Poisson bracket \\eqref{Poisson_ts}. From the Casimir determining equations, stemming from the requirement  $\\mathfrak{C}_{\\xi_i}=0$, where $\\mathfrak{C}_{\\xi_i}$ appear in the following rearrangement of \\eqref{Poisson_ts} \n$$\\{F,G\\}=\\int d^3x\\, \\Big\\{F_\\rho \\mathfrak{C}_\\rho+F_{n_e} \\mathfrak{C}_{n_e}+F_{B_z} \\mathfrak{C}_{B_z}+F_\\psi \\mathfrak{C}_\\psi+F_\\chi \\mathfrak{C}_\\chi+F_\\Upsilon \\mathfrak{C}_\\Upsilon+\\sum_p\\int d^3v\\,F_{f_p} \\mathfrak{C}_{f_p} \\Big\\}$$\nwe were able to identify the following families of Casimirs\n\\begin{eqnarray}\n {\\mathcal{C}} _1 &=& \\int d^3x\\, n_e N(\\psi) \\,,\\label{ts_Casimir_1}\\\\\n {\\mathcal{C}} _2 &=& \\int d^3x\\, B_z F(\\psi)\\,,\\label{ts_Casimir_2}\\\\\n {\\mathcal{C}} _3 &=& \\int d^3x\\, \\rho K\\left(\\varphi\\right)\\,,\\label{ts_Casimir_3}\\\\\n {\\mathcal{C}} _4 &=& \\int d^3x\\, \\Big(\\Omega+\\frac{e}{m}B_z-\\sum_p m_p \\int d^3v [\\rho^{-1}f_p,\\boldsymbol{v}\\cdot{\\mathbf x}]\\Big)G(\\varphi)\\,,\\label{ts_Casimir_4}\\\\\n {\\mathcal{C}} _{p} &=& \\int \\int d^3xd^3v\\,\\Lambda_p(f_p)\\,,\\label{ts_Casimir_5}\n\\end{eqnarray}\nwhere, $N,K,F,G$ and $\\Lambda_p$ are arbitrary functions of their respective arguments and\n\\begin{eqnarray}\n\\varphi:=u_z+\\frac{e}{m}\\psi-\\rho^{-1}\\sum_pm_p\\int d^3v f_p v_z\\,,\n\\end{eqnarray}\nis a generalized ion stream function, modified due to the presence of kinetic particle species. Here $m$ is the mass of the thermal ions. For \\(f_p\\rightarrow 0\\) this stream function and the entire set of Casimir invariants \\eqref{ts_Casimir_1}-\\eqref{ts_Casimir_5}, reduces to their translationally symmetric HMHD counterparts \\cite{Kaltsas2017}. Note that in this fluid limit, quasineutrality implies $n_e=n_i$, since $n_p=\\int d^3v\\,f_p=0$.\n\n\\subsection{Energy-Casimir equilibria}\nDue to the spatial symmetry and the scalar decomposition of the vector fields, the Casimirs of the translationally symmetric case constitute infinite families of invariants thereby allowing for the derivation of sufficient stability conditions by the Energy-Casimir method, which has been used for hybrid kinetic-MHD models in \\cite{Tronci2015} and for the extended and Hall MHD models in \\cite{Kaltsas2020}. A preliminary step though is the definition of the stationary state that serves as the initial condition for the dynamics. The set of equilibrium equations can be derived in the first step of the energy-Casimir method by setting the first order variation of the extended Hamiltonian equal to zero. In MHD and extended MHD models, this procedure results in a system of Grad-Shafranov-Bernoulli (GSB) equations. Here, we derive the corresponding GSB system for translationally symmetric hybrid kinetic-HMHD plasmas in the PC scheme. \nThe energy-Casimir functional, or extended Hamiltonian, is given by\n\\begin{eqnarray}\n\\mathfrak{F}= {\\mathcal{H}} -\\sum_{i=1}^{4} {\\mathcal{C}} _i-\\sum_p  {\\mathcal{C}} _p\\,,\\label{EC_functional}\n\\end{eqnarray}\nwhere $ {\\mathcal{H}} $ is given by \\eqref{ts_Hamiltonian} and $ {\\mathcal{C}} $'s are the Casimirs \\eqref{ts_Casimir_1}-\\eqref{ts_Casimir_5}. The first variation of \\eqref{EC_functional} can be written as \n\\begin{eqnarray}\n\\delta\\mathfrak{F} &=& \\int d^3x\\,\\Big(\\mathfrak{R}_1\\delta n_e+\\mathfrak{R}_2 \\delta\\rho+\\mathfrak{R}_3 \\delta u_z+\\mathfrak{R}_4 \\delta \\chi+\\mathfrak{R}_5 \\delta \\Upsilon\\nonumber\\\\\n&&\\hspace{15mm}+\\mathfrak{R}_6 \\delta B_z+\\mathfrak{R}_7 \\delta\\psi +\\sum_p\\int d^3v\\, \\mathfrak{R}_p\\delta f_p   \\Big)\\,,\n\\end{eqnarray}\nAssuming independent, arbitrary variations of the scalar dynamical variables, the requirement $\\delta\\mathfrak{F}=0$ is equivalent to $\\mathfrak{R}_i=0$ $\\forall i=1,...,7\\,$ and $\\mathfrak{R}_p=0$ $\\forall p$. These equations lead to the following equilibrium conditions\n\\begin{eqnarray}\n&&\\hspace{-5mm} {\\mathcal{U}} _e+n_{e}^{-1}P_{e_{\\parallel}}-N(\\psi)=0\\,, \\label{dne}\\\\\n&&\\hspace{-5mm}h_i(\\rho)+\\frac{u^2}{2} -K(\\varphi)-\\rho^{-1}K'(\\varphi)\\sum_pm_p\\int d^3v\\, f_p v_z\\nonumber \\\\\n&&\\hspace{-5mm}-\\rho^{-2}\\left(\\Omega+\\frac{e}{m}B_z -\\sum_p m_p \\int d^3v\\, [\\rho^{-1}f_p,\\boldsymbol{v}\\cdot{\\mathbf x}] \\right)G'(\\varphi) \\sum_{p'}m_{p'}\\int d^3v\\, f_{p'}v_z\\nonumber \\label{drho}\\\\\n&&\\hspace{-5mm}-\\rho^{-2}\\sum_p m_p \\int d^3v\\, f_{p}[\\boldsymbol{v}\\cdot{\\mathbf x},G]=0\\,,\\\\\n&&\\hspace{-5mm}-\\nabla\\cdot\\left(\\rho \\nabla\\Upsilon \\right)+[\\chi,\\rho]=0\\,, \\label{dUpsilon}\\\\\n&&\\hspace{-5mm}-\\nabla\\cdot(\\rho \\nabla\\chi)+[\\rho,\\Upsilon]+\\Delta G(\\varphi)=0\\,,\\label{dchi}\\\\\n&&\\hspace{-5mm}\\rho u_z-\\rho K'(\\varphi)-\\left(\\Omega+\\frac{e}{m}B_z-\\sum_p m_p\\int d^3v [\\rho^{-1}f_p,\\boldsymbol{v}\\cdot{\\mathbf x}]\\right)G'(\\varphi)=0\\,,\\label{duz}\\\\\n&&\\hspace{-5mm}\\left(\\mu_0^{-1}-\\gamma\\right)B_z-F(\\psi)-\\frac{e}{m}G(\\varphi)=0\\, \\label{dBz}\\\\\n&&\\hspace{-5mm}\\nabla\\cdot\\left[(\\mu_0^{-1}-\\gamma)\\nabla\\psi\\right]+n_e N'(\\psi)+B_zF'(\\psi)+\\frac{e}{m}\\rho K'(\\varphi)\\nonumber\\\\\n&&\\hspace{-5mm}+\\frac{e}{m} \\left(\\Omega+\\frac{e}{m}B_z-\\sum_p m_p\\int d^3v\\, [\\rho^{-1}f_p,\\boldsymbol{v}\\cdot{\\mathbf x}]\\right)G'(\\varphi)=0\\,,\\label{dpsi}\\\\\n&&\\hspace{-5mm}\\frac{1}{2}m_pv^2-\\Lambda_p'(f_p)+m_pv_z\\bigg[K'(\\varphi)\\nonumber\\\\\n&&\\hspace{-5mm}+\\rho^{-1}\\left(\\Omega+\\frac{e}{m}B_z-\\sum_p m_p\\int d^3v\\, [\\rho^{-1}f_p,\\boldsymbol{v}\\cdot{\\mathbf x}]\\right)G'(\\varphi)\\bigg]\\nonumber\\\\\n&&\\hspace{-5mm}+\\rho^{-1}m_p[\\boldsymbol{v}\\cdot{\\mathbf x},G(\\varphi)]=0\\,.\\label{dfp}\n\\end{eqnarray}\nIn Eq. \\eqref{drho}, $h_i(\\rho)$ is the specific enthalpy of the thermal ion fluid.  Equation \\eqref{dne} provides a relation between $P_{e_\\parallel}$ and the variables $n_e$, $\\psi$ and $B_z$. Upon combining \\eqref{dne} with \\eqref{dUe_dB} we find an equation for $P_{e_\\perp}$ also. This one reads as follows \n\\begin{eqnarray}\n\\hspace{-10mm}P_{e_\\perp}=n_e\\left[|{\\mathbf B}|\\frac{\\partial  {\\mathcal{U}} _e}{\\partial |{\\mathbf B}|}+N(\\psi)- {\\mathcal{U}} _(n_e,|{\\mathbf B}|)\\right]\\,. \\label{Pe_perp}\n\\end{eqnarray}\nIn terms of $B_z$ and $|\\nabla\\psi|$, Eq. \\eqref{Pe_perp} becomes\n\\begin{eqnarray}\n\\hspace{-10mm}P_{e_\\perp}=n_e\\left[|{\\mathbf B}|^2\\left(\\frac{1}{B_z}\\frac{\\partial  {\\mathcal{U}} _e}{\\partial B_Z}+\\frac{1}{|\\nabla\\psi|}\\frac{\\partial  {\\mathcal{U}} _e}{\\partial |\\nabla\\psi|}\\right)+N(\\psi)- {\\mathcal{U}} _e(n_e,B_z,|\\nabla\\psi|)\\right]\\,.\n\\end{eqnarray}\nTherefore, in order to fully define an equilibrium state we need a thermodynamic closure for the electrons, i.e., $ {\\mathcal{U}} _e= {\\mathcal{U}} _e(n_e,B_z,|\\nabla\\psi|)$. For example, such a closure, which is associated with collisionless magnetic reconnection is provided in \\cite{Le2009}. \n\nFrom equation \\eqref{dBz} we readily obtain an equation for $B_z$ in terms of $\\psi$, $\\varphi$ and $\\gamma$\n\\begin{eqnarray}\nB_z=\\frac{F(\\psi)+(e/m)G(\\varphi)}{\\mu_0^{-1}-\\gamma}\\,. \\label{B_z}\n\\end{eqnarray}\nThe anisotropy parameter $\\gamma$, however, depends on $B_z$ as well, hence \\eqref{dBz} is actually nonlinear in $B_z$. Since the dependence of $ {\\mathcal{U}} _{e}$ on $B_z$ is arbitrary, it can be selected in a particular manner so that $\\gamma$ to be independent of $B_z$. Therefore, Eq. \\eqref{B_z} in general serves as an implicit equation for $B_z$ or as an explicit equation in the special case described above.\n\nNow, multiplying Eq. \\eqref{dfp} with $f_p$, integrating over the velocity space and summing over the particle species we obtain the following equation\n\\begin{eqnarray}\n\\sum_p\\int d^3v \\frac{m_p}{2}f_p v^2+\\sum_p K'(\\varphi) \\int d^3v m_p f_p v_z-\\sum_p\\int d^3v f_p \\Lambda_p'(f_p) \\nonumber\\\\\n+\\rho^{-1}\\left(\\Omega +\\frac{e}{m} B_z -\\sum_p m_p \\int d^3v  [\\rho^{-1}f_p,\\boldsymbol{v}\\cdot{\\mathbf x}] \\right)G'(\\varphi)\\sum_p \\int d^3v m_p f_p v_z\\nonumber\\\\\n+\\sum_pm_p \\rho^{-1} \\int d^3v f_p [\\boldsymbol{v}\\cdot{\\mathbf x},G(\\varphi)]=0\\,. \\label{dfp_inter}\n\\end{eqnarray}\nCombining Eqs. \\eqref{drho} and \\eqref{dfp_inter} we find the following Bernoulli equation \n\\begin{eqnarray}\n\\rho h(\\rho)=\\rho K(\\varphi)-\\frac{1}{2}\\rho u^2-\\sum_p\\left[\\int d^3v \\, f_p \\Lambda_p'(f_p)-\\int d^3v \\frac{m_p}{2}f_p v^2\\right]\\,. \\label{Bernoulli}\n\\end{eqnarray}\nThe two Grad-Shafranov equations, one for the ion and one for the electron fluid, stem from equations \\eqref{duz} and \\eqref{dpsi}, respectively, using equations \\eqref{dUpsilon}, \\eqref{dchi} and \\eqref{dBz}, along with the definition of $\\varphi$. These GS equations are,\n\\begin{eqnarray}\n&&\\hspace{-15mm}G'(\\varphi)\\nabla\\cdot\\left(\\frac{G'}{\\rho}\\nabla\\varphi\\right)+\\rho\\left(\\varphi-\\frac{e}{m}\\psi\\right)-\\rho K'(\\varphi)-\\frac{e}{m}\\frac{F(\\psi)+\\frac{e}{m}G(\\varphi)}{\\mu_0^{-1}-\\gamma}G'(\\varphi)\\nonumber\\\\\n&&\\hspace{25mm}+\\sum_p m_p \\int d^3v \\left(f_p v_z+G'[\\rho^{-1}f_p,\\boldsymbol{v}\\cdot{\\mathbf x}]\\right)=0\\,, \\label{ion_GS}\\\\\n&&\\hspace{-15mm}\\nabla\\cdot\\left[(\\mu_0^{-1}-\\gamma)\\nabla\\psi\\right]+n_e N'(\\psi)+\\frac{F(\\psi)+\\frac{e}{m}G(\\varphi)}{\\mu_0^{-1}-\\gamma}F'(\\psi)\\nonumber\\\\\n&&\\hspace{25mm}+\\frac{e}{m}\\rho\\left(\\varphi-\\frac{e}{m}\\psi+\\rho^{-1}\\sum_p m_p \\int d^3v f_p v_z\\right)=0\\,. \\label{electron_GS}\n\\end{eqnarray}\nNote that in Eq. \\eqref{electron_GS} the anisotropy parameter $\\gamma$ appears inside the differential operator, hence this PDE might not be always elliptic \\cite{Ito2007} as is for isotropic electron pressure. Finally, we need a set of equations for determining the equilibrium distribution functions $f_{p,e}$. Using Eq. \\eqref{duz}, Eq. \\eqref{dfp} is simplified to\n\\begin{eqnarray}\n\\Lambda_p'(f_p)=\\frac{1}{2}m_p v^2+m_p \\boldsymbol{v}\\cdot{\\mathbf u}\\,, \\label{Lambda_prime}\n\\end{eqnarray}\nwhere we have used \n\\begin{eqnarray}\n{\\mathbf u}_\\perp=\\rho^{-1}\\nabla G\\times\\hat{z}\\,,\\nonumber\n\\end{eqnarray}\nwhich can be deduced by Eq. \\eqref{dchi}, and the fact that $[\\boldsymbol{v}\\cdot{\\mathbf x},G]=(\\nabla G\\times \\hat{z})\\cdot\\boldsymbol{v}$. For invertible functions $\\Lambda_p$ we take solutions to \\eqref{Lambda_prime} of the form \n\\begin{eqnarray}\nf_{p,e}=f_{p,e}\\left(\\frac{1}{2} \\left(|\\boldsymbol{v}+{\\mathbf u}|^2-|{\\mathbf u}|^2\\right)\\right)\\,,  \\label{dist_fun_1}\n\\end{eqnarray}\nas was the case in \\cite{pjmTT14} for the planar kinetic-MHD PCS model. Note that the subscript $e$ denotes equilibrium. With \\eqref{Lambda_prime} the Beroulli equation becomes\n\\begin{eqnarray}\n\\rho h(\\rho)=\\rho K(\\varphi)-\\frac{1}{2}\\rho u^2-\\sum_p m_p\\int d^3v f_p\\boldsymbol{v}\\cdot{\\mathbf u}\\,. \\label{Bernoulli_2}\n\\end{eqnarray} \nAssuming a special functional form for $f_{p,e}$ we can, in principle, compute the velocity space integrals appearing in \\eqref{ion_GS}, \\eqref{electron_GS} and \\eqref{Bernoulli_2} to obtain a Hall-MHD Grad-Shafranov-Bernoulli (GSB) system, modified by the presence of energetic particles. As a final remark, note that the Hall-MHD GSB equations with anisotropic electron pressure are retrieved from \\eqref{ion_GS} \\eqref{electron_GS}, \\eqref{Bernoulli_2}, in the limit $f_p\\rightarrow 0$, as can be seen upon comparing with variants of this system obtained in \\cite{Kaltsas2017,Kaltsas2018} for isotropic electron pressure.\n\n\n\n\n\n\\section{Conclusions}\nWe constructed two kinetic-Hall MHD models with fluid and kinetic ions and fluid, massless electrons. In the first model the coupling of the kinetic and the fluid components is effected through the current density (current coupling scheme) while in the second model this coupling is carried out through the pressure tensor in the momentum fluid equation. Also, we consider an electron fluid with a gyrotropic electron pressure tensor, which is legitimate in the Hall MHD framework. This description, that allows for both fluid and kinetic ions, bridges ordinary Hall MHD, which can be recovered in the limit of vanishing kinetic ion population, and the most common hybrid kinetic-ion/fluid-electron description, often used in hybrid simulations. Therefore, the basic structure of the HMHD dynamics is retained while enriched by kinetic effects which are consistently described by the Vlasov equation. The Hamiltonian structures of the models are derived from the Hamiltonian structure of the HMHD and the Maxwell-Vlasov system using a method introduced in \\cite{Tronci2010}. In addition, a translationally symmetric description of the Hamiltonian PCS model is obtained along with the corresponding Hamiltonian structure and the Casimir invariants which are deployed in an Energy-Casimir variational principle that leads to a generalized Grad-Shafranov-Bernoulli system of equilibrium equations. Also, we would like to note that  alternative hybrid models with anisotropic electron pressure, could have been obtained if we have considered other extended Hall MHD models, e.g. the one presented in \\cite{Tronci2013}. This model is obtained upon neglecting electron mean flow inertia in the Lagrangian of the parent kinetic theory and then taking an appropriate fluid closure rather than assuming $m_e=0$. The utilization of extended MHD models with finite electron inertia for describing the bulk plasma, along with further investigations, regarding the construction of specific equilibria and the derivation of stability criteria will be the subject of a future work.\n\n\\section*{Acknowledgements}\nThis work   was   carried   out   within   the   framework   of   the EUROfusion Consortium. This work received funding from (a) the Euratom research and training program 2014\u20132018 and 2019\u20132020 under Grant Agreement No. 633053 and (b) the National Program for  the  Controlled  Thermonuclear  Fusion,  Hellenic  Republic.  The views and opinions expressed herein do not necessarily reflect those of the European Commission. PJM was supported by the DOE Office of Fusion Energy Sciences, under DE-FG02-04ER-54742 and a Forschungspreis from the Alexander von Humboldt Foundation. He warmly acknowledges the hospitality of the Numerical Plasma Physics Division of Max Planck IPP, Garching, Germany, where a portion of this research was done. \n", "meta": {"timestamp": "2021-07-02T02:23:32", "yymm": "2106", "arxiv_id": "2106.13060", "language": "en", "url": "https://arxiv.org/abs/2106.13060"}}
{"text": "\\section{Supplemental material}\n\\subsection{Numerical methods}\n\nTo sample the joint distribution $P(\\sigma,\\dot\\gamma,t)$ of stress $\\sigma$ and strain rate $\\dot\\gamma$, we cast the dynamics as a system of stochastic equations with continuous and discontinuous transitions. If $\\s_{i,t}$ denotes the value of stress element $i$ at time $t$, then the values at $t+\\Delta t$  is given by a probabilistic Euler update:\n\\begin{equation}\\label{eq:stress_sde}\n\t\\begin{aligned}\n\t\t\\s_{i,t+\\Delta t} &= \\s_{i,t} + \\gd_t \\Delta t + \\sqrt{2D_t \\Delta t} \\,{\\cal N}_i(0,1) \\quad \\text{with\\ prob.\\ } 1 - H(\\vert\\s_{i,t}\\vert - 1) \\Delta t\t,\n\t\t\\\\\n\t\t\\s_{i,t+\\Delta t} &= 0 \\quad \\text{with\\ prob.\\ } H(\\vert\\s_{i,t}\\vert - 1) \\Delta t,\n\t\\end{aligned}\n\\end{equation}\nwhere ${\\cal N}_i(0,1)$ is a unit-variance, zero-mean Gaussian random variable. The shear rate $\\gd_t$ and diffusion constant $D_t$ are also updated at each timestep:\n\\begin{equation}\n\tD_{t+\\Delta t} = \\frac{\\alpha}{N} \\sum_{i=1}^N H(\\vert \\s_{i,t}\\vert - 1) ,\n\t\\qquad\n\t\\gd_{t+\\Delta t} = \\Sigma - \\frac{1}{\\eta N} \\sum_{i=1}^N \\s_{i,t} .\n\\end{equation}\nFor all simulations, we take $\\Delta t = 10^{-3}$ as the maximum time step value, which was sufficient for the convergence of averages \\textit{and} steady-state distribution. The local power is the product of a single stress with the shear rate:\n\\begin{equation}\n\tp_t = \\gd_t \\sigma_t .\n\\end{equation}\nIn the steady state, by binning the values of $p$ over the elements on the streamline and over some period of time, we deduce the power distribution $\\mathcal{P}(p)$.\n\n\n\n\\subsection{Steady-state solution to HL model}\n\nThe steady state solution for~\\eqref{eq:HL-fokker-plank} in the main text is ($\\s_c=\\tau=1$, as in main text)~\\cite{hebraud_mode-coupling_1998, agoritsas_relevance_2015} \n\\begin{equation} \\label{eq:fhl}\n\t\\begin{aligned}\n\t\tf_{\\rm HL}(\\s) = \\frac{1}{\\mathcal{Z}} e^{\\xi_2 \\s} \n\t\\begin{cases}\n\t\t(\\xi_2/\\xi_1) e^{\\xi_1 (\\s + 1 )}  \\qquad {\\rm for}\\quad \\s <  - 1 , \\\\\n\t\t\\sinh(\\xi_2(\\s + 1) ) + (\\xi_2/\\xi_1) \\cosh(\\xi_2(\\s + 1)) \\qquad {\\rm for}\\quad - 1  < \\s < 0 , \\\\\n\t\t\\sinh(\\xi_2(1 - \\s) ) + (\\xi_2/\\xi_1) \\cosh(\\xi_2(1 - \\s)) \\qquad {\\rm for}\\quad 0 < \\s < 1 , \\\\\n\t\t(\\xi_2/\\xi_1) e^{\\xi_1( 1 - \\s )} \\qquad {\\rm for}\\quad \\s >  1 , \\\\\n\t\\end{cases}\n\t\\end{aligned}\n\\end{equation}\nwith\n\\begin{equation}\n\t\\begin{aligned}\n\t\t\\mathcal{Z}^{-1}& = \\frac{ \\tau^{-1} + \\dot{\\gamma}^2/(4D_\\text{HL})}{2 \\chi D_\\text{HL} \\xi_2  } \\Big[ \\sinh(\\xi_2 ) + (\\xi_2/\\xi_1) \\cosh(\\xi_2) \\Big] ,\n\t\t\\\\\n\t\t\\chi &= \\Big[ (2 \\tau)^{-1} + \\dot{\\gamma}^2/(4D_\\text{HL})\\Big] \\text{sinh}( 2 \\xi_2) + (\\xi_2/\\xi_1) \\Big[\\tau^{-1} + \\dot{\\gamma}^2/(4D_\\text{HL})\\Big] \\text{cosh}(2 \\xi_2) ,\n\t\t\\\\\n\t\t\\xi_1 &= \\sqrt{(D_\\text{HL}\\tau)^{-1} + (\\dot{\\gamma}/(2D_\\text{HL}))^2} ,\n\t\t\\quad\n\t\t\\xi_2 = \\gd/(2D_\\text{HL}) .\n\t\\end{aligned}\n\\end{equation}\nThe steady-state diffusion constant $D_\\text{HL}$ is determined by the non-linear equation:\n\\begin{equation}\n\t\\alpha = F(\\sqrt{D_\\text{HL}\\tau}, \\gd/D_\\text{HL}) ,\n\t\\qquad\n\tF(x, y) = x^2 + \\frac{1}{y} \\frac{1 + \\Big[ \\sqrt{1 + 4/(xy)^2} + 2/y\\Big] \\tanh(y/2)}{\\tanh(y/2) + \\sqrt{1 + 4/(xy)^2}} .\n\\end{equation}\nThe result for $f_{\\rm HL}$ leads to the fluctuation relation for the distribution of power $p=\\s\\gd$, as given in Eq.~(7) of the main text.\n\n\n\n\n\\subsection{NHL flow curves}\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=\\textwidth]{flow_curve_alphavar.png}\n\n\t\\caption{(left) Flow curves for NHL model. Note the existence of a yield stress for the values of $\\alpha < 0.5$. Produced with $N=2^{10}, \\Sigma=0.7 $, with $\\eta$ varying along each line. (right) Dependence of the average shear rate with the force balance stress $\\Sigma$. Produced with $N=2^{10}, \\eta = 0.2$. }\n\t\\label{fig:flow-curves}\n\\end{figure*}\n\nAs mentioned in the main text, the macroscopic yield stress for NHL exhibits Hershel-Bulkley form for $\\alpha < \\alpha_c$ and is Newtonian for $\\alpha > \\alpha_c$ - this is depicted in Fig \\ref{fig:flow-curves}. As a consequence of the non linear closure relations given by (2-3) in the main text, the average shear rate depends non-trivially on the shear stress $\\Sigma$. For a steady flow to be achieved $\\Sigma$ must be larger than the macroscopic yield stress which is set by the parameter $\\alpha$ as in HL.\n\n\\subsection{Distribution of stress and strain rate in NHL model}\n\nIn NHL model, the shear rate $\\dot\\gamma$ depends on all the stresses ${\\bm\\s}=\\{\\sigma_i\\}$ in the streamline, since its value is determined by the force balance Eq.~(3) in main text. Let $\\gd_t$ be the value of the shear rate at time $t$:\n\\begin{equation}\n\t\\gd_t = \\eta^{-1}\\Big(\\Sigma - \\frac{1}{N}\\sum_{i=1}^N \\s_{i,t}\\Big) ,\n\t\\qquad\n\t\\Delta\\gd_{t} =  -\\frac{1}{\\eta N} \\sum_{i=1}^N \\Delta\\s_{i,t} ,\n\\end{equation}\nits dynamics between sucessive resettings follows from~\\eqref{eq:stress_sde} as\n\\begin{equation}\n\t\\Delta\\gd_{t} = -\\frac{1}{\\eta} \\gd_{t}\\Delta t - \\sqrt{\\frac{2D_{t}\\Delta t}{\\eta^2 N^2}} \\sum_{i=1}^N {\\cal N}_i(0,1) ,\n\\end{equation}\nwhere the random variables ${\\cal N}_i(0,1)$ are identical to those defined in~\\eqref{eq:stress_sde}. Then, the dynamics of ${\\bf x} = (\\gd, {\\bm\\s})$ between resettings can be combined into a multivariate stochastic system:\n\\begin{equation}\n\td{\\bf x}_t = {\\bf A}({\\bf x}, t) dt + {\\bf B}({\\bf x}, t) d{\\bf W}_t ,\n\t\\qquad\n\t{\\bf A} = (-\\gd/\\eta, \\gd, \\dots, \\gd) ,\n\t\\qquad\n\t{\\bf B} = \\sqrt{2D_t} \n\t\\left( \\begin{array}{@{}c|ccc@{}} \\scalebox{1.2}0 & \\scalebox{1.2} -(\\eta N)^{-1} &  \\dots & -(\\eta N)^{-1} \\\\\n\t\t\\hline\n\t\t & & &\\\\\n\t\t \\scalebox{1.2}{\\bf 0}_{n \\times 1} &  &\\scalebox{1.2}{\\bf I}_{n\\times n} & \\\\\n\t\t & & &\\\\\n\\end{array} \n\\right) ,\n\\end{equation}\nwhere $d{\\bf W}_t$ is a set of uncorrelated Wiener processes with unit variance, so that the corresponding evolution of the joint probability $P_N({\\bm\\s},\\dot\\gamma,t)$ between resettings reads~\\cite{gardiner2004handbook}:\n\\begin{equation}\n\t\\partial_t P_N = \\sum_i \\partial_{x_i}(A_i P_N) + \\frac{1}{2} \\sum_{i.j} \\partial_{x_i}\\partial_{x_j} \\Big([{\\bf B}^T{\\bf B} ]_{ij} P_N\\Big) .\n\\end{equation}\nThe jump rate $\\cal W$ describes how stresses $\\bm\\s$ and shear rate $\\dot\\gamma$ undergo resettings in a correlated manner due to force balance:\n\\begin{equation} \\label{eq:jump-rates}\n\t{\\cal W}({\\bm\\s}, \\gd \\,\\vert\\, {\\bm\\s}', \\gd' ) = \\sum_i r(\\s'_i)\\delta(\\s_i) \n\\times \\delta\\big(\\gd - (\\gd' + (N\\eta)^{-1} \\s'_i)\\big) \\Bigg[ \\prod_{j\\neq i} \\delta(\\s_j - \\s'_j)\\Bigg] .\n\\end{equation}\nThe full dynamics of $P(\\s_i, \\gd, t) = \\int P_N({\\bm\\s},\\gd,t) \\prod_{j\\neq i}d\\s_j$, including both diffusion between resettings and resetting events, follows by marginalizing over all but one the stress variable $\\s_i$:\n\\begin{equation}\n\t\\begin{aligned}\n   \t\\partial_t P(\\s_i, \\gd, t) =& -\\gd \\partial_{\\s_i} P + D(t) \\partial^2_{\\s_i}P +\\frac{1}{\\eta} \\partial_{\\gd} ( \\gd P) + \\frac{D(t)}{\\eta^2 N}\\partial_{\\gd}^2 P  - \\frac{2 D(t)}{\\eta N} \\partial_{\\gd} \\partial_{\\s_i} P\n\t\t\\\\\n   \t&+ \\int \\prod_{j\\neq i} d\\s_j \\prod_k d\\s_k' d\\gd' \\Big[ {\\cal W}({\\bm\\s}, \\gd \\,\\vert\\, {\\bm\\s}', \\gd' ) P_N({\\bm\\s}', \\gd', t) - {\\cal W}( {\\bm\\s}', \\gd' \\,\\vert\\, {\\bm\\s}, \\gd ) P_N({\\bm\\s}, \\gd, t) \\Big] .\n\t\\end{aligned}\n\\end{equation}\nAfter performing the integrals in the final term, we get\n\\begin{equation}\n\\begin{aligned}\n \t\\partial_t P(\\s_i, \\gd, t) =& -\\gd \\partial_{\\s_i} P + D(t) \\partial^2_{\\s_i}P +\\frac{1}{\\eta} \\partial_{\\gd} ( \\gd P) + \\frac{D(t)}{\\eta^2 N}\\partial_{\\gd}^2 P  - \\frac{2 D(t)}{\\eta N} \\partial_{\\gd} \\partial_{\\s_i} P\n\t\\\\\n\t&+ (N-1) \\int d\\s' r(\\s')\\Big[P_2(\\s_i, \\s', \\gd - \\s'/(N\\eta), t) - P_2(\\s_i, \\s', \\gd,t)\\Big]\n\t\\\\\n\t&+ \\delta(\\s_i) \\int d\\s' r(\\s') P(\\s', \\gd - \\s'/(N\\eta), t) - r(\\s_i) P(\\s_i, \\gd, t) .\n\\end{aligned}\n\\end{equation}\nBy integrating over either $\\gd$ or $\\s_i$, we get the dynamics for $f(\\sigma_i,t)=\\int P(\\sigma_i,\\dot\\gamma,t)d\\gd$ and $g(\\gd,t)=\\int P(\\sigma_i,\\dot\\gamma,t)d\\s_i$:\n\\begin{eqnarray}\n\t\\partial_t f(\\s_i,t) &=& - \\int \\gd \\partial_{\\sigma_i}P(\\s_i,\\gd,t) d\\gd + D(t)\\partial_{\\s_i}^2f(\\s_i, t) - r(\\s_i) f(\\s_i, t) + \\delta(\\s_i) \\int d\\s' r(\\s') f(\\s', t) ,\n\t\\\\\\label{eq:model-gd-fokker-plank}\n\t\\partial_t g(\\gd, t) &=& \\frac{1}{\\eta} \\partial_{\\gd} ( \\gd g(\\gd,t)) + \\frac{D(t)}{\\eta^2 N} \\partial_{\\gd}^2 g(\\gd,t) + N \\int d\\s' r(\\s')\\Big[P(\\s', \\gd - \\s'/(N\\eta),t) - P(\\s', \\gd,t)\\Big] .\n\\end{eqnarray}\nWe then assume that $N$ is sufficiently large that we can use a Kramers-Moyal expansion in~\\eqref{eq:model-gd-fokker-plank}. Dropping terms $\\mathcal{O}(N^{-2})$ leads to\n\\begin{equation} \n\t\\partial_t g(\\gd, t) = \\frac{1}{\\eta} \\partial_{\\gd} \\Big[ \\gd g(\\gd,t) - \\int \\s' r(\\s') P(\\s', \\gd,t) d\\s' \\Big] + \\frac{1}{2\\eta^2 N} \\partial_{\\gd}^2\\Big[ 2D(t) g(\\gd,t) + \\int \\s'^2 r(\\s') P(\\s',\\gd,t) d\\s' \\Big] .\n\\end{equation}\nTo make further progress, we then assume that the joint distribution can be decomposed as $P(\\s, \\gd,t) \\approx f(\\s,t) g(\\gd,t)$, leading to Eqs.~(4-5) in the main text.\n\n\n\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=\\textwidth]{asym_Nvar.png}\n\n\t\\caption{Numerical proof of asymptotic independence between stress $\\s$ and strain rate $\\gd$. The joint distribution $P(\\s,\\gd)$ converges to the product of the marginal distributions $f(\\s)$ and $g(\\gd)$ as $N$ increases. Made with $\\alpha=0.55, \\eta=0.1, \\Sigma=0.8$.}\n\t\\label{fig:asymptotic-independence}\n\\end{figure*}\n\n\n\\subsection{Asymptotic statistical independence of stress and strain rate}\n\nThe joint distribution $P(\\sigma_i,\\dot\\gamma)$ can be written in terms of the conditionned probability $\\rho(\\dot\\gamma|\\sigma_i)$ as $P(\\sigma_i,\\dot\\gamma)=f(\\sigma_i)\\rho(\\dot\\gamma|\\sigma_i)$. Through a Bayesian lens, $\\rho(\\gd\\vert \\s_i)$ contains information about the statistics of shear rate $\\dot\\gamma$ given the value of a single stress compnent $\\sigma_i$. Intuitively, if the number of stress elements $N$ is large, one would not expect knowledge of $\\sigma_i$ to reduce the uncertainty in the shear rate $\\dot\\gamma$ by a meaningful amount. Then, we anticipate an \\textit{asymptotic decoupling} of $\\dot\\gamma$ and $\\sigma$ in the asymptotic limit $N\\to\\infty$. To confirm this numerically, the simplest statistics to investigate is the correlation:\n\\begin{equation}\n\t{\\rm Corr} = \\frac{\\langle \\s \\gd \\rangle - \\langle \\s \\rangle \\langle \\gd \\rangle}{\\{(\\langle \\s^2\\rangle - \\langle \\s \\rangle^2)(\\langle \\gd^2 \\rangle - \\langle\\gd\\rangle^2)\\}^{1/2}}\n\\end{equation}\nwhich vanishes if $\\s$ and $\\gd$ are independent. By taking the covariance of both sides of Eq \\eqref{eq:force-balance} of the main text with $\\gd$, we can show that\n\n\\begin{equation}\n\\begin{aligned}\n\t0 &= \\eta {\\rm Cov}(\\gd, \\gd) + N^{-1} \\sum_i {\\rm Cov}(\\s_i, \\gd) \\\\\n\\implies {\\rm Corr} &= -\\eta \\Big(\\frac{{\\rm Var}(\\gd)}{{\\rm Var}(\\s)}\\Big)^{1/2}\n\\end{aligned}\n\\end{equation}\n\nTherefore the correlation should decay with $N^{-1/2}$. We should also compare the distributions themselves, not just the correlation. To this end, we consider the integrated $L_1$ difference quantifying how similar two distributions are:\n\\begin{equation}\n\t\\delta = \\frac{1}{2}\\int \\vert\\vert P(\\s, \\gd) - f(\\s) g(\\gd) \\vert\\vert_1^1 d\\s d\\gd ,\n\\end{equation}\nso that $1 > \\delta \\geq 0$, and the lower bound being saturated when the distributions are identical. It can be seen from Fig~\\ref{fig:asymptotic-independence} that both $\\rm Corr$ and $\\delta$ decrease with $N$, as a strong evidence for the decoupling of the variables in the regime of large $N$.\n\n\n\n\n\\subsection{Extreme tails of the power distribution}\n\nWe set out to derive an expression for the cumulative function $\\mathcal{P}(p > p_{l}) = \\int^{\\infty}_{p_{l}} \\mathcal{P}(p) dp$ which is asymptotically correct at large $p_l$. In the asymptotic decoupling limit ($N\\to\\infty$), we get\n\\begin{equation}\n\t{\\cal P}(p) = \\int \\delta(p-\\s\\gd) P(\\s,\\gd) d\\s d\\gd = \\int f(\\s) g(p/\\s) \\frac{d\\s}{|\\s|} ,\n\t\\qquad\n\tg(\\gd) \\propto \\exp \\bigg[ - \\frac{(\\dot\\gamma - \\langle\\gd\\rangle)^2}{2\\text{Var}(\\gd)} \\bigg] ,\n\\end{equation}\nwhere $f$ is given by $f_{\\rm HL}$ in~\\eqref{eq:fhl} with $\\langle\\dot\\gamma\\rangle\\to\\dot\\gamma$. The cumulative function follows as\n\\begin{equation}\n\t\\begin{aligned}\n\t\t\\mathcal{P}(p > p_{l}) =& \\int d\\s f(\\s) \\int^{\\infty}_{\\frac{p_{l} - \\s\\langle\\gd\\rangle}{\\sqrt{2\\text{Var}(\\gd)} \\vert\\s\\vert}}  e^{ -t^2} \\frac{dt}{\\sqrt\\pi}\n\t\t\\\\\n\t\t&= \\frac{1}{2} - \\frac{1}{2} \\int \\text{erf}\\bigg(\\frac{p_{l} - \\s\\langle\\gd\\rangle}{\\sqrt{2\\text{Var}(\\gd)} \\vert\\s\\vert} \\bigg) f(\\s) d\\s ,\n\t\\end{aligned}\n\\end{equation}\nwhere erf($x$) is error function, canonically defined as\n\\begin{equation}\n\t\\text{erf}(x) = \\frac{2}{\\sqrt \\pi} \\int_0^x e^{-t^2} dt .\n\\end{equation}\nSince $p_l \\gg \\s \\langle \\gd \\rangle$ for the parts of the integral with the most weight, we get\n\\begin{equation}\n\\begin{aligned}\\label{eq:pcumul}\n\t\\mathcal{P}(p > p_{l}) &\\approx \\frac{\\langle \\gd \\rangle}{\\sqrt{2\\pi\\text{Var}(\\gd)}} \\int  e^{-\\frac{p_{l}^2}{2 \\text{Var}(\\gd) \\s^2}} f(\\s) \\frac{|\\s|}{\\s} d\\s\n\t\\\\\n\t&\\approx \\frac{\\langle \\gd \\rangle}{\\sqrt{2\\pi\\text{Var}(\\gd)}} \\Big[ a_{-} \\mathcal{J}(s_{-}) - a_{+}\\mathcal{J}(s_{+}) \\Big] ,\n\\end{aligned}\n\\end{equation}\n\nwhere, in the second line, we approximate the stress distribution $f$ as exponentials with different decaying factors for positive and negative $\\s$ (see $f_{\\rm HL}$ in~\\eqref{eq:fhl}), which neglects contributions from the non-resetting region $|\\s|<1$. Indeed, the exponential term in~\\eqref{eq:pcumul} makes the integrand negligible for $\\vert \\s\\vert \\lessapprox p_l /\\sqrt{2\\text{Var}(\\gd)}$, which is well above $1$ for large $p_l$. The dimensionless variables $s_\\pm$, $a_\\pm$ and the integral $\\mathcal{J}$ read\n\\begin{equation}\n\t s_\\pm = \\frac{(\\beta_{\\pm} p_l)^2}{2\\text{Var}(\\gd)} ,\n\t\\qquad\n\ta_\\pm = \\frac{\\beta_{\\pm}^{-1}}{\\beta_{-}^{-1} + \\beta_{+}^{-1}},\n\t\\qquad\n\t\\beta_\\pm = \\xi_1 \\pm \\xi_2 ,\n\t\\qquad\n\t\\mathcal{J}(s) = \\int_0^{\\infty} e^{-(s/t^2 + t)} dt .\n\\end{equation}\nUsing saddle point, we approximate $\\cal J$ at large $s$ by $\\mathcal{J} \\approx e^{-(2^{1/3} + 2^{-2/3})s^{1/3}}  \\sqrt{\\frac{ \\pi2^{4/3}s^{1/3}}{3}}$. Using this, as well as an identical argument for the negative tail, we deduce that the extreme tails go as\n\\begin{equation}\n\t\\log {\\cal P}(\\pm p) \\underset{|p|\\to\\infty}{\\sim} - \\frac{2^{1/3} + 2^{-2/3}}{2^{1/3}} \\bigg[\\frac{(\\beta_\\mp |p|)^2}{\\text{Var}(\\gd)}\\bigg]^{1/3} .\n\\end{equation}\nThis confirms that the power distribution is not a pure exponential at large $|p|$, as given in Eq.~(8) of the main text.\n\n\n\n\n\\end{document}\n\n\nMultiplying~\\eqref{eq:HL-fokker-plank} by $e^{i k \\s} $ and integrating over all stress values in steady state yields a self-consistent expression for the characteristic function:\n\\begin{equation}\n\t\\langle e^{i k \\s} \\rangle = \\frac{ \\int r(\\s) ( e^{i k \\s} - 1) f_{\\rm HL}(\\sigma) d\\s \n }{ -D_\\text{HL} k^2 + i \\dot{\\gamma} k} ,\n\\end{equation}\nand, expanding at small $k$, we then get\n\\begin{equation}\n\t1 = \\f{1}{\\gd} \\langle \\s r(\\s) \\rangle ,\n\t\\qquad\n\t\\langle \\s \\rangle = -\\f{D_\\text{HL}}{\\gd}  + \\f{1}{2\\gd} \\langle \\s^2 r(\\s) \\rangle ,\n\t\\qquad\n\t\\langle \\s^2 \\rangle = -\\f{2D_\\text{HL}}{\\gd} \\langle \\s \\rangle  + \\f{1}{3\\gd} \\langle \\s^3 r(\\s) \\rangle ,\n\\end{equation}\nyileding a simple expression for the stress variance:\n\\begin{equation}\n\t\\text{Var}(\\s) = \\Big( \\f{D_\\text{HL}}{\\gd} \\Big)^2 - \\Big( \\langle \\s \\rangle - \\f{D_\\text{HL}}{\\gd}\\Big)^2 + \\f{1}{3\\gd} \\langle \\s^3 r(\\s) \\rangle .\n\\end{equation}\n\n", "meta": {"timestamp": "2021-06-25T02:17:15", "yymm": "2106", "arxiv_id": "2106.12962", "language": "en", "url": "https://arxiv.org/abs/2106.12962"}}
{"text": "\\section{Introduction}\n\nMost objects are straightforward, but some object harbor secrets.  While most\nobjects are collections of assorted fields bundled with methods that operate on\nthem, occasionally an object is a transparent fa\\c{c}ade providing an\nabstraction over an underlying complex system.  Sometimes the fa\\c{c}ade is\npierced and bad things happen. An example follows.\n\nThis is a definition of an object in the R language representing a sequence of\nelements 1--10.\n\n\\begin{verbatim}\n    simple <- as.integer(c(1,2,3,4,5,6,7,8,9,10))\n\\end{verbatim}\n\n\\noindent\nInternally, this object is a simple vector with a header and a body consisting\nof all of its member values.  The values can be accessed directly. For example,\nthe \\verb|simple[i]| operator retrieves the the \\verb|i|th indexed element by\naccessing the memory at an offset from the end of the header.\nHowever, the same sequence can be expressed as using the following simpler\nsyntax.\n\n\\begin{verbatim}\n    magic <- 1:10\n\\end{verbatim}\n\n\\noindent\nThe \\verb|magic| vector outwardly appears to be the same as the \\verb|simple|\nvector. However, internally the vector only contains two values---the beginning\nand end of the range---and the values of the sequence are calculated on demand.\nThen, \\verb|magic[i]| is redefined to run the function calculating the value,\ninstead of accessing an offset. \n\nThis alternative representation of vectors in R (ALTREP) \\cite{Bec20} allows\nimplementing custom, even user-defined, back-ends to vectors while providing a\ncompatible API to ordinary R vectors. The advantage of this is the flexibility\nof semantics and internal representation that allows implementing file-backed\npersistent vectors, larger than memory vectors, and fast sequences with low\nmemory overheads. The disadvantage is that since the internal layout of ALTREP\nvectors is so different from the layout of R vectors, the entire R runtime\nneeded retooling to handle them.\n\nThe abstraction ALTREP vectors present to the user can be pierced by\nintrospection.  While many languages provide mechanisms for observing the\ninternals of objects, in R this is perhaps easier than most.  R itself and many\nR packages are written in C, so R provides a C API that allows packages to\ninterface with the runtime internals and runtime objects. This exposes the\nlayout of objects to external programmers, who are known to circumvent\nprescribed API functions in favor of direct memory accesses into vectors.\nALTREP vectors defend themselves against this by materializing if the pointer\nto the body of a vector is accessed via an API function. On the other hand, the\nproblem persists in general, as a sufficiently stubborn programmer may reach\ninto a vector via pointer arithmetic without reference to the API at all,\ninadvertently dispelling the ALTREP abstraction and introducing segmentation\nfaults or subtle memory bugs.\n\nThere are a number of frameworks and runtime mechanisms providing similar\nfa\\c{c}ades without runtime support too. In R there are numerous libraries\nproviding transparent larger-than-memory vectors (matter \\cite{BV17}, ff \\cite{KEW13}, bigstatr \\cite{PAZB18}, disk frame\n\\cite{df},  etc.) or abstractions\nover SQL databases (dbplyr \\cite{dbplyr}), in addition to ALTREP. Abstracting\nframeworks are also found in other languages, e.g. Remote Objects \\cite{RMI} in\nJava and Dask data frames \\cite{rock15} in Python. These can be introspected\ninto by the application of nefarious means.\n\nWe attempt to create completely transparent abstractions by exploring a\ndifferent approach We introduce a framework for Userfault Objects (UFOs)\n\\footnote{\\url{https://github.com/PRL-PRG/UFOs}} \n\\;%\nUFOs expose an area of virtual memory to the program in some host language. This area is populated with the\nrepresentation of the object using the layout and contents that the host\nlanguage is expecting, but this is done lazily. Specifically, when an access to\nthe memory inside the object occurs, the UFO framework communicates with the\noperating system (i.e.  with the Linux Kernel via \\verb|userfaultfd|) to\nmaterialize and populate a section of memory. The population procedure is\nperformed by a custom (user-defined) function which provides a specific slice\nof the object. The population function can provide contents of the object by\ncalculating it or retrieving it from persistent storage (e.g. by parsing a CSV\nfile or running SQL queries), a remote site, or other external sources.\nThe ability to process data on the fly as it is being read, as well as to have\nno backing persistent storage at all distinguishes UFOs from memory mapped\nfiles.\n\n\\section{UFO core framework}\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{figs/ufo-core}\n  \\caption{UFO core framework architecture.}\n  \\label{fig:ufo-core}\n \n\\end{figure}\n\nOur proof-of-concept implementation consists of two layers: a language agnostic\ncore framework and a language specific API. This section describes the former.\n\\emph{UFO core} interacts directly with the operating system and manages the\ncreation and destruction of individual UFOs. It also handles reading and updating them.\nThe framework discharges its responsibilities via two cooperating subsystems: the\n\\emph{event API} and the \\emph{page fault loop}, each running in a separate\noperating system thread.\nThe event API is exposed as a fa\u00e7ade through which UFOs can be created or freed.\nThe UFO API calls these functions directly.\nThe page fault loop is responsible for managing UFOs as they are accessed.\nThis involves loading and unloading UFOs fragments in and out of memory,\nin response to the needs of the user application.  It provides mechanisms for\npopulating areas of memory, a garbage collector for UFO fragments, and a system\nfor persistently caching modified fragments.  The user does not interact with\nthe page fault loop directly. Instead, the page fault loop is registered as a\nhandler for page faults with the Linux kernel for a range of virtual memory\naddresses. The subsystems of the page fault loop are always reactions to\noperations performed on memory guarded by the UFO core framework.\n\n\\subsection{Objects}\n\n\n\n\n\nThese userfault objects are user-facing, logical structures representing\ncomplete larger-than-memory objects of a host language.  Logically, each\nUFO owns a range of consecutive addresses whose contents are defined by a\nsingle, specific, user-defined \\emph{population function}.\n\nWhile UFO core is agnostic with respect to the layout of host language\nobjects, we apply a simplifying assumption toward their internal representation\nto facilitate the definition of population functions for fragments\nof objects. We assume that UFOs represent arrays, each containing a header\nfollowed by a body consisting of some number of indexed, uniformly-sized elements.\nWe show the logical layout of a UFO in Fig.~\\ref{fig:ufo-layout}.\nThe boundary between the header and the elements is immutable and falls at the\nboundary between the first and second segment of the UFO. The front of the UFO\nis padded to accommodate the boundary. The rear is padded to align the UFO with\npage size.\nThe header is initially empty and its contents are not generated by UFO core.\nThe contents of elements in the body are generated by the population\nfunction. \n\n\nThe population function is executed during the lifecycle of the UFO to provide\ncontents of elements as they are accessed. The definitions of population\nfunctions are external to UFO core. \nThe function generates the contents for a range of elements for a specific UFO,\nwhere the first and last index of the generated elements are specified via function\nparameters. \nUFO core may demand that the function populate any contiguous region within a\nUFO. A generated region may overlap other regions, and regions may be populated in any\norder as well as re-populated repeatedly. To accomodate this behavior, populate\nfunctions must be deterministic and idempotent.\nSince the state of the host runtime is unknown at the time of any specific\nmemory access, population function must be careful about interacting with the\nhost runtime.\nCurrently population function may not attempt to access other UFOs, since it\nwould lead to nested userfault events.\nWe show an example population function in\nFig.~\\ref{fig:population-function-example}.\n\n\n\\subsection{Segments}\n\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{figs/ufo-layout}\n  \\caption{UFO layout.}\n  \\label{fig:ufo-layout}\n \n\\end{figure}\n\n\\begin{figure}[t]\n{ \\footnotesize\n  \\centering\n  \\begin{verbatim}\n  typedef struct { int from; int to; int by; } ufo_seq_data_t;\n  int populate_sequence(uint64_t start_ix, uint64_t end_ix, \n                        ufUserData ufo_ud, char* target) {\n    ufo_seq_data_t* data = (ufo_seq_data_t*) ufo_ud;\n    for (size_t i = 0; i < end_ix - start_ix; i++) {\n      ((int *) target)[i] = \n          data->from + data->by * (i + start_ix);\n    }\n    return 0;\n  }\n  \\end{verbatim}\n  \\caption{Population function: from-to-by sequence.}\n  \\label{fig:population-function-example}\n \n}    \n\\end{figure}\n\nInternally, UFOs are split into \\emph{segments}, each segment representing a\nmanageable chunk of the object's address range. At any point any segment can be\nactively held in memory (\\emph{materialized}) or be removed from memory\n(\\emph{dematerialized}). Dematerializing segments either destroys or caches\ndata, depending on circumstances. Materializing a segment involves (re)generating\nits data through its population function or retrieving the data from a\npre-existing cache. \nSegment management is entirely transparent to the end user.\n\nUFO core has no way of tracking accesses to segments after they are\nmaterialized.  Therefore, ensuring that written values are not forgotten at\ndematerialization requires caching.  Dirty segments are detected by comparing\nthe hash of their contents at the time of dematerialization with the hash after\ntheir most recent materialization. Hashes are computed using the 256-bit BLAKE3\nalgorithm \\cite{CANH20}. Dematerialization of dirty segments will first cause\ntheir contents to be stored in anonymous temporary persistent storage. Each UFO\nhas its own file which remains in existence as long as the UFO is alive. All\ncache files are cleaned up at program termination by the operating system.\n\n\n\nUFO core keeps count of how much memory is being used by materialized segments.\nUFO core carries two user-defined parameters: the high and low\nwater marks. When the amount of memory taken up by materialized segments\nexceeds the high water mark, the UFO garbage collector is called and it starts\ndematerializing segments until the low water mark is reached.\n\nThe garbage collector walks the loaded segments queue (implemented as a\ncircular buffer) starting from the longest residing segment. It dematerializes\nsegments one by one until enough space has been freed. Dematerialization does\nnot immediately destroy the area of memory. Instead, the kernel is signaled\nthat the page is no longer in use and can be recycled. The kernel lowers the\nresident set size immediately and but recycles the memory at its convenience.\n\n\\section{R UFO API}\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{figs/sandwich}\n\t\\caption{R UFO API architecture diagram.}\n  \\label{fig:sandwich}\n \n\\end{figure}\n\n\nWe implemented language-specific UFO API for the R language. We picked the R\nlanguage because it is used by data scientists in fields like computational\nbiology, statistics, artificial intelligence, and machine learning, which deal\nwith large volumes of data, often represented as either vectors or data frames\n(tables with uniformly sized vectors representing columns---\u00e0 la CSV files).\nThe R ecosystem contains a many larger-than-memory libraries that create\nobject-oriented abstractions to hide the details of memory management from end\nusers while transparently representing the data to the programmer as vectors or data frames\n\\cite{KEW13,df,PAZB18,BV17,dbplyr}.\n\nThe R UFO API has two levels of services (Fig.~\\ref{fig:sandwich}). The R UFOs\nlibrary ties the UFO core framework into the R runtime, providing an API to\nspecific R vector back-ends. It provides a constructor that creates a UFO\nwith a specific user-defined population function. It does so by plugging into\nR's custom allocator API (and garbage collector),\nreplacing calls to \\verb|malloc| and \\verb|free| with calls to the R UFO core\nevent API. The UFO allocator returns an area of\nmemory to the R runtime, which is the runtime populates with an appropriate header.\nIn most cases, the R runtime will not pre-fill the vector, but if this happens,\nUFOs ignore specific writes and their population function generates the\nappropriate pre-fill values.\n\nWe provide four back-end implementations built on top of R UFOs.\nBinary file-backed vectors (and matrices) read data directly from a binary\nfile. The data is located via seeking. CSV vectors each read a single column of\na CSV file. The values are parsed on-the-fly from a fragment of a pre-scanned\nCSV file.  From-to-by sequences lazily generate data from a simple formula,\nbased on the index of an element. Empty vectors are pre-filled with a default\nvalue and can be used to store large intermediate results of\ncomputation. \n\nThe biggest difficulty in implementing R vectors, is that R operations do not\nallow custom allocation to be used in the results of arithmetic operations and\nmany functions. For this reason, in addition to back-end implementations, R UFO\nAPI also provides a reimplementation of R operators that write results to UFOs,\nas well as a toolkit for chunking the execution of existing functions while\naggregating the results into a UFO.\n\n\\section{Performance}\n\n\\begin{figure*}\n    \\begin{subfigure}{.33\\textwidth}\n        \\includegraphics[width=.89\\textwidth]{figs/250M-rand-bin-create}\n    \\end{subfigure}\n    \\begin{subfigure}{.33\\textwidth}\n        \\includegraphics[width=.89\\textwidth]{figs/250M-rand-bin-sum}\n    \\end{subfigure}\n    \\begin{subfigure}{.33\\textwidth}\n        \\includegraphics[width=.89\\textwidth]{figs/250M-rand-bin-loop}\n    \\end{subfigure}\n\n    \\begin{subfigure}{.33\\textwidth}\n        \\includegraphics[width=.89\\textwidth]{figs/250M-seq-create}\n    \\end{subfigure}\n    \\begin{subfigure}{.33\\textwidth}\n        \\includegraphics[width=.89\\textwidth]{figs/250M-seq-sum}\n    \\end{subfigure}\n    \\begin{subfigure}{.33\\textwidth}\n        \\includegraphics[width=.89\\textwidth]{figs/250M-seq-loop}\n    \\end{subfigure}\n\t\\caption{Performance evaluation.}\n  \\label{fig:performance}\n\\end{figure*}\n\n\nWe benchmark UFO performance measured against ALTREP and standard R vectors. \nALTREP is a good candidate for comparison because it represents frameworks that\ncreate an object-oriented--like facade over complex functionality while\nappearing as simple vectors. ALTREP is integrated into the R\nruntime, giving it a performance edge over user-created libraries.\nWe test UFOs in two modes: read/write mode and read-only\nmode. Read-only mode does not persist changes done to UFOs, which removes the\nneed to calculate hashes of segment contents when loading and unloading them.\n\nWe use two identically implemented back-ends for UFOs and ALTREP.\nFile-backed vectors read 4-byte integers from a binary file on disk by seeking\nto the position of the vector and reading one or more consecutive values.\nThis back-end has a relatively high overhead of retrieving a single\nvalue, which can be amortized by populating entire regions at once.\nSequence vectors represent from-to-by sequences calculated on the fly (see\nFig.~\\ref{fig:population-function-example}).\nComputing an individual element of the sequence is cheap.\nWe measured the time\nit takes to create a 1GB vector (1K iterations), calculate the sum of its\ncontents (1K iterations), and execute an identity function on each of its\nelements (10 iterations).\n\nWe ran the experiments on a machine with an Intel Core\u2122 i7-10750H CPU @\n2.60GHz$\\times$12 process, 32GB RAM and a and a Samsung SSD 970 EVO 500GB drive\nrunning 64-bit Ubuntu 20.10 with 5.8.0-53-generic Linux kernel. \nWe show the results of the evaluation in Fig.~\\ref{fig:performance}. Each plot\nshows the results for either the creation, sum, or loop microbenchmark. The top\nrow shows results for the file-backed vectors, the bottom one for sequences.\nThe X-axis always shows vector implementations and Y-axis show execution time\nin nanoseconds. The results are plotted as a violin plots showing the\ndistribution of execution times over multiple iterations.\n\nWe observe that UFOs and ALTREP have similar performance for vector creation\nand the execution time is negligibly small for both frameworks, with some\noutliers we attribute to initialization and garbage collection. The startup\ntime is higher for R vectors implementing a sequence, because the vector must\npopulated up front, as opposed to UFOs and ALTREP, which calculate these values\non demand. This initialization cost for standard vectors could eventually be\namortized over multiple passes over the vector.\n\nSums also yield similar performance for all frameworks. The lightweight\ncalculation overhead involved in sequences especially washes away performance\ndifferences. \nFor file-backed vectors UFOs and ALTREP also perform similarly.  The R runtime\ncalculates the sum of a vector using a fast arithmetic function. This function\ncooperates with ALTREP to chunk the vector into regions, which allows ALTREP\nto amortize the overhead of preparing a file for reading and seeking.  While\nthe R runtime does not similarly chunk the execution for UFOs, the UFO\nframework makes sure to read no less than 1MB of elements at-a-time and cache\ndata, yielding a similar amortization. Thus, the performance for both\nframeworks is similar.  When the hashing mechanism is turned off for read-only\nUFO vectors, a significant overhead cost is removed for UFOs, yielding a small,\nbut visible improvement in performance.\n\nAn importance difference in performance between UFOs and ALTREP stems from the\nfact that ALTREP performs dynamic dispatch whenever values are accessed, be it\na region or a single value. The R runtime attempts to turn individual value\naccesses into region accesses for ALTREP, but this can only works for specific\noperations. When the loop benchmark executes, it always executes a function on\na single value from a vector, leading to repeated dispatch in ALTREP, and so,\ndeteriorates performance significantly. UFOs also have set-up costs relating to\nloading data for an accessed value, however these costs are always amortized by\nloading an entire segment into memory. This gives UFOs an advantage over\nALTREP's dispatch and produces performance close to ordinary vectors when\nconsecutive elements are accessed. However, this approach is costly if\nthe access pattern is spread out, causing the UFO to load and\nunload a segment for each single value read.\n\n\n\\section{Conclusions and future work}\n\nThe UFO framework explores avenues of cooperating with the operating system to\nuse memory in non-traditional ways. We implement a framework that uses user\nfaults to lazily provide data to a language's runtime object. This allows the\nimplementation of structures that generate data from a variety of sources, but\nfollow the memory layout of standard runtime objects, so they can be\nintrospected safely.  Nevertheless, they can implement complex back-ends and\nprovide access to larger-than-memory data that never needs to materialize into\nmemory fully. Implementing objects via userfaults also has an impact on\nperformance as overhead is amortized over loading large segments of data and\nthe host runtime can rely on direct memory accesses into userfault object.\n\nFuture work includes implementing a mechanism for supporting recursive calls\nbetween UFOs and reacting to specific memory access patterns to limit\nunnecessary memory usage. We would also like to explore the applicability of\nthis approach outside of the Linux ecosystem and in other language runtimes.\n\n\n\n\n\n\\begin{acks}\nThis work is supported by the Czech Ministry of\nEducation, \\linebreak \nYouth and Sports from the Czech Operational Programme Research,\nDevelopment, and Education, under grant agreement No.\n\\linebreak CZ.02.1.01/0.0/0.0/15\\_003/0000421 and the European Research Council (ERC)\nunder the European Union\u2019s Horizon 2020 research and innovation programme (grant\nagreement No. 695412).\n\\end{acks}\n\n\\bibliographystyle{ACM-Reference-Format}\n", "meta": {"timestamp": "2021-06-25T02:18:12", "yymm": "2106", "arxiv_id": "2106.12995", "language": "en", "url": "https://arxiv.org/abs/2106.12995"}}
{"text": "\\section{Introduction}\n\nLet $M$ be a finite volume Riemannian manifold or a finite CW-complex. One of the most powerful and well studied invariants of $M$ are its homology groups. These abelian groups decompose to a free and a torsion part. The ranks of the free parts give the rational Betti numbers of $M$. One can also compute the mod $p$ Betti numbers, from the full homology group, and the Euler characteristic, as the alternating sum of Betti numbers. For a finite sheeted cover of $M$, the Euler characteristic is multiplicative in the index of the cover, but already the rational Betti numbers can behave quite erratically in this respect. To smooth this behavior out, one can consider the $j$-th $\\ell^2$ homology, and measure the dimension of the corresponding Hilbert space using von Neumann dimension (Atiyah \\cite{Atiyah}). Alternatively, one can consider the \\defin{growth} of the $j$-th rational Betti numbers over a suitable sequence of finite sheeted covers of $M$. As shown by the L\\\"uck approximation theorem \\cite{Luck-1994-approx} these two attempts give the same result, called the $j$-th $\\ell^2$ Betti number of $M$. \nThis result naturally led to studying the growth of other homological invariants as well, like the growth of the mod $p$ Betti numbers and the growth of the torsion. \n\nOver the years, the interest of the community has shifted from the study of the homology of spaces $M$ to the homology of their fundamental groups. Our results are also expressed in terms of group homology.\n\nSince the torsion grows at most exponentially in the index of the cover, the right definition of the \\defin{$j$-th torsion growth of $M$} is to take the logarithm of the cardinality of the $j$-th torsion group of the covering space, normalized by the index of the cover and consider its limit, for a suitable sequence of covers. \n\n\nAs of now, control of the torsion (and also the mod $p$ Betti numbers) is much weaker than for the rational Betti numbers. In particular, we do not have the analogue of the L{\\\"u}ck approximation theorem, even in the most natural settings. More than that, strikingly, we do not know a single example of a \\textbf{finitely presented} group with positive \\textbf{first} homology torsion growth over a decreasing  sequence of  finite index normal subgroups with trivial intersection, while, at the same time, it is a well-accepted conjecture that all sequences of congruence subgroups of arithmetic hyperbolic $3$-manifold group have this property, see \\cite{BSV,BrockDunfield,EMS}. \n\nIn this paper we prove vanishing results on the torsion growth of higher homology groups of a natural class of residually finite groups, using a new homotopical method called effective rebuilding. We  apply this method to non-uniform higher rank lattices, mapping class groups and various Artin groups, among others.\nFor the lattice case, we also obtain explicit estimates of the convergence for covering maps with respect to principal congruence subgroups. \n\n\n\n\\subsection{Main results}\n\nWe start by stating two theorems in their simplest forms that are good showcases for the more general and technical results of the paper. \nFor a group $\\Gamma$, its homology groups decompose as $H_j (\\Gamma , \\Z)=\\Z^{b_j(\\Gamma)}\\oplus H_j (\\Gamma , \\Z)_{\\mathrm{tors}}$ \nwhere $b_j(\\Gamma)$ is the $j$-th Betti number and where $H_j (\\Gamma , \\Z)_{\\mathrm{tors}}$ is the torsion subgroup. Let $|H_j (\\Gamma , \\Z)_{\\mathrm{tors}} |$ denote its cardinality. \n\nOur first result deals with an arbitrary sequence of finite index subgroups. \n\n\\begin{introth} \\label{T00}\nLet $\\Gamma = \\SL_d (\\Z )$ ($d \\geq 3$) and let $(\\Gamma_n)_{n \\in \\mathbb N}$ be an injective sequence of finite index subgroups of $\\Gamma$. Then for every degree $j \\leq d-2$, we have:  \n\\begin{equation} \\label{E:11}\n\\frac{\\log |H_j (\\Gamma_n , \\Z)_{\\mathrm{tors}} |}{[\\Gamma : \\Gamma_n]} \\longrightarrow 0, \\quad \\mbox{as } n \\to \\infty.\n\\end{equation}\n\\end{introth}\nThere are deep number-theoretical motivations to study torsion in the homology of arithmetic groups. It is directly related to algebraic K-theory and the Vandiver conjecture, see e.g. \\cite{Soule-99,Emery}. More recently it has attracted further attention since, thanks to the deep work of Scholze \\cite{Scholze}, one can roughly say that mod $p$ torsion classes in the homology of congruence subgroups of $\\SL_d(\\Z)$\nparametrize field extensions $K/\\mathbb{Q}$ whose Galois groups are subgroups of $\\mathrm{PGL}_d (\\overline{\\mathbb{F}}_p)$. \nTheorem~\\ref{T00}  confirms a part of a general related conjecture of Venkatesh and the second author that postulates that the limit in \\eqref{E:11} is zero for all $d \\geq 3$ and all $j$ {\\bf except} when $(d,j) = (3,2)$ or $(d,j)= (4,4)$, see \\cite{BV,EMS,AGMY}. \n\nWhen restricting our attention to \\defin{principal congruence subgroups}\n$$\\Gamma (N) = \\ker \\left( \\SL_d (\\Z ) \\to \\SL_d (\\Z / N \\Z ) \\right)$$\nour second result gives quantitative upper bounds on the torsion growth. \n\n\\begin{introth} \\label{T0}\nLet $\\Gamma = \\SL_d (\\Z )$ ($d \\geq 3$). Then for all $N \\geq 1$ and $j \\leq d-2$ we have\n\\begin{equation} \\label{E:12}\n\\frac{\\log |H_j (\\Gamma (N) , \\Z)_{\\mathrm{tors}} |}{[\\Gamma:\\Gamma(N)]} = O \\left( \\frac{ \\log N}{N^{d-1}} \\right), \\quad \\mbox{as } N \\to \\infty.\n\\end{equation}\n\\end{introth}\n\nWe refer to Section \\ref{S10} for the more general Theorem~\\ref{TorsionSL} about non-uniform lattices in semisimple Lie groups. \n\nThe first homology case of Theorem \\ref{T00}, that is the case $j=1$, is known, in particular it follows from \\cite[Theorem 4]{AGN}.\nUsing the less elementary, but more classical machinery of \\defin{congruence subgroup property} (CSP) on these groups, they also show the stronger estimate that there exists a constant $c$ depending only on $d$ such that for all finite index subgroups $\\Gamma_0$ of $\\mathrm{SL}_d (\\mathbb{Z})$ we have \n\\begin{equation}\\label{eq: str estimate degree 1}\n|H_1 (\\Gamma_0 , \\mathbb{Z} )_{\\mathrm{tors}}| = |H_1 (\\Gamma_0 , \\mathbb{Z} )| = \n|\\Gamma_0^{\\rm ab}| \\leq [\\mathrm{SL}_d (\\mathbb{Z}) : \\Gamma_0 ]^c,\n\\end{equation}\n see \\cite[\\S 5.1]{AGN}. \n\nFor $d = 3$, we expect that the degree bound $j \\leq d-2$ is sharp for both theorems above. \nThe general conjecture of \\cite{BV} indeed postulates an exponential growth of torsion for $\\SL_3 (\\Z )$ in degree $2$. As an indication, big torsion groups do show up in the recent computational work of Ash, Gunnells, McConnell and Yasaki \\cite{AGMY} for both cases. The degree bound $j \\leq d-2$ is probably not optimal for higher degree. We shall see that it is a natural stopping point for our approach, though. \n\nThe non-effective version of our methods already gives applications for the growth of mod $p$ Betti numbers for any prime $p$. \nSince we obtain a good control over the number of cells of finite covers, it follows that the growth of the $j$-th mod $p$ Betti number is zero,\nunder the same conditions as in Theorem \\ref{T00}.\nThe supplementary control offered by principal congruence subgroups leads as in Theorem \\ref{T0} to explicit estimates on the growth of the mod $p$ Betti numbers (see Theorem~\\ref{TorsionSL}):\n\n\n\\begin{introth} \\label{T000}\nLet $\\Gamma = \\SL_d (\\Z )$ ($d \\geq 3$) and let $(\\Gamma_n)_{n \\in \\mathbb N}$ be an injective sequence of finite index subgroups of $\\Gamma$. Then, for every field $K$ and for every degree $j \\leq d-2$, we have  \n\\begin{equation}\n\\label{eq: torsion growth SL d}\n\\frac{\\dim_K H_j (\\Gamma_n, K)}{[\\Gamma : \\Gamma_n]} \\longrightarrow 0, \\quad \\mbox{as } n \\to \\infty.\n\\end{equation}\nFor the principal congruence subgroups $\\Gamma (N)$, we have\n\\begin{equation}\n\\frac{\\dim_K H_j (\\Gamma (N), K) }{[\\Gamma:\\Gamma(N)]} =   O \\left( N^{(1 - d)} \\right), \\quad \\mbox{as } N \\to \\infty. \n\\end{equation} \n\\end{introth}\n\n\nNote that  the $j=1$ case of \\eqref{eq: torsion growth SL d} again follows from known results. The first mod $p$ Betti number is a lower bound for the minimal number of generators, hence the rank gradient dominates the growth of the first mod $p$ Betti number.\n It is shown for instance in\n \\cite[Theorem 4.14(3)]{Carderi-Gaboriau-delaSalle} that for $\\SL_d (\\Z )$ ($d \\geq 3$), the rank gradient vanishes for arbitrary injective sequences\n    of finite index subgroups, using the work of Gaboriau on cost \\cite{Gab00}. \n In a somewhat different direction, Fraczyk proved that, for the finite field $\\mathbb F_2$, the estimate $\\frac{\\dim_{ \\mathbb F_2} H_1(\\Gamma,K)}{{\\rm vol}(\\Gamma\\bs G)}=o(1)$ holds uniformly for all lattices in any simple higher rank real Lie group $G$ \\cite{F18}.\nWe wonder whether the methods implemented in our paper can be adapted to deal similarly with arbitrary sequences of lattices in a simple higher rank real Lie group $G$.\nFor the rank $1$ case, see the nice papers \\cite{Sauer-Vol-Hom-growth-2016, Bader-Gelander-Sauer-2020}.\n\nThe results of Theorem~\\ref{T000}  are new for higher homology groups.\nThe closest result we are aware of is due to  Calegari and Emerton and concerns  $p$-adic chains like $p^n$-congruence subgroups \n$\\Gamma(p^n)$ in $\\SL_d(\\Z)$, see \\cite{CalegariEmerton09,CalegariEmerton11,GGD14}\nwhere the existence of a limit is established (not its value)  and the error term is estimated.\nIn the small degrees that our methods address, \nour error term is better, but note that their paper also deals with the difficult case around the middle dimension.\nRecent works \\cite{CE,C}  and conjectures (personal communication) of Calegari and Emerton suggest that in reality the bound on $|H_j (\\Gamma (N), \\Z)_{\\rm tors}|$ (rather than its logarithm) should be polynomial in $N$ in degrees $\\leq d-2$.\nLet us also mention the paper of Kar, Kropholler and Nikolov \\cite{KKN-2021}, in which they prove the vanishing of the $j$-th torsion growth for a wide class of amenable groups.\n\n\\medskip\nAlthough the results above are homological, we build a homotopical machinery to obtain them. This approach traces back to \\cite{Gab00,KarNikolov,BK}. Since we use a general topological approach, our work also applies to a wide class of groups such as torsion-free nilpotent groups, infinite polycyclic groups, Baumslag-Solitar groups $\\mathrm{BS}(1,n)$ and $\\mathrm{BS}(n,n)$ for any non-zero integer $n$, residually finite Artin groups,.. and does not need or assume underlying deep results like the CSP.\n\nIn particular, our method applies to mapping class groups of higher genus surfaces. For $g,b\\in \\mathbb N$ let \n$\\alpha (g,b)=2g-2$ if $g>0$ and $b=0$, $\\alpha(g,b)=2g-3+b$ if $g>0$ and $b>0$, and $\\alpha(g,b)=b-4$ if $g=0$. \n\n\n\\begin{introth}\\label{T0000} Let ${S}$ be an orientable surface of genus $g>0$ with $b$ boundary components and let $\\Gamma=\\mathcal{MCG}({S})$ be its mapping class group. Let $(\\Gamma_n)_{n\\in\\mathbb N}$ be a Farber sequence of finite index subgroups of $\\Gamma$. Then for every field coefficient $K$ and every degree $j\\leq \\alpha (g,b)$,  we have \n\\begin{equation}\n\\frac{\\dim_K H_j (\\Gamma_n, K)}{[\\Gamma : \\Gamma_n]}  \\longrightarrow 0 \\quad \\mbox{and} \\quad \\frac{\\log |H_j (\\Gamma_n , \\Z)_{\\mathrm{tors}} |}{[\\Gamma : \\Gamma_n]} \\longrightarrow 0, \\quad \\mbox{as } n \\to \\infty.\n\\end{equation} \n\\end{introth}See Definition~\\ref{def: Farber sequence} for the definition of a Farber sequence. Examples are decreasing sequences of finite index normal subgroups with trivial intersection. \nObserve that for lattices in higher rank simple Lie groups, it follows from the Stuck--Zimmer Theorem \\cite{StuckZimmer} via \\cite{7samurai} that any injective sequence is indeed a Farber sequence.\nBoth $\\SL_d(\\mathbb Z)$ and $\\mathcal{MCG}({S})$ are then handled using the same method. \n\n\\subsection{Structure, arguments and further results}\n\nThroughout this article, any group action on a CW-complex is required to respect the CW-complex structure.\nOur starting point is the obvious observation that, while the number of cells of a finite index cover  $\\overline{\\Totaldown}$ of a CW-complex $\\Totaldown$ is proportional to the index, it turns out that, for tori, the space $\\overline{\\Totaldown}$ can be \\defin{rebuilt} with a much simpler cell structure; indeed, with the same number of cells as $\\Totaldown$ itself.\n\nAt the origin  of the theory of ``(co)homology of groups'',\nthe standard homological invariants of a group $\\Gamma$ are obtained as the homological invariants of a (and hence of any) compact space $\\Totaldown$ with fundamental group $\\Gamma$ and contractible universal cover $\\widetilde{\\Totaldown}$, whenever such a $\\Totaldown$ exists.\nIn fact, instead of contractibility, the $\\alpha$-connectedness of $\\widetilde{\\Totaldown}$ is enough to compute the homological invariants up to degree $\\alpha$ from the cellular chain complex of $\\Totaldown$.\n\nOur strategy is to exploit the existence for certain groups $\\Gamma$ (like those appearing in Theorems \\ref{T00} and \\ref{T0000}) of such a complex $\\Totaldown$ with nicely embedded tori-like sub-complexes so as to capitalize on the ``obvious observation'' above.\n\n\n\\medskip\nLet $\\Gamma$ be a countable group acting on a CW-complex $\\Baseup$ and let $\\alpha$ be a positive integer. \nSuppose that the CW-complex $\\Baseup$ is $\\alpha$-connected and that for every cell $\\cellup\\subset \\Baseup$ the stabilizer $\\Gamma_\\cellup$ acts trivially on the cell $\\cellup$. We may be led to consider a finite index subgroup first or to consider a barycentric subdivision so as to ensure this last condition.\nA general construction, called the \\defin{Borel construction} then associates to the action $\\Gamma \\curvearrowright \\Baseup$ a $\\alpha$-aspherical CW-complex $\\Totaldown$ whose fundamental group is $\\Gamma$.\nThe construction in fact naturally leads to a \\defin{stack of CW-complexes} $\\Pi: \\Totaldown\\to \\Gamma \\bs \\Baseup$, and, roughly speaking, the fiber over a cell $\\Gamma \\cellup$ of $\\Gamma \\bs \\Baseup$ is a classifying space for the stabilizer $\\Gamma_{\\cellup}$ of $\\cellup$ under the action $\\Gamma\\curvearrowright \\Baseup$. \n\\begin{center}\n\\begin{tikzcd}[column sep=1.2em]\n\\Gamma_{\\cellup}\\curvearrowright \\widetilde{\\Pi^{-1}(\\Gamma \\cellup)}\n\\arrow[d]&\\Gamma\\curvearrowright \\widetilde{\\Totaldown} \\arrow[r]  \\arrow[d] \n&\\Gamma\\curvearrowright\\Baseup \\arrow[d]\n\\\\\nB(\\Gamma_\\cellup,1)=\\Pi^{-1}(\\Gamma \\cellup)  \\arrow[r]& \\Totaldown\n \\arrow[r, \"\\Pi\"]&    \\Gamma\\bs\\Baseup\n\\end{tikzcd}\n\\end{center}\nWe briefly recall  all these notions in Section~\\ref{Sect: Borel construction}  (in particular Proposition~\\ref{prop: existence of a stack}) as it will be useful for us. We refer to the excellent book of Geoghegan \\cite[\\S 6.1]{Geoghegan} for more details. \n\nAt the risk of a spoiler, we indicate right away that we shall use the complex $\\Baseup$ to be the {\\em rational Tits building} $\\Baseup$ for $\\Gamma=\\SL_d(\\Z)$. \nNote that this complex $\\Baseup$ is $(d-3)$-connected (see Section~\\ref{sect: princ. cong. in semi-simple}), which is the sole reason why we can only treat the homology groups in the range $j \\leq d-2$ in Theorems~\\ref{T0} and \\ref{T00}. Similarly in Theorem~\\ref{T0000} for the mapping class group $\\MCG(S)$, for which we shall use the {\\em curve complex} as $\\Baseup$ (see Section~\\ref{Sect: Application to mapping class groups}). \nThe key aspect of our proof is that the stabilizers $\\Gamma_\\cellup$ in these actions $\\Gamma\\curvearrowright \\Baseup$ contain non-trivial free abelian normal subgroups, so that each fiber is itself a torus bundle. \nHowever, in its original form the Borel construction is not yet suitable for us. \nMore precisely, in order to build  ``nicer classifying spaces'' for the finite index subgroups that allow \nexploiting the structure of the stabilizers\n out of the total space $\\Totaldown$ of the stack $\\Pi: \\Totaldown\\to \\Gamma \\backslash \\Baseup$, we make use of Geoghegan's \\defin{Rebuilding Lemma} \\cite[Prop. 6.1.4]{Geoghegan} (see Proposition \\ref{prop: rebuilding} below). This is not yet enough. In order to get a grip on the torsion part of the homology, \nwe make use of a proposition attributed to Gabber (see \\cite[Prop. 3, p. 214]{Soule-99}):\n\\begin{equation}\\log |H_j (\\Totaldown , \\Z)_{\\rm tors} | \\leq \n(\\# \\text{ of $j$-cells})\n \\times \\sup ( \\log ||\\p_{j+1}|| , 0).\n\\label{eq: Gabber ineq intro}\n\\end{equation}\nSee Section~\\ref{sect: proof of Gabber's prop} where, for the convenience of the reader, we give a proof of this inequality following the point of view of \\cite{BV}.\nThis reduces the problem of estimating the torsion to bounding the number of cells and the norms of the boundary maps. Thus, to be able to really make use of Gabber's proposition, we  have to turn the Rebuilding Lemma into an effective statement. This is the content of our Proposition \\ref{P2}. In the end, this \\defin{Effective Rebuilding Lemma} is a machine that provides an explicit rebuilding of the total space $\\Totaldown$ of a stack of complexes $\\Totaldown \\to  \\Gamma \\backslash \\Baseup$ given a rebuilding of its fibers, and that moreover provides bounds \non the number of cells and norms of the chain boundary maps. \n\n\nIn various interesting situations, each fiber of $\\Pi: \\Totaldown\\to \\Gamma \\backslash \\Baseup$ is itself a torus bundle. \nThe standard CW structure on the torus and an inductive procedure using the effective rebuilding lemma yield ``classifying spaces'' for the finite index subgroups which will be sufficiently ``small'' to prove our asymptotic theorems.\n\n One needs to be able to rebuild efficiently not only free abelian groups but also finitely generated torsion-free nilpotent groups. \n This is the content of the general Theorem~\\ref{thm-UnipRewiring} stated below which we believe to be of independent interest. \n  \n\nFirst let us give a precise definition of a rebuilding (of good quality). This is a central notion of this paper.\n\n\\medskip\n For a CW-complex $X$ we denote by $X^{(\\alpha )}$ its $\\alpha$-skeleton and $\\vert X^{(\\alpha )}\\vert$ its number of $\\alpha $-cells. By convention $X^{(-1)}$ is the empty set.\n\\begin{definitionintro}[Rebuilding]\n\\label{def: Rebuilding}\nLet $\\alpha \\in \\mathbb N$ and let $X$ be a CW-complex with finite $\\alpha$-skeleton. A $\\alpha$-\\defin{rebuilding} of $X$ consists of the following data $(X,X',\\mathbf g ,\\mathbf h ,  \\mathbf P )$: \\\n\\begin{enumerate}\n\\item a CW-complex $X'$ with finite $\\alpha$-skeleton;\n\\item two cellular maps \n$\\mathbf g \\colon X^{(\\alpha )}\\to X'{}^{(\\alpha )} \\quad \\mbox{and} \\quad \\mathbf h \\colon X'{}^{(\\alpha )}\\to X^{(\\alpha )}$\nthat are homotopy inverse to each other up to dimension $\\alpha-1$, i.e., $\\mathbf h \\circ \\mathbf g_{\\restriction X^{(\\alpha-1)}}\\sim \\mathrm{id}_{\\restriction X^{(\\alpha-1)}}$ within $X^{(\\alpha)}$ and \n$\\mathbf g \\circ \\mathbf h_{\\restriction X'{}^{(\\alpha-1)}}\\sim \\mathrm{id}_{\\restriction X'{}^{(\\alpha-1)}}$ within $X'{}^{(\\alpha)}$;\n\\item a cellular homotopy $\\mathbf P \\colon [0,1] \\times X^{(\\alpha-1)}\\to X^{(\\alpha)}$  between the identity and $\\mathbf h \\circ \\mathbf g$,\ni.e., $\\mathbf P (0, .)=\\mathrm{id}_{\\restriction X^{(\\alpha-1)}}$ and $\\mathbf P (1,.)=\\mathbf h \\circ \\mathbf g_{\\restriction X^{(\\alpha-1)}}$\n\\end{enumerate}\n\\end{definitionintro}\n\n\\begin{definitionintro}[Quality of a rebuilding]\n\\label{def: Rebuilding and quality}\nGiven real numbers  ${T} , {\\kappa}\\geq 1$, we say that $(X,X',\\mathbf g ,\\mathbf h , \\mathbf P )$ is a $\\alpha$-rebuilding of \\defin{quality $({T}, {\\kappa})$} if \nwe have \n\\begin{align}\n\\forall j \\leq \\alpha, \\quad |X'{}^{(j)}|  & \\leq  {\\kappa}{T}^{-1}|X^{(j)}| \\tag{cells bound}\\\\\n\\forall j \\leq \\alpha , \\quad \\log \\|g_j \\|,\\log \\| h_j \\|,\\log \\| \\rho_{j-1} \\|,\\log \\|\\p'_{j} \\|  & \\leq   {\\kappa}(1+\\log {T} ) \\tag{norms bound}\n\\end{align}\nwhere $\\p'$ is the boundary map on $X'$, we denote by $g$ and $h$ the chain maps respectively associated to $\\mathbf g$ and $\\mathbf h$, and $\\rho \\colon C_\\bullet(X)\\to C_{\\bullet+1}(X)$ is the chain homotopy induced by $\\mathbf P$ in the cellular chain complexes: \n\\begin{equation}\n\\label{eq: cellular chain homotopy}\n\\begin{tikzcd}[column sep=1.2em]\nC_{\\alpha}(X) \\arrow[r, \"\\p_{\\alpha}\"] \\arrow[d, \"g_{\\alpha}\" left]\n& \\cdots \\arrow[l, bend left, \"\\rho_{\\alpha-1}\" ] & \n\\arrow[r] \\cdots &  \nC_{1}(X) \\arrow[r] \\arrow[r, \"\\p_1\"] \\arrow[d, \"g_1\" left] \\arrow[l, bend left, \"\\rho_1\" ] &\nC_{0}(X) \\arrow[d, \"g_0\" left]\\arrow[l, bend left, \"\\rho_0\" ]  \\\\\nC_{\\alpha}(X') \\arrow[r, \"\\p'_{\\alpha}\"] \\arrow[u, xshift=0.35em, \"h_{\\alpha} \" right]\n&\n\\cdots \n& \n\\arrow[r] \\cdots &  \nC_{1}(X') \\arrow[r, \"\\p'_1 \"] \\arrow[u, xshift=0.35em, \"h_1\" right]&\nC_{0}(X'), \\arrow[u, xshift=0.35em, \"h_0\" right]\n\\end{tikzcd}\n\\end{equation}\nand the norms $\\| \\cdot \\|$ are derived from the canonical $\\ell^2$-norms on the cellular chain complexes.\n\\end{definitionintro}\n\nWe will simplify the notation and write $(X,X')$ instead of $(X,X',\\mathbf g ,\\mathbf h ,  \\mathbf P )$ when the explicit cellular maps are not relevant.\n\nThe definition above captures an intrinsic tension between ``having few cells'' and ``maintaining tame norms''. \n\nGiven a finite cover $X_1\\to X$ (of large degree) our main task is to construct a rebuilding $(X_1,X'_1)$ of sufficiently good quality $(T_1 , \\kappa_1)$. In some cases it is possible to take $\\kappa_1 = \\kappa (X)$ independently of the cover and $T_1$ linear in the degree, see  Sections \\ref{S:unip} and \\ref{S:unipext}. In particular for finitely generated torsion-free nilpotent groups (called \\defin{unipotent lattices} in the text) the precise efficient rebuilding we obtain can be stated as follows. \n\\begin{introth}\\label{thm-UnipRewiring}\nLet $\\Lambda$ be a finitely generated torsion-free nilpotent group. If $Y_0$ is a compact $K(\\Lambda,1)$ space, \n there exists a constant ${\\kappa}_{Y_0}\\geq 1$ such that for every finite index subgroup $\\Lambda_1\\leq \\Lambda$, the cover $Y_1=\\Lambda_1\\bs \\tilde Y_0$ admits a $\\alpha$-rebuilding $(Y_1,Y_1',\\mathbf g , \\mathbf h , \\mathbf P )$ \nof quality $([\\Lambda : \\Lambda_1], {\\kappa}_{Y_0})$ for every $\\alpha$.\n\\end{introth}\nRecall that a $K(\\Lambda,1)$ space is a CW complex with fundamental group isomorphic with $\\Lambda$ and whose universal cover is contractible.\n\n\n\n\\begin{remarkintro} \nSince the number of $j$-cells of $Y_1$ is $\\vert Y_1^{(j)}\\vert = [\\Lambda:\\Lambda_1] \\vert Y_0^{(j)}\\vert$, the number of $j$-cells of the $K(\\Lambda_1,1)$ space $Y_1'$ satisfies the bound $\\vert (Y_1')^{(j)}\\vert\\leq {\\kappa}_{Y_0} [\\Lambda:\\Lambda_1]^{-1}\\vert Y_1^{(j)}\\vert ={\\kappa}_{Y_0}\\vert Y_0^{(j)}\\vert$, independent of $\\Lambda_1$.\n\\end{remarkintro}\n\n\\begin{remarkintro} The proof of Theorem \\ref{thm-UnipRewiring} (in Section~\\ref{S:unip}) yields for various $\\Lambda_1$ a rebuilding $Y_1 '$ with only $O(2^{\\mathfrak h})$ cells, where $\\mathfrak h$ is the Hirsch length of $\\Lambda$. This is in fact ``the'' minimal number of cells that a $K(\\Lambda , 1)$ CW-complex can have, since $\\sum_j \\dim_\\Q H_j (\\Lambda,\\mathbb Q)=2^\\mathfrak h.$ \n\\end{remarkintro}\n\nTheorem \\ref{thm-UnipRewiring} is used for the proof of Theorem~\\ref{T0}; the latter is in fact deduced from a general result: Theorem~\\ref{Tmain} proved in Section~\\ref{sect: proof of main th}. One key feature of the \\defin{principal} congruence subgroup $\\Gamma(N)$ of Theorem~\\ref{T0} is that it intersects every infinite unipotent subgroup of $\\SL_d (\\Z)$ along a subgroup of index at least $N$. Note that general sequences of finite index subgroups of $\\SL_d (\\Z)$ do not have this property, e.g., for each positive integer $N$ the \\defin{non-principal} congruence subgroup\n$$\\Gamma_0 (N) = \\left\\{ \\left( \\begin{array}{cc} a & b \\\\ c^\\top & D \\end{array} \\right) \\in \\SL_d (\\Z ) \\; : \\; a \\in \\Z, \\ b,c \\in \\Z^{d-1}, \\ D \\in \\GL_{d-1} (\\Z) \\mbox{, and } N | c \\right\\} \\subset \\SL_d (\\Z )$$\ncontains the whole group of upper-triangular unipotent matrices with integer coefficients. To prove Theorem \\ref{T00} we need to show that if $(\\Gamma_n )$ is an injective sequence of finite index subgroups of $\\SL_d (\\Z)$, then $\\Gamma_n$ intersects most of the infinite unipotent subgroups of $\\SL_d (\\Z)$ along subgroups of large index. \n\nIn general to deal with arbitrary Farber sequences of a given residually finite group $\\Gamma$ we introduce a property that we believe to be of independent interest. \nWe say that $\\Gamma$ has the \\defin{cheap $\\alpha$-rebuilding property} (Definition \\ref{def-CheapReb}) if it admits a \n$K(\\Gamma,1)$ space $X$ with finite $\\alpha$-skeleton and a constant ${\\kappa}_X\\geq 1$ that\nsatisfy the following property. For every Farber sequence $(\\Gamma_n)_{n\\in\\mathbb N}$ for $\\Gamma$ and $T\\geq 1$, there exists $n_0$ such that for $n\\geq n_0$ the cover $X_n:=\\Gamma_n\\bs \\tilde X$ admits a $\\alpha$-rebuilding $(X_n,Y_n)$ of quality $(T,{\\kappa}_X)$. We note that cheap $0$-rebuilding simply means that $\\Gamma$ is infinite.\n\nWe establish a robust bootstrapping criterion for a group to have the cheap $\\alpha$-rebuilding property:\n\\begin{introth}[Theorem~\\ref{thm-AdjRebStack}]\\label{thm-AdjRebStack-intro}\nLet $\\Gamma$ be a residually finite group acting on a CW-complex $\\Baseup$ in such a way that any element stabilizing a cell fixes it pointwise. Let $\\alpha \\in \\mathbb{N}$ and assume that the following conditions hold:\n\\begin{enumerate}\n\\item $\\Gamma\\bs \\Baseup$ has finite $\\alpha$-skeleton; \n\\item $\\Baseup$ is $(\\alpha-1)$-connected;\n\\item The stabilizer of each cell of dimension $j \\leq \\alpha$  has the cheap $(\\alpha -j)$-rebuilding property.\n\\end{enumerate}\nThen, $\\Gamma$ itself has the cheap $\\alpha$-rebuilding property. \n\\end{introth} \n\n\n\nAs a corollary we get that the cheap $\\alpha$-rebuilding property is an invariant of commensurability. Also, we get that it holds for  \nall infinite polycyclic groups for all $\\alpha$. It also passes from a normal subgroup $N\\triangleleft \\Gamma$ to $\\Gamma$ if $N\\bs \\Gamma$ is of type $F_{\\alpha}$. See Corollary~\\ref{cor-CheapRebGroups}. \nMore significantly, we show \n\\begin{introth}\\label{TCheapReb}\nLet $\\alpha \\geq 0$. The following groups have the cheap $\\alpha$-rebuilding property:\n\\begin{enumerate}\n\\item \\label{item Arithmetic lattices-intro} Arithmetic lattices of $\\mathbb Q$-rank at least $\\alpha+1$ (Theorem~\\ref{thm-LatticesRebuilding}).\n\n\\item \\label{item Artin groups with flag complex}\nFinitely generated residually finite Artin groups satisfying the $K(\\pi,1)$ conjecture and whose nerve is $(\\alpha-1)$-connected (Theorem~\\ref{Ex:Artin groups}).\n\n\n\\item \\label{item mapping class group-intro} Mapping class groups $\\mathcal{MGC}(S)$ where $S$ is a surface of genus $g>0$ with $b>0$ boundary components and $2g-3+b\\geq \\alpha$ or $g>0$, $b=0$ and $2g-2\\geq \\alpha$ (Theorem~\\ref{th: MCG has CRP k(g,b)}).\n\n\\end{enumerate}\n\\end{introth}\n\nThe ``cells bound'' condition of Definition~\\ref{def: Rebuilding and quality} in the cheap $\\alpha$-rebuilding property is very much related to the notion of slowness introduced in \\cite{BK}. This condition already implies the vanishing of the growth of homology over arbitrary fields for a wide class of groups. In particular a group with the cheap $2$-rebuilding property is finitely presented and economical, in the sense of \\cite{KarNikolov}, with respect to any Farber chain of finite index subgroups. For controlling the torsion, however, we need the full force of the cheap $\\alpha$-rebuilding property:\n\n\\begin{introth}[Theorem~\\ref{prop-CheapRebTorsion}] \\label{Tintrotors}\nLet $\\Gamma$ be a countable group of type $F_{\\alpha+1}$ that has the cheap $\\alpha$-rebuilding property for some non-negative integer $\\alpha$. Then, for every Farber sequence $(\\Gamma_n)_{n \\in \\mathbb N}$, coefficient field $K$ and $0\\leq j\\leq \\alpha$ we have \n\\[\\lim_{n\\to \\infty} \\frac{\\dim_K H_j (\\Gamma_n , K)}{[\\Gamma : \\Gamma_n]} = 0 \\quad \\mbox{and} \\quad \\lim_{n\\to \\infty} \\frac{\\log | H_j(\\Gamma_n,\\mathbb Z)_{\\rm tors}|}{[\\Gamma:\\Gamma_n]}=0.\\]\n\\end{introth}\nIn analogy with the L\\\"uck approximation theorem \\cite{Luck-1994-approx} which identifies the first limit with the usual $\\ell^2$-Betti numbers $b_j^{(2)} (\\Gamma ; K)$ when $K=\\Q$, we will loosely write $$b_j^{(2)} (\\Gamma ; K) = \\limsup_{n\\to \\infty} \\frac{\\dim_K H_j (\\Gamma_n , K)}{[\\Gamma : \\Gamma_n]}$$ \nonce the sequence $(\\Gamma_n )$ is fixed. \nNote that non-abelian finitely generated free groups do not have the cheap $1$-rebuilding property (since $b_1^{(2)} (\\FF_r; \\Q) =r-1\\not= 0$).\n\nAccording to Theorem \\ref{TCheapReb}~\\eqref{item Arithmetic lattices-intro} $\\SL_d (\\Z)$ has the cheap $(d-2)$-rebuilding property. Theorem \\ref{Tintrotors} therefore implies Theorem \\ref{T00}. Similarly Theorem \\ref{TCheapReb}~\\eqref{item mapping class group-intro} and Theorem \\ref{Tintrotors} imply Theorem \\ref{T0000}.\n \nTheorem \\ref{Tintrotors} is related to \\cite[Theorems 7 and 9]{AGN} where Abert, Gelander and Nikolov investigate the first homology torsion growth for\n\\defin{right-angled groups}, i.e. groups that admit a finite generating list $\\{\\gamma_1 , \\ldots , \\gamma_m \\}$ of elements of infinite order such that $[\\gamma_i , \\gamma_{i+1}] = 1$ for $i=1 , \\ldots , m-1$.\nThis is a class of groups featured in \\cite{Gab00} where $\\SL_d (\\Z )$ ($d \\geq 3$) being a right-angled group is exploited to compute its cost. These groups have the cheap $1$-rebuilding property, see Proposition \\ref{P:rag}. We therefore recover that finitely presented right-angled groups have vanishing first homology torsion growth along any Farber sequence.  Note that already in degree $j=1$, Theorem~\\ref{Tintrotors} also establishes the vanishing of the homology torsion growth for a natural class of groups where \\cite{AGN} doesn't apply (see Example~\\ref{ex: non rag semidirect F x Z2}). Moreover, thanks to Theorem \\ref{TCheapReb}~\\eqref{item Artin groups with flag complex} it applies to quite general Artin groups  in higher degrees and yields:\n\n\\begin{introth} \\label{TintroArtin}\nLet $\\Gamma$ be a finitely generated residually finite Artin group satisfying the $K(\\pi,1)$ conjecture and whose nerve is a $(\\alpha-1)$-connected . Then, for every Farber sequence $(\\Gamma_n)_{n \\in \\mathbb N}$, coefficient field $K$ and $0\\leq j\\leq \\alpha$ we have \n\\[b_j^{(2)} (\\Gamma ; K ) =0 \\quad \\mbox{and} \\quad \\lim_{n\\to \\infty} \\frac{\\log | H_j(\\Gamma_n,\\mathbb Z)_{\\rm tors}|}{[\\Gamma:\\Gamma_n]}=0.\\]\n\\end{introth}\n\n\nWhen $\\Gamma$ is a right-angled Artin group (\\defin{RAAG}) the first part of the theorem is not new: \nIn fact Avramidi, Okun and Schreve \\cite{Avramidi} have even computed all the $\\ell^2$-Betti numbers $b_j^{(2)} (\\Gamma ; K)$. Their result  is that $b_j^{(2)} (\\Gamma ; K) = \\overline{b}_{j-1} (L ; K)$, the reduced Betti number of the nerve $L$ of $\\Gamma$. Thus $b_j^{(2)} (\\Gamma ; K )$ is indeed equal to $0$ when $L$ is $(j-1)$-connected. This shows that Theorem \\ref{TCheapReb}~\\eqref{item Artin groups with flag complex} is optimal for RAAGs. \nBuilding on their computation of the $\\ell^2$-Betti numbers $b_j^{(2)} (\\Gamma ; \\mathbb{F}_2)$,  Avramidi, Okun and Schreve also prove that a RAAG $\\Gamma$ whose nerve is a flag triangulation of $\\mathbb{R}P^2$ has exponential homology torsion growth in degree $j=2$, see \\cite[Corollary 3]{Avramidi}. Since $\\Gamma$ has the cheap $1$-rebuilding property (it is in fact right-angled) this shows that Theorem \\ref{Tintrotors} is in a certain sense optimal: for $\\alpha=1$, there are groups that have the cheap $\\alpha$-rebuilding property and have exponential torsion growth in degree $j={\\alpha+1}$. \n\nTo conclude this introduction, we finally raise the following:\n\\begin{question} \\label{QIntro}\nLet $\\Gamma$ be the fundamental group of a closed, or finite volume, hyperbolic $3$-manifold. Does $\\Gamma$ have the cheap $1$-rebuilding property?\n\\end{question}\n\nIt is tempting to believe that the answer is no. \nThese groups have zero first $\\ell^2$-Betti number, so the Betti number criterion does not apply. \nBut it is conjectured that for arithmetic hyperbolic $3$-manifolds groups the torsion in degree $1$ grows exponentially along Farber sequences of congruence subgroups, see \\cite{BSV,BrockDunfield,EMS}.\n\n\n\n\\subsection*{Acknowledgement}We would like to thank Ian Biringer, Ross Geoghegan, Gr{\\'e}gory Ginot, Athanase Papadopoulos, and Shmuel Weinberger\nfor helpful conversations.\nWe especially thank Boris Okun and Kevin Schreve for reading a preliminary version of this paper and making valuable comments.\nWe also thank Emma Bergeron for drawing our pictures.\nM.~A. and M.~F.\nare partially supported by ERC Consolidator Grant 648017. \nD.~G. was partially supported by the ANR project GAMME (ANR-14-CE25-0004) and by the LABEX MILYON (ANR-10-LABX-0070) of Universit{\\'e} de Lyon, within the program ``Investissements d'Avenir\" (ANR-11-IDEX-0007) operated by the French National Research Agency (ANR).\n M.~F. would like to thank IAS Princeton for an excellent working environment during a crucial stage of this project. \n\n\n\n\\section{The Borel construction and Geoghegan rebuilding} \\label{Sect: Borel construction}\n\n\nLet $\\Gamma$ be a countable group acting on a CW-complex $\\Baseup$. \n\nTechnically speaking,  the \\defin{Borel construction} is a trick that converts an\naction of a group $\\Gamma$ on a space $\\Baseup$ into a \\textbf{free action} of $\\Gamma$ on a homotopy equivalent\nspace $\\Baseup'$. See for instance \\cite[3.G.2]{Hatcher-book-Algeb-topo}.\nNamely, take $\\Baseup' = \\Baseup \\times E\\Gamma$ with the diagonal action of $\\Gamma$, $\\gamma(y, z) = (\\gamma y, \\gamma z)$ where $\\Gamma$ acts on $E\\Gamma$ (the universal cover of some classifying space $B\\Gamma$) as deck transformations. The diagonal action is free, in fact a\ncovering space action, since this is true for the action in the second coordinate. \nThe orbit space of this diagonal action is usually denoted $\\Baseup\\times_{\\Gamma} E\\Gamma$. \nWe now briefly explain the ``stack\" interpretation of Geoghegan \\cite[\\S 6.1]{Geoghegan} alluded to in the introduction.\n\n\\medskip\n\nA cellular map $\\Pi : \\Totaldown \\to \\Basedown$ between two CW-complexes is a \\defin{stack of CW-complexes} with \\defin{base space} $\\Basedown$, \\defin{total space} $\\Totaldown$ and CW-complexes called \\defin{fibers} $F_e$ over $e$, if for each $n \\geq 1$ (denoting by $E_n$ the set of $n$-cells of $\\Basedown$) there is a cellular map \n\\begin{equation}f_n : \\bigsqcup_{e \\in E_n} F_e \\times \\mathbb{S}^{n-1} \\to \\Pi^{-1} (\\Basedown^{(n-1)})\\end{equation}\nand a homeomorphism\n\\begin{equation} \\label{k}\nk_n : \\Pi^{-1} (\\Basedown^{(n-1)}) \\cup_{f_n} \\left( \\bigsqcup_{e \\in E_n} F_e \\times \\mathbb{B}^{n} \\right) \\to \\Pi^{-1} (\\Basedown^{(n)})\\end{equation}\nsatisfying:\n\\begin{enumerate}\n\\item $k_n$ agrees with the inclusion on $\\Pi^{-1} (\\Basedown^{(n-1)})$,\n\\item $k_n$ maps each cell onto a cell, and\n\\item the following diagram commutes up to homotopy relative to $\\Pi^{-1} (\\Basedown^{(n-1)})$:\n\\begin{equation} \\label{E:diag23}\n\\xymatrix{\\Pi^{-1} (\\Basedown^{(n-1)}) \\sqcup \\left( \\bigsqcup_{e \\in E_n} F_e \\times \\mathbb{B}^{n} \\right) \\ar[d]^{\\mathrm{quotient}} \\ar[rd]^{(\\Pi_{|} , \\chi_e \\circ \\mathrm{pr}_2)} & \n\\\\\n\\Pi^{-1} (\\Basedown^{(n-1)}) \\cup_{f_n} \\left( \\bigsqcup_{e \\in E_n} F_e \\times \\mathbb{B}^{n} \\right) \\ar[d]^{k_n} & \\Basedown^{(n)} \n\\\\\n\\Pi^{-1} (\\Basedown^{(n)}) \\ar[ru]^{\\Pi_{|}} & }\\end{equation}\n\\end{enumerate}\nwhere $(\\Pi_{|} , \\chi_e \\circ \\mathrm{pr}_2):\\Pi^{-1} (\\Basedown^{(n-1)}) \\sqcup \\left( \\bigsqcup_{e \\in E_n} F_e \\times \\mathbb{B}^{n} \\right) \\to \\Basedown^{(n)}$ is the restriction of $\\Pi$ on $\\Pi^{-1} (\\Basedown^{(n-1)})$, while \non each $F_e \\times \\mathbb{B}^{n}$, it is the composition of the projection onto $\\mathbb{B}^{n}$ followed by the characteristic map $\\chi_e$ of the cell $e\\in E_n$ in the\nCW-complex $\\Basedown$. Thus there is a homotopy \n$$\\mathfrak{H} : \\left( \\Pi^{-1} (\\Basedown^{(n-1)}) \\sqcup \\left( \\bigsqcup_{e \\in E_n} F_e \\times \\mathbb{B}^{n} \\right) \\right) \\times [0,1] \\to \\Basedown^{(n)}$$\nsuch that \n$$\\mathfrak{H}_{|  \\Pi^{-1} (\\Basedown^{(n-1)}) \\times [0,1]} = \\Pi_{|}, \\quad \\mathfrak{H} (\\cdot , 0) = (\\Pi_{|} , \\chi_e \\circ \\mathrm{pr}_2), \\quad \\mbox{and} \\quad \n\\mathfrak{H} (\\cdot , 1) = \\Pi_{|} \\circ k_n \\circ \\mathrm{quotient}.$$ \n\nNote that the strong commutation of diagram \\eqref{E:diag23} required in the definition of stacks in  \\cite[Section 6.1, p. 147 ]{Geoghegan} (2008 version of the book) is in fact too strong and that \\cite[Proposition 6.1.4]{Geoghegan} needs the commutation to occur up to homotopy. Both the flaw and the way to fix it have been pointed out to us by Boris Okun and Kevin Schreve. \nWe are extremely grateful to them for that.\n\n\nFollowing \\cite[\\S 6.1]{Geoghegan},\nthe Borel construction for $\\Gamma\\curvearrowright \\Baseup$  eventually takes the form of a stack $\\Pi : \\Totaldown \\to \\Gamma \\backslash \\Baseup$ below:\n\n\\begin{proposition} \\label{prop: existence of a stack}\nLet $\\Gamma$ be a countable group acting on a simply connected CW-complex $\\Baseup$ so that for every cell $\\cellup\\subset \\Baseup$ the stabilizer $\\Gamma_\\cellup$ acts trivially on $\\cellup$. Write $\\Basedown=\\Gamma\\backslash \\Baseup$. Then there exists a stack of CW-complexes $\\Pi : \\Totaldown \\to \\Basedown$ with fiber $F_e$ over $e$ such that\n\\begin{enumerate}\n\\item The fundamental group $\\pi_1 (\\Totaldown)$ is isomorphic to $\\Gamma$.\n\\item For each cell $e$ of $\\Basedown$ the fiber $F_e$ is contractible and $\\pi_1( F_e)\\cong \\Gamma_\\cellup$, where $\\cellup$ is any cell in $\\Baseup$ above $e$.\n\\end{enumerate}\nMoreover, if $\\Baseup$ is $n$-connected then the universal cover $\\widetilde{\\Totaldown}$ of the total space $\\Totaldown$ is $n$-connected. \n\\end{proposition} \nRecall that a CW-complex space $Y$ is said to be \\defin{$n$-connected} if it is connected and all its homotopy groups $\\pi_j Y$ are trivial for $0\\leq j\\leq n$. By convention, a $(-1)$-connected space will just be an arbitrary topological space.  \n\nNote that if $\\cellup$ and $\\cellup'$ are two cells of $\\Baseup$ above a cell $e$ of $\\Basedown$, they are  in the same $\\Gamma$-orbit and thus the stabilizers $\\Gamma_\\cellup$ and $\\Gamma_{\\cellup '}$ are conjugate in $\\Gamma$. In particular these stabilizers are isomorphic, so there is no harm in writing $\\Gamma_e:=\\Gamma_\\cellup$.\n\nIn practice it is often desirable to replace the fibers $F_e$ by different CW-complexes of the same homotopy types. \nGeoghegan's stack decomposition and his \\defin{rebuilding Lemma} \\cite[Proposition 6.1.4]{Geoghegan} makes this possible:\n\n\\begin{proposition}[Rebuilding Lemma, Geoghegan {\\cite[Proposition 6.1.4]{Geoghegan}}] \\label{prop: rebuilding}\nLet $\\Pi:\\Totaldown\\to \\Basedown$ be a stack of CW-complexes.\nIf for each cell $e$ of $\\Basedown$ we are given a CW-complex $F_e '$ of the same homotopy type as $F_e$, then there is a stack of CW-complexes \n$\\Pi' : \\Totaldown' \\to \\Basedown$ with fiber $F_e '$ over $e$, and a homotopy equivalence $\\mathbf g$ making the following diagram commute up to homotopy over each closed cell:\n\\begin{equation*}\n\\xymatrix{ \\Totaldown \\ar[rd]_{\\Pi}  \\ar[rr]^\\mathbf g &   & \\Totaldown ' \\ar[ld]^{\\Pi'} \n\\\\\n& \\Basedown  & }\n\\end{equation*}\nMore precisely, $\\Pi'\\circ \\mathbf g$ is homotopy equivalent to $\\Pi$ via a homotopy equivalence such that for every closed cell $e$ of $\\Basedown$, the restriction of the homotopy to $\\Pi^{-1}(e)\\times [0,1]$ has its image in $e$.\n\\end{proposition} \n\n\\medskip\n\nProposition \\ref{prop: rebuilding} allows one, up to homotopy equivalence, to replace each fiber $F_e$ in Proposition~\\ref{prop: existence of a stack} by a prescribed classifying space for $\\Gamma_\\cellup$. \n\nRecall that a group $\\Gamma$ is of type $F_{n}$ if it admits a classifying space whose $n$-skeleton is finite; equivalently if it admits a\n$(n-1)$-aspherical CW-complex $X$ with finite $n$-skeleton and $\\pi_1(X) \\simeq \\Gamma$ (since turning $X$ into an aspherical complex can be made by adding only cells of dimension $\\geq n+1$). \n\nLet $\\alpha \\in \\mathbb{N}$. Suppose that each group $\\Gamma_e$ is of type $F_{\\alpha}$. Thanks to the rebuilding lemma we may then assume that:\n\\begin{center}\nEach fiber $F_e$ of the stack $\\Pi : \\Totaldown \\to \\Basedown$ has a finite $\\alpha$-skeleton.\n\\end{center}\nIf we suppose furthermore that $\\Baseup$ is $(\\alpha-1)$-connected then the universal cover of the total space $\\widetilde{\\Totaldown}$ is also $(\\alpha-1)$-connected. The cellular chain complex $C_q (\\widetilde{\\Totaldown})$ therefore gives a partial resolution \n$$C_{\\alpha} (\\widetilde{\\Totaldown}) \\to \\cdots \\to C_0 (\\widetilde{\\Totaldown}) \\to \\Z \\to 0$$\nof $\\Z$ by free $\\Z[\\Gamma]$-modules of finite rank. \n\n\\begin{remark} \nIf $\\overline{\\Gamma}\\leq \\Gamma$ is a subgroup of finite index, then in the commuting diagram\n\\begin{equation} \n\\xymatrix{\\overline{\\Totaldown}=\\overline{\\Gamma}\\backslash\\widetilde{\\Totaldown} \\ar[r]^{\\overline{\\Pi}} \\ar[d]^{\\zeta} \\ar@{}[rd]|{\\circlearrowleft}& \\overline\\Basedown=\\overline{\\Gamma}\\backslash\\Baseup \\ar[d]\n\\\\\n\\Totaldown=\\Gamma\\backslash\\widetilde{\\Totaldown}\\ar[r]^{\\Pi} & \\Basedown=\\Gamma\\backslash\\Baseup\n}\n\\end{equation}\nthe map $\\overline{\\Pi}$ \nis naturally a stack satisfying (1) and (2) of Proposition \\ref{prop: existence of a stack}, the map $\\zeta$ is a finite cover. For any $n \\in \\mathbb{N}$ we thus have\n$$\\vert \\overline{\\Totaldown}^{(n)} \\vert = [\\Gamma:\\overline{\\Gamma}]  \\vert \\Totaldown^{(n)} \\vert|.$$ \n\\end{remark}\n\n\nProposition \\ref{prop: rebuilding} gives a way to improve the covering space $\\overline{\\Totaldown}$ by simplifying the fibers of the stack $\\overline{\\Pi}$. Our next goal is to make this procedure explicit enough to control the boundary maps in (cellular) homology. We first recall some basic facts about the latter.\n\n\n\\section{Cellular homology}\nThe material in this section covers the basic properties of cellular homology in the context of stacks of CW-complexes. We review it to set up the notation for the following sections.\nLet $\\Pi : \\Totaldown \\to \\Basedown$ be a stack of CW-complexes with fiber over each cell $e$ of $\\Basedown$ denoted by $F_e$. \n\nRecall that by definition, the $\\Z$-module of the degree $a$ cellular chains is  \n$$C_a (\\Totaldown ) = H_a ( \\Totaldown^{(a)} , \\Totaldown^{(a-1)}).$$\nNow each homeomorphism \\eqref{k} induces a map of pairs\n$$(F_e^{(k)} \\times \\mathbb{B}^n  , F_e^{(k)} \\times \\mathbb{S}^{n-1} \\cup F_e^{(k-1)} \\times \\mathbb{B}^n) \\to (\\Totaldown^{(n+k)} , \\Totaldown^{(n+k-1)} )$$ \nwhich induces an injective map\n\\begin{equation} \\label{inj1}\nH_{n+k} (F_e^{(k)} \\times \\mathbb{B}^n  , F_e^{(k)} \\times \\mathbb{S}^{n-1} \\cup F_e^{(k-1)} \\times \\mathbb{B}^n) \\to C_{n+k} (\\Totaldown).\n\\end{equation}\nConsidering the long exact sequence for the pair \n$$(F_e^{(k)} \\times \\mathbb{B}^n  , F_e^{(k)} \\times \\mathbb{S}^{n-1} \\cup F_e^{(k-1)} \\times \\mathbb{B}^n),$$\nK\\\"unneth theorem yields a natural isomorphism of $\\Z$-modules: \n\\begin{equation} \\label{isom2}\nH_k (F_e^{(k)} , F_e^{(k-1)}) \\otimes H_n (\\mathbb{B}^n , \\mathbb{S}^{n-1} ) \\stackrel{\\simeq}{\\longrightarrow} H_{n+k} (F_e^{(k)} \\times \\mathbb{B}^n  , F_e^{(k)} \\times \\mathbb{S}^{n-1} \\cup F_e^{(k-1)} \\times \\mathbb{B}^n).\n\\end{equation}\nBy definition, we have \n$$H_k (F_e^{(k)} , F_e^{(k-1)}) = C_k (F_e).$$\nThe $\\Z$-module $H_n (\\mathbb{B}^n , \\mathbb{S}^{n-1} )$ is free of rank one generated by the relative fundamental class $[\\mathbb{B}^n , \\mathbb{S}^{n-1}]$. From now on we will identify   \n\\begin{equation} \\label{subspace}\nC_k (F_e) \\otimes H_n (\\mathbb{B}^n , \\mathbb{S}^{n-1} )\n\\end{equation}\nwith the subspace of $C_{n+k} (\\Totaldown )$ spanned by the image of the composition of the maps \\eqref{isom2} and \\eqref{inj1}. \n\nThe ascending filtration of $\\Basedown$ by its $n$-skeleta $\\Basedown^{(n)}$ induced via $\\Pi : \\Totaldown \\to \\Basedown$ then yields an ascending filtration \n\\begin{equation}F^n C_\\bullet (\\Totaldown) = \\bigoplus_{e \\in \\Basedown^{(n)}} C_{\\bullet-\\dim(e)} (F_e )  \\otimes H_{\\dim e} (\\mathbb{B}^{\\dim e} , \\mathbb{S}^{\\dim e -1} )\n\\label{eq: Fn C(A) complex}\\end{equation}\non the cellular chain complex $C_\\bullet (\\Totaldown)$. Recall that the boundary operator $\\partial : C_a (\\Totaldown) \\to C_{a -1} (\\Totaldown)$ \nis defined using the homology long exact sequences of the pairs $(\\Totaldown^{(a)} , \\Totaldown^{(a-1)} )$ and $(\\Totaldown^{(a-1)} , \\Totaldown^{(a-2)} )$: \n\\begin{equation*}\n\\xymatrix{\nC_a (\\Totaldown) \\ar[d]^{\\cong} \\ar[rr]^{\\partial} & & C_{a-1} (\\Totaldown) \\ar[d]^{\\cong} \\\\\nH_a (\\Totaldown^{(a)} , \\Totaldown^{(a-1)} ) \\ar[r]^{\\delta} & H_{a-1} (\\Totaldown^{(a-1)} ) \\ar[r] & H_{a-1} (\\Totaldown^{(a-1)} , \\Totaldown^{(a-2)} ).\n}\n\\end{equation*}\nIn restriction to the image of \\eqref{inj1} the composition of maps in the diagram above is the composition of the map \n$$H_{n+k} (F_e^{(k)} \\times \\mathbb{B}^n  , F_e^{(k)} \\times \\mathbb{S}^{n-1} \\cup F_e^{(k-1)} \\times \\mathbb{B}^n) \\stackrel{\\delta = \\delta_1 \\oplus \\delta_2}{\\longrightarrow} H_{n+k-1} (F_e^{(k)} \\times \\mathbb{S}^{n-1} ) \\oplus H_{n+k-1} (F_e^{(k-1)} \\times \\mathbb{B}^n) )$$\nby the direct sum of the two natural maps\n$$H_{n+k-1} (F_e^{(k)} \\times \\mathbb{S}^{n-1} ) \\longrightarrow H_{n+k-1} (F_e^{(k)} \\times \\mathbb{S}^{n-1} , F_e^{(k-1)} \\times \\mathbb{S}^{n-1} )$$\n$$H_{n+k-1} (F_e^{(k-1)} \\times \\mathbb{B}^n ) \\longrightarrow H_{n+k-1} (F_e^{(k-1)} \\times \\mathbb{B}^n , F_e^{(k-1)} \\times \\mathbb{S}^{n-1}).$$\n\n\\begin{definition}[In the fiber, $\\partial^{\\rm vert}$]\nLet\n$$\\partial^{\\rm vert} : C_k (F_e) \\otimes H_n (\\mathbb{B}^n , \\mathbb{S}^{n-1} ) \\to C_{k-1} (F_e) \\otimes H_n (\\mathbb{B}^n , \\mathbb{S}^{n-1} )$$\nbe the map that makes the following diagram commute: \n\\begin{footnotesize}\n\\begin{equation*}\n\\xymatrix{\nC_k (F_e) \\otimes H_n (\\mathbb{B}^n , \\mathbb{S}^{n-1} ) \\ar[d]^{\\cong} \\ar[rr]^{\\partial^{\\rm vert}} & & C_{k-1} (F_e) \\otimes H_n (\\mathbb{B}^n , \\mathbb{S}^{n-1} )  \\\\\nH_{n+k} (F_e^{(k)} \\times \\mathbb{B}^n  , F_e^{(k)} \\times \\mathbb{S}^{n-1} \\cup F_e^{(k-1)} \\times \\mathbb{B}^n) \\ar[r]^{\\quad \\quad \\quad \\quad \\delta_2} & H_{n+k-1} (F_e^{(k-1)} \\times \\mathbb{B}^n)  \\ar[r] &  H_{n+k-1} (F_e^{(k-1)} \\times \\mathbb{B}^n , F_e^{(k-1)} \\times \\mathbb{S}^{n-1}) \\ar[u]^{\\cong}.\n}\n\\end{equation*}\n\\end{footnotesize}\n\\end{definition}\n\n\nGiven $e \\in E_n$, we denote by \n$$f_e : F_e \\times \\mathbb{S}^{n-1} \\to \\Totaldown$$\nthe map obtained by restricting $f_n$. \nBy construction it induces a map of pairs \n$$(F_e^{(k)}  \\times \\mathbb{S}^{n-1} , F_e^{(k-1)}  \\times \\mathbb{S}^{n-1} ) \\longrightarrow (\\Totaldown^{(n+k-1)} , \\Totaldown^{(n+k-2)} ).$$\n\n\\begin{definition}[In the base, $\\partial^{\\rm hor}$] \\label{def:dhor}\nLet\n$$\\partial^{\\rm hor} : C_k (F_e) \\otimes H_n (\\mathbb{B}^n , \\mathbb{S}^{n-1} ) \\to  C_{n+k-1} (\\Totaldown )$$\nbe the map that makes the following diagram commute: \n\\begin{footnotesize}\n\\begin{equation*}\n\\xymatrix{\nC_k (F_e) \\otimes H_n (\\mathbb{B}^n , \\mathbb{S}^{n-1} ) \\ar[d]^{\\cong} \\ar[rr]^{\\partial^{\\rm hor}} & & C_{n+k-1} (\\Totaldown )   \\\\\nH_{n+k} (F_e^{(k)} \\times \\mathbb{B}^n  , F_e^{(k)} \\times \\mathbb{S}^{n-1} \\cup F_e^{(k-1)} \\times \\mathbb{B}^n) \\ar[r]^{\\quad \\quad \\quad \\quad \\delta_1} & H_{n+k-1} (F_e^{(k)} \\times \\mathbb{S}^{n-1}) ) \\ar[r] &  H_{n+k-1} (F_e^{(k)} \\times \\mathbb{S}^{n-1} , F_e^{(k-1)} \\times \\mathbb{S}^{n-1}) \\ar[u]^{(f_e)_*}.\n}\n\\end{equation*}\n\\end{footnotesize}\n\\end{definition}\n\n\nBy naturality we get the following:\n\n\\begin{lemma} \\label{L23}\nThe boundary map $\\partial : C_\\bullet (\\Totaldown) \\to C_{\\bullet -1} (\\Totaldown)$\ndecomposes as \n\\begin{equation}\n\\partial = \\partial^{\\rm vert} + \\partial^{\\rm hor} \\label{eq: def partial vert}\n\\end{equation}\nwhere $\\partial^{\\rm vert}$ preserves each summand $C_\\bullet (F_e) \\otimes H_{\\dim e} (\\mathbb{B}^{\\dim e} , \\mathbb{S}^{\\dim e -1} )$ and acts on it by the boundary operator of the cellular chain complex $C_\\bullet (F_e)$, and where \n$$\\partial^{\\rm hor} : F^n C_\\bullet (\\Totaldown) \\to F^{n-1} C_{\\bullet-1} (\\Totaldown)$$\nmaps $c \\otimes [\\mathbb{B}^n , \\mathbb{S}^{n-1}] \\in C_\\bullet (F_e) \\otimes H_{n} (\\mathbb{B}^{n} , \\mathbb{S}^{n -1} )$ (with $e \\in E_n$) to \n$$(-1)^{\\dim c} (f_e)_* (c \\otimes [\\mathbb{S}^{n-1}]).$$\n\\end{lemma}\n\\begin{proof} The sign here comes from the isomorphism in the first vertical arrow of the commutative diagram of Definition~\\ref{def:dhor}.\n\\footnote{Analogously, the cellular chain complex of a product $X \\times Y$ of two CW-complexes is the tensor product $C_\\bullet (X) \\otimes C_\\bullet (Y)$ equipped with the boundary operator $\\partial \\otimes \\mathrm{id} + (-1)^j \\mathrm{id} \\otimes \\partial$ on $C_i (X) \\otimes C_j (Y)$.}\n\\end{proof}\n\n\\section{Effective rebuilding}\nLet $\\Basedown$ be a finite CW-complex and let $\\Totaldown\\to \\Basedown$ be a stack of CW-complexes with fibers $F_e$.\nNow, if for each cell $e$ of $\\Basedown$ we are given a CW-complex $F_e '$ of the same homotopy type as $F_e$, Proposition \\ref{prop: rebuilding} implies that there exists a stack of CW-complexes $\\Pi' : \\Totaldown' \\to \\Basedown$ with fiber $F_e '$ over $e$, and a homotopy equivalence $\\mathbf g$ making the following diagram commute up to homotopy over each cell:\n\\begin{equation}\\xymatrix{\\Totaldown \\ar[rd]^{\\Pi} \\ar[rr]^\\mathbf g & &  \\Totaldown' \\ar[ld]^{\\Pi '} \\\\\n& \\Basedown & }\\end{equation}\nFor each cell $e$ of $\\Basedown$ let $\\mathbf k_e : F_e  \\to  F_e '$ be a cellular map that induces a homotopy equivalence. In particular there exists a cellular map $\\mathbf l_e : F_e ' \\to F_e$ and a homotopy \n\\begin{equation}\n\\mathbf \\Sigma_e : [0,1] \\times F_e  \\to F_e\n\\end{equation} \nbetween $\\mathbf \\Sigma_e (0,  \\cdot ) = \\mathbf l_e \\circ \\mathbf k_e$ and $\\mathbf \\Sigma_e (1 , \\cdot )=\\mathrm{id}_{F_e}$.  \n\nThe maps $\\mathbf k_e$ and $\\mathbf l_e$ induce chain maps\n$$k_e : C_\\bullet (F_e ) \\to C_\\bullet (F_e ' ) \\quad \\mbox{and} \\quad l_e : C_\\bullet (F_e ' ) \\to C_\\bullet (F_e ).$$\nThe homotopy $\\mathbf \\Sigma_e$ induces a map $\\sigma_e \\colon C_\\bullet (F_e ) \\to C_{\\bullet+1} (F_e ),$ given by \n$$\\sigma_e (c)=(\\mathbf \\Sigma_e)_{*}([I, \\p I ]\\otimes c), \\textrm{ for any cell } c\\subset F_e .$$\nThe map $\\sigma_e$ is then a chain homotopy between $1$ and $l_e\\circ k_e$, i.e.  \n$$l_e\\circ k_e-1= \\p \\sigma_e + \\sigma_e \\p.$$ \nWe denote by \n$$k : C_\\bullet (\\Totaldown ) \\to C_\\bullet (\\Totaldown '), \\quad  l : C_\\bullet (\\Totaldown ' ) \\to C_\\bullet (\\Totaldown )$$\nand \n$$\\sigma : C_\\bullet (\\Totaldown ) \\to C_{\\bullet+1} (\\Totaldown )$$\nthe (``vertical'') maps induced by $k_e$, $l_e$ and $\\sigma_e$ on each subspace \\eqref{subspace}.\n\nThe three goals of this section are the following.\n\\begin{enumerate}\n\\item To give explicit formulas for the chain maps $g : C_\\bullet (\\Totaldown) \\to C_\\bullet (\\Totaldown')$ and $h : C_\\bullet (\\Totaldown') \\to C_\\bullet (\\Totaldown)$ respectively associated to $\\mathbf g$ and a homotopy inverse $\\mathbf h$, in terms of $k$, $l$ and $\\sigma$.\n\\item To describe an explicit chain homotopy $\\rho :  C_\\bullet (\\Totaldown) \\to C_{\\bullet+1} (\\Totaldown)$ between $1$ and $h\\circ g$, in terms of $k$, $l$ and $\\sigma$.\n\\item To give an explicit formula for the boundary operator $\\partial'$ on the cellular chain complex $C_\\bullet (\\Totaldown ')$ in terms of $k$, $l$, $\\sigma$, the boundary operator $\\partial$ on the cellular chain complex $C_\\bullet (\\Totaldown )$, and the vertical boundary operator $(\\partial ')^{\\rm vert}$. \n\\end{enumerate}\nThe precise result is Proposition \\ref{P2} below; it is a (homological) effective version of Proposition \\ref{prop: rebuilding}. Note that we do not give effective formulas for the homotopy between $\\mathbf g \\circ \\mathbf h$ and the identity as we will not use it. For the same reason we do not even name the homotopy between $g \\circ h$ and the identity. \n\n\\medskip\n\nThe proof consists in explicating the construction of the homotopy equivalence between $\\Totaldown$ and $\\Totaldown'$: both the stack of CW-complexes $\\Pi' : \\Totaldown' \\to \\Basedown$ and the map $\\mathbf g :\\Totaldown \\to \\Totaldown'$ are constructed by induction on the dimension of the cells. Suppose that \n$$\\mathbf g_{n-1} : \\Pi^{-1} (\\Basedown^{(n-1)}) \\to (\\Pi')^{-1} (\\Basedown^{(n-1)})$$\nhas been constructed. Then over $n$-cells the stack $\\Totaldown'$ is built from the composition of the maps\n\\begin{equation}f_n ' : \\bigsqcup_{e \\in E_n } F_e ' \\times \\mathbb{S}^{n-1} \\stackrel{\\mathbf l_n}{\\longrightarrow} \\bigsqcup_{e \\in E_n } F_e  \\times \\mathbb{S}^{n-1}  \\stackrel{f_n}{\\longrightarrow} \\Pi^{-1} (\\Basedown^{(n-1)}) \\stackrel{\\mathbf g^{(n-1)}}{\\longrightarrow}  (\\Pi')^{-1} (\\Basedown_{n-1}) \\end{equation}\nwhere \n$$\\mathbf l_n = \\bigsqcup_{e \\in E_n} \\mathbf l_e \\times \\mathrm{id}_{\\mathbb{S}^{n-1}}.$$\n\nFor each $n$ we denote \n$$X_n = \\bigsqcup_{e \\in E_n } F_e  \\times \\mathbb{B}^{n}, \\quad A_n = \\bigsqcup_{e \\in E_n } F_e  \\times \\mathbb{S}^{n-1} \\quad \\mbox{and} \\quad Y_n = \\Pi^{-1} (\\Basedown^{(n)})$$\nand \n$$X_n ' = \\bigsqcup_{e \\in E_n } F_e '  \\times \\mathbb{B}^{n}, \\quad A_n ' = \\bigsqcup_{e \\in E_n } F_e '  \\times \\mathbb{S}^{n-1} \\quad \\mbox{and} \\quad Y_n' = (\\Pi')^{-1} (\\Basedown^{(n)}).$$\nBy construction we have:\n$$Y_n = Y_{n-1} \\cup_{f_n} X_n \\quad \\mbox{and} \\quad Y_n ' = Y_{n-1} ' \\cup_{f_n '} X_n '.$$\n\nNow the map $\\mathbf g$, its homotopy inverse $\\mathbf h$ and an explicit homotopy\n$$\\mathbf P : [0,1] \\times \\Totaldown \\to \\Totaldown, \\quad \\mathbf P (0 , \\cdot ) = \\mathrm{Id}_{\\Totaldown}, \\ \\mathbf P (1 , \\cdot ) = \\mathbf h \\circ \\mathbf g,$$ \nare obtained as the direct limits of maps \n$$\\mathbf g_{n} : Y_n \\to Y_n', \\quad \\mathbf h_n : Y_n \\to Y_n ' \\quad \\mbox{and} \\quad \\mathbf P_n : [0,1] \\times Y_n \\to Y_n$$\ninductively constructed by considering the diagram\n\\begin{equation} \\label{diagn}\n\\xymatrix{\nX_n \\ar@<-.5ex>[d]^{\\ \\mathbf l_n} &A_n \\ar[l]^{\\iota} \\ar@<-.5ex>[d]^{\\ \\mathbf l_n} \\ar[r]^{f_n} & Y_{n-1} \\ar@<-.5ex>[d]^{\\ \\mathbf h_{n-1}} \\\\\nX'_n\\ar@<-.5ex>[u]^{\\mathbf k_n \\ } &A'_n \\ar[l]^{\\iota} \\ar@<-.5ex>[u]^{\\mathbf k_n \\ } \\ar[r]^{f_n '} & Y_{n-1} \\ar@<-.5ex>[u]^{\\mathbf g_{n-1} \\ }.\n}\n\\end{equation}\nHere $\\iota$ denotes the inclusion map, we have $f_n ' = \\mathbf g_{n-1}  \\circ f_n \\circ \\mathbf l_n$ and the maps $\\mathbf l_n \\circ \\mathbf k_n$ and $\\mathbf h_{n-1} \\circ \\mathbf g_{n-1}$ are homotopic to the identity. From these data the proposition proved in the next paragraph gives explicit formulas for the chain maps induced by $\\mathbf g_n$ and $\\mathbf h_n$. This provides a (homological) effective version of \\cite[Theorem 4.1.8]{Geoghegan}. \n\n\\subsection{Explicit gluing}\n\nLet $X,X',A,A',Y,Y'$ be CW-complexes fitting into the following diagram of cellular maps\n\\begin{equation*}\n\\xymatrix{\nX \\ar@<-.5ex>[d]^{\\ \\mathbf l} &A \\ar[l]^{\\iota} \\ar@<-.5ex>[d]^{\\ \\mathbf l} \\ar[r]^{f} & Y \\ar@<-.5ex>[d]^{\\ \\mathbf h} \\\\\nX'\\ar@<-.5ex>[u]^{\\mathbf k \\ } &A' \\ar[l]^{\\iota} \\ar@<-.5ex>[u]^{\\mathbf k \\ } \\ar[r]^{f '} & Y' \\ar@<-.5ex>[u]^{\\mathbf g \\ },\n}\n\\end{equation*}\nwhere the $\\iota$'s denote inclusions, $f'=\\mathbf g \\circ f \\circ \\mathbf l$ and the maps $\\mathbf l \\circ \\mathbf k$ and $\\mathbf h \\circ \\mathbf g$ are homotopic to the identity. Assume furthermore that any cell $e$ of $X$ that intersects $A$ is contained in $A$, similarly for $X'$ and $A'$, and  that $\\mathbf k^{-1}(A')=A$ and $\\mathbf l^{-1}(A)=A'$. \n\nLet $I=[0,1]$. Fix explicit homotopies \n$$\\mathbf \\Sigma \\colon I\\times X\\to X \\quad \\mbox{ and } \\quad \\mathbf P \\colon I \\times Y\\to Y,$$\n$$\\mathbf \\Sigma  (0,x)=x,\\quad \\mathbf \\Sigma  (1,x)=\\mathbf l \\circ \\mathbf k (x),\\quad \\mathbf P (0, y)=y,\\quad \\mathbf P (1,y)=\\mathbf h \\circ \\mathbf g (y),$$ \nsuch that $\\mathbf \\Sigma (\\cdot , A) \\subset A$.  \n\nThe cellular chain complex $C_\\bullet (X\\sqcup_f Y)$ can be naturally identified with \n$$C_\\bullet(X)\\oplus C_\\bullet(Y)/ (\\iota ,-f)C_\\bullet(A).$$ \nIn the following we use the same letter for the maps and the induced maps on a cellular chain complex except that the chain maps are not bold faced. \n\nFor any cell $e\\subset X^{(n)}$ let $[e]:=e_*([\\mathbb B^n, \\mathbb S^{n-1}]).$ The homotopy $\\mathbf \\Sigma$ induces a map $\\sigma \\colon C_\\bullet(X)\\to C_{\\bullet+1}(X),$ given by \n$$\\sigma ([e])=(\\mathbf \\Sigma )_{*}([I, \\p I ]\\otimes [e]), \\textrm{ for any cell } e\\subset X.$$\nThe map $\\sigma$ is then a chain homotopy between $1$ and $l \\circ k$, i.e.  \n$$l \\circ k -1 = \\p \\sigma + \\sigma \\p.$$ \nThe same discussion applies to $Y$; we denote by $\\rho$ the corresponding chain homotopy.\n\nDecompose the cellular chain complex $C_\\bullet (X)$ as\n$$C_\\bullet(X)=\\sum_{e\\not\\subset A}\\Z[e]\\oplus \\iota_*C_\\bullet(A)$$ \nand let $1_A\\colon C_\\bullet (X)\\to \\iota_*C_\\bullet(A)$ be the projection onto the second component. Define analogously $1_{A'}\\colon C_\\bullet (X')\\to \\iota_*'C_\\bullet (A')$. It follows from our assumptions $k^{-1}(A')\\subset A$ and $l^{-1}(A)\\subset A'$ \nthat \n$$k \\circ 1_A=1_{A'} \\circ k  \\quad \\mbox{and} \\quad l \\circ 1_{A'}=1_A \\circ l.$$ \nIn order to lighten the \nnotations we will suppress $\\iota_*$ and identify $C_\\bullet(A)$ with a sub-complex of $C_\\bullet(X)$. \n\n\\begin{proposition}[Explicit gluing Lemma] \n \\label{lem-QGluing} \n 1. There exist cellular maps \n$$\\tilde \\mathbf g \\colon X \\cup_f Y\\to X' \\cup_{f'}Y' \\quad \\mbox{and} \\quad \\tilde \\mathbf h \\colon X'\\cup_{f'} Y'\\to X\\cup_f Y$$ \nsuch that $\\tilde \\mathbf g |_Y=\\mathbf g,$ $\\tilde \\mathbf h |_Y=\\mathbf h$ and $\\tilde \\mathbf h \\circ \\tilde \\mathbf g$ is homotopy equivalent to the identity. \n\n2. The chain maps $\\tilde g : C_\\bullet (X \\cup_f Y) \\to C_\\bullet (X' \\cup_{f'}Y')$ and $\\tilde h : C_\\bullet (X' \\cup_{f'}Y') \\to C_\\bullet (X \\cup_f Y)$ respectively associated to $\\tilde \\mathbf g$ and $\\tilde \\mathbf h$ are induced by maps defined on $C_\\bullet (X)\\oplus C_\\bullet(Y)$ and $C_\\bullet (X')\\oplus C_\\bullet(Y')$ by the following formulas \n$$\\tilde g = (k \\circ (1-1_A)+ g \\circ f \\circ 1_A - g \\circ f \\circ \\sigma \\circ (1_A\\p -\\p 1_A)) \\oplus g $$\nand\n$$\\tilde h =(l  \\circ (1-1_{A'})+ h \\circ f' \\circ 1_{A'} + \\rho \\circ f \\circ l \\circ (1_{A'}\\p-\\p 1_{A'})) \\oplus h,$$ \nwhere $g$ and $h$ are the chain maps respectively induced by $\\mathbf g$ and $\\mathbf h$.\n\n3. There exists a homotopy map \n$$\\widetilde \\mathbf P \\colon I\\times ( X\\cup_f Y) \\to X\\cup_{f} Y$$ \nsuch that $\\widetilde \\mathbf P (0,z)=z$ and $\\widetilde \\mathbf P (1,z)=\\tilde \\mathbf h \\circ \\tilde \\mathbf g (z).$ \n\n4. The chain homotopy map $\\tilde \\rho (c) =\\widetilde \\mathbf P_*([I , \\p I ]\\otimes c)$ associated to $\\widetilde \\mathbf P$ is induced by a map defined on $C_\\bullet (X)\\oplus C_\\bullet(Y)$ by the following formula\n$$\\tilde \\rho = ( \\sigma \\circ (1-1_A)+ \\rho \\circ f \\circ 1_A- \\rho \\circ f \\circ \\sigma \\circ (1_A\\p-\\p 1_A) ) \\oplus \\rho.$$\n\\end{proposition}\n\nThe proof of Proposition~\\ref{lem-QGluing} consists of explicating the construction of the maps $\\tilde\\mathbf h $ and $ \\tilde\\mathbf g$. \nWe postpone it until Section \\ref{sec-QGluing}.\n The construction is quite technical so the reader may want to skip Section \\ref{sec-QGluing} on a first reading.  \nIt is instructive to check that our formulas work at the level of homology groups.\n \n \n \n\\subsection{\\texorpdfstring{Rebuilding of $\\Totaldown$}{Rebuilding of Sigma}}\n\nStarting from \n$$Y_0 = \\sqcup_{e \\in E_0} F_e, \\quad Y_0' = \\sqcup_{e \\in E_0} F_e', \\quad \\mathbf g_0 = \\mathbf k_0, \\quad \\mathbf h_0 = \\mathbf l_0, \\quad \\mbox{and} \\quad \\mathbf P_0 = \\mathbf \\Sigma_0,$$\nwe inductively apply Proposition \\ref{lem-QGluing} to \\eqref{diagn} and construct the desired extensions $\\mathbf g_n$, $\\mathbf h_n$ and $\\mathbf P_n$ of $\\mathbf g_{n-1}$, $\\mathbf h_{n-1}$ and $\\mathbf P_{n-1}$.\n\nWe identify \n$$C_\\bullet (X_n ) = \\bigoplus_{e \\in E_n} C_\\bullet (F_e ) \\otimes C_\\bullet (\\mathbb B^n)$$ \nand let \n$$[\\mathbb{B}^n , \\mathbb{S}^n] \\in H_n (\\mathbb{B}^n , \\mathbb{S}^n ) = C_n (\\mathbb{B}^n)$$ \nbe the relative fundamental class that generates the $\\Z$-module of rank one $C_n (\\mathbb{B}^n)$. Note that given $c \\in C_\\bullet (F_e)$ we have:\n$$1_{A_n} (c \\otimes [\\mathbb{B}^n , \\mathbb{S}^n] )=0 \\quad \\mbox{and} \\quad (1_{A_n}\\circ \\p -\\p \\circ 1_{A_n})(c \\otimes [\\mathbb{B}^n , \\mathbb{S}^n] ) = 1_{A_n}\\p (c \\otimes [\\mathbb{B}^n , \\mathbb{S}^n] ) = (-1)^{\\dim c} c\\otimes [\\mathbb{S}^{n-1}]$$ \nwhere the last equality follows from Lemma \\ref{L23}. There are analogous formulas for $c' \\in C_\\bullet (F_e')$ replacing $A_n$ by $A'_n$. \n\nDenoting by $g_n$ and $h_n$ the chain maps respectively associated to $\\mathbf g_n$ and $\\mathbf h_n$ and by $\\sigma_n$ and $\\rho_n$ the chain homotopies respectively associated to $\\mathbf \\Sigma_n$ and $\\mathbf P_{n}$,  Proposition \\ref{lem-QGluing} gives \n$$g_n (c \\otimes [\\mathbb{B}^n , \\mathbb{S}^n])= k_n (c \\otimes [\\mathbb{B}^n , \\mathbb{S}^n]) - (-1)^{\\dim c} g_{n-1} \\circ f \\circ  \\sigma_n (c\\otimes [\\mathbb{S}^{n-1}]),$$\n$$h_n (c' \\otimes [\\mathbb{B}^n , \\mathbb{S}^n]) = l_n (c' \\otimes [\\mathbb{B}^n , \\mathbb{S}^n])+(-1)^{\\dim c} \\rho_{n-1} \\circ f \\circ l_{n-1} (c' \\otimes [\\mathbb{S}^{n-1}])$$\nand\n$$\\rho_n (c \\otimes [\\mathbb{B}^n , \\mathbb{S}^n]) = \\sigma_n (c \\otimes [\\mathbb{B}^n , \\mathbb{S}^n]) - (-1)^{\\dim c} \\rho_{n-1} \\circ f \\circ \\sigma_n (c\\otimes [\\mathbb{S}^{n-1}]).$$\n\nWe can simplify these formulas by identifying \n$$C_\\bullet (Y_n)=(C_\\bullet(X_n)\\oplus C_\\bullet(Y_{n-1}))/ (1,-f_*)C_\\bullet(A_n),$$ \nwith \n$$\\bigoplus_{e \\in E_n} C_\\bullet(F_e) \\otimes H_n (\\mathbb{B}^n , \\mathbb{S}^{n-1}) \\subset  F^n C_\\bullet(\\Totaldown ).$$ \nThe map $\\mathbf g : \\Totaldown \\to \\Totaldown'$ induces chain maps $g_n : F^n C_\\bullet (\\Totaldown) \\to F^n C_\\bullet (\\Totaldown' )$, and by Lemma \\ref{L23} the chain maps $g_n$ are inductively defined as follows:\n\\begin{itemize}\n\\item The restriction of $g_n$ to $F^{n-1} C_\\bullet (\\Totaldown)$ is equal to $g_{n-1}$.\n\\item If $e$ is a $n$-cell of $\\Basedown$, the restriction of $g_n$ to $C_\\bullet (F_e) \\otimes H_n (\\mathbb{B}^n , \\mathbb{S}^n)$ is given by \n\\begin{equation} \\label{kn}\ng_n (c \\otimes [\\mathbb{B}^n , \\mathbb{S}^n] ) = k_e (c) \\otimes [\\mathbb{B}^n , \\mathbb{S}^{n-1}] +  g_{n-1} \\circ \\p^{\\rm hor} (\\sigma_e (c) \\otimes [\\mathbb{B}^n , \\mathbb{S}^{n-1}] )  \\quad \\left( c \\in C_\\bullet (F_e ) \\right).\n\\end{equation}\n\\end{itemize}\nThe sign changed because $\\dim \\sigma_e(c)=\\dim c +1$, so $(-1)^{\\dim c}f\\circ\\sigma_n(c\\otimes [\\mathbb S^{n-1}])=-\\partial^{\\rm hor}(\\sigma_e(c)\\otimes [\\mathbb B^n,\\mathbb S^{n-1}]),$ by Lemma \\ref{L23}.\nBy induction, we get:\n\\begin{equation} \\label{E:k}\ng = k \\circ \\left( \\sum_{i=0}^{n} \\left( \\partial^{\\rm hor} \\circ \\sigma \\right)^i \\right).\\end{equation}\nNote that the map $\\sigma$ preserves the filtration\n$F^{n} C_\\bullet (\\Totaldown)$ and that the map $\\partial^{\\rm hor}$ maps $F^n C_\\bullet (\\Totaldown)$ to $F^{n-1} C_{\\bullet-1} (\\Totaldown)$.\nIt follows that $ \\left( \\partial^{\\rm hor} \\circ \\sigma \\right)^i$ vanishes on $C_n(\\Totaldown)$ for $i\\geq n+1$.\n\nWe similarly get formulas for $h$ and $\\rho$. To sum up we get:\n\n\n\\begin{proposition}[Effective rebuilding Lemma]\\label{P2}\nWe have\n\\begin{enumerate}\n\n\\item The rebuilding lemma yields a CW-stack $\\Pi'\\colon \\Totaldown' \\to \\Basedown$, cellular maps $\\mathbf g \\colon \\Totaldown \\to \\Totaldown '$, $\\mathbf h \\colon \\Totaldown ' \\to \\Totaldown$ and a homotopy $\\mathbf P \\colon I\\times \\Totaldown \\to \\Totaldown$ such that \n$$\\mathbf P (0,\\cdot )={\\rm Id}_{\\Totaldown} \\quad \\mbox{and} \\quad \\mathbf P (1,\\cdot )=\\mathbf h \\circ \\mathbf g.$$ \n\n\\item The chain maps  $g : C_\\bullet (\\Totaldown) \\to C_\\bullet (\\Totaldown')$, $h : C_\\bullet (\\Totaldown') \\to C_\\bullet (\\Totaldown)$ and $\\rho :  C_\\bullet (\\Totaldown) \\to C_\\bullet (\\Totaldown)$ respectively associated to $\\mathbf g$, $\\mathbf h$ and $\\mathbf P$ are respectively given by \n\\begin{equation}\\label{E: effective k}\ng = k \\circ \\left( \\sum_{i=0}^{\\infty} \\left( \\partial^{\\rm hor} \\circ \\sigma \\right)^i \\right) \\quad \\mbox{ on } C_\\bullet (\\Totaldown ),\n\\end{equation}\n\\begin{equation} \nh = \\left( \\sum_{i=0}^{\\infty} \\left(  \\sigma \\circ \\partial^{\\rm hor}   \\right)^i \\right) \\circ l \\quad \\mbox{ on } C_\\bullet (\\Totaldown' ), \\end{equation}\nand\n\\begin{equation}\n\\rho = \\sigma \\circ \\left( \\sum_{i=0}^{\\infty} \\left(\\partial^{\\rm hor} \\circ \\sigma \\right)^i \\right) \\quad \\mbox{ on } C_\\bullet (\\Totaldown ). \\end{equation} \n\n\\item The formula for the boundary operator $\\partial'$ on the cellular chain complex $C_\\bullet (\\Totaldown ')$ of the rebuilt CW-complex $\\Totaldown'$ is given on $C_\\bullet (\\Totaldown ')$ by \n\\begin{equation}\\label{E:P2} \n\\partial ' = (\\partial ' )^{\\rm vert} + k \\circ \\left( \\sum_{i=0}^{\\infty} \\left( \\partial^{\\rm hor} \\circ \\sigma \\right)^i \\right) \\circ  \\partial^{\\rm hor} \\circ l. \\end{equation}\n\\end{enumerate}\n\\end{proposition}\n\\begin{proof} It only remains to prove (3). \nLemma \\ref{L23} applies to the stack $\\Pi ' : \\Totaldown' \\to \\Basedown$ so that if $e$ is an $n$-cell of $\\Basedown$ and $c \\in C_\\bullet (F_e)$ we have:\n\\begin{equation} ((\\partial ')^{\\rm hor}) (c \\otimes e) = (-1)^{\\dim c} f_e ' (c \\otimes [\\mathbb{S}^{n-1}]).\\end{equation}\nOn the other hand it follows from the construction of $\\Pi' : \\Totaldown' \\to \\Basedown$ that \n\\begin{equation}\nf_e ' (c \\otimes [\\mathbb{S}^{n-1}]) = g_{n-1} (f_e (l_e (c) \\otimes [\\mathbb{S}^{n-1}])).\\end{equation} \nWe conclude that \n\\begin{equation*}\n\\begin{split}\n((\\partial ')^{\\rm hor}) (c \\otimes e) & = (-1)^{\\dim c} f_e ' (c \\otimes [\\mathbb{S}^{n-1}]) \\\\\n& = (-1)^{\\dim c} g_{n-1} (f_e (l _e (c) \\otimes [\\mathbb{S}^{n-1}])) \\\\\n& = g_{n-1} \\circ \\partial^{\\rm hor} \\circ l (c \\otimes e),\n\\end{split}\n\\end{equation*}\nand \\eqref{E:P2} follows from \\eqref{E: effective k}.\n\\end{proof}\n\n\n\n\\section{Proof of Proposition \\ref{lem-QGluing}}\\label{sec-QGluing}\n\nLet $X$ be a CW-complex and let $A\\subset X$ be a sub-complex. Then $( \\{0 \\} \\times X ) \\cup (I \\times A )$ is a strong deformation retract of $I \\times X$; see e.g. \\cite[1.3.15]{Geoghegan}. We refine this property in the following lemma.\n\n\\begin{lemma}\\label{lem-pconst}\nThere exists a cellular map \n$$p\\colon I\\times X\\to \\{0\\}\\times X\\cup I\\times A$$ \nsuch that for all $x \\in X$, $a \\in A$ and $s \\in ]0,1]$,\n\\begin{enumerate}\n\\item $p(0,x)=(0,x) \\in \\{0\\}\\times X$;\n\\item $p (s,x) = (p^1 (s,x) , p^2 (s,x)) \\in [0 ,s] \\times X$;\n \\item $p(s,a)=(s,a) \\in I\\times A$\n\\end{enumerate}\nand the following formulas hold for any chain $c\\in C_\\bullet(X)$: \n\\begin{align*}\np_*([I, \\p I]\\otimes c)=&[I , \\p I] \\otimes 1_Ac, \\\\\n\\delta_*([I, \\p I]\\otimes c)=&[I , \\p I] \\otimes (1-1_A)c,\n\\end{align*}\nwhere \n$$\\delta\\colon I\\times  X\\to I\\times X, \\quad \\delta(s,x)=(s-p^1(s,x),p^2(s,x)).$$\n\\end{lemma}\n\\begin{proof}\nWe define the map $p$ cell by cell, starting from $0$-dimensional cells and attaching any available cell of the lowest dimension as pictured on Figure \\ref{fig:p}.\n\n\n\\begin{figure}[ht]      \n\\begin{center}\n\\includegraphics[width=.3\\textwidth]{Fig1.png}\n\\end{center}\n\\caption{The map $p$}\\label{fig:p}\n\\end{figure}\n\n\nThe proof reduces to the following statement. Let $Z$ be a CW-sub-complex of $X$ of dimension at most $n$, and let $e\\colon \\mathbb B^n\\to X$ be an $n$ cell with $e(\\mathbb S^{n-1})\\subset Z^{(n-1)}$.  Suppose we are given a cellular map \n$$p \\colon I\\times Z\\to \\{0\\}\\times Z\\cup I\\times (A\\cap Z)$$ \nsatisfying all the desired properties. Put $Z'=\\mathbb B^n\\sqcup_{e} Z$. We shall construct an extension \n$$p \\colon I \\times Z'\\to \\{0\\}\\times Z'\\cup I \\times (A\\cap Z')$$ \nof $p$, satisfying all the required properties. To do so it is enough to define $p$ on $I\\times \\mathbb B^n$ so that \n$$\\forall s \\in I , \\ \\forall x\\in \\mathbb S^{n-1},  \\quad p(s,x)=p(s,e(x)).$$\nNow if $e\\subset A$ we put \n\\begin{equation} \\label{E:ponA}\np(s,e(x))=(s,e(x)) \\in I \\times A.\n\\end{equation}\nOtherwise $p(s,e(x))$ is already defined for all $x\\in \\mathbb S^{n-1}$ so we put $p(s,x)=p(s,e(x))$ for $x\\in \\mathbb S^{n-1}$, $p(0,x)=(0,x)$ for $x\\in\\mathbb B^n$ and extend the map to $I\\times \\mathbb B^n$ using the homotopy extension property for the pair $\\{0\\}\\times \\mathbb B^n\\cup I\\times \\mathbb S^{n-1}$ (cf. \\cite[1.3.15]{Geoghegan}). By replacing $p^1(s,x)$ with $\\max\\{s, p^1(s,x)\\}$ we can ensure that the second condition is satisfied. \n\n\\begin{figure}[ht]      \n\\begin{center}\n\\includegraphics[width=.25\\textwidth]{Fig2.png}\n\\end{center}\n\\caption{The map $p(1 , -)$}\\label{fig:p1}\n\\end{figure}\n\n\n \nBy construction, the maps $p$ and $\\delta$ are cellular and both $p(I\\times e)$ and $\\delta(I\\times e)$ are contained in $I\\times e$.  Write $[e]=e_*([\\mathbb{B}^n , \\mathbb{S}^n ])$ and $[\\p e] = e_* ([\\mathbb{S}^{n-1}])$.  We check the formulas for $p_*$ and $\\delta_*$ on $[I,\\p I]\\otimes [e]$.\nWhen $e\\subset A$, the formula for $p_*$ follows from the definition (see \\eqref{E:ponA}). If $e\\not \\subset A$ then the $(n+1)$-cell $I\\times e$ is mapped by $p$ into \n$$\\{0 \\} \\times (Z')^{(n)} \\cup  I \\times Z^{(n-1)}$$ \nwhich is contained in the $n$-skeleton of $I \\times Z'$. It follows that $p_*([I , \\p I ]\\otimes[e])=0$. This proves the formula for $p_*$. \n\nUsing that $p_* \\circ \\p = \\p \\circ p_*$, the formula for $p_*$ now implies that \n\\begin{equation} \\label{E:p*formula}\np_*([1]\\otimes c)=[0]\\otimes (1-1_A)c+[I , \\p I ]\\otimes (1_A\\p-\\p 1_A)c+[1]\\otimes 1_Ac\n\\end{equation} \nand consequently that \n\\begin{equation} \\label{E:delta*formula}\n\\delta_*([1]\\otimes c)=[1]\\otimes (1-1_A)c-[I , \\p I ]\\otimes (1_A\\p-\\p1_A)c+[0]\\otimes 1_Ac.\n\\end{equation}\n\n\\begin{figure}[ht]      \n\\begin{center}\n\\includegraphics[width=.4\\textwidth]{Fig3.png}\n\\end{center}\n\\caption{The map $\\delta$}\\label{fig:delta}\n\\end{figure}\n\n\nWe finally derive the general formula for  $\\delta_*([I,\\p I]\\otimes [e])$ from \\eqref{E:delta*formula}: we know that \n$$\\delta_*([I,\\p I]\\otimes [e])\\in C_n(I\\times Z')=C_0(I)\\otimes C_{n+1}(Z')\\oplus C_1(I)\\otimes C_n(Z')=C_1(I)\\otimes C_n(Z'),$$ \nsince $Z'$ is $n$-dimensional. As $\\delta(I\\times e)\\subset I\\times e$ we must have \n$$\\delta_*([I,\\p I]\\otimes [e])=\\alpha [I,\\p I]\\otimes [e]$$ \nfor some $\\alpha\\in \\mathbb Z$. To find $\\alpha$ we compute $\\p \\delta_*([I,\\p I]\\otimes [e])$ using \\eqref{E:delta*formula} and, by induction, the formula for $\\delta_*$ on $[I , \\p I ]\\otimes [\\p e]$~:\n\\begin{equation*}\n\\begin{split}\n\\p \\delta_*([I,\\p I]\\otimes [e]) & = \\delta_* ( \\p ([I,\\p I]\\otimes [e]) ) \\\\\n& = \\delta_*([1]\\otimes [e])-\\delta_*([0]\\otimes [e]) - \\delta_*([I , \\p I ]\\otimes [\\p e] ) \\\\ \n& =\\p ([I , \\p I ]\\otimes (1-1_A)[e]).\n\\end{split}\n\\end{equation*} \nHence $\\alpha=0$ if $e\\subset A$ and $\\alpha=1$ otherwise. This proves the formula for $\\delta_*([I,\\p I]\\otimes [e]).$\n\n \\end{proof}\n\n\n\\medskip\n\n\nWe now come back to the notations of Proposition \\ref{lem-QGluing}.\n\nLet $$p\\colon I\\times (X\\cup_{\\iota} A )  \\to (\\{0\\}\\times X ) \\cup (I\\times A)$$ be the map afforded by Lemma \\ref{lem-pconst} applied to the pair $(X,A)$.\nWe write \n$$p_s(x) =p(s,x), \\quad ( s\\in I, \\ x\\in X).$$\nWe have \n$$C_\\bullet(\\{0\\}\\times X\\cup I \\times A)=[0]\\otimes C_\\bullet(X) + [I , \\p I] \\otimes C_\\bullet(A)+[1]\\otimes C_\\bullet(A)$$ \nand, according to Lemma \\ref{lem-pconst} and Equation \\eqref{E:p*formula}, the map $p$ can be chosen so that \n\\begin{equation*}\n\\begin{split}\np_*([I, \\p I ] \\otimes c) & = [I, \\p I] \\otimes1_Ac,\\\\ \n(p_1)_*(c) &= [0]\\otimes (1-1_A)c+[1]\\otimes 1_Ac+[I , \\p I] \\otimes (1_A\\p-\\p1_A)c \\\\\n & = [0]\\otimes c+\\p [I , \\p I ]\\otimes 1_Ac+[I, \\p I ] \\otimes(1_A\\p-\\p 1_A)c. \n\\end{split}\n\\end{equation*} \nAs in Lemma \\ref{lem-pconst} we write $p^1_s\\colon I\\times X\\to I$ and $p^2_s\\colon I\\times X\\to X$ for the coordinates of $p_s$. Replacing $p_s^1(x)$ with $\\min\\{s, p_s^1(x)\\}$  we may assume that $p_s^1(x)\\leq s$ for $s\\in I$ (actually the construction in Lemma \\ref{lem-pconst} already gives such $p$).\n\nSimilarly, using Lemma \\ref{lem-pconst} we choose a map \n$$q\\colon I\\times (X' \\cup_{\\iota} A' ) \\to (\\{0\\}\\times X' ) \\cup (I\\times A' ).$$ \nFor the readers' convenience we spell out the relevant properties of $q$. For all $x'\\in X'$, $a'\\in A'$ and $s\\in I$\n$$q(0,x')=(0,x'), \\quad q(s,a')=(s,a')$$  \nand  \n\\begin{equation*}\n\\begin{split}\nq_*([I , \\p I] \\otimes c) & = [I , \\p I ] \\otimes1_{A'} c, \\\\ \n(q_1)_*(c) & = [0]\\otimes (1-1_{A'} )c+[1]\\otimes 1_{A'}c+[I, \\p I] \\otimes (1_{A'}\\p-\\p1_{A'})c \\\\\n& = [0]\\otimes c+\\p [I , \\p I ]\\otimes 1_{A'}c+[I , \\p I ]\\otimes(1_{A'}\\p-\\p 1_{A'})c. \n\\end{split}\n\\end{equation*} \nWe define the map\n$$\\tilde \\mathbf g : X \\cup_f Y \\to X' \\cup_{f'} Y '$$ \nto be equal to $\\mathbf g$ on $Y$ and for $x \\in X$ by the formula \n\\begin{equation*}\n\\tilde \\mathbf g (x)=\\begin{cases} \\mathbf k \\circ p^2_1(x) &\\textrm{ if } p_1(x)\\in \\{0\\}\\times X,\\\\\n\\mathbf g \\circ f\\circ \\mathbf \\Sigma (1-p_1^1(x),p_1^2(x)) &\\textrm{ if } p_1(x)\\in I\\times A.\\end{cases}\n\\end{equation*}\nTo check that $\\tilde \\mathbf g$ is a well defined continuous map it is enough to verify that the partial formulas coincide on \n$$\\{x\\in X \\; : \\;  p_1(x)\\in \\{0\\}\\times A\\}$$ \nand that for all $a \\in A$ we have $\\tilde \\mathbf g (a)=\\mathbf g \\circ f(a)$. If $p_1(x)\\in \\{0\\}\\times A$ we have \n$$\\mathbf k \\circ p_1^2(x)=f'\\circ \\mathbf k \\circ p_1^2(x) \\quad \\mbox{in} \\quad X' \\cup_{f'} Y '$$\nand\n$$f'\\circ \\mathbf k \\circ p_1^2(x) =\\mathbf g \\circ f\\circ \\mathbf l \\circ \\mathbf k \\circ p_1^2(x)=\\mathbf g \\circ f \\circ \\mathbf \\Sigma (1,p_1^2(x))=\\mathbf g \\circ f \\circ \\mathbf \\Sigma (1-p_1^1(x),p_1^2(x)).$$\nFor $a\\in A$ we have $p_1(a)=(1,a)$ so \n$$\\tilde \\mathbf g (a)=\\mathbf g \\circ f\\circ \\mathbf \\Sigma (0,a)=\\mathbf g \\circ f(a),$$ \nas desired.\n\nAnalogously we define the map\n$$\\tilde \\mathbf h : X' \\cup_{f'} Y' \\to X \\cup_{f} Y$$ \nto be equal to $\\mathbf h$ on $Y'$ and for $x \\in X'$ by the formula \n\\begin{equation*}\n\\tilde \\mathbf h (x)= \\begin{cases} \n\\mathbf l \\circ q^2_1(x) &\\textrm{ if } q_1(x)\\in\\{0\\}\\times X',\\\\\n\\mathbf \\Sigma (q_1^1(x),f\\circ \\mathbf l \\circ q_1^2(x)) & \\textrm{ if } q_1(x)\\in I\\times A' .\n\\end{cases} \n\\end{equation*} \nThe verification that $\\tilde \\mathbf h$ is well defined and continuous is completely analogous to what we did for $\\tilde \\mathbf g$.   \n\nThe formulas for the chain maps $\\tilde g$ and $\\tilde h$ respectively associated to $\\tilde \\mathbf g$ and $\\tilde \\mathbf h$ now follow from those for $(p_1)_*$ and $(q_1)_*$; more specifically the identity (\\ref{E:p*formula}). \n\nIt remains to construct an explicit homotopy between the identity map and $\\tilde \\mathbf h \\circ \\tilde \\mathbf g$, which is the main content of the proposition. \nOn $X$ we have \n\\begin{equation*} \n\\tilde \\mathbf h \\circ\\tilde \\mathbf g (x)= \\left\\{\n\\begin{array}{ll} \n\\mathbf l \\circ q_1^2\\circ \\mathbf k \\circ p_1^2(x) & \\mbox{if } p_1(x)\\in \\{0\\}\\times X \\mbox{ and } q_1\\circ \\mathbf k \\circ p_1^2 (x)\\in \\{0\\}\\times X', \\\\\n\\mathbf P ( q_1^1\\circ \\mathbf k \\circ p_1^2(x), f\\circ \\mathbf l \\circ q_1^2\\circ \\mathbf k \\circ p_1^2(x)) & \\mbox{if } p_1(x)\\in \\{0\\}\\times X \\mbox{ and }  q_1\\circ \\mathbf k \\circ p_1^2(x)\\in I\\times A',\\\\\n\\mathbf h \\circ \\mathbf g \\circ f\\circ \\mathbf \\Sigma (1-p_1^1(x), p_1^2(x)) &  \\mbox{if } p_1(x)\\in I\\times A,\n\\end{array} \\right.\n\\end{equation*} \nwhile on $Y$ we have \n$$\\tilde \\mathbf h \\circ \\tilde \\mathbf g (y)=\\mathbf h \\circ \\mathbf g (y), \\quad \\mbox{for all } y\\in Y.$$ \nNote that for $p_1(x)\\in I\\times A,$ we have $\\mathbf k \\circ p_1^2(x)\\in A'$ so that $q_1 ( \\mathbf k \\circ p_1^2(x) ) = (1 , \\mathbf k \\circ p_1^2(x) )$ and therefore\n\\begin{equation*}\n\\begin{split}\n\\mathbf h \\circ \\mathbf g \\circ f\\circ \\mathbf \\Sigma (1-p_1^1(x), p_1^2(x)) &= \\mathbf P (1, f\\circ \\mathbf \\Sigma (1-p_1^1(x), p_1^2(x)) \\\\ \n& = \\mathbf P (q_1^1\\circ \\mathbf k \\circ p_1^2(x), f\\circ \\mathbf \\Sigma (1-p_1^1(x), p_1^2(x)).\n\\end{split}\n\\end{equation*} \nThis already suggests the rough form of the homotopy between $1$ and $\\tilde \\mathbf h \\circ \\tilde \\mathbf g$. We want to construct a map \n$$\\tilde \\mathbf P \\colon I\\times (X\\cup_f Y)\\to X\\cup_f Y \\quad \\mbox{with} \\quad \\tilde \\mathbf P (0,z)=z \\quad \\mbox{and} \\quad \\tilde \\mathbf P (1,z)=\\tilde \\mathbf h \\circ \\tilde \\mathbf g (z).$$ \nWe define it piece by piece starting with $[1/2,1]\\times (X\\cup_f Y)$. Define subsets \n\\begin{align*}\n\\mathcal C_1:=&\\{(s,x)\\in [0,1/2]\\times X |\\, p_1(x)\\in \\{0\\}\\times X \\mbox{ and } q_{2s}\\circ \\mathbf k \\circ p_1^2(x)\\in \\{0\\}\\times X'\\},\\\\\n\\mathcal C_2:=&\\{(s,x)\\in [0,1/2]\\times X |\\, p_1(x)\\in \\{0\\}\\times X \\mbox{ and } q_{2s}\\circ \\mathbf k \\circ p_1^2(x)\\in I\\times A'\\},\\\\\n\\mathcal C_3:=&\\{(s,x)\\in [0,1/2]\\times X |\\, p_1(x)\\in I\\times A\\}. \n\\end{align*} \n\nFor all $s \\in [0,1/2]$ we set \n$$\\tilde \\mathbf P (1/2+s,y)= \\mathbf P (2s,y) \\quad \\mbox{if } y \\in Y$$ \nand define \n\\begin{equation*}\n\\tilde \\mathbf P (1/2+s,x)= \\left\\{ \n\\begin{array}{ll} \n\\mathbf l \\circ q_{2s}^2\\circ \\mathbf k \\circ p_1^2(x) &  \\mbox{if } (s,x)\\in \\mathcal C_1,\\\\\n\\mathbf P ( q_{2s}^1\\circ \\mathbf k \\circ p_1^2(x), f\\circ \\mathbf l \\circ q_{2s}^2\\circ \\mathbf k \\circ p_1^2(x)) & \\mbox{if } (s,x)\\in \\mathcal C_2,\\\\\n\\mathbf P (q_{2s}^1\\circ \\mathbf k \\circ p_1^2(x), f\\circ \\mathbf \\Sigma (1-p_1^1(x), p_1^2(x))) & \\mbox{if } (s,x)\\in \\mathcal C_3, \n\\end{array} \\right.\n\\end{equation*} \nfor all $x \\in X$. \nLet's check that the partial maps agree on the common boundaries.\n\\begin{itemize}\n\\item  The common boundary to $\\mathcal C_1$ and $\\mathcal C_2$ is the set \n$$\\{(1/2+s,x) \\; : \\; p_1(x)\\in \\{0\\}\\times X \\mbox{ and } q_{2s}\\circ \\mathbf k \\circ p_1^2(x)\\in \\{0\\}\\times A'\\}.$$ \nFor $(1/2+s,x)$ therein, we have $\\mathbf l \\circ q^2_{2s}\\circ \\mathbf k \\circ p_1^2(x)\\in A$ so that\n$$\\mathbf l \\circ q_{2s}\\circ \\mathbf k \\circ p_1^2(x)=f\\circ \\mathbf l \\circ q_{2s}\\circ \\mathbf k \\circ p_1^2(x)=\\mathbf P (q_{2s}^1\\circ \\mathbf k \\circ p_1^2(x), f\\circ \\mathbf l \\circ q_{2s}\\circ \\mathbf k \\circ p_1^2(x)).$$\n\\item The common boundary of $\\mathcal C_2$ and $\\mathcal C_3$ is \n$$\\{(1/2+s,x) \\; : \\;  p_1(x)\\in \\{0\\}\\times A \\mbox{ and } q_{2s}\\circ \\mathbf k \\circ p_1^2(x)\\in I\\times A'\\}.$$ \nNote that $p_1(x)\\in \\{0\\}\\times A$ forces $\\mathbf k \\circ p_1^2(x)\\in A'$ so that $q_{2s}\\circ \\mathbf k \\circ p_1^2(x)=(2s, \\mathbf k \\circ p_1^2(x)).$  We then have \n\\begin{equation*}\n\\begin{split}\n\\mathbf P (q^1_{2s}\\circ \\mathbf k \\circ p^2_1(x), f\\circ \\mathbf l \\circ q^2_{2s}\\circ \\mathbf k \\circ p^2_1(x)) &= \\mathbf P (q^1_{2s}\\circ \\mathbf k \\circ p^2_1(x), f\\circ \\mathbf l \\circ \\mathbf k \\circ p^2_1(x))\\\\ \n& = \\mathbf P (q^1_{2s}\\circ \\mathbf k \\circ p^2_1(x), f\\circ \\mathbf \\Sigma (1-p^1_1(x), p^2_1(x)).\n\\end{split}\n\\end{equation*}\n\\item The common boundary of $\\mathcal C_1$ and $\\mathcal C_3$ is \n$$\\{(1/2,x) \\; : \\; p_1(x)\\in \\{0\\}\\times A\\}.$$ \nIt is contained in $\\mathcal C_1\\cap \\mathcal C_2\\cap \\mathcal C_3$ so that this case follows from the two previous ones. \n\\item Finally $Y$ intersects non-trivially only $\\mathcal C_3$ and their common subset is \n$$\\{(1/2+s, a) \\; : \\;  a\\in A\\}.$$ \nThere we have \n$$q^1_{2s}\\circ \\mathbf k \\circ p^2_1(a)=2s, \\quad p_1^1(a)=1 \\quad \\mbox{and} \\quad p_1^2(a)=a$$ \nso that  \n$$\\mathbf P (q^1_{2s}\\circ \\mathbf k \\circ p^2_1(a), f\\circ \\mathbf \\Sigma (1-p^1_1(a), p^2_1(a))) = \\mathbf P (2s,f(a)).$$\n\\end{itemize}\nNow that we have a well defined continuous map $\\tilde \\mathbf P (s,\\cdot)$ for $s\\in [1/2,1]$, we proceed to define $\\tilde \\mathbf P$ for $s\\in [0,1/2]$. First note that \n$\\tilde \\mathbf P (1/2,\\cdot)$ can be more simply defined by \n$$\\tilde \\mathbf P (1/2,y)=y, \\quad \\mbox{if } y\\in Y, \\quad \\mbox{and} \\quad \\tilde \\mathbf P (1/2,x)= \\mathbf \\Sigma (1-p_1^1(x),p_1^2(x)), \\quad \\mbox{if } x\\in X.$$\nThe last expression is indeed equal to \n$$\\mathbf l \\circ \\mathbf k \\circ p^2_1(x), \\quad \\mbox{if } p_1(x)\\in \\{0\\}\\times X, \\quad \\mbox{and} \\quad f\\circ \\mathbf \\Sigma (1-p^1_1(x), p^2_1(x)), \\quad \\mbox{if } p_1(x)\\in I\\times A.$$\nFor $s\\in [0,1/2]$ we then set \n\\begin{equation*}\n\\tilde \\mathbf P (s,y)=y,  \\quad \\mbox{if } y\\in Y, \\quad \\mbox{and} \\quad \\tilde \\mathbf P (s,x)= \\mathbf \\Sigma (2s-p_{2s}^1(x),p_{2s}^2(x)), \\quad \\mbox{if } x\\in X.\n\\end{equation*} \nThe total map $\\tilde \\mathbf P \\colon I\\times (X\\cup_f Y)\\to X\\cup_f Y$ is continuous by construction. It remains to compute \n$$\\tilde \\rho (c) = \\tilde \\mathbf P_*([I , \\p I ]\\otimes c) \\quad \\mbox{for all } c\\in C_\\bullet(X\\cup_f Y).$$ \nLet us refine the CW-complex structure of $I$ by taking the first barycentric subdivision of the original one. There are now two $1$-dimensional cells $I_1=(0,1/2)$ and $I_2=(1/2,1)$ such that $[I, \\p I ]=[I_1 , \\p I_1 ]+[I_2 , \\p I_2]$. It follows that\n$$\\tilde \\mathbf P_*([I , \\p I ]\\otimes c)=\\tilde \\mathbf P_*([I_1 , \\p I_1 ]\\otimes c)+\\tilde \\mathbf P_*([I_2 , \\p I_2 ]\\otimes c).$$\nGiven $c\\in C_\\bullet (Y)$ we have \n$$\\tilde \\mathbf P_*([I_1 , \\p I_1 ]\\otimes c)=0 \\quad \\mbox{and} \\quad  \\tilde \\mathbf P_*([I_2 , \\p I_2 ]\\otimes c) = \\rho (c),$$ \nwhence $\\tilde \\mathbf P_*([I , \\p I ]\\otimes c)=\\rho (c).$ \n\nNow let $c\\in C_\\bullet(X).$ We first compute $\\tilde \\mathbf P_*([I_2, \\p I_2 ]\\otimes c)$. Recall that it follows from \\eqref{E:p*formula} that\n$$(p_1)_*(c)=[0]\\otimes (1-1_A)c +[I , \\p I ]\\otimes (1_A\\p-\\p 1_A)c+ [1]\\otimes 1_A c.$$ \nFrom this and the definition of $\\tilde \\mathbf P$ on $[1/2 , 1] \\times (X\\cup_f Y)$ we get \n\\begin{multline}\\label{eq-H1} \n\\tilde \\mathbf P_*([I_2 , \\p I_2 ]\\otimes c)= \\alpha_*\\beta_*([I , \\p I ]\\otimes (1-1_A)c)) \\\\ + \\mathbf P_*(\\gamma_*([I , \\p I] \\otimes[I , \\p I ]\\otimes (1_A\\p-\\p 1_A)c)) \\\\ +\\mathbf P_*([I , \\p I ]\\otimes f \\circ 1_A (c) ),\n\\end{multline} \nwhere \n$$\\alpha\\colon (\\{0\\}\\times X' ) \\cup (I\\times A' ) \\to X\\cup_f Y, \\quad \\beta\\colon I\\times X\\to (\\{0\\}\\times X' ) \\cup (I\\times A') \\quad \\mbox{and} \\quad  \\gamma\\colon I\\times I \\times A\\to I\\times Y$$ \nare respectively given by \n\\begin{equation*} \n\\alpha(s,x)= \\left\\{\n\\begin{array}{ll} \n\\mathbf l (x), & \\mbox{if } (s,x)\\in \\{0\\}\\times X',\\\\ \n\\mathbf P (s, f\\circ \\mathbf l (x)), &  \\mbox{if }  (s,x)\\in I\\times A',\n\\end{array} \\right.  \\qquad \\beta(s,x)=q_s\\circ \\mathbf k (x)\n\\end{equation*}\nand\n$$\\gamma(s,t,a) =( q_s^1\\circ \\mathbf k (a), f\\circ \\mathbf \\Sigma (1-t,a))=(s,f\\circ \\mathbf \\Sigma (1-t,a)).$$\nRecall the notations \n$$\\sigma (u)=\\mathbf \\Sigma_*([I , \\p I ]\\otimes u) \\quad \\mbox{and} \\quad \\rho (u)=\\mathbf P_*([I , \\p I ]\\otimes u)$$ \nfor respectively $u\\in C_\\bullet(X)$ and $C_\\bullet (Y)$. \n\nFor all $u\\in C_\\bullet(X)$ we have\n$$\\beta_*([I , \\p I ]\\otimes u)= q_*([I , \\p I ]\\otimes k (u) )=[I , \\p I ]\\otimes 1_{A'} \\circ k (u)$$\nand\n$$\\gamma_*([I , \\p I ]\\otimes [I , \\p I ]\\otimes u)= -[I , \\p I ]\\otimes f \\circ \\sigma (u).$$\nNote that it follows from the formula for $\\beta_*$ that \n$$\\beta_*([I , \\p I ]\\otimes (1-1_A)(c))=[I , \\p I ]\\otimes 1_{A'} \\circ k \\circ (1-1_A)(c)=[I , \\p I ]\\otimes k \\circ 1_A \\circ (1-1_A) (c)=0.$$\nSo we do not need to compute $\\alpha_*$ in (\\ref{eq-H1}) to conclude that \n$$\\tilde \\mathbf P_* ([I_2 , \\p I_2 ]\\otimes c)=-\\rho \\circ f \\circ \\sigma \\circ (1_A\\p-\\p 1_A) (c)+ \\rho \\circ f \\circ 1_A (c).$$\n\nWe finally compute $\\tilde \\mathbf P_*([I_1 , \\p I_1 ]\\otimes c).$ Let \n$$\\delta\\colon I \\times X\\to I\\times X, \\quad \\delta(s,x)=(s-p^1_s(x), p_s^2(x)).$$ \nBy Lemma \\ref{lem-pconst}, we can choose $p$ so that $\\delta$ is cellular and \n$$\\delta_*([I , \\p I ]\\otimes c)=[I , \\p I]\\otimes (1-1_A)c.$$ \nIt then follows from the definition of $\\tilde \\mathbf P$ on $[0 , 1/2] \\times (X\\cup_f Y)$ that\n$$\\tilde \\mathbf P_* ([I_1 , \\p I_1 ]\\otimes c)=\\mathbf \\Sigma_* \\circ \\delta_* ([I , \\p I ]\\otimes c)=\\sigma \\circ (1-1_A)(c).$$ \nWe conclude that \n$$\\tilde \\rho = (\\sigma \\circ (1-1_A) - \\rho \\circ f \\circ \\sigma \\circ (1_A\\p-\\p 1_A) + \\rho \\circ f \\circ 1_A) \\oplus \\rho .$$\nThis finishes the proof of Proposition \\ref{lem-QGluing}. \\qed\n\n\n\n\\section{Quality of rebuilding for nilpotent groups (Proof of Theorem \\ref{thm-UnipRewiring})} \\label{S:unip}\n\nThee goal of the section is to prove the following:\n\n\n\\begin{theorem}[Theorem~\\ref{thm-UnipRewiring}]\\label{thm-UnipRewiring-in-text}\nLet $\\Lambda$ be a finitely generated torsion-free nilpotent group. If $Y_0$ is a compact $K(\\Lambda,1)$ space, \n there exists a constant ${\\kappa}\\geq 1$ such that for every finite index subgroup $\\Lambda_1\\leq \\Lambda$, the cover $Y_1=\\Lambda_1\\bs \\tilde Y_0$ admits a $\\alpha$-rebuilding $(Y_1,Y_1',\\mathbf g ,\\mathbf h ,  \\mathbf P )$\nof quality $([\\Lambda : \\Lambda_1], {\\kappa})$ for every $\\alpha$.\n\\end{theorem}\n\n\n\n\\subsection{Generalities on rebuildings and quality}\n\nLet $\\alpha \\in \\mathbb N$ and let $X$ be a CW-complex with finite $\\alpha$-skeleton. Recall (Definition~\\ref{def: Rebuilding} of the introduction) that a $\\alpha$-\\defin{rebuilding} of $X$ is a collection $(X,X',\\mathbf g ,\\mathbf h ,  \\mathbf P )$ that consists of a CW-complex with finite $\\alpha$-skeleton $X'$, two cellular maps  between the $\\alpha$-skeleta\n$$\\mathbf g \\colon X^{(\\alpha)}\\to X'{}^{(\\alpha )} \\quad \\mbox{and} \\quad \\mathbf h \\colon X'{}^{(\\alpha )}\\to X^{(\\alpha )}$$\nthat are homotopy inverse to each other up to dimension $\\alpha-1$, and a cellular homotopy $\\mathbf P \\colon [0,1] \\times X^{(\\alpha -1)}\\to X^{(\\alpha)}$  between the identity and $\\mathbf h \\circ \\mathbf g$ on $X^{\\alpha-1}$.\n\nRecall furthermore (Definition~\\ref{def: Rebuilding and quality} of the introduction) that given two real numbers  ${T} , {\\kappa}\\geq 1$, a $\\alpha$-rebuilding $(X,X',\\mathbf g ,\\mathbf h , \\mathbf P)$ is of \\defin{quality $(T , \\kappa )$} if \n\\begin{align}\n\\forall j \\leq \\alpha , \\quad |X'{}^{(j)}|  & \\leq  {\\kappa}{T}^{-1}|X^{(j)}| \\tag{cells bound}\\\\\n\\forall j \\leq \\alpha , \\quad \\log \\|g_j \\|,\\log \\| h_j \\|,\\log \\| \\rho_{j-1} \\|,\\log \\|\\p'_{j} \\|  & \\leq   {\\kappa}(1+\\log {T} ) \\tag{norms bound}\n\\end{align}\nwhere $g,h$ are the chain maps respectively associated to $\\mathbf g,\\mathbf h$ and where $\\rho \\colon C_\\bullet(X)\\to C_{\\bullet+1}(X)$ is the chain homotopy induced by $\\mathbf P$ in the cellular chain complexes \\eqref{eq: cellular chain homotopy}.\n\n\n\nIn this paragraph we make two general observations regarding rebuildings and their quality. \n\nFirst note that if $X_1\\to X$ is a finite cover, every $\\alpha$-rebuilding $(X,X',\\mathbf g ,\\mathbf h , \\mathbf P )$ of $X$ induces (via the lifting property \\cite[Prop. 1.33 and 1.34]{Hatcher-book-Algeb-topo}) a $\\alpha$-rebuilding $(X_1,X_1',\\mathbf g_1 ,\\mathbf h_1 , \\mathbf P_1 )$ with $X_1'=\\widetilde{X'}/\\pi_1(X_1)$.\n\n\\begin{lemma}[Rebuilding induced to finite cover]\\label{lem:Induced rebuilding to finite cover}\nLet $X$ be a finite CW-complex. \nThere is a constant $\\delta_X$ such that for every $\\alpha$-rebuilding $(X,X')$ of quality $({T}, {\\kappa})$ and for every finite cover $X_1\\to X$, the induced $\\alpha$-rebuilding $(X_1,X_1' )$ is of quality $({T},{\\kappa}\\delta_X)$.\n\\end{lemma}\n\\begin{proof}\nA covering of CW-complexes induces a trivial covering over each open cell. In particular: \n\\begin{enumerate}\n\\item both sides of the cells bounds are multiplied by the degree of the cover leaving the quality of the cells bounds unchanged, and\n\\item the degrees of the attaching map of each cell remains bounded along coverings by a constant depending only on $X$. \n\\end{enumerate}\nIt follows from this last observation that the norms (induced by the $\\ell^2$-norms) of the boundary map and of the maps $g$, $h$ and $\\rho$ remain bounded by a constant $\\delta_X$ along coverings. \n\\end{proof}\n\n\\begin{lemma}[Composition of rebuildings]\\label{lem-composition}\nLet $(X,X',\\mathbf g_1,\\mathbf h_1,\\mathbf P_1)$ and $(X',X'', \\mathbf g_2, \\mathbf h_2 ,\\mathbf P_2)$ be two $\\alpha$-rebuildings of respective quality $(T_1,{\\kappa}_1)$ and $(T_2,{\\kappa}_2)$, with $T_1, T_2, {\\kappa}_1 ,{\\kappa}_2\\geq 1$. \nLet \n$$\\mathbf g_3= \\mathbf g_2\\circ \\mathbf g_1, \\quad \\mathbf h_3= \\mathbf h_1\\circ \\mathbf h_2, \\quad \\mbox{and} \\quad \\mathbf P_3(t,x)=\\left\\{ \\begin{array}{ll} \\mathbf P_1(2t,x) & \\mbox{if } 0\\leq t\\leq 1/2,\\\\\n\\mathbf h_1\\circ \\mathbf P_2 (2t-1, \\mathbf g_1(x)) & \\mbox{if } 1/2<t\\leq 1.\\end{array} \\right.$$ \nThen $(X,X'',\\mathbf g_3,\\mathbf h_3, \\mathbf P_3)$ is a $\\alpha$-rebuilding of quality $(T_1 T_2, 4{\\kappa}_1{\\kappa}_2).$\n\\end{lemma}\n\\begin{proof} The fact that $(X,X'',\\mathbf g_3,\\mathbf h_3, \\mathbf P_3)$ is a $\\alpha$-rebuilding follows from the definition. The quality of the cells bounds being  multiplicative, it remains to check the norms bounds. Since we have\n\\begin{equation*}\n\\begin{split}\n\\log \\|g_3\\| & \\leq  \\log \\|g_1\\|+\\log\\|g_2\\|\\leq {\\kappa}_1(1+\\log T_1 )+{\\kappa}_2(1+\\log T_2 )\\\\ \n& \\leq {\\kappa}_1+{\\kappa}_2+{\\kappa}_1 \\log T_1+{\\kappa}_2\\log T_2  \\\\ \n& \\leq 2{\\kappa}_1{\\kappa}_2(1+\\log T_1 T_2 ),\n\\end{split}\n\\end{equation*}\nsimilarly for $h_3$, and \n\\begin{equation*}\n\\begin{split}\n\\log \\| \\rho_3\\| &= \\log \\| \\rho_1+ h_1\\circ \\rho_2\\circ g_1\\|\\leq \\log 2+\\max\\{ \\log \\|\\rho_1\\|, \\log \\|h_1\\circ \\rho_2\\circ g_1\\|\\}\\\\\n& \\leq \\log2+  {\\kappa}_1(1+\\log T_1 )+ {\\kappa}_2 (1+\\log T_2) + {\\kappa}_1(1+\\log T_1)\\\\\n& \\leq 4{\\kappa}_1{\\kappa}_2(1+\\log T_1 T_2 ),\n\\end{split}\n\\end{equation*}\nHere the degrees match well, e.g. $(\\rho_3)_{\\alpha-1}=(\\rho_1)_{\\alpha -1} + (h_1)_{\\alpha} \\circ (\\rho_2)_{\\alpha -1} \\circ (g_1)_{\\alpha -1}$ in top degree.\nThe lemma follows.\n\\end{proof}\n\n\\subsection{Unipotent lattices}\n\nNow let $\\Gamma$ be a finitely generated, torsion free nilpotent group, equivalently --- by a theorem of Malcev \\cite{Malcev1} --- the group $\\Gamma$ is a \\defin{unipotent lattice}, i.e., it is isomorphic to a lattice in a connected, finite dimensional unipotent Lie group.\n\nWe define a central series $L_i(\\Gamma)$ by \n$$L_0(\\Gamma)=\\Gamma \\quad \\mbox{and} \\quad L_{i+1}(\\Gamma)=\\ker \\left( L_i(\\Gamma)\\to (L_i(\\Gamma)/[\\Gamma,L_i(\\Gamma)])\\otimes_{\\mathbb Z}\\mathbb Q \\right).$$ \nBy definition, the quotients $L_i(\\Gamma)/L_{i+1}(\\Gamma)$ are torsion free abelian. We define the graded group \n$$\\gr \\Gamma=\\bigoplus_{i=0}^\\infty L_i(\\Gamma)/L_{i+1}(\\Gamma).$$ \nWe have $\\gr \\Gamma\\simeq \\mathbb Z^{\\mathfrak h}$ where $\\mathfrak h$ is the \\defin{Hirsch length} of $\\Gamma$. For the purpose of this paper this can be taken as the definition of Hirsch length.   \n\nThe goal of this section is to prove Theorem  \\ref{thm-UnipRewiring} according to which finite covers of classifying spaces of unipotent lattices admit a rewiring of quality proportional to the degree of the cover. This is, up to a constant, the best quality one could hope for. \n\nTo prepare for the proof we will need a few simple lemmas. \n\n\\begin{definition}\nAn automorphism $\\sigma$ of an unipotent lattice $\\Gamma$ is a \\defin{unipotent automorphism} if the induced automorphism of the graded group \n$$\\gr \\sigma\\in \\Aut(\\gr \\Gamma)\\simeq \\GL_{\\mathfrak h}(\\mathbb Z)$$ \nis unipotent. \n\\end{definition}\nEquivalently $\\sigma\\in \\Aut(\\Gamma)$ is unipotent if and only if the associated semi-direct product \n$$\\Gamma \\rtimes_\\sigma \\Z  =  \\langle (\\gamma , t ) \\; | \\;  t \\gamma t^{-1} =\\sigma( \\gamma ) \\rangle$$ \nis nilpotent. \n\n\\begin{lemma} \\label{lem-UnipAuto}\nLet $\\Gamma$ be a non-trivial unipotent lattice and $\\sigma$ be a unipotent automorphism of $\\Gamma$. There exists a normal subgroup $\\Gamma'\\leq \\Gamma$ with quotient $\\Gamma/ \\Gamma'\\simeq \\mathbb Z$ such that $\\Gamma'$ is preserved by $\\sigma$ and $\\sigma$ acts trivially on $\\Gamma/ \\Gamma'$.\n\\end{lemma}\n\\begin{proof}\nThe group $L_1(\\Gamma)$ is preserved by $\\sigma$ and by construction we have $\\Gamma/ L_1(\\Gamma)\\simeq \\mathbb Z^{d}$ for some $d>0$. Write $\\sigma_0\\in \\GL(d,\\mathbb Z)$ for the automorphism induced on $\\Gamma/ L_1(\\Gamma)\\simeq \\mathbb Z^{d}$. Since $\\sigma_0$ is unipotent we can find a codimension $1$ subspace $W\\subset V=\\mathbb Q^d$ such that $\\sigma_0 W=W$ and the induced action on $V/W$ is trivial. Let $U=\\mathbb Z^d\\cap W$ and put $\\Gamma'=UL_1(\\Gamma).$ Then $\\sigma(\\Gamma')=\\sigma_0(U)L_1(\\Gamma)=\\Gamma'$, the quotient \n$$\\Gamma/\\Gamma'\\simeq \\mathbb Z^d/ \\mathbb Z^d\\cap W\\simeq \\mathbb Z$$ \nand the induced action of $\\sigma$ on this quotient is trivial.\n\\end{proof}\n\n\\begin{lemma} \\label{lem-InnerAut}\nLet $(X_0,x_0)$ be a pointed CW-complex. Let $G=\\pi_1(X_0,x_0)$ and assume that $X_0$ is a classifying space for $G$. Let $\\alpha$ and $\\beta\\colon (X_0,x_0)\\to (X_0,x_0)$ be two cellular maps such that the induced morphisms \n$$\\alpha_* \\quad \\mbox{and} \\quad \\beta_* : G \\to G$$\nare conjugated by some element $g_0 \\in G$~: $\\alpha_* = g_0 \\beta_* g_0^{-1}$. Then, there exists a cellular homotopy \n$$H\\colon I\\times X\\to X \\quad \\mbox{such that} \\quad H(0, \\cdot )=\\alpha, \\quad H(1,\\cdot )=\\beta$$ \nand the loop \n$$[0,1] \\to X_0, \\quad s\\mapsto H(s,x_0)$$ \nrepresents $g_0\\in G$. \n\\end{lemma}\n\\begin{proof}\nWe first construct an explicit cellular map $q\\colon (X_0,x_0)\\to (X_0,x_0)$ that induces the morphism \n$$q_* : G \\to G, \\quad g \\mapsto g_0^{-1} g g_0 $$ \nand a (non-pointed!) homotopy between the identity map and $q$. Start with a map\n$$q_0\\colon (\\{0\\}\\times X_0) \\cup (I\\times \\{x_0\\}) \\to X_0, \\quad q_0(0, \\cdot)= \\mathrm{id}_{X_0} \\quad \\mbox{and} \\quad q_0( \\cdot ,x_0)=s$$ \nwhere $s\\colon I\\to X_0$ is some loop based at $x_0$ and representing $g_0^{-1} \\in G$. By Lemma \\ref{lem-pconst} there is a map \n$$p\\colon I\\times X_0\\to (\\{0\\}\\times X_0) \\cup (I\\times \\{x_0\\}) \\quad \\mbox{such that} \\quad p(0, \\cdot )=\\{ 0 \\} \\times \\mathrm{id}_{X_0} \\quad \\mbox{and} \\quad p(1,x_0)=(1,x_0).$$ \nLet \n$$q\\colon (X_0,x_0)\\to (X_0,x_0) ; \\quad x \\mapsto q_0 (p(1,x)).$$ \nIt follows from the construction that $q$ induces the morphism \n$$q_* : G \\to G, \\quad g \\mapsto g_0^{-1} g g_0$$ \nand the map \n$$H_0\\colon I \\times X_0 \\to X_0;  (t,x) \\mapsto q_0 (p(t,x))$$\ngives a homotopy between the identity map and $q$. \nThe maps \n$$q\\circ \\alpha \\quad \\mbox{and} \\quad  \\beta\\colon (X_0,x_0)\\to (X_0,x_0)$$ \ninduce the same endomorphisms of $G$. It therefore follows from e.g. \\cite[Proposition 7.1.6]{Geoghegan} that there exists a \nhomotopy \n$$H_1\\colon I\\times X_0\\to X_0 \\quad \\mbox{such that} \\quad H(0,\\cdot )=q \\circ \\alpha , \\quad H(1, \\cdot )=\\beta \\quad \\mbox{and} \\quad H( \\cdot , x_0)=x_0.$$ \nTherefore, the map \n$$H(t,x)=\\begin{cases}\nH_0(2t,\\alpha(x)) & \\textrm{ if } 0\\leq t\\leq 1/2,\\\\\nH_1(2t-1,x) & \\textrm{ if } 1/2\\leq t\\leq 1,\n\\end{cases}$$ \nis a homotopy between $\\alpha$ and $\\beta$ and the loop $s\\mapsto H(s,x_0)$ represents $g_0.$\n\\end{proof}\n\n\n\\begin{lemma}\\label{lem-NilpotentAutmorphism}\nLet $\\Gamma$ be a unipotent lattice of Hirsch length $\\mathfrak h$ and let $\\sigma\\colon \\Gamma\\to \\Gamma$ be a unipotent automorphism. There exists a $K(\\Gamma,1)$ CW-complex $X_0$ with a distinguished point $x_0$ and a cellular map $\\theta \\colon X_0\\to X_0$ fixing $x_0$ such that for every $\\gamma \\in \\Gamma$, \n$$\\theta_* (\\gamma)= \\sigma (\\gamma).$$ \nand for every $m\\in \\N$ we have \n$$\\log \\|\\theta^m \\|\\leq  \\mathfrak h\\log |m|+O_\\Gamma(1),$$\nwhere $\\| \\theta^m \\|$ denotes the norm  induced by the $\\ell^2$-norm of the action of $\\theta^m$ on the cellular chain complex associated to $X_0$. \n\\end{lemma}\n\\begin{proof}\nWe prove the lemma by induction on the Hirsch length of $\\Gamma$. The base case $\\mathfrak h=0$ corresponds to the trivial group and one can take $X_0$ to be the CW-complex that consists of exactly one $0$-cell. \n\nSuppose now that the lemma is proved up to Hirsch length $\\mathfrak h-1$ and consider a unipotent lattice $\\Gamma$ of Hirsch length $\\mathfrak h$. By Lemma \\ref{lem-UnipAuto} there exists a $\\sigma$-invariant normal subgroup $\\Gamma'\\leq\\Gamma$ such that $\\Gamma/\\Gamma'\\simeq \\mathbb Z$ and the induced action of $\\sigma$ on $\\Gamma/\\Gamma'$ is trivial. Choose $t_0\\in \\Gamma$ such that $t_0\\Gamma'$ generates  $\\Gamma/\\Gamma'$.\n\nBy the inductive hypothesis there exists a $K(\\Gamma',1)$ space $X_0 '$ with a distinguished point $x_0'$ and a cellular automorphism $\\theta ' \\colon X'_0\\to X'_0$ that fixes $x_0'$, induces $\\sigma '$ on $\\pi_1(X_0',x_0')= \\Gamma'$ and satisfies \n$$\\log \\|(\\theta ')^m\\|\\leq (\\mathfrak h-1)\\log m+O_{\\Gamma'}(1)$$ \nfor $m\\in \\mathbb N$. Choose a cellular map $\\tau_0 \\colon X_0'\\to X_0'$ which fixes $x_0'$ and induces the automorphism $\\gamma\\mapsto t_0^{-1}\\gamma t_0$ on the fundamental group. Take $X_0$ to be the quotient \n$$X_0 =(X_0'\\times [0,1]) / \\sim \\quad \\mbox{where } \\sim \\mbox{ is generated by }  (x,1)\\sim (\\tau_0 (x ),0)  \\ (x \\in X_0 ')$$ \nand let $x_0$ be the image of $(x_0',0)$ in the quotient. The CW-complex $X_0$ is a $K(\\Gamma , 1)$ space and the image of $\\{x_0 ' \\} \\times [0,1]$ in $X_0$ is a loop based at $x_0$ that represents $t_0$ in $\\pi_1(X_0)\\simeq \\Gamma$. We proceed to construct the desired cellular map $\\theta \\colon X_0\\to X_0$. Since $\\sigma$ acts trivially on $\\Gamma / \\Gamma'$ the two maps \n$$\\tau_0 \\circ \\theta' \\quad \\mbox{and} \\quad \\theta' \\circ \\tau_0 : (X_0 , x_0) \\to (X_0 , x_0)$$ \ninduce two endomorphisms of $\\Gamma$ that are conjugated by $t_0^{-1} \\sigma (t_0) \\in \\Gamma '$. Lemma \\ref{lem-InnerAut} therefore implies that \nthere exists a homotopy \n$$H\\colon I\\times X_0'\\to X_0' \\quad \\mbox{such that} \\quad H(0, \\cdot )= \\tau_0 \\circ \\theta' ,  \\quad H(1,\\cdot )=\\theta' \\circ \\tau_0 $$ \nand the loop \n$$[0, 1 ] \\to X_0 , \\quad s\\mapsto H(s,x_0)$$ \nrepresents $t_0^{-1} \\sigma (t_0)  \\in \\Gamma'$.\nWe define $\\theta : X_0 \\to X_0$ by \n\\begin{align*}\n\\theta (x,t) =\\begin{cases} (\\theta ' (x ),2t) &\\textrm{ if } 0\\leq t\\leq 1/2\\\\\n(H(2t-1 , x), 0) &\\textrm{ if } 1/2<t\\leq 1.\\end{cases}\n\\end{align*}\nWe claim that the endomorphism of $\\Gamma$ induced by $\\theta$ is $\\sigma$. Since we already know that $\\theta$ induces $\\sigma$ on $\\Gamma'$ it is enough to compute its value on $t_0$. By construction, $t_0$ is represented by the loop in $X_0$ image of $\\{x_0 ' \\} \\times [0,1]$. This loop is sent  \nby $\\theta$ to the concatenation of $s\\mapsto (x_0,s)$ with $s\\mapsto H(s,x_0)$. The latter represents $t_0^{-1} \\sigma (t_0)  \\in \\Gamma'$ so the concatenation represents $\\sigma (t_0 )$ as desired. \n\nIt remains to compute the chain maps and to check the bound on the norm. Recall that as $\\mathbb Z$-modules, \n$$C_\\bullet(X_0) = (C_\\bullet (X_0')\\otimes [0] ) \\oplus (C_\\bullet(X_0')) \\otimes [I, \\p I ].$$ \nIn these coordinates, the chain map induced by $\\theta$ is given by the following formulas \n$$\\theta ( c\\otimes [0])= \\theta '(c)\\otimes [0] \\quad \\mbox{and} \\quad  \\theta (c\\otimes [I, \\p I ])= \\theta ' (c)\\otimes [I , \\p I ]+ r(c)\\otimes [0],$$ \nwhere $r(c) =H([I , \\p I ]\\otimes c).$\nBy induction we have \n$$\\theta^m (c\\otimes [0])=\\theta'^m (c)\\otimes [0] \\quad \\mbox{and} \\quad \\theta^m (c\\otimes [I , \\p I ])=\\theta'^m(c)\\otimes [I , \\p I ]+\\sum_{i=0}^{m-1}\\theta'^{i}(r(c))\\otimes [0].$$ \nIt follows that \n$$\\|\\theta^m \\| \\leq m \\sup \\{  \\|\\theta'^m \\| ,  \\| \\theta'^{i} \\circ r \\|  \\; : \\; i = 1 , \\ldots , m-1 \\} + || r ||$$\nwhich, by induction, gives \n$$\\log\\| \\theta^m \\| \\leq \\mathfrak h\\log m+2\\log \\|r\\|+O_{\\Gamma',\\sigma'}(1)= \\mathfrak h\\log m+ O_{\\Gamma,\\sigma}(1).$$\n\\end{proof}\n\n\n\n\n\n\n\\subsection{The Proof of Theorem \\ref{thm-UnipRewiring}}\n\n\n\\begin{proof}[Proof of Theorem \\ref{thm-UnipRewiring}]\nWe prove the theorem by induction on the Hirsch length $\\mathfrak h$. The base case $\\mathfrak h=0$ holds trivially since then $\\Gamma$ is the trivial group.  \n\nSuppose now that the lemma is proved up to Hirsch length $\\mathfrak h-1$ and consider a unipotent lattice $\\Gamma$ of Hirsch length $\\mathfrak h>0$. Thanks to Lemma~\\ref{lem:Induced rebuilding to finite cover} (Rebuilding induced to finite cover) and Lemma~\\ref{lem-composition} (Composition of rebuildings) we only need to prove the theorem for a single classifying space of $\\Gamma$. Therefore our first step is the construction of a convenient $Y_0$. \n\nLet $\\Lambda$ be a normal subgroup of $\\Gamma$ such that $\\Gamma/\\Lambda\\simeq \\mathbb Z$. Choose $t_0\\in \\Gamma$ such that $t_0\\Lambda$ generates $\\Gamma/\\Lambda$. Then $\\Gamma$ decomposes as a semidirect product \n$$\\Gamma=\\langle t_0\\rangle \\ltimes \\Lambda.$$\nThe automorphism \n$$\\Lambda \\to \\Lambda; \\quad \\lambda \\mapsto t_0 \\lambda t_0^{-1}$$ \nis unipotent. By Lemma \\ref{lem-NilpotentAutmorphism} there exists a pointed CW-complex $(X_0,x_0)$ which is a $K( \\Lambda , 1)$ space and a cellular map $\\theta \\colon (X_0,x_0)\\to (X_0,x_0)$ such that the induced endomorphism $\\theta_* : \\Lambda \\to \\Lambda$ is precisely equal to the unipotent automorphism $\\lambda \\mapsto t_0 \\lambda t_0^{-1}$ and \n$$\\|\\theta^m\\|\\leq (\\mathfrak h-1)\\log m+ O_{\\Gamma', t_0}(1).$$ \nWe let $Y_0$ be the quotient  \n$$Y_0= (X_0\\times [0,1] ) / (x,1)\\sim (\\theta (x),0)$$\nand take the image $y_0$ of $(x_0 , 0)$ as basepoint. Then $\\pi_1(Y_0,y_0)$ is isomorphic to $\\Gamma$ and $Y_0$ is a $K(\\Gamma , 1)$ space. Note for future reference that the projection of $\\{ x_0 \\} \\times [0,1]$ into $Y_0$ is a loop that represents the element $t_0$ in $\\Gamma$. \n\nNow let $\\Gamma_1\\leq \\Gamma$ be a finite index subgroup. Set $\\ell=[\\Gamma:\\Lambda\\Gamma_1]$ and let $\\Lambda_1 =\\Lambda\\cap \\Gamma_1$. Since $t_0^\\ell\\in \\Lambda \\Gamma_1$ we can choose $a\\in \\Lambda$ such that $t_1=a t_0^\\ell \\in \\Gamma_1.$ The group \n$\\Gamma_1$ then decomposes as a semi-direct product $\\Gamma_1=\\langle t_1\\rangle \\ltimes \\Lambda_1.$ It follows that $[\\Gamma:\\Gamma_1]=\\ell[\\Lambda:\\Lambda_1].$ Put \n$$Y_1=\\Gamma_1\\bs \\tilde Y_0.$$\nThis is the complex we want to rebuild. It is naturally a stack over the CW-complex obtained by lifting the standard CW-complex structure on the circle by a $\\ell$-fold self-covering map. We rebuild $Y_1$ in two steps. The first rebuilding $(Y_1,Y_1')$ consists of changing the base of the stack to get a stack over the standard CW-complex on the circle. It decreases the number of cells to $|Y_1'|=|Y_1|\\ell^{-1}$. The second rebuilding $(Y_1',Y_1'')$ is obtained by applying our effective version of the rebuilding lemma after applying the inductive hypothesis to the fibers of the stack. This second rebuilding will bring the number of cells down to $|Y_1''|=|Y_1|[\\Gamma:\\Gamma_1]=|Y_0|.$\n \nBefore performing the two rebuildings we give an explicit description of the stack structure of $Y_1$: the map $\\theta \\colon X_0\\to X_0$ lifts to a map $\\theta \\colon \\tilde X_0\\to \\tilde X_0$ such that for all $\\lambda \\in \\Lambda$ and for all $\\tilde x \\in \\tilde X_0$, we have\n$$\\theta (\\lambda \\cdot \\tilde x) = \\theta_* (\\lambda ) \\cdot \\tilde x.$$\nIn particular $\\theta$ maps each $(t_0^i \\Lambda_1 t_0^{-i})$-orbit of $\\tilde x$ (with $i \\in \\mathbb Z$) to the $(t_0^{i+1} \\Lambda_1 t_0^{-i-1})$-orbit of $\\tilde x$. Writing\n$$X_{1,(i)} =(t_0^i \\Lambda_1 t_0^{-i}) \\bs \\tilde X_0 \\quad \\mbox{and simply} \\quad  X_1 =X_{1,(0)},$$\nwe conclude that $\\theta$ induces a well-defined map \n$$\\theta \\colon X_{1,(i)}\\to X_{1,(i+1)}.$$ \nFinally since \n$$a (t_0^\\ell \\Lambda_1 t_0^{-\\ell}) a^{-1} = t_1 \\Lambda_1 t_1^{-1} = \\Lambda_1 ,$$\nthe map \n$$\\tilde X_0 \\to \\tilde X_0; \\quad \\tilde x \\mapsto a \\cdot \\tilde x$$\ninduces a well-defined map \n$$a  \\colon X_{1,(\\ell )}\\to X_0; \\quad (t_0^\\ell \\Lambda_1 t_0^{-\\ell} ) \\tilde x \\mapsto  \\Lambda_1 a\\tilde x.$$\nIt follows that \n\\begin{equation} \\label{E:Y1}\nY_1\\simeq \\bigsqcup_{i=0}^{\\ell-1} (X_{1,(i)} \\times [0,1] ) / \\sim,\n\\end{equation}\nwhere $\\sim$ is generated by \n$$(x_{(i)},1)\\sim (\\theta (x_{(i)}),0), \\quad \\mbox{for} \\quad i\\in \\{0,\\ldots, \\ell-2 \\}, \\quad \\mbox{and} \\quad (x_{(\\ell-1)},1)\\sim (a \\circ \\theta (x_{(\\ell-1)}),0).$$ \nIndeed: we have\n$$\\Lambda_1\\bs \\tilde Y_0 = \\bigsqcup_{i\\in \\mathbb Z} (X_{1,(i)} \\times [0,1] ) / (x_{(i)},1) \\sim (\\theta (x_{(i)}) , 0)$$ \nand quotienting it further by the action of $\\langle t_1\\rangle$, which has the effect of identifying $(x_{(i)},t)$ with $(a (x_{(i+\\ell)}),t)$, we get \\eqref{E:Y1}.  \n\n\\subsection*{First rebuilding} \nLet \n$$Y_1'= X_1\\times [0,1]/ (x,1)\\sim (a \\circ \\theta^\\ell (x),0).$$ \nIt is aspherical and $\\pi_1(Y_1')\\simeq \\langle t_1\\rangle \\ltimes \\Lambda_1=\\Gamma_1$ so $Y_1'$ is another $K(\\Gamma_1 , 1)$ space. Note that $|Y_1'|=\\ell^{-1}|Y_1|.$\nWe define three maps \n$$\\mathbf g \\colon  Y_1\\to Y_1', \\quad \\mathbf h \\colon Y_1'\\to Y_1 \\quad \\mbox{and} \\quad  \\mathbf P \\colon I\\times Y_1\\to Y_1$$ \nby the following formulas, with $x_{(i)} \\in X_{1,(i)}$, $x \\in X_1$ and $s,t \\in [0,1]$,\n\\begin{equation*}\n\\begin{split}\n\\mathbf g (x_{(i)} ,t) & =\\left\\{ \\begin{array}{ll} (x_{(i)},t) & \\mbox{if } i=0,\\\\ (a \\circ \\theta^{\\ell-i} (x_{(i)}),0) & \\mbox{if } \\quad i=1,\\ldots, \\ell-1, \\end{array} \\right. \\\\\n\\mathbf h (x,t) & =(\\theta^{\\lfloor \\ell t\\rfloor}(x), \\ell t-\\lfloor \\ell t\\rfloor),\n\\end{split}\n\\end{equation*}\nand\n\\begin{equation*}\n\\mathbf P (s,(x_{(i)},t)) = \\left\\{ \\begin{array}{ll} (\\theta^{\\lfloor \\alpha \\rfloor}(x_{(i)}), \\alpha-\\lfloor \\alpha \\rfloor) & \\mbox{if } i+t\\leq (\\ell-1)s \\mbox{ and } \\alpha =\\frac{\\ell(i+t-s(\\ell-1))}{\\ell-s(\\ell-1)},\\\\\n(a \\circ \\theta^{\\ell-i}(x_{(i)}) , 0) & \\mbox{if } i+t\\geq (\\ell-1)s.\n\\end{array} \\right.\n\\end{equation*}\nThe maps $\\mathbf g$ and $\\mathbf h$ are cellular and $\\mathbf P$ is a cellular homotopy between the identity and $\\mathbf h \\circ \\mathbf g$. The composition $\\mathbf g \\circ \\mathbf h$ is also homotopic to the identity since it induces the identity on $\\pi_1(Y_1')$. Therefore $(Y_1,Y_1',\\mathbf g ,\\mathbf h ,\\mathbf P )$ is a rebuilding. \n\nWe now compute the chain maps induced by $\\mathbf g$, $\\mathbf h$ and $\\mathbf P$ as well as the boundary map on $C_\\bullet (Y_1')$, which we shall denote by \n$\\p_{Y_1'}$. First remark that\n$$\nC_\\bullet(Y_1) = \\bigoplus_{i=0}^{\\ell-1}\\left(C_\\bullet(X_{1,(i)} )\\otimes [0]\\oplus C_\\bullet(X_{1,(i)}) \\otimes [I]\\right) \\quad \\mbox{and} \\quad \nC_\\bullet(Y_1')\\simeq C_\\bullet(X_1)\\otimes [0]\\oplus C_\\bullet(X_1)\\otimes [I],  $$ \nas $\\Z$-modules. In these coordinates, for $c \\in C_\\bullet (X_1)$ we have\n$$\\p_{Y_1'}(c\\otimes [0])= \\p_{X_1}c\\otimes [0] \\quad \\mbox{and} \\quad \n\\p_{Y_1'}(c\\otimes [I])= \\p_{X_1}c\\otimes [I]+ (-1)^{\\dim c}(-c\\otimes [0]+a\\circ \\theta^\\ell(c))\\otimes [0]),$$\nfor $c_{(i)} \\in C_\\bullet(X_{1,(i)})$ we have\n$$g (c_{(i)} \\otimes [0])= \\left\\{ \\begin{array}{ll}  c_{(i)} \\otimes [0] &\\mbox{if } i=0,\\\\\na\\circ \\theta^{\\ell-i}(c_{(i)} ) \\otimes [0] & \\mbox{if } i>0,\n\\end{array} \\right. \\quad \\mbox{and} \\quad \ng (c_{(i)} \\otimes [I])=  \\left\\{ \\begin{array}{ll}  c_{(i)} \\otimes [I] & \\textrm{ if } i=0,\\\\\n0 & \\textrm{ if } i>0,\\end{array} \\right.$$\nfor $c \\in C_\\bullet (X_1)$ we have\n$$h (c\\otimes [0])= c\\otimes [0] \\quad \\mbox{and} \\quad \nh (c\\otimes [I])= \\sum_{i=0}^{\\ell-1} \\theta^i(c)\\otimes [I],$$\nand for $c_{(i)} \\in C_\\bullet(X_{1,(i)})$ we have\n$$\\rho (c_{(i)} \\otimes [0])= \\left\\{ \\begin{array}{ll} \n0 & \\mbox{if } i=0,\\\\ \n\\sum_{j=i}^{\\ell-1} \\theta^j(c)\\otimes [I], & \\mbox{if } i>0,\n\\end{array}\\right. \\quad \\mbox{and} \\quad  \n\\rho (c_{(i)} \\otimes [I])= 0.$$\nThe  map $a \\colon C_\\bullet (X_{1,  (\\ell)} )\\to C_\\bullet(X_1)$ is unitary and  by Lemma \\ref{lem-NilpotentAutmorphism} $\\|\\theta^\\ell\\| \\leq  O_{X_0,\\sigma} (1+ \\log \\ell ).$ We deduce that \n\\begin{equation}\\label{eq-NormEstimates}\n\\|\\p_{Y_1'}\\|, \\quad \\|g \\|, \\quad \\|h \\|, \\quad \\| \\rho \\| \\leq O_{Y_0} (1+ \\log \\ell ). \n\\end{equation} \nThis concludes the first step.\n\n\\subsection*{Second rebuilding} \nBy induction there exists a rebuilding $(X_1,X_1', \\mathbf k, \\mathbf l , \\mathbf \\Sigma )$ of quality $([\\Lambda:\\Lambda_1], O_{X_0}(1)).$ \nTo shorten notation we shall write $\\theta_1 =a \\circ \\theta^\\ell.$ Let \n$$A =X_1\\times\\{0,1\\} \\quad \\mbox{and} \\quad A' =X_1'\\times\\{0,1\\}.$$ \nDefine a map $f\\colon A\\to X_1$ by \n$$f(x,0)=x \\quad \\mbox{and} \\quad f(x,1)=\\theta_1(x).$$ \nWe then have \n$$Y_1'=X_0\\times [0,1]/(x,1)\\sim (\\theta_1 (x),0)=(X_1\\times [0,1])\\cup_f X_1.$$\nNow consider the following diagram \n\\begin{equation*}\n\\begin{tikzcd}[sep=large]\nX_1\\times [0,1]\\arrow[d,\"\\mathbf k \\times \\mathrm{id}\",swap, shift right] &\\arrow[l,hook] A\\arrow[d,swap,\"\\mathbf k \\times \\mathrm{id}\",shift right] \\arrow[r,\"f\"] & X_1\\arrow[d,swap,\"\\mathbf k\", shift right]\\\\\nX_1'\\times [0,1]\\arrow[u,\" \\mathbf l\\times \\mathrm{id}\",swap, shift right]&\\arrow[l,hook] A'\\arrow[r,\"f'\"] \\arrow[u,\" \\mathbf l\\times \\mathrm{id}\",swap, shift right]& X_1'\\arrow[u,\" \\mathbf l\",swap, shift right],\n\\end{tikzcd}\n\\end{equation*}\nwhere $f'= \\mathbf k\\circ f\\circ ( \\mathbf l \\times \\mathrm{id})$. Define \n$$Y_1''=(X_1'\\times [0,1])\\sqcup_{f'} X_1'.$$ The number of cells in $Y_1''$ is $2|X_1'|=O_{X_0}(1)$ and \n$$C_\\bullet(Y_1'') = C_\\bullet(X_1)\\otimes [I]\\oplus C_\\bullet(X_1),$$ \nas $\\mathbb Z$-modules. The boundary map is given by \n$$\\p_{Y_1'}(c\\otimes [I])=\\p_{X_1'}(c)\\otimes [I]+(-1)^{\\dim c}(-k (\\theta_1 (l (c)))+k ( l (c)) \\quad \\mbox{and} \\quad \\p_{Y_1'}(c)=\\p_{X_1'}(c),$$ \nfor $c\\in C_\\bullet(X_1').$\n\nLet \n$$\\mathbf g ' \\colon Y_1'\\to Y_1'' \\quad \\mbox{and} \\quad \\mathbf h ' \\colon Y_1''\\to Y_1'$$ \nbe the cellular maps afforded by Proposition \\ref{lem-QGluing} and let \n$$\\mathbf P ' \\colon I\\times Y_1'\\to Y_1'$$ \nbe the homotopy between the identity and $\\mathbf h ' \\circ \\mathbf g '$, also provided by Proposition \\ref{lem-QGluing}. Let \n$$ \\sigma \\colon C_\\bullet(X_1)\\to C_{\\bullet+1}(X_1) \\quad \\mbox{and} \\quad \\rho ' \\colon C_\\bullet(Y_1')\\to C_{\\bullet+1}(Y_1')$$ \nbe the chain homotopies respectively induced by $\\mathbf \\Sigma$ and $\\mathbf P '$.\n\nBy induction we have \n$$\\log \\|k \\|, \\quad \\log \\|l \\|, \\quad \\log \\|\\sigma \\|\\leq O_{X_0} ( 1+ \\log [\\Lambda:\\Lambda_1] ) \\quad \\mbox{and} \\quad \\log \\|f\\|\\leq O_{Y_0} (1+\\log \\ell ).$$  \nUsing the explicit formulas for the chain maps from Lemma \\ref{lem-QGluing} we deduce that \n\\begin{equation}\\label{eq-NormEstimates2}\n\\log\\| \\p_{Y_1'} \\|, \\log\\| g ' \\|, \\log \\| h ' \\|, \\log \\| \\rho ' \\|\\ll_{Y_0} \\log\\ell+ \\log [\\Lambda:\\Lambda_1]+1=\\log [\\Gamma:\\Gamma_1]+1.\n\\end{equation}\nLet\n$$\\mathbf g ''=\\mathbf g ' \\circ \\mathbf g, \\quad \\mathbf h ''=\\mathbf h \\circ \\mathbf h '$$ \nand \n$$\\mathbf P ''(s,y)= \\left\\{ \n\\begin{array}{ll} \n\\mathbf P (2s, y) & \\mbox{if } 0\\leq s\\leq 1/2,\\\\\n\\mathbf h ( \\mathbf P ' (2t-1,g(y)) & \\mbox{if } 1/2\\leq s\\leq 1,\n\\end{array} \\right. \\quad \\mbox{for} \\quad y\\in Y_1.$$ \nUsing \\eqref{eq-NormEstimates} and \\eqref{eq-NormEstimates2} we see that $(Y_1,Y_1'', \\mathbf g '', \\mathbf h'', \\mathbf P'')$ is a rebuilding of quality $([\\Gamma:\\Gamma_1],O_{Y_0}(1))$. \nThis proves Theorem~\\ref{thm-UnipRewiring}.\n\\end{proof}\n\n\n\\section{Quality of rebuilding of  extensions by unipotent lattices} \\label{S:unipext}\n\nConsider a countable group $\\Glemma$ that contains a finitely generated, torsion-free, nilpotent, \\defin{normal} subgroup $\\Alemma$. Suppose furthermore that $\\Glemma / \\Alemma$ is of type $F_{\\alpha}$ for some integer $\\alpha \\geq 0$. \n\nLet $\\basezero$ be a classifying space (CW-complex) for $\\Glemma / \\Alemma$ with finite $\\alpha$-skeleton.\nThe group $\\Glemma$ acts on its universal cover $\\widetilde{\\basezero}$ with cell-stabilizers all equal to $\\Alemma$.\n\nLet $Y$ be a finite classifying space for $\\Alemma$. The Borel construction followed by the rebuilding Lemma~\\ref{prop: rebuilding}\nyields a stack of CW-complexes $\\stackzero \\to \\basezero$ with base $\\basezero$ where each fiber is $Y$.  \n\nThe goal of this section is to prove the following effective rebuilding statement. \n\n\n\\begin{proposition} \\label{L3}\nLet $\\Glemone \\leq \\Glemma$ be a finite index subgroup, let $\\stackone \\to \\baseone$ be the associated stack and let $ \\Alemone = \\Alemma \\cap \\Glemone$. Then the total space $\\stackone$ is a classifying space for $\\Glemone$ and there exists a $\\alpha$-rebuilding $(\\stackone , \\stackone ' , \\mathbf g , \\mathbf h , \\mathbf P )$ of quality $([\\Alemma : \\Alemone ] , O_{Y,D} (1) )$.\n\\end{proposition}\n\\begin{proof} \nEach fiber of $\\stackzero \\to \\basezero$ has the same number of cells $n_i \\in \\mathbb{N}$ in each dimension $i$ (with $n_i=0$ above the dimension of $Y$). \n\nThe CW-complex $\\basezero$ has finitely many cells in each dimension $j \\leq \\alpha$; we denote by $m_j$ this number. The total number of cells of dimension $\\ell \\leq \\alpha$ of the total complex $\\stackzero$ is then finite and equal to \n$$N_\\ell = \\sum_{i+j=\\ell } n_i m_j.$$  \n\nNow consider the stack $\\stackone \\to \\baseone$. The total space $\\stackone$ has $[\\Glemma : \\Glemone ]N_\\ell$ cells in each dimension $\\ell \\leq \\alpha$. More precisely, each fiber $Y_1$ of $\\stackone \\to \\baseone$ is a classifying space for $ \\Alemone = \\Alemma \\cap \\Glemone$ which contains $[\\Alemma : \\Alemone] n_i$ cells in each dimension $i$ and $\\baseone$ contains $[\\Glemma / \\Alemma : \\Glemone /  \\Alemone] m_j$ cells in each dimension $j \\leq \\alpha$. Thus:\n$$N_\\ell [\\Glemma : \\Glemone ]=\\sum_{i+j=\\ell} [\\Alemma :  \\Alemone ] n_i\\ [\\Glemma / \\Alemma : \\Glemone  /  \\Alemone] m_j.$$\n\nThe group $\\Alemma$ being finitely generated, torsion-free and nilpotent, Theorem \\ref{thm-UnipRewiring} implies that there exists a $\\alpha$-rebuilding $(Y_1 , Y_1 ' , \\mathbf k , \\mathbf l , \\mathbf \\Sigma)$ of quality $([\\Alemma :  \\Alemone] , O_{Y} (1))$. In particular $Y_1'$ is a classifying space for $A_1$ and letting $\\stackone ' \\to \\baseone$ be the rebuilding of the stack $\\stackone \\to \\baseone$ associated to the rebuilding $Y_1'$ of the fibers we conclude that $\\stackone ' $ has\n$$O_Y \\left( \\sum_{i+j=\\ell} n_i \\ [\\Glemma / \\Alemma : \\Glemone  /  \\Alemone] m_j \\right) = O_Y \\left( \\frac{N_\\ell [\\Glemma : \\Glemone ]}{[\\Alemma :  \\Alemone ]} \\right)$$\ncells in each dimension $\\ell \\leq \\alpha$.  \n\nIn fact Proposition \\ref{P2} provides $(\\stackone , \\stackone ' , \\mathbf g , \\mathbf h , \\mathbf P)$ and expresses the boundary map $\\partial '$ on  $C_n (\\stackone ' )$, the chain maps induced by $\\mathbf g$ and $\\mathbf h$, and the chain homotopy induced by $\\mathbf P$ in terms of $k$, $l$, $\\sigma$, the (vertical) boundary map on $C_\\bullet (Y )$ and the horizontal boundary map on $C_\\bullet (\\basezero )$. The norms of the two boundary maps are bounded by $O_{Y,D} (1)$ and, since the rebuilding $(Y_1 , Y_1 ' , \\mathbf k , \\mathbf l , \\mathbf \\Sigma)$ is of quality $([\\Alemma :  \\Alemone] , O_{Y} (1))$,  the log of the norms of $k$, $l$ and $\\sigma$ are bounded by $O_{Y,D} ( 1 + \\log [\\Alemma :  \\Alemone] )$. The formulas \\eqref{E:P2} therefore imply that $(\\stackone , \\stackone ' , \\mathbf g , \\mathbf h , \\mathbf P)$ is of quality $([\\Alemma : \\Alemone ] , O_{Y,D} (1) )$. Indeed, consider for example the boundary map $\\partial ' $ on $C_j (\\stackone ' )$ for some $j \\leq \\alpha$. We have: \n\\begin{eqnarray*}\n\\Vert \\partial ' \\Vert &\\leq & \\Vert(\\partial ' )^{\\rm vert} \\Vert+ \\Vert k \\Vert \\ \\ \\left\\Vert \\left( \\sum_{i=0}^{j} \\left(  \\partial^{\\rm hor} \\circ \\sigma  \\right)^i \\right) \\right\\Vert\\ \\  \\Vert \\partial^{\\rm hor} \\Vert\\ \\Vert l \\Vert\n\\\\\n&\\leq & \\Vert(\\partial ' )^{\\rm vert} \\Vert+\nP_j([\\Alemma :  \\Alemone])\n\\end{eqnarray*}\nwhere $P_j$ is a polynomial of degree $O_{Y,D} (1)$ whose coefficients do not depend on the subgroup $\\Alemone$. It follows that \n$$\\log \\Vert \\partial ' \\Vert = O_{Y,D} (1 + \\log [\\Alemma :  \\Alemone] ) .$$\nThe bounds for $g$, $h$ and $\\rho$ are obtained similarly. \n\\end{proof}\n\n\n\n\n\\section{A general quantitative rebuilding theorem}\n\\label{sect: proof of main th}\n\n\n\n\nLet $\\alpha$ be a positive integer and let $\\Gamma \\curvearrowright \\Baseup$ be a CW-complex action of a countable group $\\Gamma$ that satisfies the following assumptions. \n\\begin{description}\n\\item[(Cond 1) \\namedlabel{Cond1}{(Cond 1)}] The CW-complex $\\Baseup$ is $(\\alpha-1)$-connected.\n\\item[(Cond 2) \\namedlabel{Cond2}{(Cond 2)}]  For every cell $\\cellup\\subset \\Baseup$ the stabilizer $\\Gamma_\\cellup$ acts trivially on $\\cellup$.\n\\label{item: H2}\n\\item[(Cond 3) \\namedlabel{Cond3}{(Cond 3)}]  The quotient CW-complex $\\Basedown:=\\Gamma \\backslash \\Baseup$ has finite $\\alpha$-skeleton.\n\\item[(Cond 4) \\namedlabel{Cond4}{(Cond 4)}]  The group $\\Gamma$ is of type $F_{\\alpha+1}$. \n\\item[(Cond 5) \\namedlabel{Cond5}{(Cond 5)}]  Each stabilizer group $\\Gamma_\\cellup$ is of type $F_{\\alpha}$.\n\\end{description}\nRecall from Proposition \\ref{prop: existence of a stack} that, as long as $\\Baseup$ is assumed to be simply connected, the Borel construction associates to this action a stack of CW-complexes $\\Totaldown \\to \\Basedown :=\\Gamma \\backslash \\Baseup$ such that $\\Totaldown$ has a $(\\alpha-1)$-connected universal cover and the fundamental group $\\pi_1 (\\Totaldown)$ isomorphic to $\\Gamma$. \n\n\\begin{theorem}[Quantitative rebuilding] \\label{Tmain}\nSuppose that $\\Gamma$ is finitely presented and that for each cell $e$ of $\\Basedown^{(\\alpha)}$ the group $\\Gamma_e$ is finitely generated and contains a finitely generated, torsion-free, nilpotent, normal subgroup $Z_e$. \nThere exists a constant ${\\kappa}$ such that for every finite index normal subgroup $\\Gammaprime \\leq \\Gamma$, there exists a CW-complex $\\Totaldown_1^+$ with finite $(\\alpha+1)$-skeleton such that the following hold.\n\\begin{enumerate}\n\\item The CW-complex $\\Totaldown_1^+$ has a $\\alpha$-connected universal cover.\n\\item The fundamental group $\\pi_1 (\\Totaldown _1 ^+)$ is isomorphic to $\\Gammaprime$. \n\\item In each dimension $\\leq \\alpha$, the total number of cells of $\\Totaldown_1^+$ is \n\\begin{equation}\n\\leq {\\kappa} \\sum_{e \\in \\Basedown^{(\\alpha)}} \\frac{[\\Gamma : \\Gammaprime]}{[Z_e : \\Gammaprime \\cap Z_e ]}.\\end{equation}\n\\item In each degree $\\leq \\alpha +1$, the norm of the boundary operator on the chain complex $C_\\bullet (\\Totaldown_1 ^+ )$ is \n\\begin{equation}\\leq {\\kappa} [\\Gamma : \\Gammaprime]^{\\kappa}.\\end{equation}\n\\end{enumerate}\n\\end{theorem}\n\nTo prove the theorem we will consider the stack of CW-complexes $\\Totaldown \\to \\Basedown :=\\Gamma \\backslash \\Baseup$. However for the fundamental group of the latter to be isomorphic to $\\Gamma$, we need $\\Omega$ to be simply connected. This follows from the hypothesis for $\\alpha \\geq 2$ but in certain interesting cases, e.g., for $\\Gamma=\\SL_3(\\mathbb Z)$, the natural candidate for $\\Omega$ is the Tits complex which fails to be simply connected. We need a way to ``fix\" these spaces before we start using them. This is the content of the following:\n\n\\begin{lemma}\\label{lem-SimplyConn} Let $\\Gamma$ be a finitely presented group. Consider a $1$-dimensional co-compact $\\Gamma$-CW-complex $\\Baseup$ whose cells are fixed pointwise by their stabilizers.\nAssume that all vertex stabilizers are finitely generated. Then there exists a simply connected co-compact $\\Gamma$-CW-complex $\\Baseup^+$ \nwhose $1$-skeleton is $\\Baseup$.\n\\end{lemma}\n\n\\begin{proof}\n\nUp to refining the structure of the CW-complex on $\\Baseup$, we may assume that it is a graph.\nThe point is to kill the fundamental group of $\\Baseup$ by gluing some $2$-cells along finitely many $\\Gamma$-orbits of loops in $\\Baseup$.\nBy \\cite[Theorem 9.2, p. 39]{Dicks-Dunwoody-book} the group $\\Gamma$ sits in a short exact sequence\n\\begin{equation}\\label{eq: short exact sequ Dicks-Dunwoody}\n1\\to \\pi_1(\\Baseup)\\to G \\overset{\\zeta }{\\to} \\Gamma\\to 1\n\\end{equation}\nwhere $G$ is the fundamental group of the natural quotient graph of groups associated with the action $\\Gamma\\curvearrowright \\Baseup$.\nThe assumption that the quotient space is compact and the vertex stabilizers are finitely generated imply that $G$ is finitely generated.\nThus $G$ is the quotient of a free group $\\FF(\\tilde{S})$ (where $\\tilde{S}$ is finite).\nBy \\eqref{eq: short exact sequ Dicks-Dunwoody}, $\\Gamma$ is also such a quotient. \nThus we have surjective morphisms:\n\\[\\FF(\\tilde{S})\\overset{\\eta }{\\to} G\\overset{\\zeta }{\\to} \\Gamma.\\]\nBy standard arguments, since it is finitely presented, $\\Gamma$ admits a finite presentation with these generators $\\tilde{S}$.\n\\[\\Gamma=\\langle \\tilde{S} | r_1, r_2, \\cdots, r_p\\rangle.\\]\nIt follows that the normal subgroup of $G$ generated by $\\eta (r_1), \\eta (r_2), \\cdots, \\eta (r_p)$ is exactly the kernel of $\\zeta $, i.e., it is $\\pi_1(\\Baseup)$.\n\n\nConsider now the action of $G$ on its Bass-Serre tree $T$, a base vertex $v$ in $T$ and the paths $c_j$ in $T$ from $v$ to $\\eta (r_j)(v)$.\nThey project to loops $\\sigma_1, \\sigma_2, \\cdots, \\sigma_p$ in $\\Baseup=\\pi_1(\\Baseup)\\bs T$. Since these loops normally generate $\\pi_1(\\Baseup)$, gluing a $2$-cell along the $\\sigma_i$ and extending $\\Gamma$-equivariantly produces the required simply connected $\\Gamma$-cocompact $\\Baseup^+$.\n\\end{proof}\n\n\n\n\n\n\\begin{proof} [Proof of the Quantitative Rebuilding Theorem~ \\ref{Tmain}]\nWe need to work with an $\\Baseup$ that is simply connected. If $\\alpha \\geq 2$ this follows from the assumptions. For $\\alpha=1$ we can use Lemma \\ref{lem-SimplyConn} to replace $\\Baseup$ by (its $1$-skeleton and then by) a simply connected $2$-dimensional complex $\\Baseup^+$ satisfying all the assumptions of the theorem. In any case, the Borel construction followed by the rebuilding Lemma~\\ref{prop: rebuilding} \nthen associates to the action of $\\Gamma$ on $\\Omega$ a stack of CW-complexes $\\Totaldown \\to \\Basedown :=\\Gamma \\backslash \\Baseup$ such that the fundamental group of $\\Totaldown$ is isomorphic to $\\Gamma$ and such that the fiber over each cell $e=\\Gamma \\cellup$ is a classifying space $F_e$ of $\\Gamma_{\\cellup}$ with finite $\\alpha$-skeleton.\n\n\nNow consider the stack of CW-complexes\n\\begin{equation} \\label{E:stackprime}\n\\Totaldown_1  \\to \\Basedownprime\n\\end{equation}\nassociated to the finite index subgroup $\\Gammaprime \\leq \\Gamma$, so that \n$$\\Totaldown_1 = \\Gammaprime \\backslash \\widetilde{\\Totaldown} \\quad \\mbox{and} \\quad \\Basedownprime = \\Gammaprime \\backslash \\Baseup.$$\nEach cell $e \\in \\Basedown$ is covered by $\\# ( \\Gamma_e \\backslash \\Gamma / \\Gammaprime)$ cells in $\\Gammaprime \\backslash \\Baseup$ and, since $\\Gammaprime$ is a normal subgroup of $\\Gamma$, the fibers of \\eqref{E:stackprime} over each of these cells are all isomorphic to the finite cover $D_e$ of $F_e$ associated to $\\Gammaprime \\cap \\Gamma_e \\leq \\Gamma_e$. \n\nBy hypothesis, as long as $\\dim e \\leq \\alpha$, the group $\\Gamma_e$ contains a finitely generated, torsion-free, nilpotent, normal subgroup $Z_e$. We may therefore apply Proposition \\ref{L3} with $G = \\Gamma_e$ and $G_1 = \\Gamma_e \\cap \\Gammaprime$. It follows that there exists a rebuilding $(D_e , D_e ' , \\mathbf k_e , \\mathbf l_e , \\mathbf \\Sigma_e )$ of quality $( [Z_e : \\Gammaprime \\cap Z_e] , O_{\\Gamma , \\Baseup} (1))$ of each fiber $D_e$ of \\eqref{E:stackprime}. \n\nApplying the rebuilding lemma (Proposition \\ref{P2}) to \\eqref{E:stackprime} we get a stack of CW-complexes \n$$\\Pi ' : \\Totaldown_1 ' \\to \\Basedownprime,$$ \ncellular homotopy equivalences $\\mathbf g_1 : \\Totaldown_1 \\to \\Totaldown_1 '$, $\\mathbf h_1 :  \\Totaldown_1' \\to \\Totaldown_1 $ and a homotopy $\\mathbf P_1$ between the identity and $\\mathbf h_1 \\circ \\mathbf g_1$. Note that over each cell of $\\Basedownprime$ covering a cell $e$ of $\\Basedown$, the fiber of $\\Pi'$ is $D_e '$.\n\nThe total space $\\Totaldown_1'$ is homotopy equivalent to $\\Totaldown_1$ \nand therefore has an $(\\alpha-1)$-connected universal cover and a fundamental group isomorphic to $\\Gammaprime$. Moreover, in each dimension $n \\leq \\alpha$, the total number of cells of $\\Totaldown_1 '$ is\n$$O \\left( \\sum_{e \\in \\Basedown^{(n)}} \\# ( \\Gamma_e \\backslash \\Gamma / \\Gammaprime) \\frac{[\\Gamma_e : \\Gammaprime \\cap \\Gamma_e]}{[Z_e : \\Gammaprime \\cap Z_e]} \\right) = O \\left(  \\sum_{e \\in \\Basedown^{(\\alpha)}} \\frac{[\\Gamma : \\Gammaprime]}{[Z_e : \\Gammaprime \\cap Z_e ]} \\right).$$\n\nTo control the norm of the boundary operator on the chain complex $C_\\bullet (\\Totaldown_1 ')$ we apply Proposition \\ref{P2}  as in the proof of Proposition \\ref{L3}. Equation \\eqref{E:P2} implies that the boundary operator $\\partial' $ of the (rebuilt) chain complex $C_\\bullet (\\Totaldown_1 ')$ can be written in terms of the (vertical) boundary operator in the fibers, the (vertical) maps ($k$, $l$ and $\\sigma$) and the boundary operators $\\partial$ and $\\partial^{\\rm vert}$ acting on the chain complex $C_\\bullet ( \\Totaldown_1 ) = C_\\bullet ( \\Gammaprime \\backslash \\widetilde{\\Totaldown} )$ (before rebuilding).\n\nThe norm of the boundary operators $\\partial$ and $\\partial^{\\rm vert}$ are bounded by constants that depend only on the local combinatorial structure of $\\Totaldown$, see Lemma \\ref{lem:Induced rebuilding to finite cover}.\n\nNow over each cell $e$ of $\\Basedown$ of dimension $\\leq \\alpha$, the rebuilding $(D_e , D_e ' , \\mathbf k_e , \\mathbf l_e , \\mathbf \\Sigma_e )$ is of quality $( [Z_e : \\Gammaprime \\cap Z_e] , O_{\\Gamma , \\Baseup} (1))$ and it follows that the norm of the vertical boundary operator $(\\partial ')^{\\rm vert}$ is  \n$O([\\Gamma_e : \\Gamma_e \\cap \\Gammaprime ]^{O(1)} )$ and therefore $O([\\Gamma : \\Gammaprime ]^{O(1)} )$. For the same reason, the norms of the vertical maps $k_e$, $l_e$ and $\\sigma_e$ are bounded by $O([\\Gamma_e : \\Gamma_e \\cap \\Gammaprime ]^{O(1)} )$, and formula \n\\eqref{E:P2} finally implies that the norm of the boundary operator on the chain complex $C_{\\leq \\alpha} (\\Totaldown_1 ')$ is a $O([\\Gamma : \\Gammaprime ]^{O(1)} )$.\n\nThe universal cover of $\\Totaldown_1'$ is `only' $(\\alpha -1)$-connected. However, since $\\Gamma$ is of type $F_{\\alpha+1}$, it follows from \\cite[Theorem 8.2.1]{Geoghegan} that it is possible to attach \\emph{finitely many} $\\Gamma$-orbits of $(\\alpha+1)$-cells to $\\widetilde{\\Totaldown}$ to make a $\\alpha$-connected $\\Gamma$-CW complex. \nWrite the quotient as $\\Totaldown^+ = \\Totaldown \\sqcup_f \\Basedown$,\nwith $\\Basedown=\\sqcup_{I} \\mathbb B^{\\alpha+1}$ a finite collection of $(\\alpha+1)$-cells and $f\\colon \\partial \\Basedown=\\sqcup_{I} \\mathbb S^{\\alpha}\\to \\Totaldown^{(\\alpha)}$ the map that attaches these $(\\alpha+1)$-cells to $\\Totaldown$. Then write \n$$\\Gamma_1 \\bs \\widetilde{\\Totaldown^+} = \\Totaldown_1 \\sqcup_{f_1} \\Basedown_1$$ \nwhere $\\Basedown_1 = \\sqcup_{I_1} \\mathbb B^{\\alpha+1}$ is the preimage of $\\Basedown$ in $\\Totaldown_1^+$ and $f_1 \\colon \\partial \\Basedown_1 =\\sqcup_{I_1} \\mathbb S^{\\alpha}\\to \\Totaldown_1^{(\\alpha)}$ is the lift of $f$. We have a diagram \n\\begin{equation*}\n\\begin{tikzcd}[sep=large]\n\\Basedown_1 \\arrow[d,\"\\mathrm{id}\",swap, shift right] &\\arrow[l,hook] \\partial \\Basedown_1 \\arrow[d,swap,\"\\mathrm{id}\",shift right] \\arrow[r,\"f_1\"] & \\Totaldown_1 \\arrow[d,swap,\"g\", shift right]\\\\\n\\Basedown_1 \\arrow[u,\"\\mathrm{id}\",swap, shift right]&\\arrow[l,hook] \\partial \\Basedown_1 \\arrow[r,\"\\varphi\"] \\arrow[u,\"\\mathrm{id}\",swap, shift right]& \\Totaldown_1' \\arrow[u,\"h\",swap, shift right],\n\\end{tikzcd}\n\\end{equation*} \nwith $\\varphi =g\\circ f_1.$ It then follows from Proposition \\ref{lem-QGluing} that the space \n$$\\Totaldown_1^+ = \\Totaldown_1 ' \\sqcup_{\\varphi} \\Basedown_1$$ \nis homotopy equivalent to $\\Totaldown_1 \\sqcup_{f_1} \\Basedown_1  .$ \nThe map $f_1$, being a lift of $f$, is of norm $\\|f_1 \\|\\leq \\|f\\|$. Finally the norm of the boundary map on $\\Totaldown_1^+$ in degree $\\alpha+1$ is bounded by $\\|\\varphi\\|\\leq \\|f_1\\|\\|g\\|.$  \nThe latter being of norm $O([\\Gamma : \\Gammaprime ]^{O(1)} )$ we conclude that the norm of the degree $\\alpha+1$ boundary operator on the chain complex $C_\\bullet (\\Totaldown_1^+ )$ is $O([\\Gamma : \\Gammaprime ]^{O(1)} )$. \n\\end{proof}\n\n\n\n\\section{Bounding torsion - A proof of Gabber's proposition~\\ref{Prop: Gabber}}\n\\label{sect: proof of Gabber's prop}\n\nIn this section we prove the following useful proposition attributed to Gabber (see \\cite[Prop. 3, p. 214]{Soule-99}). Our proof here follows the viewpoint of \\cite[Section 2]{BV}.\n\nLet $(C_{\\bullet} , \\p)$ be the cellular chain complex associated to a finite CW-complex $\\Totaldown$. Each $C_{j}$ is a free $\\Z$-module of finite rank with a canonical basis associated to the $j$-cells of $\\Totaldown$. We equip each finite dimensional vector space $C_j \\otimes \\R$ with the associated Euclidean norm. For any coefficient field $K$ and for any integer $j \\geq 0$, it follows from the definition of the homology groups that\n\\begin{equation} \\label{E:bettiK}\n\\dim_K H_j (C_\\bullet \\otimes K) \\leq \\mathrm{rank} \\ C_j .\n\\end{equation}\nTo bound the torsion homology we will use the following analogous observation. \n\n\\begin{proposition}[Gabber]\n \\label{Prop: Gabber}\nFor any $j \\geq 0$, \n\\begin{equation}\n\\log |H_j (C_\\bullet )_{\\rm tors} | \\leq (\\mathrm{rank} \\ C_j ) \\times \\sup ( \\log ||\\p_{j+1}|| , 0).\n\\label{eq: Gabber ineq}\\end{equation}\n\nHere $|| \\p_{j+1} ||$ denotes the operator norm. \n\\end{proposition}\n\\begin{proof}\nGiven a finite rank free $\\Z$-module $A$, such that $A_{\\R} = A\\otimes\\R$ is endowed with a positive definite inner product $(\\cdot , \\cdot )$ (a \\defin{metric} for short), we define $\\mathrm{vol} (A)$ to be the volume of $A_{\\R} / A$. When considered without further precision, the free $\\Z$-module $\\Z^a$ ($a \\in \\mathbb{N}^*$) will denote the standard one where $\\R^a= \\Z^a \\otimes \\R$ is endowed with the canonical metric. \n\nLet $a>0$, $b>0$ be integers and \n\\begin{equation}f : \\Z^a \\to \\Z^b\\end{equation} \nbe a $\\Z$-linear map. We set $\\det ' (f)$ to be the product of all nonzero singular values of $f$. Recall that the nonzero singular values of $f$ are --- with multiplicity --- the positive square roots of the nonzero eigenvalues of $ff^*$. Note that we have \n$$\\det {}' (f) \\leq \\sup (||f|| , 1)^{\\min (a,b)}$$\nwhere $||f||$ denotes the operator norm of $f_\\R : \\R^a \\to \\R^b$.\n\nNow recall from \\cite[(2.1.1)]{BV} the ``metric rank formula''\n\\begin{equation} \\label{BT1}\n\\det {}' (f) = \\mathrm{vol} (\\mathrm{image} \\ f ) \\ \\mathrm{vol} (\\mathrm{ker} \\ f).\n\\end{equation}\nHere we understand the metrics on $(\\mathrm{ker} \\ f) \\otimes \\R$ and $(\\mathrm{image} \\ f ) \\otimes \\R$ as those induced from $\\R^a$ and $\\R^b$. Let $Q = \\mathrm{coker} \\ f$ be the cokernel of $f$. It is a finitely generated $\\Z$-module. Write $Q= Q_{\\rm tors} \\oplus Q_{\\rm free}$ its decomposition into a torsion and a free part.\n\nWriting \n\\begin{equation}\\R^b = (\\mathrm{image} \\ f ) \\otimes \\R \\oplus (\\mathrm{image} \\ f )^{\\perp}\\end{equation}\nand applying \\cite[(2.1.1)]{BV} to the orthogonal projection $\\Z^b \\to (\\mathrm{image} \\ f )^{\\perp}$ we conclude that the quotient $1 / {\\mathrm{vol} (\\mathrm{image} \\ f )}$ is the product of $|Q_{\\mathrm{tors}}|^{-1}$ with the `regulator' $\\mathrm{vol} (Q_{\\rm free})$, where the metric on $Q_{\\rm free} \\otimes \\R$ is obtained by identifying it as a subspace of $(\\mathrm{image} \\ f )^{\\perp}$. In summary,\n\\begin{equation} \\label{BT2}\n\\frac{1}{\\mathrm{vol} (\\mathrm{image} \\ f )} = \\frac{\\mathrm{vol} (Q_{\\rm free})}{|Q_{\\mathrm{tors}}|^{-1}}.\n\\end{equation}\nIt follows from \\eqref{BT1} and \\eqref{BT2} that \n\\begin{equation} \\label{BT3}\n|Q_{\\rm tors} | = \\det {}' (f) \\frac{\\mathrm{vol} (Q_{\\rm free})}{\\mathrm{vol} (\\mathrm{ker} \\ f)} \\leq \\det {}' (f) .\n\\end{equation}\nThe last inequality follows from two facts:\n(1)\n$\\mathrm{ker} \\ f $ being a sub-lattice of $\\Z^a$\n we   have $\\mathrm{vol} (\\mathrm{ker} \\ f) \\geq 1$, and\n(2) $\\mathrm{vol} (Q_{\\rm free}) \\leq 1$ since $Q_{\\rm free}$ is spanned by vectors of length at most one.\n\n\nProposition \\ref{Prop: Gabber} follows from the equation \\eqref{BT3} applied to \n$$Q = \\mathrm{coker} \\left( \\p_{j+1} : C_{j+1} \\to C_j \\right).$$\nIndeed, the homology group $H_j (C_\\bullet )$ is contained in $Q$ and it follows from \\eqref{BT3} that the size of the torsion part of $Q$ is smaller than \n$$\\det {}' (\\p_{j+1}) \\leq \\sup(||\\p_{j+1} || , 1)^{\\mathrm{rank} C_j} .$$\n\\end{proof}\n\n\n\\section{Farber sequences and cheap rebuilding property} \\label{S:Farber}\n\\subsection{Farber neighborhoods}\n\\label{Farber neighborhoods}\n\n\n\n\n\nLet $\\Gamma$ be a countable group.\nLet $\\Sub_\\Gamma$ denote the space of subgroups of $\\Gamma$ equipped with the topology induced from the topology of pointwise convergence on $\\{0,1\\}^\\Gamma$. \nThe subset $\\Sub^{\\findex}_\\Gamma\\subseteq \\Sub_\\Gamma$ of finite index subgroups is equipped with the induced topology. It is countable when $\\Gamma$ is finitely generated.\nThe group $\\Gamma$ continuously acts by conjugation on both $\\Sub_\\Gamma$ and $\\Sub^{\\findex}_\\Gamma$.\n\n\n\n\nWe consider the fixed point ratio function defined for finite index subgroups $\\Gamma'\\leq \\Gamma$:\n$${\\rm fx}_{\\Gamma,\\gamma}\\colon \\Sub^{\\findex}_\\Gamma\\to  [0,1], \\quad \\Gamma' \\mapsto \\frac{|\\{g\\Gamma'| \\gamma g\\Gamma'=g\\Gamma'\\}|}{[\\Gamma:\\Gamma']}.$$\nThus ${\\rm fx}_{\\Gamma,\\gamma}(\\Gamma')$ is just the proportion of fixed points of $\\gamma\\in \\Gamma$ in the action $\\Gamma\\curvearrowright \\Gamma/\\Gamma'$.\n\n\\begin{definition}\n\\label{def: Farber sequence}\n A sequence $(\\Gamma_n)_{n\\in\\mathbb N}$  of subgroups of $\\Gamma$ is called a \\defin{Farber sequence} if it consists of finite index subgroups and for every $\\gamma\\in \\Gamma\\setminus \\{1\\}$ we have \n$\\lim_{n\\to\\infty} {\\rm fx}_{\\Gamma,\\gamma}(\\Gamma_n)=0$.\n\\end{definition}\nThough we won't use it, note that if $\\Gamma$ is finitely generated and $S\\Subset \\Gamma$ is a finite symmetric generating set, the sequence $(\\Gamma_n)_{n\\in\\mathbb N}$ is Farber if and only if the sequence of Schreier graphs $\\Sch(\\Gamma_n\\bs \\Gamma, S)$ converges to the Cayley graph $\\Cay(\\Gamma,S)$ in the Benjamini--Schramm topology \\cite{BS}; if and only if the sequence of actions $\\Gamma\\curvearrowright\\Gamma/ \\Gamma_n$ defines a sofic approximation of $\\Gamma$.\n\n\nObserve that $\\Gamma$ admits Farber sequences if and only if it is residually finite.\nThe notion is designed to encompass non-normal finite index subgroups. \n\n\\begin{definition}[Farber neighborhood]\nLet $\\Gamma$ be a residually finite group.\nAn open subset $U$ of $\\Sub^{\\findex}_\\Gamma$ is a \\defin{$\\Gamma$-Farber neighborhood} if it is $\\Gamma$-invariant and every Farber sequence in $\\Sub^{\\findex}_\\Gamma$ eventually belongs to $U$. \n\\end{definition}\nWe can think of these $\\Gamma$-Farber neighborhoods as neighborhoods of $\\{\\mathrm{id}\\}$ in $\\Sub^{\\findex}_\\Gamma$, except that $\\{\\mathrm{id}\\}\\not\\in \\Sub^{\\findex}_\\Gamma$.\n\\begin{example}\n\\label{ex: basis of Farber neighborhoods}\nAssume $\\Gamma$ is residually finite. The sets \\[U_{\\Gamma,S,\\delta}=\\left\\{\\Gamma'\\in \\Sub^{\\findex}_\\Gamma \\; \\left| \\; {\\rm fx}_{\\Gamma,\\gamma}(\\Gamma')< \\delta \\textrm{ for }\\gamma\\in S\\right.\\right\\},\\]\nwhere  $S\\Subset \\Gamma\\setminus\\{1\\}$ is a finite subset and $\\delta>0$, are non-empty and form a basis of $\\Gamma$-Farber neighborhoods.\nLet  $(\\gamma_j)_{j\\in \\N}$ be an enumeration of $\\Gamma$ and \n$S_n=\\{\\gamma_0, \\gamma_1, \\cdots, \\gamma_n\\}$, then the same holds for $U_{\\Gamma,S_n,\\frac{1}{n}}$.\nIf $\\Gamma_n\\in U_{\\Gamma,S_n,\\frac{1}{n}}$,\n then $(\\Gamma_n)_{n\\in \\N}$ is a Farber sequence.\nIt follows that if $V\\subseteq  \\Sub^{\\findex}_{\\Gamma}$ does not contain any $U_{\\Gamma,S,\\delta}$, then we can construct a Farber sequence $(\\Gamma_n)_{n\\in \\N}$ as above that does\n not meet $V$. \n\\end{example}\n\n\\begin{lemma}\\label{lem-FarberIntersection}\nLet $\\Gamma$ be a residually finite  group and let $\\Lambda\\leq \\Gamma$ be an infinite subgroup. For every $\\Lambda$-Farber neighborhood  $U\\subseteq  \\Sub^{\\findex}_{\\Lambda}$ and $\\delta>0$,  there exists a $\\Gamma$-Farber neighborhood $V\\subseteq  \\Sub^{\\findex}_{\\Gamma}$ such that for any $\\Gamma'\\in V$ we have \n\\[\\frac{\\left\\vert\\left\\{\\gamma\\in \\Gamma / \\Gamma' \\colon \\gamma \\Gamma'\\gamma^{-1}\\cap \\Lambda\\in U\\right\\}\\right\\vert}{[\\Gamma:\\Gamma']}\\geq 1- \\delta.\\]\n\\end{lemma}\n\nIn words, the lemma says that the finite index subgroups $(\\Gamma')^\\gamma\\cap \\Lambda$ of $\\Lambda$ belong to a prescribed $\\Lambda$-Farber neighborhood  for a large proportion of the conjugates $(\\Gamma')^\\gamma$, as soon as $\\Gamma'$ belongs to a small enough $\\Gamma$-Farber neighborhood.\n\n\\begin{proof}\nIt is enough to prove the statement for $U=U_{\\Lambda,S,\\delta}$ (from Example~\\ref{ex: basis of Farber neighborhoods}) for any finite $S\\Subset \\Lambda$ and $1>\\delta>0$. \n\n\n\n\nLet $V = U_{\\Gamma, S , \\frac{\\delta^2}{\\vert S\\vert}}$. Then, for $\\Gamma'\\in V$ and each $\\gamma \\in S$ simple combinatorics arguments give \n\\begin{equation*}\n\\begin{split}\n \\frac{\\delta^2}{\\vert S\\vert}  \\geq  \\textrm{fx}_{\\Gamma, \\gamma }(\\Gamma') & =\\frac{1}{[\\Gamma:\\Gamma']} \\sum_{g \\in \\Gamma / \\Gamma '} 1_{(\\Gamma')^g} (\\gamma  ) =\n\\frac{1}{[\\Gamma:\\Gamma']} \\sum_{g \\in \\Lambda \\bs \\Gamma / \\Gamma' } \\sum_{\\lambda \\in \\Lambda / ((\\Gamma')^g\\cap \\Lambda )} 1_{((\\Gamma')^g\\cap \\Lambda)^\\lambda} (\\gamma) \\\\\n& = \\frac{1}{[\\Gamma:\\Gamma']} \\sum_{g \\in \\Gamma / \\Gamma '} \\left(\\frac{1}{[\\Lambda:(\\Gamma')^g\\cap \\Lambda]}\\sum_{\\lambda \\in \\Lambda / ((\\Gamma')^g\\cap \\Lambda )} 1_{((\\Gamma')^g\\cap \\Lambda)^\\lambda} (\\gamma) \\right) \\\\\n& = \\frac{1}{[\\Gamma:\\Gamma']}\\sum_{g \\in \\Gamma / \\Gamma '}  {\\rm fx}_{\\Lambda,\\gamma } ((\\Gamma')^g \\cap \\Lambda).\n\\end{split}\n\\end{equation*}\nHence, \n$$|\\{g \\in \\Gamma / \\Gamma' \\colon  {\\rm fx}_{\\Lambda,\\gamma }((\\Gamma')^g \\cap \\Lambda)\\geq \\delta\\}|\\leq [\\Gamma:\\Gamma']  \\frac{\\delta}{\\vert S\\vert}$$ \nand \n$$|\\{g \\in \\Gamma / \\Gamma' \\colon \\exists \\gamma\\in S \\text{ s. t. } {\\rm fx}_{\\Lambda,\\gamma }((\\Gamma')^g \\cap \\Lambda)\\geq \\delta\\}|\\leq [\\Gamma:\\Gamma'] \\delta .$$ \nThus \n$$\\left\\vert\\{g \\in \\Gamma / \\Gamma' \\colon {\\rm fx}_{\\Lambda,\\gamma }((\\Gamma')^g \\cap \\Lambda)\\leq \\delta \\text{ for all } \\gamma\\in S\\}\\right\\vert\\geq [\\Gamma:\\Gamma'] (1-\\delta).$$  \nWe conclude that \n\\[\\frac{1}{[\\Gamma : \\Gamma']}\\sum_{g \\in \\Gamma / \\Gamma' }1_{U_{\\Lambda,S,\\delta}}((\\Gamma')^g \\cap \\Lambda)\\geq 1-\\delta,\\] \nas desired.\n\\end{proof}\n\n\\subsection{The cheap rebuilding property}\n\n\\begin{definition}[Cheap $\\alpha$-Rebuilding Property, Farber sequences]\n\\label{def-CheapReb -Farb-sequ} \nLet $\\Gamma$ be a residually finite group and let $\\alpha$ be a non-negative integer.\nA Farber sequence  $(\\Gamma_n)_n$ of $\\Gamma$ has the \\defin{Cheap $\\alpha$-Rebuilding Property}  if\nthere exists a $K(\\Gamma , 1)$ space $X$ with finite $\\alpha$-skeleton and a constant ${\\kappa}_X\\geq 1$ such that for every real number $T\\geq 1$, there is $n_0$ such that when $n\\geq n_0$\nthe finite covers $Y_n\\to X$ with $\\pi_1(Y_n)=\\Gamma_n$ admit a  $\\alpha$-rebuilding $(Y_n,Y_n')$ of quality $(T,{\\kappa}_X).$ \n\\end{definition}\n\nThe group $\\Gamma$ itself has the \\defin{Cheap $\\alpha$-Rebuilding Property} if the existence of the complex $X$ and of the constant ${\\kappa}_X$ holds in a ``uniform way'' for all Farber sequences. More precisely:\n\\begin{definition}[Cheap $\\alpha$-Rebuilding Property, groups]\n\\label{def-CheapReb} \nLet $\\Gamma$ be a countable group and let $\\alpha$ be a non-negative integer.\nThe group $\\Gamma$ has the \\defin{Cheap $\\alpha$-Rebuilding Property} if it is residually finite and\nthere exists a $K(\\Gamma , 1)$ space $X$ with finite $\\alpha$-skeleton and a constant ${\\kappa}_X\\geq 1$ such that for every real number $T\\geq 1$, \nthere exists a $\\Gamma$-Farber neighborhood $U=U(X,T)\\subseteq \\Sub^{\\findex}_\\Gamma$ such that\n  every finite cover $Y\\to X$ with $\\pi_1(Y)\\in U$ admits a  $\\alpha$-rebuilding $(Y,Y')$ of quality $(T,{\\kappa}_X).$ \n\\end{definition} \n\n\\begin{remark}\n\\label{rem: cheap 0-reb prop}\nNote that a residually finite group has the cheap $0$-rebuilding property if and only if it is infinite.\n\\end{remark}\n\n\\begin{lemma}\n\\label{lem: exist-> forall in def CPR}\nOne can replace in both Definitions  ``there exists a $K(\\Gamma , 1)$ space with finite $\\alpha$-skeleton and there is a constant ${\\kappa}_X$''\nby ``for every $K(\\Gamma , 1)$ space with finite $\\alpha$-skeleton,  there is a constant ${\\kappa}_X$''.\n\\end{lemma}\n\n\\begin{proof}\nLet $X$ and $X'$ be $k$-aspherical CW-complexes with finite $\\alpha$-skeleta and $\\pi_1(X) \\simeq \\pi_1(X') \\simeq \\Gamma$.\nAssume that  $X'$ satisfies the  Definition~\\ref{def-CheapReb -Farb-sequ}, then by basic obstruction theory,  there exist cellular maps \n$$\\mathbf g_1 \\colon X^{(\\alpha)}\\to X'{}^{(\\alpha)} \\quad \\mbox{and} \\quad \\mathbf h_1 \\colon X'{}^{(\\alpha)}\\to X^{(\\alpha)}$$\nthat are homotopy inverse to each other up to dimension $\\alpha-1$, \nand a cellular homotopy $\\mathbf P_1 \\colon [0,1] \\times X^{(\\alpha-1)}\\to X^{(\\alpha)}$  between the identity of $X^{(\\alpha-1)}$ and the restriction of $\\mathbf h_1 \\circ \\mathbf g_1$ to $X^{(\\alpha-1)}$.\nObserve for the case $\\alpha=1$ that the isomorphism between the fundamental groups allows us to define $\\mathbf g_1$ and $\\mathbf h_1$ up to the $2$-skeleta.\n\n\nLet $\\kappa_1$ be an upper bound for all the norms $ \\|(h_1)_{j}\\|$, $\\|(g_1)_{j}\\|$ for $j\\in \\{ 0, \\ldots , \\alpha\\}$ and $\\|(\\rho_1)_{j}\\|$ for $j=0, 1, \\cdots, \\alpha-1$\n that moreover satisfies\n$|X'{}^{(j)}| \\leq \\kappa_1|X^{(j)}| $ for all  $j\\in \\{ 0, \\ldots , \\alpha\\}$.\nThus $(X,X')$ is  a $\\alpha$-rebuilding of quality $(1,\\kappa_1)$.\n\nFor every real number $T\\geq 1$, \nthere exists a $\\Gamma$-Farber neighborhood $U=U(X',T)\\subseteq \\Sub^{\\findex}_\\Gamma$ such that\n  every finite cover $Y'\\to X'$ with $\\pi_1(Y')\\in U$ admits a  $\\alpha$-rebuilding $(Y',Y'')$ of quality $(T,{\\kappa}_{X'}).$ \nLet $Y\\to X$ be the finite cover associated with $(\\mathbf h_1)_*(\\pi_1(Y'))\\leq \\pi_1(X)\\simeq \\Gamma$. \nBy Lemma~\\ref{lem:Induced rebuilding to finite cover} (Rebuilding induced to finite cover) the $\\alpha$-rebuilding $(X,X')$ induces a rebuilding $(Y,Y')$ of quality $(1,\\kappa_1\\delta_X)$. Applying Lemma~\\ref{lem-composition} (Composition of rebuildings) to the rebuildings $(Y,Y')$ and $(Y', Y'')$ we get a $\\alpha$-rebuilding $(Y, Y'')$ of quality $(T, 4\\kappa_1\\delta_X\\kappa_{X'})$.\n\\end{proof}\n\n\\begin{example}\nIt is an interesting exercise to check that $\\Z$ has the cheap $\\alpha$-rebuilding property for every $\\alpha$. See Lemma~\\ref{lem-ZRebuilding} for a detailed solution.\nMany other groups have this cheap $\\alpha$-rebuilding property. See Corollary~\\ref{cor-CheapRebGroups} for some examples.\nFinite groups do not have the cheap $\\alpha$-rebuilding property, for any $\\alpha$.\n\\end{example}\n\n\n\\begin{theorem}\\label{thm-AdjRebStack}\nLet $\\Gamma$ be a finitely presented residually finite  group acting on a CW-complex $\\Baseup$ in such a way that any element stabilizing a cell fixes it pointwise. Let $\\alpha$ be a non-negative integer and assume that the following conditions hold:\n\\begin{enumerate}\n\\item $\\Gamma\\bs \\Baseup$ has finite $\\alpha$-skeleton; \n\\item $\\Baseup$ is $(\\alpha-1)$-connected;\n\\item For each cell $\\cellup\\in \\Baseup$ of dimension $j\\leq \\alpha$ the stabilizer $\\Gamma_\\cellup$ has the cheap $(\\alpha-j)$-rebuilding property.\n\\end{enumerate}\nThen, $\\Gamma$ itself has the cheap $\\alpha$-rebuilding property. \n\\end{theorem}\n\\begin{proof}\nIf $\\alpha = 0$, this follows from Remark~\\ref{rem: cheap 0-reb prop} and (3): \nthe stabilizer of any $0$-cell is infinite, thus so is $\\Gamma$.\n\\\\\n From now on we assume $\\alpha \\geq 1$. \nAs in the proof of Theorem \\ref{Tmain} we need to work with a simply connected $\\Baseup$. For $\\alpha \\geq 2$ this follows from the assumptions.  For $\\alpha=1$, \nwe note that the vertex stabilizers ($j=0$) have the $1$-rebuilding property and thus are finitely generated, so we can use Lemma \\ref{lem-SimplyConn}\n to replace $\\Baseup$ by (its $1$-skeleton and then by) a $1$-connected $2$-dimensional complex $\\Baseup^+$ satisfying all the assumptions of the theorem.\n\nSince all the information we need is located in the $\\alpha$-skeleton (or $2$-skeleton for $\\alpha=1$), we may assume that $\\Baseup$ has dimension at most $\\alpha$ (or $2$ for $\\alpha=1$).\n\nGiven a contractible CW-complex $E\\Gamma$  with a free action of $\\Gamma$ (i.e., the universal cover of some classifying space for $\\Gamma$), the Borel construction (Section~\\ref{Sect: Borel construction}) \nconsiders the product $\\Baseup \\times E\\Gamma$ with the diagonal action. It is $(\\alpha-1)$-connected, the fundamental group of the quotient $\\Gamma\\bs (\\Baseup \\times E\\Gamma)$ is isomorphic to $\\Gamma$ and \n the projection map $\\Gamma\\bs (\\Baseup \\times E\\Gamma)\\to \\Gamma \\bs \\Baseup$ is interpreted (by Proposition~\\ref{prop: existence of a stack}) as a stack of CW-complexes  \n with fiber $\\simeq \\cellup\\times (\\Gamma_\\cellup \\bs E\\Gamma)\\simeq \\Gamma_\\cellup \\bs E\\Gamma$ \nover each cell $\\Gamma \\cellup$ of $\\Gamma\\bs \\Baseup$. \n\n\n\nLet $[\\Baseup]:=\\{\\cellup_1,\\ldots, \\cellup_N\\}\\subseteq \\Baseup$ be a list of distinct representatives \nof the $\\Gamma$-orbits of cells of dimension $\\leq \\alpha$; there are finitely many of them, by hypothesis~(1).\nLet $\\cellup\\in [\\Baseup]$ be a cell of dimension $j \\leq \\alpha$. \nBy hypothesis the stabilizer $\\Gamma_\\cellup$ has the cheap $(\\alpha -j)$-rebuilding property and is therefore of type $F_{\\alpha-j}$. \nLet $X_\\cellup$ be a classifying space for $\\Gamma_\\cellup$ with finite $(\\alpha-j)$-skeleton. Since $X_\\cellup$ and $\\Gamma_\\cellup \\bs E\\Gamma$ are homotopy equivalent, the Geogeghan's rebuilding Lemma (Proposition \\ref{prop: rebuilding}) yields a stack $\\Totaldown\\to \\Gamma\\bs \\Baseup$ with fiber $X_\\cellup$ over each cell $\\Gamma\\cellup$ of $ \\Gamma\\bs \\Baseup$. \nPer condition (1), the quotient $\\Gamma\\bs \\Baseup$ has finite $\\alpha$-skeleton so the whole stack $\\Totaldown$ has finite $\\alpha$-skeleton. At this point we might as well forget how we constructed $\\Totaldown$, the only important properties to keep in mind are that it fits into a stack  $\\Pi: \\Totaldown\\to\\Gamma\\bs\\Baseup$, it has a $(\\alpha-1)$-connected universal cover, it has finite $\\alpha$-skeleton, that $\\pi_1(\\Totaldown)$ is isomorphic to $\\Gamma$\nand each fiber over $\\Gamma\\cellup$ is a CW-complex $X_\\cellup$ with $(\\alpha-1)$-connected universal cover, finite $\\alpha$-skeleton,  and $\\pi_1(X_\\cellup) \\simeq \\Gamma_\\cellup$.\n\n\nNow let $\\Gamma_1 \\leq \\Gamma$ be a finite index subgroup of $\\Gamma$ and let $\\Totaldown_1$ be the finite cover of $\\Totaldown$ corresponding to $\\Gamma_1$. \nAny $\\Gamma$-orbit of cells $\\Gamma\\cellup\\subseteq \\Baseup$ splits into a family of $\\Gamma_1$-orbits indexed by the double cosets:\n\\[\\Gamma\\cellup=\\bigsqcup_{\\gamma\\in \\Gamma_1\\bs\\Gamma/\\Gamma_{\\cellup}}  \\Gamma_1\\gamma\\cellup\\]\n\nThe complex $\\Totaldown_1$ is naturally the total space of a stack over $\\Gamma_1 \\bs \\Baseup$.\nIf $\\gamma\\cellup$ is any cell of $\\Baseup$ $(\\gamma\\in \\Gamma$) and $\\Gamma_{\\gamma\\cellup} \\cap \\Gamma_1=\\gamma\\Gamma_{\\cellup}\\gamma^{-1}\\cap \\Gamma_1 $ is its stabilizer for the $\\Gamma_1$-action, then the fiber of this stack over the cell $\\Gamma_1\\gamma\\cellup$ of $\\Gamma_1\\bs\\Baseup$ takes the form $X_{1,\\gamma\\cellup}=(\\gamma\\Gamma_{\\cellup}\\gamma^{-1}\\cap \\Gamma_1 ) \\bs \\widetilde X_{\\gamma\\cellup}\n \\simeq (\\Gamma_{\\cellup}\\cap \\gamma^{-1}\\Gamma_1\\gamma )\\bs \\widetilde X_\\cellup$. \n\\begin{center}\n\\begin{tikzcd}[column sep=1.2em]\n\\widetilde{\\Totaldown} \\arrow[rrr]\n \\arrow[d]& &&\\Totaldown_1 =\\Gamma_1\\bs \\widetilde{\\Totaldown}\\arrow[r] \\arrow[d]& \\Totaldown=\\Gamma\\bs \\widetilde{\\Totaldown} \\arrow[d]\\\\\n\\Baseup \\arrow[rrr]& && \\Gamma_1\\bs\\Baseup \\arrow[r]&   \\Gamma\\bs\\Baseup\n\\end{tikzcd}\n\\end{center}\n\n\n\nWe remark that since $\\Totaldown_1$ is a cover of $\\Totaldown$, we know that the norm (induced from the $\\ell^2$-norm) of the boundary map $\\partial\\colon C_{\\bullet}(\\Totaldown_1 )\\to C_{\\bullet-1}(\\Totaldown_1 )$ is bounded in degrees $\\bullet\\leq \\alpha$\nby a constant depending only on $\\Totaldown$, not on $\\Gamma_1$. \n\nWe will perform some rebuilding of $\\Totaldown_1$ and  determine conditions on $\\Gamma_1$ under which its quality is good enough.\n\n\\subsection*{Step 1. Rebuilding the stack} \n\n The cheap -rebuilding property (assumption (3)) of the stabilizers $\\Gamma_{\\cellup_i}$ applied to $X_{\\cellup_i}$ (by Lemma~\\ref{lem: exist-> forall in def CPR}, we have the freedom of the space in Definition~\\ref{def-CheapReb}) gives constants $c_{X_{\\cellup_i}}\\geq 1$ for which the following choice is possible:\n \\\\\nLet $T\\geq 1$ be a real number.\n We\n choose for each $i=1,\\ldots, N$ a $\\Gamma_{\\cellup_i}$-Farber neighborhood $U_i\\subseteq \\Sub^{\\findex}_{\\Gamma_{\\cellup_i}}$ and for every finite index subgroup $\\Lambda\\leq \\Gamma_{\\cellup_i}$ with $\\Lambda\\in U_i$ we choose an $(\\alpha-\\dim(\\cellup_i))$-rebuilding \n $R(\\cellup_i,T,\\Lambda)$ of $\\Lambda \\bs \\widetilde{X}_{\\cellup_i}$ \n of quality $(T, \\kappa_{X_{\\cellup_i}})$.  \n\nWe want to use the effective rebuilding Lemma (Proposition \\ref{P2}) to rebuild the stack $\\Totaldown_1=\\Gamma_1\\bs \\widetilde{\\Totaldown}$. To do that, we need to specify a rebuilding of each fiber $X_{1, \\cellup}$, with $\\Gamma \\cellup \\subset \\Gamma \\bs \\Baseup.$  The full set of fibers of $\\Totaldown_1\\to \\Gamma_1\\bs\\Baseup$\nconsists of the CW-complexes $X_{1 , \\gamma\\cellup_i}$, with  $i=1,\\ldots,N$ and $\\gamma\\in \\Gamma_1 \\bs \\Gamma/\\Gamma_{\\cellup_i}$. Note that \n$$X_{1,\\gamma{\\cellup_i}}=(\\Gamma_{\\gamma{\\cellup_i}} \\cap \\Gamma_1) \\bs \\widetilde X_{\\gamma{\\cellup_i}}\n \\simeq (\\Gamma_{{\\cellup_i}}\\cap \\gamma^{-1}\\Gamma_1\\gamma )\\bs \\widetilde X_{\\cellup_i}.$$\n\nWe rebuilt $X_{1,\\gamma{\\cellup_i}}$ according to whether $\\Gamma_{1,\\cellup_i, \\gamma}:=(\\Gamma_{\\cellup_i}\\cap \\gamma^{-1}\\Gamma_1\\gamma )\\in U_i\\subseteq \\Sub^{\\findex}_{\\Gamma_{\\cellup_i}}$ or not by using the rebuilding:\n\\begin{equation}\\label{eq: X'1 gamma omega i}\n (X_{1, \\gamma\\cellup_i},X'_{1, \\gamma\\cellup_i}, \\mathbf k_{\\gamma\\cellup_i} , \\mathbf l_{\\gamma\\cellup_i},\\mathbf \\Sigma_{\\gamma\\cellup_i}):=\n\\begin{cases}\nR(\\cellup_i,T,\\Gamma_{1,\\cellup_i, \\gamma}) &\\text{ if } \\Gamma_{1,\\cellup_i, \\gamma}\\in U_i\\subseteq \\Sub^{\\findex}_{\\Gamma_{\\cellup_i}}\\\\\n\\text{ $X'_{1, \\gamma\\cellup_i}=X_{1, \\gamma\\cellup_i}$ }\n &\\text { if } \\Gamma_{1,\\cellup_i, \\gamma}\\not\\in U_i\n\\end{cases}\n\\end{equation}\ni.e., we simply use the trivial rebuilding $(X_{1, \\gamma\\cellup_i},X_{1, \\gamma\\cellup_i}, id ,id,0)$ when \n$\\Gamma_{1,\\cellup_i, \\gamma}\\not\\in U_i$.\n\n\nBy virtue of Proposition \\ref{P2} we obtain a global rebuilding $(\\Totaldown_1, \\Totaldown_1 ' , \\mathbf g ,  \\mathbf h , \\mathbf P )$.\nOur goal is now to estimate its quality. \nRecall that the tension in Definition~\\ref{def: Rebuilding and quality} of quality (in the introduction) \n is between ``having few cells'' and ``maintaining tame norms''.\nObserve that for the fibers associated with $\\Gamma_{1,\\cellup_i, \\gamma}\\in U_i$, \nboth are \n controlled by definition of the $\\Gamma_{\\cellup_i}$-Farber neighborhood. In particular, the norms of the vertical maps in these fibers are bounded by a polynomial in $T$. \n As for the fibers associated with $\\Gamma_{1,\\cellup_i, \\gamma}\\not\\in U_i$, \nthe quality is ``very good'' as far as the ``norms bound\" is concerned.\nThe number of cells will be controlled later on in Step 2.\n\n\nLet us simply denote by $k \\colon C_\\bullet(X_{1,\\cellup} )\\to C_\\bullet(X'_{1,\\cellup})$ and $l \\colon C_\\bullet( X'_{1,\\cellup})\\to C_\\bullet(X_{1,\\cellup})$ the maps respectively induced by $\\mathbf k_\\cellup$ and $\\mathbf l_\\cellup$. Similarly, let $\\sigma \\colon C_\\bullet(X_{1,\\cellup} )\\to C_{\\bullet+1}(X_{1,\\cellup})$ be the chain homotopy map induced by $\\mathbf \\Sigma_\\cellup$. Finally let $g \\colon C_\\bullet(\\Totaldown_1 ) \\to C_\\bullet(\\Totaldown_1 ' )$ and $h \\colon C_\\bullet(\\Totaldown_1 ' )\\to C_\\bullet(\\Totaldown_1)$ be the chain maps induced by $\\mathbf g$ and $\\mathbf h$ and let $\\rho \\colon C_\\bullet (\\Totaldown_1 ) \\to C_{\\bullet +1} (\\Totaldown_1  )$ be the chain homotopy map induced by $\\mathbf P$.\n\nAs in the paragraphs preceding and preparing to Proposition \\ref{P2}, we consider the decomposition   \n\\[ C_\\bullet(\\Totaldown_1 )=\\bigoplus_{\\Gamma_1 \\cellup \\in \\Gamma_1 \\bs \\Baseup}  [\\cellup] \\otimes C_\\bullet(X_{1,\\cellup} ) \\quad \\mbox{and} \\quad   C_\\bullet(\\Totaldown_1 ')=\\bigoplus_{\\Gamma_1 \\cellup \\in \\Gamma_1 \\bs \\Baseup}  [\\cellup] \\otimes C_\\bullet(X'_{1,\\cellup}).\\] \nBy a slight abuse of notation we will simply write $\\heartsuit$ instead of the $1\\otimes \\heartsuit$ for $\\heartsuit=k$, $k_\\cellup$, $l$, $l_\\cellup$, $\\sigma$ or $\\sigma_\\cellup.$  We write $\\partial$, $\\partial^{\\rm hor}$, $\\partial^{\\rm vert}$ for the boundary, horizontal boundary and vertical boundary maps in the stack $C_\\bullet(\\Totaldown_1)$ and $\\partial '$, $(\\partial ')^{\\rm hor}$, $(\\partial ')^{\\rm vert}$ for the boundary, horizontal boundary and vertical boundary maps on $C_\\bullet(\\Totaldown_1 ')$. By Proposition \\ref{P2} formulae \\eqref{E: effective k}--\\eqref{E:P2} \nwe have \n\\begin{equation*}\n\\begin{split}\ng & = k \\circ \\left( \\sum_{i=0}^{\\infty} \\left( \\partial^{\\rm hor} \\circ \\sigma \\right)^i \\right), \\\\\nh & = \\left( \\sum_{i=0}^{\\infty} \\left(  \\sigma \\circ \\partial^{\\rm hor}   \\right)^i \\right) \\circ l, \\\\\n\\rho & = \\sigma \\circ \\left( \\sum_{i=0}^{\\infty} \\left( \\partial^{\\rm hor} \\circ \\sigma \\right)^i \\right), \\\\\n\\partial ' & = (\\partial ' )^{\\rm vert} + k \\circ \\left( \\sum_{i=0}^{\\infty} \\left( \\partial^{\\rm hor} \\circ \\sigma \\right)^i \\right) \\circ  \\partial^{\\rm hor} \\circ l.\n\\end{split}\n\\end{equation*}\nRecall that in each of these expressions the internal sum is in fact finite since the summands vanish for $i$ large enough. \nThe norms of $ (\\partial ')^{\\rm vert}$, $k$, $l$, $\\sigma$ and $\\partial^{\\rm hor}$ are bounded by polynomials in $T$ with coefficients depending on $\\Totaldown$ and on the constants $\\kappa_{X_{\\cellup_i}}$, but independent of $\\Gamma_1$. This is thus also the case for the norms of $g$, $h$, $\\rho$ and $\\partial'$ the chain maps appearing in the definition of the quality of a rebuilding. Their $\\log$ is thus bounded by a constant (independent of $\\Gamma_1$) times $(1+\\log T)$. This ensures the norms bound condition for every $\\Gamma_1$.\n\n\n\\subsection*{Step 2. Counting cells} In remains to ensure a control on the number of cells of $\\Totaldown_1'$.\nLet  \n$\\card{d}(X)$ denote the number of $d$-cells of $X$.\nWe need to exhibit a $\\Gamma$-Farber neighborhood $V\\subseteq \\Sub^{\\findex}_\\Gamma$ such that if $\\Gamma_1$ belongs to $V$ then \n$$\\card{j}(\\Totaldown'_1) \\leq O( \\card{j}(\\Totaldown_1)/T )\\quad \\mbox{for each } j \\in \\{ 0,\\ldots , \\alpha \\}.$$ \n\nGiven an arbitrary finite index  $\\Gamma_1\\leq \\Gamma$, we intend to count the number of cells in $\\Totaldown_1 '$. For that purpose we introduce $N$ functions $F_i : \\Gamma \\to \\R$, with $i \\in \\{ 1 , \\ldots , N\\}$, corresponding to \nthe chosen distinct representatives $[\\Baseup]:=\\{\\cellup_1,\\ldots, \\cellup_N\\}\\subseteq \\Baseup$ of the $\\Gamma$-orbits of cells in $\\Baseup$, \n by the following formulas\n$$F_i (\\gamma) = \n\\left\\{ \n\\begin{array}{ll}\nT^{-1} & \\text{ if } \n(\\Gamma_{\\cellup_i}\\cap \\gamma^{-1}\\Gamma_1\\gamma )\\in U_i\\\\\n1 &  \\text{ otherwise},\n\\end{array} \\right.$$\nwhere $U_i$ is the $\\Gamma_{\\cellup_i}$-Farber neighborhood \nchosen at the beginning of Step 1.\nDenoting $q:=\\dim(\\omega_i)$, recall to that each $\\Lambda\\in U_i\\subseteq \\Sub^{\\findex}_{\\Gamma_{\\cellup_i}}$\nwe have associated a cheap $(\\alpha-q)$-rebuilding $(X_{1, \\gamma\\cellup_i},X'_{1, \\gamma\\cellup_i})$ in equation~\\eqref{eq: X'1 gamma omega i} of quality $(T, \\kappa_{X_{\\cellup_i}})$.  \nIn particular, the cells bound gives\n$\\card{\\ell}(X'_{1,\\gamma\\cellup_i})\\leq \\kappa_{X_{\\cellup_i}}\\times  F_i (\\gamma)\\times \\card{\\ell}(X_{1,\\gamma\\cellup_i})$, for every $\\ell\\in \\{0, 1, \\cdots, \\alpha -q\\}$.\n\nBy our choices of fibers rebuildings we thus get:\n\\begin{equation*} \n\\begin{split}\n\\card{j}(\\Totaldown_1 ') & = \\sum_{q=0}^{j}\\sum_{\\substack{\\cellup\\in \\Gamma_1 \\bs \\Baseup\\\\ \\dim(\\cellup)=q}} \\card{j-q}(X'_{1,\\cellup})= \\sum_{\\begin{smallmatrix}\\cellup\\in \\Gamma_1 \\bs \\Baseup\\\\ \\dim(\\cellup)\\leq j\\end{smallmatrix}} \\card{j-\\dim(\\cellup)}(X'_{1,\\cellup})\\\\\n&= \\sum_{q=0}^{j} \\sum_{\\substack{\\cellup_i\\in [\\Baseup]\\\\ \\dim(\\cellup_i)=q}}\n\\sum_{\\gamma\\in \\Gamma_1 \\bs \\Gamma/\\Gamma_{\\cellup_i}}  \\card{j-q}(X'_{1,\\gamma\\cellup_i})\\\\\n&\\leq \\sum_{q=0}^{j} \\sum_{\\substack{\\cellup_i\\in [\\Baseup]\\\\ \\dim(\\cellup_i)=q}}\n\\sum_{\\gamma\\in \\Gamma_1 \\bs \\Gamma/\\Gamma_{\\cellup_i}} \\kappa_{X_{\\cellup_i}}\\times F_i (\\gamma) \\times \\card{j-q}(X_{1,\\gamma\\cellup_i})\\\\\n&\\leq {\\kappa}^{\\Baseup} \\sum_{q=0}^{j}  \\sum_{\\begin{smallmatrix}\\cellup_i\\in [\\Baseup]\\\\ \\dim(\\cellup_i)=q\\end{smallmatrix}}\n \\left(\n\\underbrace{\\sum_\n{\\substack{\\gamma\\in \\Gamma_1 \\bs \\Gamma/\\Gamma_{\\cellup_i}\\\\{\\phantom{F_i(\\gamma)=1}}}}\n  \\frac{\\card{j-q}(X_{1,\\gamma\\cellup_i})}{T}}_{(A)}\n+\n(1-\\frac{1}{T})\n\\underbrace{\\sum_{\\substack{\\gamma\\in \\Gamma_1 \\bs \\Gamma/\\Gamma_{\\cellup_i}\\\\ \nF_i(\\gamma)=1}\n} \\card{j-q}(X_{1,\\gamma\\cellup_i})}_{(B)}\n\\right)\n\\end{split}\n\\end{equation*} \nwhere ${\\kappa}^{\\Baseup}:=\\max\\{ \\kappa_{X_{\\cellup_i}}: i\\in \\{1, \\cdots, N\\}\\} $.\n\nIn order to bound the part associated with (A), observe that\n\\[\\sum_{q=0}^{j}  \\sum_{\\begin{smallmatrix}\\cellup_i\\in [\\Baseup]\\\\ \\dim(\\cellup_i)=q\\end{smallmatrix}}\n\\sum_{\\gamma\\in \\Gamma_1 \\bs \\Gamma/\\Gamma_{\\cellup_i}} \\card{j-q}(X_{1,\\gamma\\cellup_i})\n=\\card{j}(\\Totaldown_1).\\]\n\nIn order to bound the part associated with (B), observe that\\\\\n\\begin{itemize}\n\\item the covering $\\Totaldown_1\\to \\Totaldown$ induces the covering $X_{1,\\gamma\\cellup_i}\\to X_{\\cellup_i}$; and thus \n\\[\\card{j-q}(X_{1,\\gamma\\cellup_i})=[\\Gamma_{\\gamma \\cellup_i} : \\Gamma_{\\gamma \\cellup_i}\\cap \\Gamma_1]\n\\card{j-q}(X_{\\cellup_i});\\]\n\\item $[\\Gamma_{\\gamma \\cellup_i} : \\Gamma_{\\gamma \\cellup_i}\\cap \\Gamma_1]=[\\Gamma_{\\cellup_i}:\\gamma^{-1}\\Gamma_1\\gamma\\cap \\Gamma_{\\cellup_i}]$ is exactly the number of $\\Gamma_1\\bs\\Gamma$-classes that are gathered together to form the $\\Gamma_1\\bs\\Gamma/\\Gamma_{\\cellup_i}$-class of $\\gamma$; \n\\item the invariance under conjugation of the $\\Gamma_{\\cellup_i}$-Farber neighborhoods ensures that $F_i(\\gamma)=F_i(\\gamma \\lambda)$ for every $\\lambda\\in \\Gamma_{\\cellup_i}$. It follows that\n\\[\n\\sum_{\\begin{smallmatrix}\\gamma\\in \\Gamma_1 \\bs \\Gamma/\\Gamma_{\\cellup_i}\\\\ F_i(\\gamma)=1\n\\end{smallmatrix}} \n\\card{j-q}(X_{1,\\gamma\\cellup_i})=\\sum_{\\begin{smallmatrix}\\gamma\\in \\Gamma_1 \\bs \\Gamma/\\Gamma_{\\cellup_i}\\\\ F_i(\\gamma)=1\n\\end{smallmatrix}} \n[\\Gamma_{\\gamma \\cellup_i} : \\Gamma_{\\gamma \\cellup_i}\\cap \\Gamma_1]\n\\card{j-q}(X_{\\cellup_i})=\n\\sum_{\\begin{smallmatrix}\\gamma\\in \\Gamma_1 \\bs \\Gamma\\\\ F_i(\\gamma)=1\n\\end{smallmatrix}} \n\\card{j-q}(X_{\\cellup_i}).\n\\]\n\\end{itemize}\n\n\nBy Lemma \\ref{lem-FarberIntersection}, there exists a $\\Gamma$-Farber neighborhood $V\\subseteq \\Sub^{\\findex}_\\Gamma$ such that for $\\Gamma_1 \\in V$ and $i \\in \\{ 1,\\ldots, N \\}$ we have \n\\[\\left\\vert\\left\\{\\gamma\\in \\Gamma_1\\bs \\Gamma \\colon F_i(\\gamma)=1\\right\\} \n\\right\\vert=\n\\left\\vert\\left\\{\\gamma\\in \\Gamma / \\Gamma_1\\colon \\gamma \\Gamma_1\\gamma^{-1}\\cap \\Gamma_{\\cellup_i} \\not\\in U_i\\right\\}\\right\\vert\\leq T^{-1}[\\Gamma:\\Gamma_1].\\]\nThus\n\\begin{eqnarray*}\n\\sum_{\\begin{smallmatrix}\\gamma\\in \\Gamma_1 \\bs \\Gamma/\\Gamma_{\\cellup_i}\\\\ F_i(\\gamma)=1\n\\end{smallmatrix}} \n\\card{j-q}(X_{1,\\gamma\\cellup_i})&\\leq& T^{-1}[\\Gamma:\\Gamma_1]\\card{j-q}(X_{\\cellup_i})\n\\\\\n\\sum_{q=0}^{j}  \\sum_{\\begin{smallmatrix}\\cellup_i\\in [\\Baseup]\\\\ \\dim(\\cellup_i)=q\\end{smallmatrix}}\n\\sum_{\\begin{smallmatrix}\\gamma\\in \\Gamma_1 \\bs \\Gamma/\\Gamma_{\\cellup_i}\\\\ F_i(\\gamma)=1\n\\end{smallmatrix}} \n\\card{j-q}(X_{1,\\gamma\\cellup_i})\n&\\leq&\n\\underbrace{\\sum_{q=0}^{j}  \\sum_{\\begin{smallmatrix}\\cellup_i\\in [\\Baseup]\\\\ \\dim(\\cellup_i)=q\\end{smallmatrix}} T^{-1}[\\Gamma:\\Gamma_1]\\card{j-q}(X_{\\cellup_i})\n}_{=T^{-1}\\card{j}(\\Totaldown_1)}.\n\\end{eqnarray*}\nThis finishes the proof of Theorem~\\ref{thm-AdjRebStack}.\n\\end{proof}\n\nAs particular cases of Theorem~\\ref{thm-AdjRebStack} we obtain:\n\\begin{example}[Graphs of groups]\\label{ex: graphs of groups}\nLet $\\Gamma$ be a residually finite group that splits as a finite graph of groups with edge and vertex stabilizers that satisfy the cheap $(\\alpha-1)$-rebuilding property (and cheap $\\alpha$-rebuilding property respectively) for every $\\alpha$. \nConsidering its Bass-Serre tree $\\Baseup$, we obtain that $\\Gamma$ itself has the cheap $\\alpha$-rebuilding property.\nThis applies for instance to the residually finite Baumslag-Solitar groups $\\mathrm{BS}(1,n)$ and $\\mathrm{BS}(n,n)$, for any non-zero integer $n$.\n\\end{example}\n\n\\begin{example}[Groups acting on graphs] \\label{Ex:T10.10}\nIf a residually finite group $\\Gamma$ acts co-compactly on a \\emph{connected} graph $\\mathcal{G}$ such that \n\\begin{itemize}\n\\item vertex stabilizers have the cheap $1$-rebuilding property; \n\\item edge stabilizers are infinite, \n\\end{itemize}\nthen $\\Gamma$ has the cheap $1$-rebuilding property.\n\\end{example}\n\n\n\\subsection{First example}\n\n\n\\begin{lemma}\\label{lem-ZRebuilding}\n$\\mathbb Z$ has the cheap $\\alpha$-rebuilding property for any $\\alpha$. \n\\end{lemma}\n\\begin{proof}\nWe will verify the property for $X=[0,1]/\\{0\\sim 1\\}$ being the circle with one $0$-cell and one $1$-cell. Given a positive integer $m$ we denote by $X_m=[0,m]/\\{0\\sim m\\}$ the $m$-fold cover of $X$ associated to the subgroup $m \\Z$ of $\\Z$ and equipped with the cell structure lifted from $X$.\n\nLet $T$ be a positive real number. The subset $U \\subset \\mathrm{Sub}^{\\findex}_{\\Z}$ that consists of all the finite index subgroups $N \\Z$ in $\\Z$ with $N \\geq 4T$ is a $\\Z$-Farber neighborhood. Pick some subgroup $N\\Z$ in $U$. We now explain how to construct a rebuilding of quality $(T,O (1))$ of $X_N$.\nTo do so, first pick some sequence of integers \n$$0=a_0<a_1\\ldots< a_{m} = N \\quad \\mbox{with} \\quad T/2\\leq a_{i+1}-a_i\\leq T.$$ \nFor each $t\\in [0,N)$ let $\\iota(t)$ be the integer in $[0 , m-1]$ determined by $t\\in [a_{\\iota(t)},a_{\\iota(t)+1}).$ \nThe cellular maps $\\mathbf g \\colon X_N\\to X_m$ and $\\mathbf h \\colon X_m\\to X_N$ defined by \n\\[ \\mathbf g (t) = \\iota(t) + \\min\\{ t-a_{\\iota(t)},1\\} \\quad \\mbox{and} \\quad \\mathbf h (t)=a_{\\lfloor t\\rfloor}+(a_{\\lfloor t\\rfloor+1}-a_{\\lfloor t\\rfloor})(t-\\lfloor t\\rfloor)\\]\nare homotopy inverses to each other. The explicit homotopy $\\mathbf P \\colon X_N\\times [0,1]\\to X_N$ between $1$ and $\\mathbf h \\circ \\mathbf g$ is given by\n\\[ \\mathbf P (t,s)=\\min\\{a_{\\iota (t)}+(t-a_{\\iota (t)})(1+s(a_{\\iota(t)+1}-a_{\\iota(t)}-1)) , a_{\\iota(t)+1}\\}.\\]\nOne easily verifies that the $\\ell^2$-operator norms of the induced chain maps satisfy \n$$\\|g \\|, \\ \\|h \\|, \\ \\|\\rho \\|=O( T+1) \\text{ and } \\|\\p\\|, \\|\\p'\\|\\leq 2.$$\nThe number of cells in $X_m$ is \n$m\\in [N/T,2N/T]$\nin dimension $0$ and $1$. Hence, $\\mathbb Z$ has the cheap $\\alpha$-rebuilding property for any $\\alpha \\in\\mathbb N$.\n\\end{proof}\n\n\n\\subsection{Applications}\n\nRecall that a group $\\Gamma$ is called \\defin{polycyclic} if there is a sequence of subgroups \n$$\\Gamma=A_0\\triangleright A_1\\triangleright \\cdots \\triangleright A_n=\\{1\\}$$ \nsuch that each quotient group $A_i/A_{i+1}$ is cyclic. As a corollary of Theorem~\\ref{thm-AdjRebStack} and Lemma \\ref{lem-ZRebuilding} we obtain\n\n\\begin{corollary}\\label{cor-CheapRebGroups} Let $\\Gamma$ be a residually finite countable group. The following holds:\n\\begin{enumerate}\n\n\\item \\label{cor-stab under commens}  Let $\\Gamma'\\leq \\Gamma$ be a finite index subgroup. Then $\\Gamma$ has the cheap $\\alpha$-rebuilding property if and only if $\\Gamma'$ does. \n\n\\item \\label{it: cheap k-reb. normal subgroup and F k+1 quotient} If $\\Gamma$ has an infinite normal subgroups $N$ such that $\\Gamma/N$ is of type $F_{\\alpha}$ and $N$ has the cheap $\\alpha$-rebuilding property, then $\\Gamma$ has the cheap $\\alpha$-rebuilding property.\n\n\\item $\\Z^m$ has the cheap $\\alpha$-rebuilding property for every $\\alpha$.\n\n\\item Infinite polycyclic groups have the cheap $\\alpha$-rebuilding property for every $\\alpha$.\n\\end{enumerate}\n\\end{corollary}\n\\begin{proof} 1. If $\\Gamma$ has the cheap $\\alpha$-rebuilding property, then the witnesses $(X, {\\kappa}, U(T))$ for $\\Gamma$ yield witnesses for $\\Gamma'$.\nConversely, if $\\Gamma'$ has the cheap $\\alpha$-rebuilding property one can assume, up to passing to a further finite index subgroup, that it is normal in $\\Gamma$. Then pick any $(\\alpha-1)$-connected CW-complex $\\Baseup$ with a free action of $\\Gamma/\\Gamma'$ that is finite in every dimension $\\leq \\alpha$ and apply Theorem~\\ref{thm-AdjRebStack}\nto the $\\Gamma$-action on $\\Baseup$ defined through the quotient map $\\Gamma\\to \\Gamma/\\Gamma'$.\n\n2. Let $\\basezero$ be a classifying space (CW-complex) $\\basezero$ for $\\Gamma / N$ with finite $\\alpha$-skeleton.\nThe group $\\Gamma$ acts on its universal cover $\\widetilde{\\basezero}$ with cell-stabilizers all equal to $N$ and one can apply Theorem~\\ref{thm-AdjRebStack} to this action. \n\n3. and 4. finally follow from Lemma \\ref{lem-ZRebuilding} and 2. by induction. \n\\end{proof}\n\n\n\\begin{example}\nCorollary~\\ref{cor-CheapRebGroups}, item~\\eqref{it: cheap k-reb. normal subgroup and F k+1 quotient} implies for instance that $\\SL_d(\\Z)\\ltimes \\Z^d$ has the cheap $\\alpha$-rebuilding property for every $\\alpha$. The same is true for the standard braid groups since it has an infinite cyclic center.\n\\end{example}\n\n\n\\subsection{Right-angled groups}\nThe following proposition connects the cheap $1$-rebuilding property to the rewirings of right-angled groups considered in \\cite{AGN}.\n\n\\begin{proposition} \\label{P:rag}\nLet $\\Gamma$ be a residually finite group that is \\defin{right-angled}, i.e., that admits a finite generating list $\\{ \\gamma_1 , \\ldots , \\gamma_m \\}$ of elements of infinite order such that $[\\gamma_i , \\gamma_{i+1} ] = 1$ for all $i \\in \\{1 , \\ldots , m-1 \\}$. Then $\\Gamma$ has the cheap $1$-rebuilding property. \n\\end{proposition}\n\n\n\n\n\\begin{proof}[Proof of Proposition~\\ref{P:rag}] According to Example~\\ref{Ex:T10.10} it suffices to produce a cocompact action $\\Gamma \\curvearrowright \\mathcal{G}$ on a \\emph{connected} graph such that \n\\begin{itemize}\n\\item vertex stabilizers have the $1$-rebuilding property, and\n\\item edge stabilizers are infinite. \n\\end{itemize} \n\nWe build the graph $\\mathcal{G}$ as follows: for each $i \\in \\{1 , \\ldots , m \\}$, let $H_i$ be the (free abelian) infinite subgroup of $\\Gamma$ generated by $\\gamma_i$. The vertex set of $\\mathcal{G}$ consists of all the conjugates of the $H_i$ in $\\Gamma$, and two distinct conjugates $gH_ig^{-1}$ and $hH_jh^{-1}$ represent an edge if and only if the subgroup they generate is abelian. \n\nThe conjugation action of $\\Gamma$ on the set of its subgroups induces an action of $\\Gamma$ on $\\mathcal{G}$. Observe that, by hypothesis, the vertices corresponding to $H_1, \\ldots , H_m$ belong to the same connected component of $\\mathcal{G}$. It follows that for every $g \\in \\Gamma$ the vertices $gH_1g^{-1}, \\ldots , gH_m g^{-1}$ also belong to one connected component of $\\mathcal{G}$. Now for any $j \\in \\{ 1 , \\ldots , m \\}$ we have \n$$g H_j g^{-1} = (g\\gamma_j^{-1}) H_j (g \\gamma_j^{-1})^{-1},$$\nand, since $\\{ \\gamma_1 , \\ldots , \\gamma_m \\}$ is a generating set of $\\Gamma$, we conclude that the graph $\\mathcal{G}$ is connected. \n\nThe stabilizer of each vertex, or each edge, contains a normal subgroup that is a free infinite abelian subgroup (a conjugate of some $H_j$). These stabilizers therefore have the cheap $1$-rebuilding property (by Corollary \\ref{cor-CheapRebGroups}(2) and Lemma \\ref{lem-ZRebuilding}). \n\nFinally, the quotient $\\Gamma \\backslash \\mathcal{G}$ is a finite graph (on $m$ vertices).Theorem~\\ref{thm-AdjRebStack} applies to the action of $\\Gamma$ on $\\mathcal{G}$ and we conclude that $\\Gamma$ itself has the cheap $1$-rebuilding property.\n\\end{proof}\n\n \nThe cheap $\\alpha$-rebuilding property goes beyond right-angled groups, even for $\\alpha =1$. We now give an example. \n\n\n\\begin{example}[Non right-angled examples $\\Lambda\\ltimes \\Z^2$]\n\\label{ex: non rag semidirect F x Z2}\nIf a free group $\\Lambda\\leq \\SL_2(\\Z)$ does not contain any non-trivial unipotent element, \nthen any embedding of $\\Z^2$ in the canonical semi-direct product $\\Lambda\\ltimes \\Z^2$ lies inside the obvious normal $\\Z^2$:\nIf $(\\lambda_1,a_2), (\\lambda_2, a_2)\\in \\Lambda\\ltimes \\Z^2$ commute (and generate $\\Z^2$), then up to considering some power of them, one can assume \n$\\lambda_1=\\lambda_2$. It follows that $\\lambda_1^{-1} a_2^{-1} a_1 \\lambda_1=a_2^{-1} a_1$, so that $\\lambda_1$ is unipotent, thus trivial.\n\nIn order to find such a subgroup $\\Lambda$ of $\\SL(2,\\Z)$, it's enough to consider a finite cover of the modular orbifold $\\SL_2(\\Z) \\backslash \\HH^2$  that is a surface $S$ of genus $g\\geq 2$ (with cusps). The fundamental group of the compact surface $S'$ obtained by adding one point to each cusp of $S$ surjects onto the free group $\\FF_2$. Finally pick a  pull-back  of this free group along the maps $\\SL_2(\\Z)\\geq \\pi_1(S) \\twoheadrightarrow \\pi_1(S')\\twoheadrightarrow \\FF_2$. It does not contain any non-trivial unipotent element.\n\nSuch a semi-direct product $\\Gamma=\\Lambda\\ltimes \\Z^2$ is thus not right-angled while it has the cheap $\\alpha$-rebuilding property for all~$\\alpha$ by\nCorollary~\\ref{cor-CheapRebGroups}~\\eqref{it: cheap k-reb. normal subgroup and F k+1 quotient}.\n\\end{example}\n\n\n\n\n\n\n\n\\subsection{Artin groups}\n\nLet $I$ be a finite set. A \\defin{Coxeter matrix} $M=(m_{ij})$ on $I$ is an $I$-by-$I$ symmetric matrix $M=(m_{ij})$ with entries in $\\mathbb{N} \\cup \\{ \\infty \\}$ such that $m_{ii}=1$ for all $i$ and such that whenever $i \\neq j$, $m_{ij}\\geq 2$. Associated to $M$ are \n\\begin{itemize}\n\\item a \\defin{Coxeter group} $W_I$, or simply $W$, given by the presentation \n$$W = \\langle s_i, \\  i \\in I \\; | \\; (s_i s_j )^{m_{ij}} = 1, \\ (i,j) \\in I^2 \\rangle$$\nwith the convention that the relation is ignored if $m_{ij} = \\infty$, \n\\item a \\defin{Artin group} $A_I$, or simply $A$, given by the presentation \n$$A = \\langle a_i, \\ i \\in I \\; | \\; \\underbrace{a_i a_j a_i a_j a_i \\cdots}_{m_{ij}} = \\underbrace{a_j a_i a_j a_i a_j \\cdots}_{m_{ij}}, \\ (i,j) \\in I^2 \\rangle, \\quad \\mbox{and}$$\n\\item a simplicial complex, called the \\defin{nerve} of $M$, that we define below.\n\\end{itemize} \nIf the corresponding Coxeter group $W$ is finite we say that $A$ is \\defin{spherical}. These Artin groups are sometimes also referred to as `Artin groups of finite type' in the literature.\n\nIf $J \\subseteq I$, let $M_J$ denote the minor of $M$ whose rows and columns are indexed by $J$, and let $W_J$, resp. $A_J$, be the corresponding Coxeter group, resp. Artin group. It is known \\cite{Bourbaki} that the natural map \n$W_J \\to W_I$, resp. $A_J \\to A_I$, is injective and hence $W_J$, resp. $A_J$, can be identified with the subgroup of $W_I$, resp. $A_I$, generated by $\\{ s_i \\; : \\; i \\in J \\}$, resp. $\\{ a_i \\; : \\; i \\in J \\}$.\n\nThe \\defin{nerve} of $M$ is the simplicial complex $L$ whose vertex set is $I$ and a subset $\\sigma \\subseteq I$ spans a simplex if and only if $W_\\sigma$ is finite. \n\nCharney and Davis \\cite[Section 3]{CharneyDavis} have associated to $M$ a simple complex of groups $\\mathcal{C}$, in the sense of \\cite[Definition 12.11]{BridsonHaefliger}. The construction goes as follows: first consider the partially ordered set (poset) \n$$\\mathcal{P} = \\{ J \\subseteq I \\; : \\; W_J \\mbox{ is finite} \\}$$\nordered by inclusion. To any $\\sigma \\in \\mathcal{P}$ one associates the Artin group $A_\\sigma$, and for each $\\tau \\leq \\sigma$ the associate homomorphism $A_\\tau \\to A_\\sigma$ is the natural inclusion. The geometric realization of $\\mathcal{C}$ is simply connected (in fact contractible), since the empty set is an initial object for the poset $\\mathcal{P}$. In particular, the fundamental group of the complex of groups $\\mathcal{C}$ is \n$A$ (the direct limit of the $A_\\sigma$).\nThe complex of groups $\\mathcal{C}$ is developable, that is, it arises from the action of a group on a simplicial complex. More precisely, let \n$$A \\mathcal{P} = \\{ g A_J \\; : \\; g \\in A , \\ J \\in \\mathcal{P} \\},$$\nordered by inclusion. The stabilizer of $gA_j \\in A \\mathcal{P}$ is the subgroup $gA_J g^{-1}$; hence, $A$ acts without inversion on $A \\mathcal{P}$. Let \n$$X = | A \\mathcal{P} '|$$ \nbe the geometric realization of the derived poset of $A \\mathcal{P}$; it is the simplicial complex, referred to as the  `modified Deligne complex' in \\cite{CharneyDavis}, where $k$-simplices correspond to totally ordered chains of elements in $A\\mathcal{P}$. The complex of groups $\\mathcal{C}$ explicitely arises from the action of $A$ on $X$.\n\n It is conjectured that $X$ is contractible and this conjecture is equivalent to the celebrated \\defin{$K(\\pi,1)$ conjecture} for Artin groups. The main theorem of \n\\cite{CharneyDavis} is that this conjecture holds when the nerve $L$ is a \\defin{flag complex},  i.e., when the Coxeter group $W_J$ associated to every clique $J$ is finite.\nThis holds for instance if $A$ is either spherical or a right-angled Artin group. The conjecture also holds whenever  $L$ has dimension  at most $1$.  Recently, Paolini and Salvetti proved that the $K(\\pi,1)$ conjecture holds for all affine Artin Groups \\cite{Paolini-Salvetti-2021}.\n\n\n\\begin{theorem}[Artin groups with $(\\alpha-1)$-connected nerve]\n\\label{Ex:Artin groups}\nLet $M$ be a Coxeter matrix whose nerve $L$ is $(\\alpha-1)$-connected. Suppose that the $K(\\pi,1)$-conjecture holds for the associated Artin group $A$ and that $A$ is residually finite. Then $A$ has the cheap $\\alpha$-rebuilding property.\n\\end{theorem}\n\n\n\\begin{proof} We first consider the case where $A$ is spherical, or equivalently where $L$ is a simplex. In \\cite[Main Theorem]{Bestvina}, Bestvina proves that $A$ is then commensurable with $G \\times \\Z$, where $G=A/\\langle \\Delta^2 \\rangle$ is the quotient of $A$ by some central element $\\Delta^2$, and moreover $G$ acts cocompactly on a contractible simplicial complex $X$ and the action is transitive on the vertices with stabilizers $\\Z/2\\Z$. It follows that $G$ is of type $F_\\alpha$ for every $\\alpha$ (see e.g., \\cite[Theorem 7.3.1]{Geoghegan}). Applying Corollary~\\ref{cor-CheapRebGroups}~\\eqref{it: cheap k-reb. normal subgroup and F k+1 quotient} we get that \n $G \\times \\Z$ has the cheap $\\alpha$-rebuilding property for every $\\alpha$. By Corollary~\\ref{cor-CheapRebGroups}~\\eqref{cor-stab under commens}, so has $A$. \n \nConsider now the case where $A$ is of general type. We would like to apply our Theorem~\\ref{thm-AdjRebStack} to the action of $A$ on the modified Deligne complex $X$; but the stabilizers of the vertices of the form $A_J$ for $J=\\emptyset$ are trivial.\nWe thus modify $X$ as follows:\nLet $\\Baseup \\subset X$ be the sub-complex associated to the sub-poset $\\mathcal{P}_0$ of $\\mathcal{P}$ consisting of the non-empty subsets $J \\subseteq I$. The $A$-action on $X$ preserves $\\Baseup$ and the cell stabilizers are now isomorphic to the non trivial spherical Artin groups $A_J$, $J\\not=\\emptyset$. The complement $X \\setminus\\Baseup$ is the disjoint union of the open stars $St(v)$ of the vertices $v$ of $X$  corresponding to the elements $g A_\\emptyset \\in A \\mathcal{P}$ ($g \\in A$). \nEach such star $St(v)$  is the cone over the link of $v$ in $X$ and this link is isomorphic to the simplicial complex $|\\mathcal{P}_0'|$ associated to the poset of non-empty spherical subsets of $I$, i.e., the nerve $L$ of $M$. By hypothesis the latter is $(\\alpha-1)$-connected and $X$ is contractible, it therefore follows from Mayer-Vietoris and Hurewicz Theorems (and van Kampen Theorem in degree $1$) that $\\Baseup$ is $(\\alpha-1)$-connected.   Then a direct application of Theorem~\\ref{thm-AdjRebStack} shows that the Artin group $A$ has the cheap $\\alpha$-rebuilding property as soon as it is residually finite.\n\\end{proof}\n\n\\begin{example}\nThe direct product $\\Gamma=\\FF_2\\times \\FF_2\\times \\cdots \\times \\FF_2$ of $k$ copies of the free group on $2$ generators has the cheap $(k-1)$-rebuilding property.\nThis follows from Theorem~\\ref{Ex:Artin groups} when $k\\geq 2$.\nThe nerve of these right-angled Artin groups has the homotopy type of a $(k-1)$-dimensional sphere.\n\n\\end{example}\n\n\n\\begin{remark}\nIn fact, to obtain Theorem \\ref{Ex:Artin groups}, instead of the $K(\\pi,1)$ conjecture, it's enough to assume that the modified Deligne complex $X$ is $(\\alpha-1)$-connected: this is the condition in the proof for $\\Baseup$ to be $(\\alpha-1)$-connected.\n\n\\end{remark}\n\n\n\n\n\n\n\\subsection{Homology growth}\n\n\\begin{theorem}\\label{prop-CheapRebTorsion}\nLet $\\Gamma$ be a finitely presented residually finite group of type $F_{\\alpha+1}$ that has the cheap $\\alpha$-rebuilding property for some non-negative integer $\\alpha$. Then, for every Farber sequence $(\\Gamma_n)_{n \\in \\mathbb N}$, coefficient field $K$ and $0\\leq j \\leq \\alpha$ we have \n\\[ \\lim_{n\\to \\infty} \\frac{\\dim_K H_j (\\Gamma_n , K)}{[\\Gamma : \\Gamma_n]} =0 \\quad \\mbox{and} \\quad \\lim_{n\\to \\infty} \\frac{\\log | H_j (\\Gamma_n,\\mathbb Z)_{\\rm tors}|}{[\\Gamma:\\Gamma_n]}=0.\\]\n\\end{theorem}\n\\begin{proof}\nLet $X$ be a $K(\\Gamma , 1)$ CW-complex with finite $\\alpha$-skeleton. Let $T\\geq 1$. Then there exists a $\\Gamma$-Farber neighborhood $U\\subseteq \\Sub^{\\findex}_\\Gamma$ such that for any finite index subgroup $\\Gamma_1 \\in U$ there exists a $\\alpha$-rebuilding $(X_1 = \\Gamma_1 \\bs \\tilde X, X_1 ')$ of quality $(T,O(1)).$ In degree $\\leq \\alpha$, the complex \n$X'_1$ has $O([\\Gamma:\\Gamma_1 ]/T)$ cells of dimension and the norms of the boundary maps are $O(T+1).$ It follows from \\eqref{E:bettiK} that for all $j \\leq \\alpha$ we have \n$$\\dim_K H_j (\\Gamma_n , K) = O ( [\\Gamma: \\Gamma_1 ]T^{-1} ).$$\nLetting $T\\to\\infty$ this proves the first part of the theorem. By Proposition \\ref{Prop: Gabber} we similarly get \n\\[ \\log | H_j (\\Gamma_1,\\mathbb Z)_{\\rm tors}| \\leq O( [\\Gamma:\n\\Gamma_1 ]T^{-1}\\log(T+1)), \\] \nfor $j=0,\\ldots, \\alpha-1$. \nLetting $T\\to\\infty$ this proves the second part of the theorem for all $0\\leq j \\leq \\alpha-1$. \n\nTo handle the remaining case $j=\\alpha$, we take advantage of the fact that in Proposition \\ref{Prop: Gabber} the only information needed from dimension $\\alpha +1$ is the norm of the boundary maps $\\p_{\\alpha +1}$ (but not the number of $(\\alpha+1)$-cells) and that the norm of the boundary maps is bounded when taking coverings. We proceed exactly as in the end of the proof of Theorem \\ref{Tmain}: by adding finitely many  $(\\alpha +1)$-cells to $X$, we can make it $\\alpha$-aspherical (see \n\\cite[Theorem 8.2.1]{Geoghegan}). Write $\\Basedown=\\sqcup_{I} \\mathbb B^{\\alpha +1}$ for this collection of $(\\alpha +1)$-cells and let $f\\colon \\partial \\Basedown=\\sqcup_{I} \\mathbb S^{\\alpha}\\to X^{(\\alpha)}$ be the map that attaches the $(\\alpha +1)$-cells to $X$. Then $X^+:=X\\sqcup_f \\Basedown$ is a finite $\\alpha$-aspherical \nCW-complex with fundamental group $\\Gamma$.\n\nLet $T,U$ be as above and let $\\Gamma_1\\in U$ be a finite index subgroup. Let $X_1 =\\Gamma_1 \\bs \\tilde X$ and let $(X_1 ,X_1' ,\\mathbf g , \\mathbf h ,\\mathbf P )$ be an $\\alpha$-rebuilding of quality $(T,O(1))$ given by the rebuilding property of $\\Gamma$.\nWe also consider  $X_1^+ =\\Gamma_1 \\bs \\widetilde{X^+}$. The CW-complex $X_1^+$ can be written as  $X_1^+ =X_1 \\sqcup_{f_1} \\Basedown_1$ where $\\Basedown_1 = \\sqcup_{I_1} \\mathbb B^{\\alpha+1}$ is the preimage of $\\Basedown$ in $X_1^+$ and $f_1 \\colon \\partial \\Basedown_1 =\\sqcup_{I_1} \\mathbb S^{\\alpha}\\to X_1^{(\\alpha)}$ is the lift of $f$.  We have a diagram \n\\begin{equation*}\n\\begin{tikzcd}[sep=large]\n\\Basedown_1 \\arrow[d,\"\\mathrm{id}\",swap, shift right] &\\arrow[l,hook] \\partial \\Basedown_1 \\arrow[d,swap,\"\\mathrm{id}\",shift right] \\arrow[r,\"f_1\"] & X_1 \\arrow[d,swap,\"\\mathbf g\", shift right]\\\\\n\\Basedown_1 \\arrow[u,\"\\mathrm{id}\",swap, shift right]&\\arrow[l,hook] \\partial \\Basedown_1 \\arrow[r,\"\\varphi\"] \\arrow[u,\"\\mathrm{id}\",swap, shift right] & X_1' \\arrow[u,\"\\mathbf h\",swap, shift right],\n\\end{tikzcd}\n\\end{equation*} \nwith $\\varphi =\\mathbf g \\circ f_1$. By Proposition \\ref{lem-QGluing} the space $X_1 ' \\sqcup_{\\varphi} \\Basedown_1$ is homotopy equivalent to \n$$X_1 \\sqcup_{f_1} \\Basedown_1 = X_1^+ .$$ \nTherefore, $H_{\\alpha}(\\Gamma_1 ,\\mathbb Z)=H_{\\alpha}(X_1' \\sqcup_{\\varphi} \\Basedown_1 ).$  The map $f_1$ being of lift of $f$, its norm satisfies $\\|f_1 \\|\\leq \\|f\\|=O(1)$. The norm of the boundary map on $X_1' \\sqcup_{\\varphi} \\Basedown_1$ in degree $\\alpha+1$ is bounded by $\\|\\varphi\\|\\leq \\|f_1 \\|\\|g \\|=O(T+1)$.  By Proposition \\ref{Prop: Gabber} we have \n\\[ \\log | H_{\\alpha}(X_1' \\sqcup_{\\varphi} \\Basedown_1 ,\\mathbb Z)_{\\rm tors}|\\leq \\|\\partial_{\\alpha+1}\\|_{(X_1' \\sqcup_{\\varphi} \\Basedown_1 )}\n\\times \\card{\\alpha}(X_1' \\sqcup_{\\varphi} \\Basedown_1 )\\leq O( \\log(T+1) [\\Gamma:\\Gamma_1 ]T^{-1}).\\]\nSince any Farber sequence eventually falls into $U$, we get the theorem by letting $T\\to \\infty$. \n\\end{proof}\n\nWe end this section with a question.\n\\begin{question}\\label{quest: F k+1 amenable have cheap k-reb. prop.?}\nDoes every residually finite amenable group of type $F_{\\alpha}$ have the cheap $\\alpha$-rebuilding property? \n\\end{question}\nIn this context, Kar, Kropholler and Nikolov \\cite{KKN-2021} prove the vanishing of the $j$-th torsion growth for $j\\leq \\alpha$.\n\n\n\n\n\\section{Lattices in semi-simple Lie groups (Proof of Theorem \\ref{T0})} \\label{S10}\n\\label{sect: princ. cong. in semi-simple}\n\n\nLet $\\mathbf G$ be an affine algebraic group defined over $\\Q$. The \\defin{radical} of $\\mathbf G$ is the greatest connected normal solvable subgroup. The group $\\mathbf G$ is called \\defin{semi-simple} if its radical is trivial. In the following, we assume that $\\mathbf G$ is connected and semi-simple. \n\nAn algebraic group $\\mathbf T$ over $\\Q$ is an \\defin{algebraic torus} if the group of its complex points $\\mathbf{T} (\\C)$ is isomorphic to a product of $\\GL_1 (\\C )$. If $\\mathbf T$ is isomorphic to a product of $\\GL_1$ themselves over $\\Q$, then $\\mathbf T$ is said to be \\defin{split} over $\\Q$. \nThe maximal dimension of a torus in $\\mathbf G$ that is split over $\\Q$ is called the \\defin{rational rank} of $\\mathbf G$. \n\nA Zariski-closed subgroup $\\mathbf P$ of $\\mathbf G$ is called \\defin{parabolic} if $\\mathbf P$ contains a connected solvable subgroup, i.e., a \\defin{Borel subgroup} of $\\mathbf G$. If $\\mathbf P$ is defined over $\\Q$, then $\\mathbf P$ is called a rational parabolic subgroup.  The \\defin{unipotent radical} of $\\mathbf P$ is defined to be its largest normal subgroup consisting entirely of unipotent elements.\n\nA $\\Z$-structure of $\\mathbf G$ is given by a faithful $\\Q$-embedding of $\\mathbf G$ in some $\\GL_n$. We define $\\Gamma = \\mathbf{G}_\\Z$ to be the intersection of $\\mathbf G$ with $\\GL_n (\\Z)$ and the level $N$ \\defin{principal congruence subgroup} $\\Gamma (N)$ to be the intersection of $\\mathbf G$ with the kernel of $\\GL_n (\\Z) \\to \\GL_n (\\Z / N \\Z)$. Here $N$ is some positive integer.\n\n\\begin{theorem}\\label{TorsionSL}\nLet $\\mathbf G$ be a connected semi-simple affine algebraic group defined over $\\Q$ equipped with a $\\Z$-structure. Let $r$ be the rational rank of $\\mathbf G$. Then there exists a constant ${\\kappa}={\\kappa}(\\mathbf{G})$ such that for every \\defin{principal} congruence subgroup $\\Gamma(N) \\leq \\mathbf{G}_\\Z$, every $j\\leq r-1$ and every coefficient field $K$, we have \n\\begin{equation}\n\\dim_K |H_j (\\Gamma (N), K) |= {\\kappa}  N^{(1- \\delta) \\dim \\mathbf{G}} \\quad \\mbox{and} \\quad \\log |H_j (\\Gamma(N),\\Z)_{\\rm tors}| \\leq {\\kappa} N^{(1-\\delta ) \\dim \\mathbf{G}} \\log N ,\n\\end{equation}\nwhere \n$$\\delta = \\mathrm{min} \\left\\{ \\frac{\\dim \\mathbf{U}}{\\dim \\mathbf{G}} \\; : \\; \\mathbf{U} \\mbox{ is the unipotent radical of a parabolic subgroup of } \\mathbf{G} \\right\\} .$$    \n\\end{theorem}\n\n\\begin{remark} \n\\label{rem: case SL(d), computation of delta}\nIn case $\\mathbf{G} = \\SL_d$ we have $r=d-1$ and $\\dim \\mathbf{G} = d^2-1$. Any parabolic subgroup of $\\mathbf{G}$ is conjugated to a subgroup of block upper triangular matrices, and the unipotent radical of such is the subgroup where the block diagonal elements are the identity. It follows that\n$$\\delta = \\frac{d-1}{d^2-1} = \\frac{1}{d+1}.$$\nSince there exists a universal constant $C$ such that \\[1>\\frac{[\\SL_d (\\Z):\\Gamma(N)]}{N^{d^2-1}} >C>0,\\]\nTheorem \\ref{TorsionSL} implies Theorem \\ref{T0}, as well as the congruence case of Theorem \\ref{T000}, of the introduction.\n\\end{remark}\n\n\\begin{proof}[Proof of Theorem~\\ref{TorsionSL}]\nWe first recall the spherical Tits building $\\Delta_\\Q (\\mathbf{G})$ associated with $\\mathbf G$ over $\\Q$ (see \\cite{Tits1,Tits2}); it is a simplicial set whose non-degenerate simplices are in bijection with proper rational parabolic subgroups of $\\mathbf G$. Each proper maximal rational parabolic subgroup corresponds to a vertex of $\\Delta_\\Q (\\mathbf{G})$ and $k+1$ proper parabolic subgroups $\\mathbf{P}_0 , \\ldots , \\mathbf{P}_k$ are the vertices of a $k$-simplex if and only if their intersection $\\mathbf{P}_0 \\cap \\ldots \\cap \\mathbf{P}_k$ is a rational parabolic subgroup, and this simplex corresponds to the parabolic subgroup $\\mathbf{P}_0 \\cap \\ldots \\cap \\mathbf{P}_k$.\n\nIf the rational rank $r$ of $\\mathbf G$ is equal to $1$, then $\\Delta_\\Q (\\mathbf{G})$ is a countable collection of points. Otherwise $\\Delta_\\Q (\\mathbf{G})$ is a spherical building (see \\cite[Theorem 5.2]{Tits2}). For any maximal $\\Q$-split torus $\\mathbf T$, all the rational parabolic subgroups containing $\\mathbf T$ form an apartment in this building and each of these sub-complexes give a simplicial triangulation of the sphere of dimension $\\alpha=r-1$ so that $\\Delta_\\Q (\\mathbf{G})$ has the homotopy type of a bouquet of $\\alpha$-spheres (Solomon--Tits theorem \\cite{Solomon-1969}); in particular it is $(\\alpha-1)$-connected. We refer to \\cite[\\S V.5]{BrownBuildings} for this and more about spherical buildings. \n\nThe rational points $\\mathbf{G} (\\Q)$ of $\\mathbf G$ act on the set of rational parabolic subgroups by conjugation and hence on $\\Delta_\\Q (\\mathbf{G})$: for any $g \\in \\mathbf{G} (\\Q)$ and any rational parabolic $\\mathbf{P}$, the simplex associated to $\\mathbf P$ is mapped to the simplex associated to $g\\mathbf{P} g^{-1}$. Let $\\Gamma \\subset \\mathbf{G} (\\Q)$ be an arithmetic subgroup of $\\mathbf{G} (\\Q)$. Then by reduction theory (see e.g. \\cite[Theorem 4.15]{PlatonovRapinchuk}) there are only finitely many $\\Gamma$-conjugacy classes or rational parabolic subgroups. Therefore, the quotient $\\Gamma \\backslash \\Delta_\\Q (\\mathbf{G})$ is a finite simplicial complex. \n\nFrom now on we let $\\Gamma = \\mathbf{G}_\\Z$, we fix some principal congruence subgroup $\\Gamma (N)$ and let $\\Baseup$ denote the rational Tits building $\\Delta_\\Q (\\mathbf{G})$. Up to replacing $\\Baseup$ by its barycentric subdivision we may furthermore assume that for every cell $\\sigma \\subset \\Baseup$ the stabilizer $\\Gamma_\\sigma$ acts trivially on $\\sigma$. \n\nThe stabilizer $\\Gamma_\\sigma$ of a simplex $\\sigma \\subset \\Baseup$ associated to a rational parabolic subgroup $\\mathbf P$ contains the intersection $\\mathbf{P}_\\Z$ of $\\mathbf P$ with $\\GL_d (\\Z)$. In particular $\\Gamma_\\sigma$ contains the $\\Z$-points $\\mathbf{U} (\\Z)$ of the unipotent radical  $\\mathbf{U}$ of $\\mathbf P$ as a normal subgroup. The group $\\mathbf{U} (\\Z)$ is finitely generated, torsion-free, nilpotent and its intersection with the stabilizer $\\Gamma(N)_\\sigma$ is equal to \n$$\\mathrm{ker} (\\mathbf{U} (\\Z) \\to \\mathbf{U} (\\Z /N \\Z))$$\nwhich is of index at least $N^{ \\delta \\dim \\mathbf{G}}$ in $\\mathbf{U} (\\Z)$. On the other hand it is well known that\n$$[\\Gamma : \\Gamma(N) ] \\leq O( N^{\\dim \\mathbf{G}}),$$\nsee e.g. \\cite[bottom of page 3134]{BelolipetskyLubotsky}. \nIt therefore follows from Theorem \\ref{Tmain} (with $\\Gamma_1=\\Gamma(N)$)\n that there exists \n a constant $c=c(\\mathbf{G})$ that depends on $\\mathbf{G}$ but not on $\\Gamma(N)$ and a finite CW-complex $\\Totaldown_N^+$\n such that the following properties hold. \n\\begin{enumerate}\n\\item The CW-complex $\\Totaldown_N^+$ has a $\\alpha$-connected universal cover.\n\\item In each dimension $\\leq \\alpha$, the total number of cells of $\\Totaldown_N^+$ is bounded by \n\\begin{equation}\\label{eq-NbOfCelles}\nc  N^{(1- \\delta) \\dim \\mathbf{G}} .\n\\end{equation}\n\\item In each degree $\\leq \\alpha +1$ the norm of the boundary operator on the chain complex $C_\\bullet (\\Totaldown_N^+ )$ is bounded by \n\\begin{equation}\nc N^{c} .\n\\end{equation}\n\\end{enumerate}\nWe conclude by applying \\eqref{E:bettiK} and Proposition \\ref{Prop: Gabber}. Let ${\\kappa}= c^2 \\log c$. Then, in each degree $j \\leq \\alpha$ we have both\n$$\\dim_K |H_j (\\Gamma (N), K) | \\leq {\\kappa} N^{(1- \\delta) \\dim \\mathbf{G}} \\quad \\mbox{and} \\quad \\log |H_j (\\Gamma(N) )_{\\rm tors} | \\leq {\\kappa} N^{(1- \\delta) \\dim \\mathbf{G}} \\log N .$$\n\\end{proof}\n\nWe used the fact that the stabilizers of cells in the action of $\\Gamma$ on $\\Baseup$ have infinite normal unipotent subgroups. Hence, by Lemma \\ref{cor-CheapRebGroups} they all have the cheap $\\alpha$-rebuilding property, for every $\\alpha$. Upon applying Theorem~\\ref{thm-AdjRebStack} to the action of $\\Gamma$ on the rational Tits building $\\Baseup$ we get\n\\begin{theorem}\\label{thm-LatticesRebuilding}\nLet $\\mathbf G$ be a connected semi-simple affine algebraic group defined over $\\Q$ equipped with a $\\Z$-structure. Let $r$ be the rational rank of $\\mathbf G$. Then $\\mathbf{G}_\\Z$ has the cheap $(r-1)$-rebuilding property. \n\\end{theorem}\nTheorem \\ref{prop-CheapRebTorsion} now yields.\n\\begin{theorem}\\label{thm-LatticesFarberTors}\nLet $\\mathbf G$ be a connected semi-simple affine algebraic group defined over $\\Q$ equipped with a $\\Z$-structure. Let $r$ be the rational rank of $\\mathbf G$. Then for any Farber sequence $(\\Gamma_n)_{n\\in \\mathbb N}$ in $\\mathbf{G}_\\Z $ and for any coefficient field $K$ we have \n\\[ \\lim_{n\\to\\infty} \\frac{\\dim_K H_j (\\Gamma_n , K)}{[\\Gamma : \\Gamma_n]} = 0 \\quad \\mbox{and} \\quad \\lim_{n\\to\\infty} \\frac{\\log | H_j(\\Gamma_n,\\mathbb Z)_{\\rm tors}|}{[\\mathbf{G}_\\Z :\\Gamma_n]}=0,\\] \nfor $j=0,\\ldots, r-1$.\n\\end{theorem}\nThis implies Theorem \\ref{T0} and the first part of Theorem \\ref{T000} of the introduction. \n\n\\section{Application to mapping class groups (Proof of Theorem \\ref{T0000})} \\label{Sect: Application to mapping class groups}\n\n\nLet ${S}$ be a closed orientable surface of genus $g$ with $b$ connected boundary components. We assume that \n$$\\chi ({S} ) = 2-2g - b <0  \\quad \\mbox{and} \\quad b \\geq 4 \\mbox{ if } g=0.$$  \nWe write $\\MCG({S})$ for the mapping class group of ${S}$. Let us recall the construction of the curve complex $\\mathcal C({S})$. \n\nThe curve complex $\\mathcal C({S})$ is a combinatorial cell complex whose $k$-cells consist of collections of $k+1$ closed, disjoint, essential, pairwise non-homotopic simple closed curves on ${S}$, considered up to homotopy. In particular, a $0$-dimensional cell corresponds to the homotopy class of a simple closed curve and top dimensional cells correspond to maximal families of pairwise non-homotopic essential closed curves in ${S}$. \nIt is equipped with a co-compact $\\MCG({S})$-action.\n\nUnder our hypotheses ${S}$ admits a hyperbolic structure and the size of any maximal family of pairwise non-homotopic essential closed curves in ${S}$ is finite, equal to $3g-3+b$. So the dimension of $\\mathcal C({S})$ is $3g-4+b$. \n\n\n\nNow let $\\mathcal F$ be a finite collection of $m$ closed, disjoint, essential, pairwise non-homotopic simple closed curves on ${S}$. The stabilizer of the homotopy class of $\\mathcal F$ in $\\MCG({S})$, denoted $\\Stab \\mathcal F$, fits into a short exact sequence: \n\\[\\begin{tikzcd}\n1 \\arrow[r]& \\mathbb Z^m \\arrow[r]& \\Stab \\mathcal F\\arrow[r]& \\MCG'({S} \\setminus \\mathcal F)\\arrow[r]& 1, \n\\end{tikzcd}\\] where $\\MCG'({S}\\setminus \\mathcal F)$ is the subgroup of those mapping classes in $\\MCG({S}\\setminus \\mathcal F)$ that\npermute the boundary in such a way that it can still be glued back to obtain $\\mathcal{F}$ in ${S}$\n ; see e.g. \\cite[Equation (1)]{Minsky}. \n The group $\\MCG'({S}\\setminus \\mathcal F)$ has finite index in $\\MCG({S}\\setminus \\mathcal F)$, which is of class $F_k$ for every $k\\geq 0$\n by \\cite[Theorem 5.4.A]{Ivanov}. \nHence the stabilizers $ \\Stab \\mathcal F$ are of class $F_k$  for every $k\\geq 0$. \n \n \nIn order to ensure the assumption \\ref{Cond2} that the cell-stabilizers act freely on them, we consider instead the action of $\\Gamma= \\MCG({S})$ on the barycentric subdivision $\\Baseup$ of $\\mathcal C({S})$. Now the stabilizer of any cell $\\sigma$ of $\\Baseup$ is a finite index subgroup of some $\\Stab \\mathcal F$ as above.\nTherefore $\\Gamma_\\sigma$ is of class $F_\\alpha$  for every $\\alpha \\geq 0$ and it has a finite rank normal free abelian subgroup,\n  so by Corollary~\\ref{cor-CheapRebGroups} it has the cheap $\\alpha$-rebuilding property for every $\\alpha$. \n  \n The homotopy type of $\\mathcal C({S})$ (and therefore that of $\\Baseup$) was identified by Harer \\cite[Thm 3.5]{Harer}; it is homotopy equivalent to a bouquet of spheres of dimension $\\alpha (g,b)$ where \n$$\\alpha (g,b):=\\left\\{ \\begin{array}{ll}\n2g-2 & \\mbox{ if } g\\geq 2 \\mbox{ and } b=0; \\\\\n2g-3+b & \\mbox{ if } g \\geq 1 \\mbox{ and } b \\geq 1; \\\\ \nb-4 &  \\mbox{ if } g=0  \\mbox{ and } b\\geq 4.\n\\end{array} \\right.$$\nThus  the space $\\Baseup$ is $(\\alpha(g,b)-1)$-connected.\nBy Theorem~\\ref{thm-AdjRebStack}, we obtain:\n\\begin{theorem}\\label{th: MCG has CRP k(g,b)}\nThe mapping class group $\\MCG({S})$ has the cheap $\\alpha(g,b)$-rebuilding property. \n\\end{theorem}\nBy \\cite[Theorem 5.4.A]{Ivanov}, $\\MCG({S})$ is of class $F_\\alpha$ for every $\\alpha \\geq 0$.\nApplying Proposition \\ref{prop-CheapRebTorsion} finishes the proof of Theorem \\ref{T0000}. \n\\hfill $\\square$\n\n\\medskip\nCuriously we had a hard time trying to apply our method to $Out(\\mathbf{F}_n)$, thus our question:\n\\begin{question}\nWhat is the range of $\\alpha$ for which $Out(\\mathbf{F}_n)$ has the cheap $\\alpha$-rebuilding property?\n\\end{question}\n\n\n\\bibliographystyle{alpha}\n\n\\newcommand{\\etalchar}[1]{$^{#1}$}\n", "meta": {"timestamp": "2021-06-25T02:20:01", "yymm": "2106", "arxiv_id": "2106.13051", "language": "en", "url": "https://arxiv.org/abs/2106.13051"}}
{"text": "\\section{Introduction}\r\n\\pagenumbering{arabic}\r\nIn this paper, we study the actions of real reductive groups on real submanifolds of \\Keler manifolds.\r\n\r\nLet $U$ be a compact connected Lie group with Lie algebra $\\mathfrak{u}$ and let $U^\\C$ be its complexification. We say that a subgroup $G$ of $U^\\C$ is\r\ncompatible if $G$ is closed and the map $K\\times \\mathfrak{p} \\to G,$ $(k,\\beta) \\mapsto k\\text{exp}(\\beta)$ is a diffeomorpism where $K := G\\cap U$\r\nand $\\mathfrak{p} := \\mathfrak{g}\\cap \\text{i}\\mathfrak{u};$ $\\mathfrak{g}$ is the Lie algebra of $G.$ The Lie algebra $\\mathfrak{u}^\\C$ of $U^\\C$ is the direct\r\nsum $\\mathfrak{u}\\oplus i\\mathfrak{u}.$ It follows that $G$ is compatible with the Cartan decomposition $U^\\C = U\\text{exp}(\\text{i}\\mathfrak{u})$\r\n(see Section \\ref{comp-subgrous}), $K$ is a maximal compact subgroup of $G$ with Lie algebra $\\mathfrak{k}$ and that\r\n$\\mathfrak{g} = \\mathfrak{k}\\oplus \\mathfrak{p}.$\r\n\r\nLet $(Z,\\omega)$ be a \\Keler manifold with an holomorphic action of the complex reductive group $U^\\C$. We also assume $\\omega$ is $U$-invariant and that there is a $U$-equivariant moment mapping $\\mu : Z \\to \\mathfrak{u}^*.$ By definition, for any $\\xi \\in \\mathfrak{u}$ and $z\\in Z,$ $d\\mu^\\xi = i_{\\xi_Z}\\omega,$ where $\\mu^\\xi(z) := \\mu(z) (\\xi)$, and $\\xi_Z$ denotes the fundamental vector field induced on $Z$ by the action of $U,$\r\n$$\r\n\\xi_Z(z) := \\frac{d}{dt}\\bigg \\vert_{t=0} \\text{exp}(t\\xi)\\cdot z.\r\n$$\r\n\r\n\r\nThe inclusion i$\\mathfrak{p}\\hookrightarrow \\mathfrak{u}$ induces by restriction, a $K$-equivariant map $\\mu_{\\text{i}\\mathfrak{p}} : Z \\to (\\text{i}\\mathfrak{p})^*.$ Using a $U$-invariant inner product on $\\mathfrak u^\\C$ to  identify $(\\text{i}\\mathfrak{p})^*$ and $\\mathfrak{p},$ so $\\mu_{\\text{i}\\mathfrak{p}}$ can be viewed as a map $\\mu_{\\mathfrak{p}} : Z \\to \\mathfrak{p}.$  For $\\beta \\in \\mathfrak{p}$ let $\\mu_\\mathfrak{p}^\\beta$ denote $\\mu^{-\\text{i}\\beta}.$ i.e., $\\mu_\\mathfrak{p}^\\beta(z) = -\\sx\\mu(z), i\\beta\\xs.$  Then grad$\\mu_\\mathfrak{p}^\\beta = \\beta_Z,$ where grad is computed with respect to the Riemannian metric induced by the \\Keler structure. The map $\\mu_\\mathfrak{p}$ is called the gradient map associated with $\\mu$ (see Section \\ref{subsection-gradient-moment}). For a $G$-stable locally closed real submanifold $X$ of $Z,$ we consider $\\mu_\\mathfrak{p}$ as a mapping $\\mu_\\mathfrak{p} : X\\to \\mathfrak{p}.$ Using the inner product  on $\\mathfrak p\\subset i\\mathfrak u$, we define the norm square of $\\mu_\\mathfrak{p}$ by\r\n$$f(x) := \\frac{1}{2}\\parallel \\mu_\\mathfrak{p}(x) \\parallel^2; \\quad x \\in X.$$\r\nThe set of semistable points associated with the critical points of the norm square of $\\mu_\\mathfrak{p}$ was studied in great details in \\cite{heinzner-schwarz-stoetzel}. The norm square of $\\mu_\\mathfrak{p}$ is in general far from being Morse-Bott and it's critical sets may be very complicated. But can we have a particular case when the norm square will be Morse-Bott? We find this question to be true for two orbits variety. From now on, we always assume that $X$ is connected and compact.\r\n\r\nSuppose that the action of $G$ on $X$ has two orbits. $X$ is called a two orbit variety. S. Cupit-Foutou obtained the classification of a complex algebraic varieties on which a reductive complex algebraic group acts with two orbits \\cite{Cupit-Foutou}. Applying standard\r\nMorse-theoretic results in \\cite{Kirwan} and \\cite{heinzner-schwarz-stoetzel}, we prove that the norm square is Morse-Bott and obtain information on the\r\ncohomology and $K$-equivariant cohomology of $X$ (Theorem \\ref{two-orbit-Morse}), generalizing \\cite{Anna}.\r\n\r\nA central ingredient to prove this result is the Ness Uniqueness Theorem which asserts that any two critical points of $f$ in the same $G$-orbit in fact belong to the same $K$-orbit (Theorem \\ref{critt}). Moreover,  although we do not assume that $X$ is real analytic manifold, we point out that for any $G$-invariant compact and connected submanifold of $Z$, the Lojasiewicz gradient inequality holds for the norm square. Therefore, the limit of the negative gradient flow exists and it is unique and any $G$-orbit collapses to a single $K$-orbit (Theorem \\ref{corr}).\r\n We use the original ideas from \\cite{Salamon} in a different context. By the stratification theorem, we have\r\n$$\r\n\\{p\\in X : \\overline{G \\cdot p}\\cap \\mu_\\mathfrak{p}^{-1}(0) \\neq \\emptyset \\} = \\{p\\in X : \\lim_{t\\to +\\infty}\\phi_t(p)\\in \\mu_\\mathfrak{p}^{-1}(0)\\} = S_G(\\mu_\\mathfrak{p}^{-1}(0)),\r\n$$\r\nwhere $\\phi_t(p)$ is the flow of the vector field -grad$f.$ Then, there exist a $K$-equivariant strong deformation of $S_G(\\mu_\\mathfrak{p}^{-1}(0))$ onto the set $\\mu_\\mathfrak{p}^{-1}(0)$ (Theorem \\ref{retraction-theorem}). Hence no analyticity assumption is necessary in the statement of the retraction Theorem answering Question $1$ in \\cite[$p.219$]{heinzner-stoetzel}.\r\n\r\nBiliotti and Ghigi \\cite{LG} proved a convexity theorem along orbits in a very general setting using only so-called Kempf-Ness function. The behaviour of the corresponding gradient map is encoded in the Kempf-Ness function. Recently, Biliotti \\cite{LB} gives a new proof of the Hilbert-Mumford criterion for real reductive Lie groups stressing the properties of the Kempf-Ness functions. He shows that the Kempf-Ness function is Morse-Bott and it is convex along geodesics for the action of a linear group on $\\mathbb{P}(V)$ where $V$ is a finite dimensional dimensional vector space. We prove this result in a general setting.\r\n\r\nResults on convexity theorems are obtained. Let $\\mathfrak{a}\\subset\\mathfrak{p}$ be an Abelian subalgebra. Let $\\mu_\\mathfrak{a} : X\\to \\mathfrak{a}$ denote the gradient map of $A = \\text{exp}(\\mathfrak{a}).$ If $\\pi_\\mathfrak{a} : \\mathfrak{p} \\to \\mathfrak{a}$ is the orthogonal projection, then $\\mu_\\mathfrak{a} = \\pi_\\mathfrak{a} \\circ \\mu_\\mathfrak{p}.$ Although there is no counterexample, we do not know if the Abelian Convexity Theorem holds for any $G$-invariant connected  submanifold (see for instance \\cite{biliotti-ghigi-heinzner-document,LG,heinzner-schuetzdeller} for more details on the subject).\r\nIf $G$ has a unique closed orbit $\\OO,$ then we prove that $\\mu_\\mathfrak{a}(X) = \\mu_\\mathfrak{a}(\\OO)$ and so a polytope. This result is new also if $G = U^\\C$ and $X = Z.$ This means that $\\OO$ captures all the information of the $A$-gradient map. As an application, we prove that the Abelian convexity Theorem holds for a two orbits variety.\r\n\r\nIf $Z$ is connected and compacts,\nand $X\\subset Z$ is a $A$-stable closed coisotropic submanifold, then we prove $\\mu_\\mathfrak{a}(X) = \\mu_\\mathfrak{a}(Z)$ (Theorem \\ref{convexity-coisotropic}) and so it is a polytope as well. More precisely, there exists an open and dense subset $W$ of $X$  such that for any $p\\in W,$ we have\r\n$\\mu_\\mathfrak{a}(X) = \\mu_\\mathfrak{a}(\\overline{A\\cdot p})$. Finally, If $X$ is a two orbits variety, then we prove Non-Abelian convexity Theorem (Theorem \\ref{NACT}).\r\n\r\n\r\n\\section{Preliminaries}\r\n\r\n \\subsection{Convex geometry}\r\n\\label{pre-convex}\r\n\r\n\r\n In this section, some definitions and results in convex geometry are recalled. The reader can see e.g. \\cite{schneider-convex-bodies} and \\cite{biliotti-ghigi-heinzner-1-preprint} for further details on the topic. Let $V$ be a real vector space with a scalar product $\\scalo$ and let $E\\subset V$ be a \\changed{compact} convex subset. The \\emph{relative interior} of $E$, denoted by $\\relint E$, is the interior of $E$ in its affine hull. For $a,b\\in E,$ denote the close segment joining $a$ and $b$ by $[a,b]$. Then, a face of $E$ is a convex subset $F$ of $E$ such that if $a,b\\in E$ and $\\relint[a,b]\\cap F\\neq \\vacuo$, then\r\n $[a,b]\\subset F$.  The \\emph{extreme points} of $E$ denoted by $\\ext E$ are the points\r\n $a\\in E$ such that $\\{a\\}$ is a face. Since  $E$ is compact the faces\r\n are closed \\cite[p. 62]{schneider-convex-bodies}. The empty set and $E$ are faces of $E:$ the other faces are called \\enf{proper}.\r\n \\begin{definition}\r\n  The support function of $E$ is defined by the function $ h_E : V \\ra \\R$, $\r\n h_E(u) = \\max \\{\\sx x, u \\xs : x\\in E\\}$.  If $ u\\neq 0$, the\r\n hyperplane $H(E, u) : = \\{ x\\in E : \\sx x, u \\xs = h_E(u)\\}$ is\r\n called the supporting hyperplane of $E$ for $u$.\r\n\r\n\r\n The set\r\n   \\begin{gather}\r\n     \\label{def-exposed}\r\n     F_u (E) : = E \\cap H(E,u)\r\n   \\end{gather}\r\n   is a face and it is called the \\enf{exposed face} of $E$ defined by\r\n   $u$.\r\n \\end{definition\n\n\n\n\r\n Intuitively, the meaning of the support function is simple. For instance, consider a nonempty closed convex set $E\\subset \\mathbb{R}^n.$ Then for a unit vector $u\\in S^{n-1}\\cap \\text{dom}h_E,$ the supporting function $h_E(u)$ is the signed distance of the support plane to $E$ with exterior normal vector $u$ from the origin; the distance is negative if and only if $u$ points into the open half-space containing the origin.  In general not all faces of a convex subset are exposed. For instance, consider the convex hull of a closed disc and a point outside the disc: the resulting convex set is the union of the\r\n disc and a triangle. The two vertices of the triangle that lie on the\r\n boundary of the disc are non-exposed 0-faces.\r\n\r\n A subset $E\\subset V$ is called a \\emph{convex cone} if $E$ is convex, non empty and closed under multiplication by non negative real numbers.\r\nThe following results about a compact convex set $E$ and it's faces are recalled from \\cite{biliotti-ghigi-heinzner-1-preprint}\r\n \\begin{lemma}\r\n   \\label{u-cono}\r\n   If $F \\subset E$ is an exposed face, the set $\\CF : = \\{ u\\in V:\r\n   F=F_u(E) \\}$ is a convex cone. If $G$ is a compact subgroup of\r\n   $O(V)$ that preserves both $E$ and $F$, then $\\CF$ contains a fixed\r\n   point of $G$.\r\n \\end{lemma}\r\n\\begin{theorem} [\\protect{\\cite[p. 62]{schneider-convex-bodies}}]\r\n   \\label{schneider-facce} If $E$ is a compact convex set and\r\n   $F_1,F_2$ are distinct faces of $E$, then $\\relint F_1 \\cap \\relint\r\n   F_2=\\vacuo$. If $G$ is a nonempty convex subset of $ E$ which is\r\n   open in its affine hull, then $G \\subset\\relint F$ for some face\r\n   $F$ of $E$. Therefore $E$ is the disjoint union of \\changed{the\r\n     relative interiors of its} faces.\r\n \\end{theorem}\r\n\\begin{proposition}\\label{convex-criterium}\r\nLet $C_1 \\subseteq C_2$ be two compact convex set of $V$. Assume that for any $\\beta \\in V$ we have\r\n\\[\r\n\\mathrm{max}_{y\\in C_1} \\langle y , \\beta \\rangle=\\mathrm{max}_{y\\in C_2} \\langle y , \\beta \\rangle.\r\n\\]\r\nThen $C_1=C_2$.\r\n\\end{proposition}\r\n\\begin{proof}\r\nWe may assume without loss of generality  that the affine hull of $C_2$ is $V$.\r\nAssume by contradiction that $C_1 \\subsetneq C_2$. Since $C_1$ and $C_2$ are both compact, it follows that there exists $p\\in \\partial C_1$ such that $p\\in \\stackrel{o}{C_2}$. Since every face of a convex compact set is contained in an exposed face \\cite{schneider-convex-bodies}, there exists $\\beta \\in V$ such that\r\n\\[\r\n\\mathrm{max}_{y\\in C_1} \\langle y , \\beta \\rangle=\\langle p, \\beta \\rangle.\r\n\\]\r\nThis means the linear function $x\\mapsto \\langle x, \\beta \\rangle$ restricted on $C_2$ achieves its maximum at an interior point which is a contradiction.\r\n\\end{proof}\r\n\r\n\r\n\r\n \\subsection{Compatible subgroups}\r\n\\label{comp-subgrous}\r\nIn this section, the notion of compatible subgroup is discussed.\nLet $U$ be a Lie group with Lie algebra $\\mathfrak{u}$ and $E, F \\subset \\mathfrak{u}$. Then, we set\r\n \\begin{gather*}\r\n   E^F :=     \\{\\eta\\in E: [\\eta, \\xi ] =0, \\forall \\xi \\in F\\} \\\\\r\n   G^F = \\{g\\in G: \\Ad g (\\xi ) = \\xi, \\forall \\xi \\in F\\}.\r\n \\end{gather*}\r\n If $F=\\{\\beta\\}$ we write simply $E^\\beta$ and $G^\\beta$. Suppose $U$ is a\r\n compact Lie group and $U^\\C$ its universal complexification. $U^\\C$\r\n is a linear reductive complex algebraic group with lie algebra $\\mathfrak{u}^\\C$. $\\mathfrak{u}^\\C$ have the Cartan decomposition\r\n $$\r\n \\mathfrak{u}^\\C = \\mathfrak{u} + i\\mathfrak{u}\r\n $$ with a conjugation map $\\theta : \\liu^\\C \\ra \\liu^\\C$ and the corresponding group isomorphism $\\theta : U^\\C \\ra\r\n   U^\\C$.  Let $f: U \\times i\\liu \\ra U^\\C$ be the diffeomorphism\r\n $f(g, \\xi) = g \\text{exp}  \\xi$. The decomposition $U^\\C = U\\ext(i\\mathfrak{u})$ is refered to as the Cartan decomposition.\r\n\r\n Let $G\\subset U^\\C$ be a closed real\r\n subgroup of $U^\\C$ We say\r\n that $G$ is \\enf{compatible} with the Cartan decomposition of $U^\\C$ if $f (K \\times \\liep) = G$ where $K:=G\\cap U$ and $\\liep:= \\lieg \\cap i\\liu$. The\r\n restriction of $f$ to $K\\times \\liep$ is then a diffeomorphism onto\r\n $G$. It follows that $K$ is a maximal compact subgroup of $G$ and\r\n that $\\lieg = \\liek \\oplus \\liep$.  Note that $G$ has finitely many\r\n connected components. Since $U$ can be embedded in $\\Gl(N,\\C)$ for\r\n some $N$, and any such embedding induces a closed embedding of\r\n $U^\\C$, any compatible subgroup is a closed linear group. Moreover\r\n $\\lieg$ is a real reductive Lie algebra, hence $\\lieg =\r\n \\liez(\\lieg)\\oplus [\\lieg, \\lieg]$. Denote by $G_{ss}$ the analytic\r\n subgroup tangent to $[\\lieg, \\lieg]$. Then $G_{ss}$ is closed and\r\n $G^o=Z(G)^o\\cd G_{ss}$ \\cite[p. 442]{knapp-beyond}, where $G^o$ is the connected component of $G$ containing $e$ and $Z(G)^o$ is the connected component of the center of $G$ containing $e$.\r\n\\begin{lemma}[\\protect{\\cite[Lemma 7]{LA}}]\r\n\\label{lemcomp}\r\n   \\begin{enumerate}\r\n   \\item \\label {lemcomp1} If $G\\subset U^\\C$ is a compatible\r\n     subgroup, and $H\\subset G$ is closed and $\\theta$-invariant,\r\n     then $H$ is compatible if and only if $H$ has only finitely many connected components.\r\n   \\item \\label {lemcomp2} If $G\\subset U^\\C$ is a connected\r\n     compatible subgroup, then $G_{ss}$ is compatible.\r\n     \\item \\label{lemcomp3} If $G\\subset U^\\C$ is a compatible\r\n       subgroup, and $E\\subset \\liep$ is any subset, then $G^E$ is\r\n       compatible.\r\n   \\end{enumerate}\r\n \\end{lemma}\r\n\n \n  \n   \n    \n    \n    \n   \n   \n   \n   \n   \n   \n   \n   \n   \n    \n  \n   \n  \n  \n \n \n\n\n\n\n\n \n \n\r\n\r\n\r\n\\subsection{Parabolic subgroups}\r\nLet $G \\subset U^\\C$ be a compatible, and let $\\lieg = \\liek \\oplus \\liep$ be the corresponding decomposition.  A subalgebra $\\lieq \\subset \\lieg$ is \\enf{parabolic} if\r\n$\\lieq^\\C$ is a parabolic subalgebra of $\\lieg^\\C$.  One way to\r\ndescribe the parabolic subalgebras of $\\lieg$ is by means of\r\nrestricted roots.  If $\\lia \\subset \\liep$ is a maximal subalgebra,\r\nlet $\\roots(\\lieg, \\lia)$ be the (restricted) roots of $\\lieg$ with\r\nrespect to $\\lia$, let $\\lieg_\\la$ denote the root space corresponding\r\nto $\\la$ and let $\\lieg_0 = \\liem \\oplus \\lia$, where $\\liem =\r\n\\liez_\\liek(\\lia)$.  Let $\\simple \\subset \\roots(\\lieg, \\lia)$ be a\r\nbase and let $\\roots_+$ be the set of positive roots. If $I\\subset\r\n\\simple$ set $\\roots_I : = \\spam(I) \\cap \\roots$. Then\r\n\\begin{gather}\r\n  \\label{para-dec}\r\n  \\lieq_I:= \\lieg_0 \\oplus \\bigoplus_{\\la \\in \\roots_I \\cup \\roots_+}\r\n  \\lieg_\\la\r\n\\end{gather}\r\nis a parabolic subalgebra. Conversely, if $\\lieq \\subset \\lieg$ is a\r\nparabolic subalgebra, then there are a maximal subalgebra $\\lia\r\n\\subset \\liep$ contained in $\\lieq$, a base $\\simple \\subset\r\n\\roots(\\lieg, \\lia)$ and a subset $I\\subset \\simple $ such that $\\lieq\r\n= \\lieq_I$.  We can further introduce\r\n\\begin{gather}\r\n\\label{notaz-I}\r\n\\begin{gathered}\r\n  \\lia_I : = \\bigcap_{\\la \\in I} \\ker \\la \\qquad \\lia^I := \\lia_I^\\perp \\\\\r\n  \\lien_I = \\bigoplus_{\\la \\in \\roots_+ \\setminus \\roots_I} \\lieg_\\la\r\n  \\qquad \\liem_I : = \\liem \\oplus \\lia^I \\oplus \\bigoplus_{\\la \\in\r\n    \\roots_I}\\lieg_\\la.\r\n\\end{gathered}\r\n\\end{gather}\r\nThen $\\lieq_I = \\liem_I \\oplus \\lia_I \\oplus \\lien_I$. Since\r\n$\\theta\\lieg _ \\la = \\lieg_{-\\la}$, it follows that $ \\lieq_I \\cap\r\n\\theta\\lieq_I = \\lia_I \\oplus \\liem_I$.  This latter \\changed{Lie algebra} coincides\r\nwith the centralizer of $\\lia_I$ in $\\lieg$. It is a \\enf{Levi factor}\r\nof $\\lieq_I$ and\r\n\\begin{gather}\r\n  \\label{liaI}\r\n  \\lia_I =\\liez (\\lieq_I \\cap \\theta\\lieq_I) \\cap \\liep.\r\n\\end{gather}\r\nAnother way to describe parabolic subalgebras of $\\lieg$ is the\r\nfollowing.  If $\\beta \\in \\liep$, the endomorphism $\\ad \\beta \\in \\End\r\n\\lieg$ is diagonalizable over $\\R$. Denote by $ V_\\la (\\ad \\beta) $ the\r\neigenspace of $\\ad \\beta$ corresponding to the eigenvalue $\\la$.  Set\r\n\\begin{gather*}\r\n  \\lieg^{\\beta+}: = \\bigoplus_{\\la \\geq 0} V_\\la (\\ad \\beta).\r\n\\end{gather*}\r\n\\begin{lemma}\r\n  \\label{para-beta}\\cite[Lemma 8]{LA}\r\n  For any $\\beta$ in $ \\liep$, $\\lieg^{\\beta+}$ is a parabolic\r\n  subalgebra of $\\lieg$.  If $\\lieq \\subset \\lieg$ is a parabolic\r\n  subalgebra, there is some vector $\\beta \\in \\liep$ such that $\\lieq\r\n  = \\lieg^{\\beta+}$.  The set of all such vectors is an open convex\r\n  cone in\r\n$\\liez(\\lieq \\cap \\theta\\lieq) \\cap \\liep$.\r\n\\end{lemma}\r\n \n \n \n \n \n\n \n\n\n \n   \n   \n     \n\n \n \n \n \n \n  \n   \n   \nA \\enf {parabolic subgroup} of $G$ is a subgroup of the form $Q =\r\nN_G(\\lieq)$ where $\\lieq $ is a parabolic subalgebra of $\\lieg$.\r\nEquivalently, a parabolic subgroup of $G$ is a subgroup of the form\r\n$P\\cap G$ where $P$ is parabolic subgroup of $G^\\C$ and $\\liep$ is the\r\ncomplexification of a subspace $\\lieq \\subset \\lieg$.  If $\\beta\r\n\\in \\liep$ set\r\n\\begin{gather*}\r\n\\begin{gathered}\r\n  G^{\\beta+} :=\\{g \\in G : \\lim_{t\\to - \\infty} \\text{exp} ({t\\beta}) g\r\n  \\text{exp} ({-t\\beta}) \\text { exists} \\}\\\\\r\n  R^{\\beta+} :=\\{g \\in G : \\lim_{t\\to - \\infty} \\text{exp} ({t\\beta})\r\n  g \\text{exp} ({-t\\beta}) =e \\}\\\\\r\n  G^\\beta=\\{g\\in G:\\, \\mathrm{Ad}(g)(\\beta)=\\beta\\}\r\n\\end{gathered}\r\n\\qquad\r\n  \\lier^{\\beta+}: = \\bigoplus_{\\la > 0} V_\\la (\\ad \\beta).\r\n\\end{gather*}\r\nNote that $ \\lieg^{\\beta+}= \\lieg^\\beta \\oplus \\lier^{\\beta+}$.\r\n\\begin{lemma}\\label{parabolicc}\\cite[Lemma 9]{LA}\r\n  $G^{\\beta +} $ is a parabolic subgroup of $G$ with Lie algebra\r\n  $\\lieg^{\\beta+}$.  Every parabolic subgroup of $G$ equals\r\n  $G^{\\beta+}$ for some $\\beta \\in \\liep$.  Finally, $R^{\\beta+}$ is connected and it is the unipotent radical of $G^{\\beta+}$ and $G^\\beta$ is a Levi factor.\r\n\\end{lemma}\r\n\n\n \n \n \n \n  \n \n\n \n \n\n \n\n \n\n \n  \n\n \n \n\n \n \n   \n\\subsection{Gradient map}\r\n\\label{subsection-gradient-moment}\r\nLet $(Z, \\om)$ be a \\Keler manifold. Let $U^\\C$ acts\r\nholomorphically on $Z$ i.e., the action $U^\\C \\times Z \\to Z$ is holomorphic. Assume that $U$ preserves $\\om$ and that there is a $U$-equivariant\r\nmomentum map $\\mu: Z \\ra \\liu$.  If $\\xi \\in \\liu$, we denote by $\\xi_Z$\r\nthe induced vector field on $Z$ and we let $\\mu^\\xi \\in \\cinf(Z)$ be\r\nthe function $\\mu^\\xi(z) := \\sx \\mu(z),\\xi\\xs$, where $\\sx \\cdot , \\cdot \\xs$ is an $\\mathrm{Ad}(U)$-invariant scalar product on $\\mathfrak u^\\C$. We may also assume that the moltiplication by $i$ is an isometry form $\\mathfrak u$ and $i\\mathfrak u$ (\\cite[$p. 428$]{LA}. By definition, we have\r\n$$\r\nd\\mu^\\xi =\r\ni_{\\xi_Z} \\om.\r\n$$\r\n\r\nLet $G \\subset U^\\C$ be a compatible subgroup of $U^\\C$.\r\nFor $x\\in Z$, let $\\mu_\\mathfrak{p}^\\beta(x)$ denote $-i$ times the component of $\\mu (x)$ along $\\beta$ in the direction of $i\\mathfrak{p},$ i.e.,\r\n\\begin{equation}\\label{mu3}\r\n\\mu_\\mathfrak{p}^\\beta(x) := \\langle \\mu_\\mathfrak{p}(x), \\beta \\rangle = -\\langle \\mu(x), i\\beta \\rangle = \\mu^{-i\\beta} (x)\r\n\\end{equation} for any $\\beta \\in \\mathfrak{p}.$\r\nThen, the map defined by\r\n\\begin{gather*}\r\n  \\mu_\\liep : Z \\ra \\liep.\r\n\\end{gather*} is called the \\emph{gradient map}.\r\nLet $\\metrica$ be the \\Keler\r\nmetric associated to $\\om$, i.e. $(v, w) = \\om (v, Jw),$ for all $z\\in Z$ and $v,w\\in T_zZ$ where $J$ denotes the complex structure on $TZ$. Then\r\n$\\beta_Z $ is the gradient of $\\mup^\\beta$, where $\\beta_Z$ is the vector field on $Z$ corresponding to $\\beta$ and the gradient is computed with respect to $(\\cdot,\\cdot)$. For the rest of this paper, fix a $G$-invariant locally closed submanifold $X$ of $Z.$ We denote the restriction of $\\mu_\\mathfrak{p}$ to $X$ by $\\mu_\\mathfrak{p}.$ Then\r\n$$\r\n\\text{grad}\\mu_\\mathfrak{p}^\\beta = \\beta_X,\r\n$$ where grad is now computed with respect to the induced Riemannian metric on $X.$ Since $X$ is $G$-stable, $\\beta_X=\\beta_Z$. Similarly, $\\bot$ denotes perpendicularity relative to the Riemannian metric on $X.$\r\nWe will now recall some of the properties of the gradient map.\r\n\\begin{lemma}\r\nLet $x\\in X$ and let $\\beta \\in \\mathfrak{p}.$ Then either $\\beta_X(x) = 0$ or the function $t\\mapsto \\mu_\\mathfrak{p}^\\beta(\\text{exp}(t\\beta)\\cdot x)$ is strictly increasing.\r\n\\end{lemma}\r\n\r\n\\begin{proof}\r\nLet $f(t) = \\mu_\\mathfrak{p}^\\beta(\\text{exp}(t\\beta)\\cdot x) = \\langle \\mu_\\mathfrak{p}(\\text{exp}(t\\beta)\\cdot x), \\beta \\rangle.$ Then\r\n$$f'(t) = ( \\beta_X(\\text{exp}(t\\beta)\\cdot x), \\beta_X(\\text{exp}(t\\beta)\\cdot x)  \\geq 0\r\n$$\r\n\\end{proof}\r\nFor any subspace $\\mathfrak{m}$ of $\\mathfrak{g}$ and $x\\in X,$ let $$\\mathfrak{m}\\cdot x := \\{\\xi_X(x) : \\xi \\in \\mathfrak{m}\\}.$$\r\n\\begin{lemma}\r\nLet $x\\in X$. Then $$\\text{ker}\\,\\, d\\mu_\\mathfrak{p}(x) = (\\mathfrak{p}\\cdot x)^\\bot $$\r\n\\end{lemma}\r\n\r\n\\begin{proof}\r\nFrom (\\ref{mu3}),  $v\\in \\text{ker}\\,\\, d\\mu_\\mathfrak{p}(x)$ if and only if for all $\\beta \\in \\mathfrak{p}$\r\n\\begin{align*}\r\n     & \\langle d\\mu_\\mathfrak{p}(x)(v), \\beta \\rangle = 0\\\\\r\n     & \\Longleftrightarrow d\\mu_\\mathfrak{p}^\\beta (v) = 0\\\\\r\n    & \\Longleftrightarrow \\langle \\beta_X(x), v\\rangle = 0.\r\n\\end{align*}\r\n\\end{proof}\r\n\r\n\r\n\r\n\\begin{lemma}\r\nLet $x\\in X.$ The following are equivalent:\r\n\r\n\\begin{enumerate}\r\n    \\item $d\\mu_\\mathfrak{p} : T_xX \\to \\mathfrak{p}$ is onto;\r\n    \\item $d\\mu_\\mathfrak{p} : \\mathfrak{g}\\cdot x \\to \\mathfrak{p}$ is onto;\r\n    \\item the map $\\mathfrak p  \\to T_x X$, $\\beta \\mapsto \\beta_X$, is injective.\r\n\\end{enumerate}\r\n\\end{lemma}\r\n\\begin{proof}\r\n$$\\text{ker}\\,\\, d\\mu_\\mathfrak{p}(x) = (\\mathfrak{p}\\cdot x)^\\bot,$$ it follows that $d\\mu_\\mathfrak{p}(x)$ \\text{is surjective} if and only if\r\n$d\\mu_\\mathfrak{p} : \\mathfrak{p}\\cdot x \\to \\mathfrak{p}$ \\text{is surjective} if and only if $\\dim \\mathfrak p \\cdot x=\\dim \\mathfrak p$,  concluding the proof.\r\n\\end{proof}\r\nWe recall the Slice Theorem, see \\cite{heinzner-schwarz-stoetzel}. For any Lie group $G,$ a closed subgroup $H$ and any set $S$ with an $H-$action, the $G-$bundle over $G/H$ associated with the $H-$principal bundle $G\\to G/H$ is denoted by $G\\times^H S.$ This is the orbit space of the $H-$action on $G\\times S$ given by $h\\cdot(g,s) = (gh^{-1}, h\\cdot s)$ where $g\\in G,$ $s\\in S$ and $h\\in H.$ The $H-$orbit of $(g,s),$ considered as a point in $G\\times^H S,$ is denoted by $[g,s].$\r\n\\begin{theorem}\r\n  [Slice Theorem \\protect{\\cite[Thm. 3.1]{heinzner-schwarz-stoetzel}}]\r\n  If $x \\in X$ and $\\mup(x) = 0$, there are a $G_x$-invariant\r\n  decomposition $T_xX = \\lieg \\cd x \\oplus W$, open $G_x$-invariant\r\n  subsets $S \\subset W$, $\\Omega \\subset X$ and a $G$-equivariant\r\n  diffeomorphism $\\Psi : G \\times^{G_x}S \\ra \\Omega$, such that $0\\in\r\n  S, x\\in \\Omega$ and $\\Psi ([e, 0]) =x$.\r\n\\end{theorem}\r\nLet $\\beta \\in \\mathfrak{p}$. It is well-known that $G^\\beta$ is compatible and\r\n\\[\r\nG^\\beta=K^\\beta \\text{exp}  (\\liep^\\beta),\r\n\\]\r\nwhere $K^\\beta=K\\cap G^\\beta=\\{g\\in K:\\, \\mathrm{Ad}(g)(\\beta)=\\beta\\}$ and\r\n$\\liep^\\beta=\\{v\\in \\liep:\\, [v,\\beta]=0\\}$ (see \\cite{knapp-beyond}).\r\n\r\n\r\n\\begin{corollary} \\label{slice-cor} If $x \\in X$ and $\\mup(x) = \\beta$,\r\n  there are a $G^\\beta$-invariant decomposition $T_xX = \\lieg^\\beta\r\n  \\cd x \\, \\oplus W$, open $G^\\beta$-invariant subsets $S \\subset W$,\r\n  $\\Omega \\subset X$ and a $G^\\beta$-equivariant diffeomorphism $\\Psi\r\n  : G^\\beta \\times^{G_x}S \\ra \\Omega$, such that $0\\in S, x\\in \\Omega$\r\n  and $\\Psi ([e, 0]) =x$.\r\n\\end{corollary}\r\nThis follows by applying the previous theorem to the action of $G^\\beta$\r\nwith the momentum map $\\widehat{\\mu_{\\liu^\\beta}} := \\mu_{\\liu^\\beta} -\r\ni\\beta$, where $\\mu_{\\liu^\\beta}$ denotes the projection of $\\mu$ onto\r\n$\\mu_{\\liu^\\beta}$.\r\nSee \\cite[p. 169]{heinzner-schwarz-stoetzel} for more details.\r\n\r\n\r\nIf $\\beta \\in \\liep$, \\changed{then $\\beta_X$ is a vector field on\r\n  $X$, i.e. a section of $TX$. For $x\\in X$, the differential is a map\r\n  $T_xX \\ra T_{\\beta_X(x)}(TX)$. If $\\beta_X(x) =0$, there is a\r\n  canonical splitting $T_{\\beta_X(x)}(TX) = T_xX \\oplus\r\n  T_xX$. Accordingly $d\\beta_X(x)$ splits} into a horizontal and a\r\nvertical part. The horizontal part is the identity map. We denote the\r\nvertical part by $d\\beta_X(x)$.  It belongs to $\\End(T_xX)$.  Let\r\n$\\{\\phi_t=\\text{exp} (t\\beta)\\} $ be the flow of $\\beta_X$.  \\changed{There\r\n  is a corresponding flow on $TX$. Since $\\phi_t(x)=x$, the flow on\r\n  $TX$ preserves $T_xX$ and there it is given by $d\\phi_t(x) \\in\r\n  \\Gl(T_xX)$.  Thus we get a linear $\\R$-action on $T_xX$ with\r\n  infinitesimal generator $d\\beta_X(x) $.}\r\n\\begin{corollary}\r\n  \\label{slice-cor-2}\r\n  If $\\beta \\in \\liep $ and $x \\in X$ is a critical point of $\\mupb$,\r\n  then there are open invariant neighbourhoods $S \\subset T_xX$ and\r\n  $\\Omega \\subset X$ and an $\\R$-equivariant diffeomorphism $\\Psi : S\r\n  \\ra \\Omega$, such that $0\\in S, x\\in \\Omega$, $\\Psi ( 0) =x$. \\changed{(Here\r\n  $t\\in \\R$ acts as $d\\phi_t(x)$ on $S$ and as $\\phi_t$ on $\\Omega$.)}\r\n\\end{corollary}\r\n\\begin{proof}\r\n  The subgroup $H:=\\text{exp} (\\R \\beta)$ is compatible.  It is enough to\r\n  apply the previous corollary to the $H$-action at $x$.\r\n\\end{proof}\r\n\r\n\r\n\r\n\r\n\r\nAssume now that $\\beta \\in \\liep$ and that $x \\in\r\n\\Crit(\\mu^\\beta_\\liep)$. Let $D^2\\mup^\\beta(x) $ denote the Hessian,\r\nwhich is a symmetric operator on $T_xX$ such that\r\n\\begin{gather*}\r\n  ( D^2 \\mup^\\beta(x) v, v) = \\frac{\\mathrm d^2}{\\mathrm\r\n    dt^2}\n  (\\mup^\\beta\\circ \\ga)(0)\r\n \n \n\\end{gather*}\r\nwhere $\\ga$ is a smooth curve, $\\ga(0) = x$ and $ \\dot{\\ga}(0)=v$.\r\nDenote by $V_-$ (respectively $V_+$) the sum of the eigenspaces of the\r\nHessian of $\\mupb$ corresponding to negative (resp. positive)\r\neigenvalues. Denote by $V_0$ the kernel.  Since the Hessian is\r\nsymmetric we get an orthogonal decomposition\r\n\\begin{gather}\r\n  \\label{Dec-tangente}\r\n  T_xX = V_- \\oplus V_0 \\oplus V_+.\r\n\\end{gather}\r\nLet $\\alfa : G \\ra X$ be the orbit map: $\\alfa(g) :=gx$.  The\r\ndifferential $d\\alfa_e$ is the map $\\xi \\mapsto \\xi_X(x)$.\r\n\\begin{proposition}\r\n    \\label{tangent}\r\n  If $\\beta \\in \\liep$ and $x \\in \\Crit(\\mu^\\beta_\\liep)$ then\r\n  \\begin{gather*}\r\n    D^2\\mup^\\beta(x) = d \\beta_X(x).\r\n  \\end{gather*}\r\n  Moreover $d\\alfa_e (\\lier^{\\beta\\pm} ) \\subset V_\\pm$ and $d\\alfa_e(\r\n  \\lieg^\\beta) \\subset V_0$.  If $X$ is $G$-homogeneous these are\r\n  equalities.\r\n\\end{proposition}\r\n\r\n\\begin{proof}\r\n  The first statement is proved in\r\n  \\cite[Prop. 2.5]{heinzner-schwarz-stoetzel}.  Denote by $\\rho : G_x\r\n  \\ra T_xX$ the isotropy representation: $\\rho(g) = dg_x$.  Observe\r\n  that $\\alfa$ is $G_x$-equivariant where $G_x$ acts on $G$ by\r\n  conjugation, hence $d\\alfa_e$ is $G_x$-equivariant, where $G_x$ acts\r\n  on $\\lieg$ by the adjoint representation and on $T_xX$ by the\r\n  isotropy representation.  Since $\\beta_X(x)=0$, $\\text{exp} ({t\\beta }) \\in\r\n  G_x$ for any $t$ and $d\\alfa_e$ is $\\R$-equivariant. Therefore it\r\n  interchanges the infinitesimal generators of the $\\R$-actions,\r\n  i.e. $d\\alfa_e \\circ \\ad \\beta = d\\beta_X = D^2\\mupb(x)$.  The\r\n  required inclusions follow. If $G$ acts transitively on $X$ we must\r\n  have $T_xX = d\\alfa_e(\\lieg)$. Hence the three inclusions must be\r\n  equalities.\r\n\\end{proof}\r\n\r\n\\begin{corollary}\r\n  \\label{MorseBott}\r\n  For every $\\beta \\in \\liep$, $\\mupb$ is a Morse-Bott function.\r\n\\end{corollary}\r\n\\begin{proof}\r\n  Let $X^\\beta:=\\{ x\\in X: \\beta_X(x) =0\\}$.  Corollary\r\n  \\ref{slice-cor-2} implies that $X^\\beta$ is \\changed{a smooth\r\n    submanifold}. Since $T_xX^\\beta = V_0$ for $x\\in X^\\beta$, the\r\n  first statement of Proposition \\ref{tangent} shows that the Hessian\r\n  is nondegenerate in the normal directions.\r\n\\end{proof}\r\n\r\n\\subsection{Stratifications of the Norm Square of the Gradient map.}\\label{ssss}\r\nWe recall the stratification theorem for actions of reductive group. First, we define a stratification of $X.$ For details see \\cite{heinzner-schwarz-stoetzel}.\r\n\r\nGiven a maximal subalgebra $\\mathfrak{a}\\subset \\mathfrak{p},$ we pick $\\mathfrak{a}_+ \\subset \\mathfrak{a}$ a positive Weyl-chamber. Let $f: X \\to \\mathbb{R}$ be the norm square of the gradient map $\\mu_\\mathfrak{p}.$ i.e.,\r\n$$\r\nf(x) := \\frac{1}{2}\\parallel \\mu_\\mathfrak{p}(x) \\parallel^2,\r\n$$ where $\\parallel \\cdot \\parallel$ denotes the norm functions associated to an $\\mathrm{Ad}(K)$-invariant scalar product $\\langle\\cdot , \\cdot\\rangle.$ This function $f$ will be studied in more details in Section \\ref{Norm_squared}. Let\r\n$C$ denote the critical set of $f$, $\\mathfrak{B} := \\mu_\\mathfrak{p}(C)$ and $\\mathfrak{B}_+ := \\mathfrak{B}\\cap \\mathfrak{a}_+.$\r\n\r\nLet $X^{ss} := \\{x\\in X : \\overline{G\\cdot x} \\cap \\mu_\\mathfrak{p}^{-1}(0) \\neq \\emptyset \\},$ where $X$ is a compact $G$-invariant subset of $Z.$ For $\\beta \\in \\mathfrak{B}_+$, following the notation introduced in \\cite{heinzner-schwarz-stoetzel},  set\r\n\r\n\r\n\\begin{align*}\r\n    & X|_{\\parallel \\beta\\parallel^2} := \\{x\\in X : \\overline{\\text{exp}(\\mathbb{R}\\beta)\\cdot x} \\cap (\\mu_\\mathfrak{p}^\\beta)^{-1}(\\parallel \\beta \\parallel^2) \\neq \\emptyset \\}\\\\\r\n    & X^\\beta := \\{x\\in X : \\beta_X(x) = 0 \\}\\\\\r\n   & X^\\beta|_{\\parallel \\beta \\parallel^2} := X^\\beta \\cap X|_{\\parallel \\beta\\parallel^2}\\\\\r\n   & X^{\\beta +}|_{\\parallel \\beta\\parallel^2} := \\{x\\in X|_{|\\beta|^2}: \\lim_{t\\to -\\infty}\\text{exp}(t\\beta)\\cdot x \\; \\text{exists and it lies in} \\; X^\\beta|_{|\\beta|^2}\\}\r\n\\end{align*}\r\n\r\nThe set $X^{\\beta +}|_{|\\beta|^2}$ is $G^{\\beta +}$-invariant. $\\mu_{\\mathfrak{p}^\\beta}$ is a gradient map of the $G^\\beta$-action on $X^{\\beta +}|_{|\\beta|^2}.$ Set $$\\widehat{\\mu_{\\mathfrak{p}^\\beta}} := \\mu_{\\mathfrak{p}^\\beta} - \\beta.$$ Since $\\beta$ is in the center of $\\mathfrak{g}^\\beta$ and $G^\\beta$ is a compatible subgroup of $(U^\\beta)^\\C = (U^\\C)^\\beta, $ it is a gradient map too. Let\r\n$$\r\nS^{\\beta +} := \\{x \\in X^{\\beta +}|_{|\\beta|^2}: \\overline{G^\\beta \\cdot x} \\cap \\mu_{\\mathfrak{p}^\\beta}^{-1}(\\beta) \\neq \\emptyset \\}.\r\n$$ The set $S^{\\beta +}$ coincides with the set of semistable points of the group $G^\\beta$ in $X^{\\beta +}|_{|\\beta|^2}$ after shifting.\r\n\r\n\\begin{definition}\r\nThe $\\beta$-stratum of $X$ is given by $S_\\beta := G\\cdot S^{\\beta +}.$\r\n\\end{definition}\r\n\r\n\\begin{theorem}\\label{Stratification}\r\n(Stratification Theorem)\\cite[7.3]{heinzner-schwarz-stoetzel}. Suppose $X$ is a compact $G$-invariant submanifold of $Z.$ Then $\\mathfrak{B}_+$ is finite and\r\n$$\r\nX = \\bigsqcup_{\\beta \\in \\mathfrak{B}_+}S_\\beta.\r\n$$\r\nMoreover\r\n$$\r\n\\overline{S_\\beta}\\subset S_\\beta \\cup \\bigcup_{|\\gamma| > |\\beta|}S_\\gamma.\r\n$$\r\n\\end{theorem}\r\n\r\n\\begin{proposition}\\cite[6.12]{heinzner-schwarz-stoetzel} \\label{lemm}\r\nIf $z\\in X$ satisfies $$f(z) = max_{x\\in X}f(x).$$ Then $G\\cdot z = K\\cdot z$ and so it is closed orbit.\r\n\\end{proposition}\r\nIf $v\\in T_p X$, then $|v|=\\sqrt{(v,v)}$, where $(\\cdot,\\cdot)$ is the scalar product induced by the K\\\"ahler form $\\omega$.\r\nThe following proposition give the Hessian of $f.$\r\n\\begin{proposition}\\label{Hessian}\r\n\\cite[Prop. 2.5, 2]{heinzner-schwarz-stoetzel}. Let $v\\in T_xX$ be an eigenvector of $\\beta \\in \\mathfrak{p}_x$ with eigenvalue $\\lambda(\\beta).$ Let $\\gamma(t)$ be a smooth curve in $X$ with $\\gamma(0) = x$ and $\\dot{\\gamma}(0) = v.$ Then if $x$ is a critical point of $f,$\r\n\\begin{equation}\\label{Hess}\r\n\\frac{d^2}{dt^2}(f\\circ\\gamma)(0) = \\lambda(\\beta)|v|^2 + \\parallel d\\mu_\\mathfrak{p}(x)v \\parallel^2\r\n\\end{equation}\r\n\\end{proposition}\r\n\r\nThe Hessian of $f$ at critical points satisfies the following:\r\n\r\n\\begin{proposition}\\label{Hessian comp}\r\n\\cite[Prop. 6.6]{heinzner-schwarz-stoetzel}\r\nLet $x\\in C$ be a critical point of $f : X \\to \\mathbb{R}$ and let $S_\\beta$ be the associated stratum. Let $H_x(f)$ denote the Hessian of $f$ at $x.$ Then\r\n\\begin{enumerate}\r\n    \\item $H_x(f) = 0$ on $T_x(K\\cdot x),$\r\n    \\item $H_x(f) > 0$ on $\\mathfrak{p}^\\beta\\cdot x + \\lier^{\\beta+}\\cdot x,$ where $\\lier^{\\beta+}$ is the Lie algebra of $R^{\\beta+},$\r\n    \\item $H_x(f)\\geq 0$ on $T_x(S_\\beta) = \\lieg\\cdot x + T_x(S^{\\beta+}) = \\liek\\cdot x + T_x(S^{\\beta+})$ and\r\n    \\item $H_x(f) < 0$ on $T_x(S_\\beta)^\\bot = (\\lieg\\cdot x)^\\bot \\cap (T_x(S^{\\beta+}))^\\bot = (\\liek\\cdot x)^\\bot \\cap (T_x(S^{\\beta+}))^\\bot.$\r\n\\end{enumerate}\r\n\\end{proposition}\r\n\r\n\\begin{remark}\\label{Hessian comp1}\r\n\\cite[6.7]{heinzner-schwarz-stoetzel} The tangent space $T_x(G\\cdot x)$ decompose to $T_x(G\\cdot x) = T_x(K\\cdot x)\\oplus \\liep^\\beta\\cdot x \\oplus \\lier^{\\beta+}\\cdot x.$ This follows from the decomposition $G = K G^{\\beta+},$ $G^{\\beta+} = G^{\\beta}R^{\\beta+},$ the identity $K\\cap G^{\\beta+} = K^\\beta$ and the fact that $G^\\beta$ acts on $X^\\beta$ whereas $R^{\\beta+}$ acts on the fibers of $\\liep^{\\beta+}.$ Thus the behaviour of $H_x(f)$ on $T_x(G\\cdot x)$ is precisely described by Proposition \\ref{Hessian comp}.\r\n\r\n\\end{remark}\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\\section{The Norm Square of the Gradient Map}\r\n\\label{Norm_squared}\r\n\r\nWe assume throughout that $X$ is compact and connected.\r\nLet $\\parallel \\cdot\\parallel$ denote the norm functions associated to the $\\mathrm{Ad}(K)$-invariant scalar product $\\langle\\cdot , \\cdot\\rangle$ on $\\mathfrak p$.\r\nDefine the function $f : X \\rightarrow \\mathbb{R}$ by\r\n\\begin{equation}\\label{ns}\r\nf(x) := \\frac{1}{2}\\parallel\\mu_\\mathfrak{p}(x)\\parallel^2, \\qquad \\text{for}\\quad x \\in X.\r\n\\end{equation}\r\nIn this section, the critical points of this function will be of central importance.\r\n\r\n\\begin{lemma}\\label{nmg}\r\nThe gradient of $f$ is given by\r\n\\begin{equation}\\label{nmge}\r\n    \\triangledown f(x) = \\beta_X(x), \\quad \\beta := \\mu_\\mathfrak{p}(x)\\in \\mathfrak{p} \\quad \\text{and}\\quad x\\in X.\r\n\\end{equation} Hence, $x\\in X$ is a critical point of $f$ if and only if $\\beta_X(x) = 0.$\r\n\\end{lemma}\r\n\r\n\\begin{proof}\r\n\r\nDefine a curve $\\gamma(t)$ such that $\\gamma(0) = x$ and $\\gamma'(0) = v \\in T_xX.$\r\n$f(x) = \\frac{1}{2}\\parallel \\mu_\\mathfrak{p}(x)\\parallel^2 = \\frac{1}{2}\\langle \\mu_\\mathfrak{p}(x), \\mu_\\mathfrak{p}(x)\\rangle$\r\n\\begin{align*}\r\n    df(x)v &= \\frac{d}{dt}\\bigg \\vert_{t=0}f(\\gamma(t))\\\\\r\n    & = \\frac{1}{2}\\frac{d}{dt} \\bigg \\vert_{t=0}\\langle \\mu_\\mathfrak{p}(\\gamma(t)), \\mu_\\mathfrak{p}(\\gamma(t))\\rangle \\\\\r\n    & = \\langle d\\mu_\\mathfrak{p}(\\gamma(t))\\gamma'(t), \\mu_\\mathfrak{p}(\\gamma(t))\\rangle|_{t=0}\\\\\r\n    & = \\langle d\\mu_\\mathfrak{p}(x)v, \\mu_\\mathfrak{p}(x)\\rangle = \\langle \\beta_X(x), v), \\quad \\beta = \\mu_\\mathfrak{p}(x).\r\n\\end{align*}\r\nHence, $\\triangledown f(x) = \\beta_X(x).$\r\n\\end{proof}\r\n\r\n\r\n\\begin{corollary}\r\n  Let $x\\in X$ and set $\\beta := \\mu_\\mathfrak{p}(x).$ The following are equivalent.\r\n \\begin{enumerate}\r\n      \\item $\\beta_X(x) = 0,$\r\n      \\item $d\\mu_\\mathfrak{p}^\\xi(x) = 0,$ $\\xi \\in \\mathfrak{p},$\r\n      \\item $df(x) = 0.$\r\n  \\end{enumerate}\r\n\\end{corollary}\r\n\r\nFor the remaining part of this work, we fix $\\beta = \\mu_\\mathfrak{p}(x).$ The negative gradient flow line of $f$ through $x\\in X$ is the solution of the differential equation\r\n\r\n\r\n\\[ \\left\\{ \\begin{array}{ll}\r\n         \\dot{x}(t) = -\\beta_X(x(t)), \\quad t\\in \\mathbb{R} \\\\\r\n        x(0) = x.\\end{array} \\right. \\]\r\n\r\nThe $G$-orbits are invariant under the gradient flow.\r\n\\begin{lemma}\\label{Gradient}\r\nLet $g: \\mathbb{R} \\rightarrow G$ be the unique solution of the differential equation\r\n\\[ \\left\\{ \\begin{array}{ll}\r\n         g^{-1}\\dot{g}(t) = \\beta_X(x(t)) \\\\\r\n        g(0) = e,\\quad \\text{where $e$ is the identity of $G$}.\\end{array} \\right. \\]\r\n\r\nThen, $$x(t) = g^{-1}(t)x$$ for all $t\\in \\mathbb{R}.$\r\n\\end{lemma}\r\n\r\n\\begin{proof}\r\nDefine $y : \\mathbb{R} \\to X$ by $$y(t) = g^{-1}(t)x.$$ Since $\\dot{g^{-1}} = -g^{-1}\\dot{g}g^{-1}$ and $g^{-1}\\dot{g} = \\beta_X(x),$ it follows that\r\n$$\\dot{y} = -g^{-1}\\dot{g}g^{-1}x =  -\\beta_X(g^{-1}x) = -\\beta_X(y(t))$$ and\r\n$$y(0) = (g(0))^{-1}x = e^{-1}x = x.$$\r\n\r\nHence $x(t) = y(t) = g^{-1}(t)x$ for all $t\\in \\mathbb{R}.$\r\n\r\n\\end{proof}\r\n\r\nThe proof of the following Theorem is based on the Lojasiewicz gradient inequality, which holds in general for analytic gradient flows. A proof for the case of an action of a complex reductive group is given in \\cite{Salamon}.\r\n\\begin{theorem}\\label{teo}\r\nLet $x_0\\in X$ and $x: \\mathbb{R} \\rightarrow X$ the negative gradient flow line of $f$ through $x_0$. There exist positive constants $\\alpha,$ $C,$ $\\psi,$ and $\\frac{1}{2} < \\gamma < 1$ such that\r\n\r\n$$x_\\infty := \\lim_{t \\rightarrow \\infty} x(t) $$\r\nexists. Moreover, there exist a constant $T > 0$ such that for any $t > T,$\r\n\\begin{align*}\r\n    d(x(t), x_\\infty) & \\leq \\int_t^\\infty |\\dot{x}(s)|ds\\\\\r\n    &\\leq \\frac{\\alpha}{1 - \\gamma}(f(x(t)) - f(x_\\infty))^{1-\\gamma}\\\\\r\n    & \\leq \\frac{C}{(t - T)^\\psi}.\r\n\\end{align*}\r\n\\end{theorem}\r\n\\begin{proof}\r\nLet $X = Z.$ Using the Marle-Guiliemin-Sternberg local normal form, the moment map is locally real analytic.\r\nSince $\\mu_\\mathfrak{p} = -\\pi_\\mathfrak{p}\\circ i\\mu$ where $\\pi_\\mathfrak{p} :i\\mathfrak{u} \\rightarrow \\mathfrak{p}$ is the orthogonal projection.\r\nThen $\\mu_\\mathfrak{p}$ is locally real analytic. This implies that $f = \\frac{1}{2}\\parallel \\mu_\\mathfrak{p} \\parallel^2 :Z \\rightarrow \\mathbb{R}$ satisfies the Lojasiewicz gradient inequality. By Lemma \\ref{nmg}, the gradient of $f:Z \\rightarrow \\mathbb{R}$ coincide with the gradient of $f: X \\rightarrow \\mathbb{R}.$\nHence $f|_X$ also satisfies Lojasiewicz gradient inequality: there exists constants $\\delta > 0,$ $\\alpha > 0,$ and $\\frac{1}{2} < \\gamma < 1$ such that, for every critical value $a$ of $f$ and every $x\\in X,$\r\n\\begin{equation}\\label{loj}\r\n    |f(x)-a| < \\delta \\qquad \\implies |f(x)-a|^\\gamma \\leq \\alpha|\\triangledown f(x)|.\r\n\\end{equation}\r\n\r\nLet $x : \\mathbb{R} \\rightarrow X$ be a nonconstant negative gradient flow line of $f.$\r\n$$\r\na = \\lim_{t \\to \\infty}f(x(t))\r\n$$ is a critical value of $f.$ Choose a constant $T > 0$ such that $a < f(x(t)) < a + \\delta$ for $t \\geq T.$ Then, for $t\\geq T,$\r\n$$\r\n\\frac{d}{dt}(f(x)-a)^{1-\\gamma} = (1-\\gamma)(f(x)-a)^{-\\gamma}|\\triangledown f(x)| \\geq \\frac{1-\\gamma}{\\alpha}|\\dot{x}|.\r\n$$\r\n\r\nIntegrating the inequality over the interval $[t, \\infty)$ gives\r\n\\begin{equation}\\label{equ}\r\n\\int_t^\\infty|\\dot{x}(s)|ds \\leq \\frac{\\alpha}{1-\\gamma}(f(x(t))-a)^{1-\\gamma} \\qquad \\text{for} \\; \\; t \\geq T.\r\n\\end{equation}\r\n\r\nThis shows that $$x_\\infty := \\lim_{t\\to \\infty} x(t)$$ exists and it is a critical point of $f$ and hence satisfies $\\mu_\\mathfrak{p}(x_\\infty)_X = 0.$\r\n\r\nSet $\\xi(t) = (f(x(t))-a)^{1-2\\gamma}.$\r\n$$\\dot{\\xi}(t) = (2\\gamma - 1)(f(x(t))-a)^{-2\\gamma}|\\triangledown f(x(t))|^2\\geq \\frac{2\\gamma-1}{\\alpha^2} \\qquad \\text{for} \\; \\; t\\geq T.\r\n$$ Which implies that\r\n$$\r\n\\xi(t) \\geq \\frac{2\\gamma-1}{\\alpha^2}(t-T) \\qquad \\text{for} \\; \\; t\\geq T.\r\n$$\r\nHence\r\n$$(f(x(t))-a)^{1-\\gamma} = \\xi(t)^{-\\frac{1-\\gamma}{2\\gamma-1}}\\leq \\left(\\frac{2\\gamma-1}{\\alpha^2}(t-T)\\right)^{-\\frac{1-\\gamma}{2\\gamma-1}} \\qquad \\text{for} \\; \\; t\\geq T.\r\n$$\r\nThus\r\n$$\r\n\\frac{\\alpha}{1-\\gamma}(f(x(t))-a)^{1-\\gamma}\\leq \\frac{c}{(t-T)^\\psi}, \\quad \\psi := \\frac{1-\\gamma}{2\\gamma-1}, \\quad c := \\frac{\\alpha}{1-\\gamma}\\left(\\frac{\\alpha^2}{2\\gamma-1}\\right)^\\psi\r\n$$ and by (\\ref{equ}) the result follows.\r\n\r\n\\end{proof}\r\n\r\nFrom the proof of Theorem \\ref{teo}, it was observed that $f$ satisfies the Lojasiewicz\u2019s gradient inequality. As an application, we prove a well-known result that the stratum corresponding to the minimum of $f$ is open.\r\n\\begin{theorem}\r\nIf $\\beta \\in \\mathfrak{B}$ is such that $\\frac{1}{2}\\parallel \\beta\\parallel^2$ is a minimum value of $f.$ Then the corresponding strata $S_\\beta$ is open in $X.$\r\n\\end{theorem}\r\n\r\n\\begin{proof} From the proof of Theorem \\ref{teo}, there exists constants $\\delta > 0,$ $\\alpha > 0,$ and $\\frac{1}{2} < \\gamma < 1$ such that, for every critical value $a$ of $f$ and every $x\\in X,$\r\n\\begin{equation\n    |f(x)-a| < \\delta \\qquad \\implies |f(x)-a|^\\gamma \\leq \\alpha|\\triangledown f(x)|.\r\n\\end{equation} In particular, if $x$ is critical point, then $f(x) = a.$ Since $X$ is compact, by  Theorem \\ref{Stratification}, $$X = \\bigsqcup_{i = 1}^kS_{\\beta_i},$$ where $\\beta_i \\in \\mathfrak{B}_+.$ We may assume that $\\beta = \\beta_1$ and so, $|\\beta_j| > |\\beta|$ for any $j = 2, \\cdots, k.$\r\nLet $$0< \\delta' < \\text{min} \\left(\\delta, \\frac{\\parallel \\beta_2 \\parallel^2 - \\parallel \\beta\\parallel^2}{2}, \\cdots, \\frac{\\parallel \\beta_k\\parallel^2 - \\parallel \\beta\\parallel^2}{2}\\right).$$ Let\r\n$$\r\nU = \\{x\\in X : |f(x) - \\parallel \\beta\\parallel^2|< \\delta'\\}.\r\n$$ Let $x_0\\in U$ and let $x(t)$ the gradient flow of $-\\nabla f$ through $x_0.$ Therefore,\r\n\\begin{align*}\r\n    f(x(t)) \\leq f(x_0) \\leq |f(x_0) - \\parallel \\beta\\parallel^2| + \\parallel \\beta\\parallel^2\\\\\r\n    & < \\delta' + \\parallel \\beta\\parallel^2\\\\\r\n    & < \\frac{\\parallel \\beta_j \\parallel^2 - \\parallel\\beta\\parallel^2}{2} + \\parallel \\beta\\parallel^2 \\quad j = 2, \\cdots, k\\\\\r\n    & < \\frac{\\parallel \\beta_j \\parallel^2 + \\parallel \\beta\\parallel^2}{2}\\leq \\parallel \\beta_j \\parallel^2, \\quad j = 2, \\cdots, k.\r\n\\end{align*}\r\nTherefore, $f(x_\\infty) = \\parallel \\beta\\parallel^2$. This implies that $U \\subset S_\\beta$ and so, using standard arguments of the gradient flow,  $S_\\beta$ is open.\r\n\\end{proof}\r\n\r\nAs in \\cite{E. Lerman}, we have the following deformation retraction.\r\n\\begin{theorem}[Retraction Theorem] \\label{retraction-theorem}\r\nLet $\\beta \\in \\mathfrak{a}_+$ be a critical value of $f.$ Let $S_\\beta$ be the stratum associated to $\\beta.$ Let $\\phi_t(x)$ denote the gradient flow of $-\\nabla f.$ Then there exist a $K$-equivariant strong deformation retraction of $S_\\beta$ onto $S_\\beta \\cap \\mu_\\mathfrak{p}^{-1}(K\\cdot \\beta)$ given by\r\n$$[0, \\infty] \\times S_\\beta \\to S_\\beta \\cap \\mu_\\mathfrak{p}^{-1}(K\\cdot \\beta), \\quad (t, p) \\mapsto \\phi_t(p),$$ and $$\\phi_\\infty(p) = \\lim_{t\\to +\\infty}\\phi_t(p).$$\r\n\\end{theorem}\r\n\r\n\r\n\\section{Kempf-Ness Function}\\label{Kempf-Ness Function}\r\n\r\nGiven $G$ a real reductive group which acts smoothly on $Z;$ $G = K\\text{exp}(\\mathfrak{p}),$ where $K$ is a maximal compact subgroup of $G.$ Let $X$ be a $G$-invariant locally closed submanifold of $Z.$ As Mundet pointed out in \\cite{MUNDET}, there exists a function $\\Phi : X \\times G \\rightarrow \\mathbb{R},$ such that\r\n$$\r\n\\langle \\mu_\\mathfrak{p}(x), \\xi\\rangle = \\frac{d}{dt}\\bigg \\vert_{t=0}\\Phi (x, \\text{exp}(t\\xi)), \\qquad \\xi \\in \\mathfrak{p},\r\n$$ and satisfying the following conditions:\r\n\r\n\\begin{enumerate}\r\n    \\item For any $x\\in X,$ the function $\\Phi (x, .)$ is smooth on $G.$\r\n    \\item The function $\\Phi(x, .)$ is left-invariant with respect to $K,$ i.e., $\\Phi(x, kg) = \\Phi(x, g).$\r\n    \\item For any $x\\in X,$ $v\\in \\mathfrak{p}$ and $t\\in \\mathbb{R};$\r\n\r\n    $$\\frac{d^2}{dt^2}\\Phi (x, \\text{exp}(tv)) \\geq 0.$$\r\n    Moreover: $$\\frac{d^2}{dt^2}\\Phi (x, \\text{exp}(tv)) = 0$$ if and only if exp$(\\mathbb{R}v)\\subset G_x.$\r\n\r\n    \\item For any $x\\in X,$ and any $g, h \\in G;$\r\n    $$\\Phi(x, hg) = \\Phi(x, g) + \\Phi(gx, h).$$ This equation is called the cocycle condition. The proof is given in \\cite{LM}, see also \\cite{bgs}.\r\n\\end{enumerate}\r\n\r\nThe function $\\Phi : X \\times G \\rightarrow \\mathbb{R}$ is called the Kempf-Ness function for $(X, G, K)$\r\n\r\nLet $M = G/K$ and $\\pi : G \\rightarrow M.$ $M$ is a symmetric space of non-compact type \\cite{borel-ji-libro}. By $(b),$ $\\Phi(x, kg) = \\Phi(x, g),$ the function $\\Phi$ descend to $M$ as $$\\Phi : X \\times M \\rightarrow \\mathbb{R};$$\r\n\\begin{equation}\r\n\\Phi(x,\\pi(g)) = \\Phi(x,g^{-1}).\r\n\\end{equation}\r\n\r\nFirst, we compute the differential of $\\Phi$ at any point in a given direction.\r\n\\begin{lemma}\\label{differential}\r\nFor $x\\in X$, let $\\Phi_x(gk) = \\Phi(x,g^{-1}).$ The differential of $\\Phi_x$ is given as\r\n$$\r\nd(\\Phi_x)_{\\pi(g)}(v_x) = -\\langle \\mu_\\mathfrak{p}(g^{-1}x), \\xi\\rangle\r\n$$ where, $v_x(g) = d\\pi_gg\\xi$ and $\\xi \\in \\mathfrak{p}.$\r\n\r\nTherefore, $\\nabla \\Phi_x(\\pi(g)) = -d_{\\pi(g)}(dL_g(\\mu_\\mathfrak{p}(g^{-1}x)))$\r\n\\end{lemma}\r\n\r\n\\begin{proof}\r\nLet $\\pi(g)\\in M,$ $\\xi \\in \\mathfrak{p}$ and $v_x\\in T_{\\pi(g)}G/K.$ There exist $\\xi \\in \\mathfrak{p}$ such that\r\n\r\n$$v_x = \\frac{d}{dt}\\bigg \\vert_{t=0} g\\text{exp}(t\\xi)K.$$\r\n\r\nTake\r\n$$ \\gamma(t) = \\pi(g\\text{exp}(t\\xi)), \\qquad t\\in [a,b], \\; \\xi \\in \\mathfrak{p}.$$\r\nThen\n$v_x = (d\\pi)_g((dL_g)(\\xi)),$\r\n\r\n\\begin{align*}\r\n    d(\\Phi_x)_{\\pi(g)}(v_x) &= \\frac{d}{dt}\\bigg \\vert_{t = 0}\\Phi(x, \\gamma(t))\\\\\r\n    & = \\frac{d}{dt}\\bigg \\vert_{t = 0}\\Phi(x, \\pi(g\\text{exp}(t\\xi))\\\\\r\n   \n    & = \\frac{d}{dt}\\bigg \\vert_{t = 0}\\Phi(x, \\text{exp}(-t\\xi)g^{-1}) \\qquad (\\mbox{by the deifintion of} \\,\\, \\Phi)\\\\\r\n    & = \\frac{d}{dt}\\bigg \\vert_{t = 0} [\\Phi(x, g^{-1}) + \\Phi(g^{-1}x, \\text{exp}(-t\\xi)) ] \\qquad (\\mbox{by condition (d)})\\\\\r\n    & = \\frac{d}{dt}\\bigg \\vert_{t = 0} \\Phi(g^{-1}x, \\text{exp}(-t\\xi))\\\\\r\n    & = -\\langle \\mu_\\mathfrak{p}(g^{-1}x), \\xi\\rangle.\r\n \\end{align*}\r\nThis implies that\r\n$$ d(\\Phi_x)_{\\pi(g)}(v_x) = -\\langle \\mu_\\mathfrak{p}(g^{-1}x), \\xi\\rangle.\n$$\r\n\\end{proof}\r\n\r\nLet $\\Phi_x : G \\rightarrow \\mathbb{R},$ be the Kempf-Ness function of $x$ define on $G.$ Define $\\phi_x: G\\to G\\cdot x$ as follows $$\\phi_x(g) = g^{-1}x.$$\r\n\\begin{lemma}\\label{differential2}\r\nThe map $\\phi_x$ intertwines the gradient of $\\Phi_x$ and the gradient of $f.$ i.e., $\\forall g\\in G$\r\n$$\r\nd(\\phi_x)_g\\nabla\\Phi_x = \\nabla f((\\phi_x(g)).\r\n$$\r\n\\end{lemma}\r\n\r\n\\begin{proof}\r\nLet $\\beta\\in\\mathfrak{p}.$ Since\r\n$$\\phi_x(g\\text{exp}(t\\beta)) = \\text{exp}(-t\\beta)g^{-1}x,$$\r\nwe have\r\n$$(d\\Phi_x)_g(dL_g(\\beta)) = -\\beta_X(g^{-1}x).$$ The result follow from taking $\\beta = \\mu_\\mathfrak{p}(x).$\r\n\\end{proof}\r\n\r\n\\begin{remark}\r\nA smooth curve $\\gamma(t):= \\pi \\circ g : \\mathbb{R} \\to M$ is a negative gradient flow line of $\\Phi_x$ if and only if the smooth curve $g : \\mathbb{R} \\to M$ satisfies $g^{-1}\\dot{g} = \\mu_\\mathfrak{p}(g^{-1}x).$ Indeed, from Lemma \\ref{differential}, $$\\nabla \\Phi_x(\\pi(g)) = -d_{\\pi(g)}(dL_g(\\mu_\\mathfrak{p}(g^{-1}x))$$ and by Lemma \\ref{differential2}, $\\phi_x$ interwines the gradient of $\\Phi_x$ with $\\nabla f.$ Therefore, the gradient flow of $\\Phi_x$ is such that $g^{-1}\\dot{g} = \\mu_\\mathfrak{p}(g^{-1}x)$. On the other hand if $g: \\mathbb{R}\\to G$ satisfies $g^{-1}\\dot{g} = \\mu_\\mathfrak{p}(g^{-1}x)$, the curve $\\gamma = \\pi \\circ g(t)$ is a geodesic and\r\n$$\r\n\\frac{d}{dt}(\\Phi_x\\circ \\gamma) = -\\langle \\mu_\\mathbb{p}(g^{-1}x), g^{-1}\\dot{g}\\rangle.\r\n$$\r\n\\end{remark}\r\n\r\nWe recall some result from Riemannian geometry. We refer the reader to Apendix A in \\cite{Salamon} for further details. Suppose $M$ is an Hadamard manifold, i.e., connected, complete, simply-connected with non-positive curvature. Let $\\gamma_1, \\gamma_2 : [a, b] \\to M$ be smooth curves and for each $t\\in [a, b],$ $\\gamma(s, t): [0, 1] \\to M$ be the unique geodesic such that $$\r\n\\gamma(0, t) = \\gamma_1(t), \\qquad \\gamma(1,t) = \\gamma_2(t).\r\n$$\r\nDefine the function $\\rho: [a, b] \\to \\mathbb{R}$ by\r\n$$\\rho (t) := d(\\gamma_1(t), \\gamma_2(t)).$$ The following Lemma is proved in \\cite[Lemma A.2 ]{Salamon}\r\n\\begin{lemma}\\label{rmm} Suppose $f: M\\to \\mathbb{R}$ is a smooth function that is convex along geodesics. Let $\\gamma_1, \\gamma_2 : \\mathbb{R} \\to M$ be the negative gradient flow lines of $f,$ and let $\\gamma$ and $\\rho$ be as defined above. Then $\\rho$ is nonincreasing and, if $\\rho(t) \\neq 0,$ then\r\n\\begin{equation}\\label{rho}\r\n    \\dot{\\rho}(t) = -\\frac{1}{\\rho(t)}\\int_0^1\\frac{\\partial^2}{\\partial s^2}(f\\circ \\gamma)(s,t)ds.\r\n\\end{equation}\r\n\\end{lemma}\r\n\r\n\r\n\\begin{theorem}\\label{ConvexKempf}\r\nLet $\\Phi_x : M \\to \\mathbb{R}.$ Then\r\n\\begin{enumerate}\r\n    \\item $\\Phi_x$ is a Morse-Bott function and it is convex along geodesics.\r\n    \\item If $\\gamma : \\mathbb{R}\\to M$ is a negative gradient flow of $\\Phi_x,$ then, $$\\lim_{t\\to \\infty}\\Phi_x(\\gamma(t)) = \\text{inf}_{x\\in M}\\Phi_x.$$\r\n\\end{enumerate}\r\n\r\n\\end{theorem}\r\n\r\n\\begin{proof}\r\n(a): By lemma \\ref{differential}, $g\\in G$ is a critical of $\\Phi_x$ if and only if $$\\mu_\\mathfrak{p}(g^{-1}x) = 0.$$\r\n\\begin{equation}\r\n\\text{Crit}(\\Phi_x) = \\{\\pi(g)\\in M : \\mu_\\mathfrak{p}(g^{-1}x) = 0\\}.\r\n\\end{equation}\r\nThe next is to show that the $\\text{Crit}(\\Phi_x)$ is a submanifold of $M.$ To do this, the Hessian of the function is computed along geodesics. The geodesic on $M$ passing through $\\pi(g)$ in the direction $v = d\\pi_gg\\xi$ has the form $\\pi(g\\text{exp}(t\\xi))$ \\cite{LM}. Hence, $M$ is complete and by the Hadamard theorem, $$\\mathfrak{p} \\rightarrow M, \\qquad \\xi \\mapsto \\pi(g\\text{exp}(\\xi))$$ is a diffeomorphism. This implies that $\\pi(g\\text{exp}(\\xi))\\in \\text{Crit}(\\Phi_x)$ if and only if $\\mu_\\mathfrak{p}(g^{-1}\\text{exp}(-\\xi)x) = 0.$\r\n$$\r\n\\text{Hess}(\\Phi_x) = d^2(\\Phi_x)_{\\pi(g)}(v), \\qquad \\pi(g)\\in \\text{Crit}(\\Phi_x)\r\n$$\r\n\\begin{align*}\r\nd^2(\\Phi_x)_{\\pi(g)}(v) & = \\frac{d^2}{dt^2}\\bigg \\vert_{t = 0}\\Phi(x, \\gamma(t))\\\\\r\n    & = \\frac{d^2}{dt^2}\\bigg \\vert_{t = 0}\\Phi(x, \\pi(g\\text{exp}(-t\\xi))\\\\\r\n    & = \\frac{d^2}{dt^2}\\bigg \\vert_{t = 0}\\Phi(x, g\\text{exp}(-t\\xi)K)\\\\\r\n    & = \\frac{d^2}{dt^2}\\bigg \\vert_{t = 0}\\Phi(x, \\text{exp}(t\\xi)g^{-1})\\\\\r\n    & = \\frac{d^2}{dt^2}\\bigg \\vert_{t = 0} [\\Phi(x, g^{-1}) + \\Phi(g^{-1}x, \\text{exp}(t\\xi)) ]\\\\\r\n    & = \\frac{d^2}{dt^2}\\bigg \\vert_{t = 0}\\Phi(g^{-1}x, \\text{exp}(t\\xi) \\geq 0 \\qquad (\\mbox{by condition (c)})\r\n \\end{align*}\r\nMoreover, $$ \\frac{d^2}{dt^2}\\bigg \\vert_{t = 0}\\Phi(g^{-1}x, \\text{exp}(t\\xi) = 0$$ if and only if exp$(t\\xi) \\subset G_{g^{-1}x}.$ Hence,\r\n$$\r\n\\text{Crit}(\\Phi_x) = \\{\\pi(g\\text{exp}t\\xi)\\in M : \\text{exp}t\\xi \\subset G_{g^{-1}x}\\}.\r\n$$\r\nWhich is a submanifold and the kernel of the Hessian. Therefore, $\\Phi$ is a Morse-Bott function and since the Hessian is non-negative along geodesics, it is convex along geodesics.\r\n\r\n(b). Let $\\gamma_1, \\gamma_2$ be negative gradient flow of $\\Phi_x.$ There exists $g_1, g_2: \\mathbb{R}\\to G$ such that $\\gamma_1 = \\pi(g_1(t))$ and $\\gamma_2 = \\pi(g_2(t)).$ Let $\\beta: \\mathbb{R}\\to \\mathfrak{p}$ and $k: \\mathbb{R}\\to K$ be such that $g_2(t) = g_1(t)\\text{exp}(\\beta(t))k(t).$ Define $H: \\mathbb{R}\\times \\mathbb{R}\\to M$ by\r\n$$\r\nH(t,s) =  \\pi(g_1(t)\\text{exp}(s\\beta(t))).\r\n$$\r\nThe curve $s\\mapsto H(t, s)$ is the unique geodesic joining $\\gamma_1$ and $\\gamma_2.$ By Lemma \\ref{rmm} the function $\\rho(t) = d_M(\\gamma_1(t), \\gamma_2(t)) = \\parallel\\beta(t)\\parallel$ is nonincreasing.\r\n\r\nAssume that Crit$(\\Phi_x)$ is not empty. Hence we may suppose that $\\mu_\\mathfrak{p}(g_1(0)^{-1}x) = 0$. This implies that the curve $\\gamma_1$ is constant. Since $\\rho$ is nonincreasing, the image of  $\\gamma_2$ is contained in a compact subset of $M.$ Since $\\Phi_x$ is Morse-Bott, then $\\gamma_2$ converges to a critical point of $\\Phi_x.$ This implies that if the critical manifold of $\\Phi_x$ is nonempty, then $\\Phi_x$ has a global minimal and every negative flow line of $\\Phi_x$ converges to a critical point. Now suppose Crit$(\\Phi_x)$ is empty. Assume by contradiction that $$a := \\lim_{t\\to\\infty}\\Phi_x(\\gamma_1(t)) \\geq \\text{inf}_M\\Phi_x.$$ Then, $\\Phi_x(\\gamma_1(t))$ is bounded from below. We can choose $\\gamma_2$ such that $\\Phi(\\gamma_2(0))< a.$ Since the function $\\rho = \\parallel \\beta(t)\\parallel$ is nonincreasing, then there exists a constant $C > 0$ such that $\\rho(t) = \\parallel \\beta(t)\\parallel \\leq C.$ Hence,\r\n\\begin{align*}\r\n\\frac{d}{ds}\\bigg \\vert_{s=0}\\Phi(H(t,s)) &= (d\\Phi_x)_{\\gamma_1(t)} (\\dot{H}(t,0))\\\\\r\n& = -\\langle \\mu_\\mathfrak{p}(g_1(t)^{-1}x), \\beta(t)\\rangle\\\\\r\n& \\geq - \\parallel \\mu_\\mathfrak{p}(g_1(t)^{-1}x)\\parallel \\parallel \\beta(t)\\parallel\\\\\r\n&\\geq -C\\parallel\\mu_\\mathfrak{p}(g_1(t)^{-1}x)\\parallel.\r\n\\end{align*}\r\nSince for a fixed $t,$ the function $s\\to \\Phi_x(H(t,s))$ is convex, this implies that the derivative $\\frac{d}{ds}\\Phi(H(t,s))$ increases. It follows that\r\n\\begin{align*}\r\n    \\Phi_x(\\gamma_2(t)) &= \\Phi_x(H(t,1))\\\\\r\n    & = \\Phi_x(H(t,0)) + \\int_0^1\\frac{d}{ds}\\Phi(H(t,s)) \\mathrm d s\\\\\r\n    & \\geq \\Phi_x(\\gamma_1(t)) - C\\parallel \\mu_\\mathfrak{p}(g_1(t)^{-1}x)\\parallel.\r\n\\end{align*} Since the function $\\Phi_x(\\gamma_1(t))$ is bounded below and $\\frac{d}{dt}\\Phi_x(\\gamma_1(t)) = -\\parallel \\mu_\\mathfrak{p}(g_1(t)^{-1}x)\\parallel^2,$ there exists a sequence $t_i\\to \\infty$ such that $\\lim_{i\\to \\infty}\\parallel \\mu_\\mathfrak{p}(g_1(t_i)^{-1}x)\\parallel^2 = 0.$ This implies that\r\n$$\r\n\\lim_{i\\to\\infty}\\Phi_x(\\gamma_2(t_i))\\geq \\lim_{i\\to\\infty}\\Phi_x(\\gamma_1(t_i)) = a.\r\n$$ This is a contradiction since by assumption $\\Phi_x(\\gamma_2(t)) < a$ and so $\\lim_{i\\to\\infty}\\Phi_x(\\gamma_1(t)) < a.$\r\n\r\n\r\n\\end{proof}\r\nThe following result asserts that any critical points of $f$ in the same $G$-orbit in fact belong to\r\nthe same $K$-orbit. We use original ideas from \\cite{Salamon} in a different context.\r\n\\begin{theorem}\r\n\\label{critt}\r\nLet $x_0, x_1 \\in X$ be critical points of the norm square $f.$ Then $$x_1 \\in G\\cdot x_0 \\implies x_1 \\in K\\cdot x_0.$$\r\n\\end{theorem}\r\n\r\n\\begin{proof}\r\nSince $x_0, x_1 \\in X$ are critical points of $f,$ then by Lemma \\ref{nmg}\r\n\\begin{equation}\r\n    \\mu_\\mathfrak{p}(x_0)_X = 0, \\qquad \\mu_\\mathfrak{p}(x_1)_X = 0\r\n\\end{equation}\r\nSuppose $x_1 \\in G\\cdot x_0.$ Let $g_0\\in G$ such that\r\n$$x_1 = g_0^{-1}x_0$$\r\n\r\nand $g,h : \\mathbb{R} \\rightarrow G$ be defined by\r\n$$\r\ng(t) := \\text{exp}(t\\mu_\\mathfrak{p}(x_0)), \\qquad \\qquad h(t) := g_0\\text{exp}(t\\mu_\\mathfrak{p}(x_1)).\r\n$$\r\nNow, $g(t)^{-1}x_0 = \\text{exp}(-t\\mu_\\mathfrak{p}(x_0))x_0 = x_0$, since $\\mu_\\mathfrak{p}(x_0)_X = 0$.\r\nSimilarly one can check that $h(t)^{-1}x_0 = \\text{exp}(-t\\mu_\\mathfrak{p}(x_1))g_0^{-1}x_0 = g_0^{-1}x_0 = x_1$\r\nfor all $t.$ Thus $g(t)$ and $h(t)$ satisfy the differential equation $g^{-1}\\dot{g} = \\mu_\\mathfrak{p}(g^{-1}x_0).$ These implies that the curves $\\gamma_1 := \\pi \\circ g$ and $\\gamma_2 := \\pi \\circ h$ are geodesics and are negative gradient flow lines of the Kempf-Ness function. Define $\\xi(t)\\in \\mathfrak{p}$ and $k(t) \\in K$ so that\r\n$$\r\nh(t) := g(t)\\text{exp}(\\xi(t))k(t).\r\n$$\r\n\r\n$$\r\nx_1 = h(t)^{-1}g(t)x_0 = k(t)^{-1}\\text{exp}(-\\xi(t))x_0.\r\n$$\r\n\r\nLet\r\n$$\r\n\\rho(t) := d_M(\\gamma_1(t), \\gamma_2(t)) = \\parallel \\xi(t)\\parallel, \\quad \\text{for all t.}\r\n$$\r\n\r\nIf $\\rho \\equiv 0,$ then $\\xi(t) = 0$ for all $t$ and $$\r\nx_1 = k(t)^{-1}x_0\r\n$$ and this means that $x_1\\in K \\cdot x_0.$ Otherwise, for each $t$, let $\\gamma(s,t): [0, 1] \\to M$ defined by\r\n$$\r\n\\gamma(s, t) := \\pi(g(t)\\text{exp}(s\\xi(t)))\r\n$$ be the unique geodesic. Note that $\\gamma(0,t) = \\gamma_1(t)$ and $\\gamma(1,t) = \\gamma_1(t).$\r\n\r\nBy equation \\ref{rho}, we have\r\n\r\n\\begin{align}\\label{eqq}\r\n    \\dot{\\rho}(t) &= -\\frac{1}{\\rho(t)}\\int_0^1\\frac{\\partial^2}{\\partial s^2}\\Phi_{x_0}(g(t)\\text{exp}(s\\xi(t)) \\mathrm d s\\\\\r\n    &= -\\frac{1}{\\rho(t)}\\int_0^1 ( \\xi(t)_X(\\text{exp}(s\\xi(t))x_0), \\xi(t)_X(\\text{exp}(s\\xi(t))x_0) \\mathrm d s.\\label{eqqq}\r\n\\end{align}\r\nChoose a sequence $t_n\\to \\infty$ such that the limits\r\n\r\n$$\r\n\\lim_{n\\to \\infty} \\dot{\\rho}(t_n) = 0, \\quad \\xi_\\infty :=\\lim_{n\\to \\infty}\\xi(t_n), \\quad k_\\infty := \\lim_{n\\to \\infty}k(t_n)\r\n$$ exist. By (\\ref{eqqq}) and since $\\lim_{n\\to \\infty} \\dot{\\rho}(t_n) = 0,$ $\\xi_{\\infty X}(x_0) = 0.$ Then,\r\n\r\n$$\r\nx_1 =  \\lim_{n\\to \\infty}k(t_n)^{-1}\\text{exp}(-\\xi(t_n))x_0 = k_\\infty^{-1}\\text{exp}(-\\xi_\\infty)x_0 = k_\\infty^{-1}x_0.\r\n$$ Hence $x_1\\in K\\cdot x_0$\r\n\\end{proof}\r\n\r\nThe following theorems also hold in the real case.\r\n\\begin{theorem}\\label{MLT} Let $x_0\\in X$ and $x: \\mathbb{R} \\rightarrow X$ the negative gradient flow line of $f$ through $x_0.$ Define $x_\\infty := \\lim_{t\\to\\infty}x(t).$ Then\r\n$$\r\n\\parallel \\mu_\\mathfrak{p}(x_\\infty)\\parallel = \\text{inf}_{g\\in G}\\parallel \\mu_\\mathfrak{p}(gx_0)\\parallel.\r\n$$\r\nMoreover, the $K$-orbit of $x_\\infty$ depends only on the $G$-orbit of $x_0.$\r\n\\end{theorem}\r\n\\begin{proof}\r\nThe limit $x_\\infty$ exists by Theorem \\ref{teo}. The solution of the negative gradient flow line of $f$ through $x_0$ by Lemma \\ref{Gradient} is given by\r\n$$\r\nx(t) = g(t)^{-1}x_0,\r\n$$\r\nwhere $g : \\mathbb{R} \\to G$ is the solution of\r\n\r\n\\[ \\left\\{ \\begin{array}{ll}\r\n         g^{-1}\\dot{g}(t) = \\beta_X(x(t)) \\\\\r\n        g(0) = e,\\quad \\text{where $e$ is the identity of $G$}.\\end{array} \\right. \\]\r\n\r\nFix an element $g_0\\in G$ and let $y : \\mathbb{R}\\to X$ and $h : \\mathbb{R}\\to G$ be the solutions of the differential equations\r\n$$\r\n\\dot{y} = -\\beta_X(y(t)), \\qquad y(0) = g_0^{-1}x_0,\r\n$$ and\r\n$$\r\nh^{-1}\\dot{h} = \\beta_X(y(t)), \\qquad h(0) = g_0.\r\n$$\r\nDefine $\\xi(t) \\in \\mathfrak{p}$ and $k(t)\\in k$ by\r\n$$\r\nh(t) =: g(t)\\text{exp}(\\xi(t))k(t).\r\n$$\r\n\r\nBy Lemma \\ref{Gradient},\r\n$$\r\ny(t) = h(t)^{-1}x_0 = k(t)^{-1}\\text{exp}(-\\xi(t))x(t), \\quad \\forall t\\in \\mathbb{R}.\r\n$$\r\nLet $d_M : M \\times M \\to [0,\\infty)$ be the distance function of the Riemannian metric on $M.$ $\\gamma_1 := \\pi \\circ g$ and $\\gamma_2 := \\pi \\circ h$ are geodesics and are negative gradient flow lines of the so called Kempf-Ness function. Since $M$ is simply connected with nonpositive sectional curvature. Then\r\n$$\r\nd_M(\\gamma_1(t), \\gamma_2(t)) = \\parallel\\xi(t)\\parallel\r\n$$ is nonincreasing. Hence there exist a sequence $t_n\\to \\infty$ such that the limits\r\n\r\n$$\r\n\\xi_\\infty :=\\lim_{n\\to \\infty}\\xi(t_n), \\quad k_\\infty := \\lim_{n\\to \\infty}k(t_n)\r\n$$ exist. Hence\r\n\r\n\r\n$$\r\ny_\\infty = \\lim_{t\\to \\infty}y(t) = \\lim_{t\\to \\infty}k(t)^{-1}\\text{exp}(-\\xi(t))x(t) = k_\\infty^{-1}\\text{exp}(-\\xi_\\infty)x_\\infty.\r\n$$ Which implies that $y_\\infty$ and $x_\\infty$ are critical points of the normed sqaure of the gradient map belonging to the same $G$-orbit. Hence they belong to the same $K$-orbit by Theorem \\ref{critt} and therefore,\r\n$$\r\n\\parallel \\mu_\\mathfrak{p}(x_\\infty)\\parallel = \\parallel \\mu_\\mathfrak{p}(y_\\infty)\\parallel\\leq \\parallel \\mu_\\mathfrak{p}(g_0^{-1}x_0) \\parallel.\r\n$$\r\n\r\n\\end{proof}\r\n\\begin{theorem}\\label{SNUT}. Let $x_0\\in X$ and\r\n$$\r\nm := \\text{inf}_{g\\in G}\\parallel \\mu_\\mathfrak{p}(gx_0)\\parallel.\r\n$$Then\r\n$$\r\nx,y\\in \\overline{G\\cdot x_0}, \\quad \\parallel \\mu_\\mathfrak{p}(x)\\parallel = \\parallel \\mu_\\mathfrak{p}(y)\\parallel = m \\quad \\implies y\\in K\\cdot x.\r\n$$\r\n\\end{theorem}\r\n\\begin{proof}\r\nThe solution of the negative gradient flow line of $f$ through $x_0$ is given by\r\n$$\r\nx(t) = g(t)^{-1}x_0,\r\n$$\r\nFix $g_0\\in G$ and we know that the limit $x_\\infty$ of $x(t)$ exists. Then by Theorem \\ref{MLT},\r\n$$\r\nx_\\infty \\in \\overline{G\\cdot x}, \\quad \\parallel \\mu_\\mathfrak{p}(x_\\infty)\\parallel = m.\r\n$$\r\nLet $x \\in \\overline{G\\cdot x}$ such that $\\parallel \\mu_\\mathfrak{p}(x)\\parallel = m.$\r\n\r\nChoose a sequence $g_n\\in G$ such that\r\n$$\r\nx = \\lim_{n\\to \\infty}g_n^{-1}x_0\r\n$$ and define $y_n : \\mathbb{R}\\to X$ and $x_i\\in X$ by\r\n$$\r\n\\dot{y_n} = -\\beta_X(y_n), \\quad y_n(0) = g_n^{-1}x_0, \\quad x_n := \\lim_{t\\to \\infty}y_n(t).\r\n$$\r\nThen from the estimate of Theorem \\ref{teo}, there exists a constant $c > 0$ such that, for $n$ sufficiently large,\r\n\r\n$$\r\nd(x_n, g_n^{-1}x_0) \\leq \\int_0^\\infty |\\dot{y_n}(t)|dt \\leq c(\\parallel \\mu_\\mathfrak{p}(g_n^{-1}x_0)\\parallel^2 - m^2)^{1-\\alpha}.\r\n$$\r\nSince\r\n$$\r\nm = \\parallel \\mu_\\mathfrak{p}(x)\\parallel = \\lim_{n\\to \\infty}\\parallel \\mu_\\mathfrak{p}(g_n^{-1}x_0)\\parallel,\r\n$$ which implies that $x = \\lim_{n\\to \\infty}x_n$ and $x_n\\in K\\cdot x_\\infty$ for all $n$ by Theorem \\ref{MLT}. Therefore, $x\\in K\\cdot x_\\infty$ because the group orbit $K\\cdot x_\\infty$ is compact.\r\n\\end{proof}\r\n\r\nLet $x\\in X$ be a critical point of $f$ and $y : \\mathbb{R}\\to M$ be the unique solution of the equation\r\n$$\r\n\\dot{y} = -\\beta_X(y(t)), \\quad y(0) = y_0\\in X; \\quad \\beta = \\mu_\\mathfrak{p}(y).\r\n$$\r\nWe define the stable manifold of the critical set $K\\cdot x$ by\r\n\\begin{equation}\\label{Stable manifold}\r\nS(K\\cdot x) := \\{y_0 \\in X : \\lim_{t\\to \\infty}y(t) = kx\\quad \\text{for some}\\quad k\\in K\\}.\r\n\\end{equation}\r\nBy Theorem \\ref{teo}, $X$ is the union of these stable manifolds and each stable manifold is a union of $G$-orbits by Theorems \\ref{MLT} and \\ref{SNUT}. The stable manifolds of the gradient flow  have a structure close to a stratification by stable manifolds corresponding to a Morse-Bott function.\r\n\r\nBy Theorems \\ref{teo}, \\ref{MLT} and \\ref{SNUT}, we have the following result. This result generalises Theorem 5.2 proved by Jablonski for the G-gradient map of a projective representation \\cite{Jab}.\r\n\\begin{theorem}\\label{corr}\r\n  Let $x\\in X$ be a critical point of $f$ and $S(K\\cdot x)\\subset X$ be as defined above. The following holds:\r\n \\begin{enumerate}\r\n     \\item $X = \\bigcup_{x\\in \\text{Crit}(f)}S(K\\cdot x).$\r\n     \\item Let $y_o\\in X.$ Then $y_0\\in S(K\\cdot x)$ if and only if\r\n     \\begin{equation}\\label{eequa}\r\n         x\\in \\overline{G\\cdot y_0}, \\quad \\parallel \\mu_\\mathfrak{p}(x)\\parallel = \\text{inf}_{g\\in G}\\parallel\\mu_\\mathfrak{p}(gy_0)\\parallel\r\n     \\end{equation}\r\n     \\item $S(K\\cdot x)$ is a union of $G$-orbits.\r\n \\end{enumerate}\r\n\\end{theorem}\r\n\r\n\\begin{proof}\r\n(a) follows by Theorem \\ref{teo}.\r\n\r\nTo proof (b); let $y : \\mathbb{R}\\to M$ be the unique solution of the equation\r\n$$\r\n\\dot{y} = -\\beta_X(y(t)), \\quad y(0) = y_0\\in X; \\quad \\beta = \\mu_\\mathfrak{p}(y)\r\n$$ and $y_\\infty := \\lim_{t\\to \\infty}y(t).$\r\nThen, by Lemma \\ref{Gradient} and Theorem \\ref{MLT}, we have\r\n\\begin{equation}\\label{eeqqa}\r\ny_\\infty \\in \\overline{G\\cdot y_0}, \\quad \\parallel \\mu_\\mathfrak{p}(y_\\infty)\\parallel = \\text{inf}_{g\\in G}\\parallel \\mu_\\mathfrak{p}(gy_0)\\parallel.\r\n\\end{equation}\r\nFrom (\\ref{Stable manifold}), $y_0\\in S(K\\cdot x)$ if and only if $y_\\infty \\in K\\cdot x.$ Thus $y_0\\in S(K\\cdot x)$ implies (\\ref{eequa}). Conversely, if $y_0$ satisfies (\\ref{eequa}), then it follows from (\\ref{eeqqa}\r\n) and Theorem \\ref{SNUT} that $y_\\infty \\in K\\cdot x$ and hence, $y_0\\in S(K\\cdot x).$\r\n\r\nTo proof (c); From (ii) and the uniqueness in Theorem \\ref{MLT} that $S(K\\cdot x)$ is a union of $G$-orbits.\r\n\\end{proof}\r\n\r\n\r\n\r\n\r\n\\section{Convexity Properties of Gradient map}\r\n\r\nIn this section, we prove some convexity properties of the gradient map. Both the Abelian and Non-Abelian cases are studied.\r\n\\subsection{The Abelian Convexity Theorem}\\label{Convexity Property of Gradient map}\r\nSuppose $X$ is compact and connected. Let $\\beta \\in \\mathfrak{p}$ and let $$Y = \\{z\\in X : \\text{max}_{x\\in X}\\mu_\\mathfrak{p}^\\beta = \\mu_\\mathfrak{p}^\\beta(z)\\}.$$\r\n\r\nBy corollary \\ref{slice-cor-2}, $Y$ is a smooth, possibly disconnected, submanifold of $X$.\r\n\\begin{lemma}\\label{lemmm}\r\n$Y$ is $G^{\\beta +}$ invariant.\r\n\\end{lemma}\r\n\\begin{proof}\r\n\r\nLet $g\\in G$ and let $\\xi \\in \\liep$. It is easy to check that\r\n\\[\r\n(\\mathrm{d} g)_p (\\xi_X)=(\\mathrm{Ad}(g)(\\xi) )_X (gp ),\r\n\\]\r\nand so $G^\\beta$ preserves $X^\\beta$. We claim that $Y$ is $G^\\beta$-stable. In fact, $G^\\beta = K^\\beta\\text{exp}(\\mathfrak{p}^\\beta)$ and $Y$ is $K^\\beta$ invariant by $K$-invariant property of the gradient map. For each $y\\in Y,$ let $\\xi\\in \\liep^\\beta$ and let $\\gamma(t)= \\text{exp}(t\\xi)\\cdot y.$ Since $\\beta_X (\\gamma(t))=0$ it follows that $\\mup^\\beta (\\gamma(t) )$ is constant and so $\\text{exp} (t\\xi)\\cdot y\\in Y.$ By lemma \\ref{parabolicc}, $G^{\\beta+} = G^\\beta R^{\\beta+}$ where $R^{\\beta+}$ is connected and then unipotent radical of $G^{\\beta+}$. By proposition \\ref{tangent}, $\\lier^{\\beta+} \\subset V_+$ and so $\\lier^{\\beta+}\\cdot z \\subset G_z$ for all $z\\in Y.$ This implies that $R^{\\beta+}$ does not act on $Y$ and the result follows.\r\n\\end{proof}\r\n\r\n\r\n\r\n\\begin{proposition}\\label{closed-orbit-parabolic}\r\n$Y$ contains a closed orbit of $G^{\\beta+}$ which coincides with a $K^\\beta$ orbit.\r\n\\end{proposition}\r\n\\begin{proof}\r\nBy Lemma \\ref{lemmm}, $(G^{\\beta})^o$ preserves any connected component of $Y$ and the restriction of $\\mup$ on any connected component defines a gradient map with respect to $(G^{\\beta})^o$ \\cite{heinzner-schwarz-stoetzel}. By Corollary 6.11 in \\cite{heinzner-schwarz-stoetzel} pag. $21$, $(G^{\\beta+})^o$ has closed orbit which coincides with a $(K^{\\beta})^o$ orbit. Since $G^{\\beta}$ has a finite number of connected components and any connected component of $G^\\beta$ intersects $K^\\beta$, it follows that $G^{\\beta}$ has a closed orbit which coincides with a $K^{\\beta}$ orbit. This is also a closed orbit of $G^{\\beta+}$ since $R^{\\beta+}$ acts freely on $Y$, concluding the proof.\r\n\\end{proof}\r\n\r\nLet $\\mathfrak{a} \\subset \\mathfrak{p}$ be an Abelian subalgebra of $\\mathfrak{p}$ and let $\\pi_\\mathfrak{a} : \\mathfrak{p} \\rightarrow \\mathfrak{a}$\r\nbe the orthogonal projection onto $\\mathfrak{a}.$ It is well known that $\\mu_\\mathfrak{a} := \\pi_\\mathfrak{a} \\circ \\mu_\\mathfrak{p}$ is the gradient map\r\nassociated to $A = \\text{exp}(\\mathfrak{a}).$ Denote $P = \\text{Conv}(\\mu_\\mathfrak{a}(X)).$\r\nIt is well-known, see for instance \\cite[Prop. $3$]{heinzner-schuetzdeller} and \\cite[Prop. $3.1$]{LAP},  that $P$ is\r\nis the convex hull of $\\mu_\\mathfrak{a}(X^A)$, where\r\n$\r\nX^A=\\{p\\in X:\\, A\\cdot p=p\\}.\r\n$\r\nSince $\\mu(X^A)$ is finite, then the convex hull of $\\mu_\\mathfrak{a}(X)$ is a polytope.\r\n\r\nSuppose that the $G$ action on $X$ has a unique closed orbit, which is a $K$ orbit \\cite{heinzner-schwarz-stoetzel} denoted by $\\mathcal O$. Let $\\mathfrak{a}'$ be a maximal Abelian subalgebra containing $\\mathfrak{a}.$ Since $\\mu_\\mathfrak{a}(\\mathcal O) = \\pi_\\mathfrak{a}(\\mu_{\\mathfrak{a}'}(\\mathcal O)),$ By a Theorem of Kostant \\cite{kostant-convexity} it follows that $\\mu_\\mathfrak{a}(\\mathcal O)$ is a polytope.\r\n\r\n\r\n\\begin{theorem}\\label{convexity property}\r\nSuppose the $G$-action on $X$ has a unique closed orbit $\\mathcal O.$ Then $\\mu_\\mathfrak{a}(X) = \\mu_\\mathfrak{a} (\\OO)$ and so it is a convex polytope.\r\n\\end{theorem}\r\n\r\n\\begin{proof}\r\n\r\nLet $\\sigma$ be a face of $P.$ Since $P$ is a polytope, any face is exposed \\cite{schneider-convex-bodies}. Therefore, there exists $\\xi \\in \\lia$ such that\r\n\\[\r\n\\sigma =\\{\\alpha \\in P:\\, \\mathrm{max}_{\\gamma \\in P} \\langle \\gamma, \\xi \\rangle = \\langle \\alpha, \\xi \\rangle \\}.\r\n\\]\r\nThen\r\n$$Y = \\mu_\\lia^{-1}(\\sigma) = \\{z\\in X : \\text{max}_{x\\in X}\\mu_\\mathfrak{p}^\\xi(x) = \\mu_\\mathfrak{p}^\\xi(z)\\}.$$\r\nIn fact, it is easy to see that $Y\\subset \\mu_\\lia^{-1}(\\sigma).$ Suppose $z\\in \\mu_\\lia^{-1}(\\sigma),$ then $\\mu_\\lia(z) \\in \\sigma$ and $\\mathrm{max}_{\\gamma \\in P} \\langle \\gamma, \\xi \\rangle = \\langle \\mu_\\lia(z), \\xi \\rangle = \\mu_\\mathfrak{p}^\\xi(z).$ Hence, $z\\in Y.$ By Lemma \\ref{lemmm}, $Y$ is $G^{\\xi+}$-invariant. By Proposition \\ref{closed-orbit-parabolic}, let $z\\in Y$ be such that $G^{\\xi+} \\cdot z$ is closed. But $G = KG^{\\xi+},$ then $G\\cdot z$ is closed. Since the action of $G$ has a unique orbit, $G\\cdot z = \\OO.$ Since $X$ is compact and $\\OO$ is closed in $X,$ both $P$ and $\\mu_\\mathfrak{a}(\\OO)$ are convex bodies and for all $\\beta \\in \\mathfrak{a},$\r\n$$\\text{max}_{x\\in P}\\langle x, \\beta \\rangle = \\text{max}_{x\\in\\mu_\\mathfrak{a}(\\OO)}\\langle x, \\beta \\rangle.$$ By Proposition \\ref{convex-criterium}, $\\mu_\\mathfrak{a}(\\OO) = P.$ Hence, $\\mu_\\mathfrak{a}(X) = \\mu_\\mathfrak{a} (\\OO)$ is a polytope.\r\n\\end{proof}\r\n\\begin{remark}\r\nIn the above assumption, applying the main Theorem in \\cite{LAP}, the convex hull of $\\mup(X)$ coincides with the convex hull of $\\mup(\\OO)$. Hence the convex hull of $\\mup(X)$ is the convex hull of a $K$-orbit in $\\liep$ and so a polar orbitope \\cite{LA}.\r\n\\end{remark}\r\n\\subsection{Abelian convexity fron Non-Abelian convexity}\r\nThe Non-Abelian convexity theorem implies the Abelian convexity theorem. This is the purpose of this section.\r\n\r\nLet $\\mathfrak{a}\\subset \\mathfrak{p}$ be a maximal Abelian, $\\mathfrak{a}_+$ positive Weyl Chamber. If $\\lambda \\in \\mathfrak{a}_+,$ we denote by\r\n$$\r\n\\roots_\\la = \\text{Conv}\\{w\\lambda : w\\in W\\},\r\n$$ where $W = \\frac{N_k(\\mathfrak{a})}{C_k(\\mathfrak{a})}$ is the Weyl-Group. If $G=U^\\C$, then the following result is proved in \\cite{gs}. The authors applyed the Kirwan'Theorem \\cite{kirwan-convexity} for the action $U\\times \\mathbb T$ on the cotangent bundle $T^* U$, where $\\mathbb T$ is a maximal torus of $U$. Our proof uses a result of Gichev \\cite{gichev-polar}.\r\n\\begin{theorem}\r\nIf $S\\subset \\mathfrak{a}_+$ is a convex subset, then,\r\n\\begin{equation}\\label{Convex}\r\nS^\\# = \\bigcup\\{\\roots_\\la : \\la \\in S\\}\r\n\\end{equation} is convex subset of $\\mathfrak{a}.$\r\n\\end{theorem}\r\n\\begin{proof}\r\nLet\r\n$$\r\n\\roots_0 = \\{(\\la, \\mu) \\in \\mathfrak{a}_+\\times \\mathfrak{a} : \\mu \\in \\roots_\\la\\}.\r\n$$ We claim that $\\roots_0$ is convex. Let $(\\la_1, \\mu_1), (\\la_2, \\mu_2) \\in \\roots_0$\r\n$$\r\nt(\\la_1, \\mu_1) + (1-t)(\\la_2, \\mu_2) = (t\\la_1 + (1-t)\\la_2, t\\mu_1 + (1-t)\\mu_2)\r\n$$\r\nNow, from \\cite{gichev-polar} we have\r\n$$\r\n\\roots_{(t\\la_1 + (1-t)\\la_2)} = \\roots_{t\\la_1} + \\roots_{(1-t)\\la_2} = t\\roots_{\\la_1} + (1-t)\\roots_{\\la_2}\r\n$$\r\nand so\r\n$$\r\nt\\mu_1 + (1-t)\\mu_2\\in \\roots_{(t\\la_1 + (1-t)\\la_2)}.\r\n$$\r\nThis proves $\\roots_0$ is convex.\r\nLet\r\n$$\r\n\\pi_1 : \\mathfrak{a}_+\\times \\mathfrak{a} \\to \\mathfrak{a}_+\r\n$$ and\r\n$$\r\n\\pi_2 : \\mathfrak{a}_+\\times \\mathfrak{a} \\to \\mathfrak{a}.\r\n$$\r\nThen,\r\n$$\r\nS^{\\#} = \\pi_2(\\pi_1^{-1}(S)\\cap \\roots_0)\r\n$$\r\nand so it is convex.\r\n\\end{proof}\r\n\\begin{theorem}\r\nLet $\\mu_\\mathfrak{p} : X \\to \\mathfrak{p}$ be the gradient map. Let $\\mathfrak{a}\\subset \\mathfrak{p}$ be a maximal Abelian subalgebra and let $\\mu_\\mathfrak{a} = \\pi_\\mathfrak{a} \\circ \\mu_\\mathfrak{p}$ be the corresponding gradient map. If $\\mu_{\\mathfrak p} (X) \\cap \\mathfrak a_+$ is convex, then\r\n\\begin{equation}\\label{Convex2}\r\n\\mu_\\mathfrak{a}(X) = (\\mu_\\mathfrak{p}(X) \\cap \\mathfrak{a}_+)^\\# ,\r\n\\end{equation}\r\nand so convex.\r\n\\end{theorem}\r\n\\begin{proof}\r\nThe action of $K$ on $\\mathfrak{p}$ is polar and $\\mathfrak{a}$ is a section \\cite{Da}. Moreover, if $x\\in \\mathfrak{p},$ then\r\n$$\r\nK\\cdot x \\cap \\mathfrak{a}_+ = \\{\\la\\}.\r\n$$\r\nBy a beautiful Theorem of Kostant \\cite{kostant-convexity}, $\\pi_\\mathfrak{a} (K\\cdot x )=\\Delta_\\lambda$ and so a polytope.\r\nTherefore\r\n$$\r\n\\mu_\\mathfrak{a}(X) = \\{\\mu\\in \\mathfrak{a} : \\mu \\in \\roots_\\la, \\quad \\text{where}\\quad \\la \\in \\mu_\\mathfrak{p}(X)\\cap \\mathfrak{a}_+\\} = (\\mu_\\mathfrak{p}(X) \\cap \\mathfrak{a}_+)^\\#.\r\n$$\r\nBy the above Theorem, $\\mu_\\mathfrak{a}(X)$ is convex.\r\n\\end{proof}\r\n\\subsection{Convexity Results of the Gradient Map}\r\nIn this section, we investigate convexity Abelian property of the gradient map. First of all, we give a new proof of the convexity Abelian property of the gradient map for $X = Z,$ avoiding the Linearization Theorem.\r\n\\begin{theorem}\\label{Kahlar}\r\nSuppose $(Z, \\omega)$ is a connected and compact \\Keler manifold. Then\r\n$$\\mu_\\mathfrak{a} : Z \\to \\mathfrak{a}$$ is a convex polytope.\r\n\\end{theorem}\r\n\r\n\\begin{proof}\r\nLet $\\xi\\in \\mathfrak{a}$ and $\\{t_n\\}_{n\\in \\mathbb{N}}$ be a sequence such that $t_n \\to \\infty.$ Denote by $g_n = \\text{exp}(t_n\\xi).$\r\nBy Theorem 2 in \\cite{LG2}, up to passing to a subsequence, there exist a proper analytic subset $U$ of $Z$ such that.\r\n$$\r\ng_n : Z\\setminus U \\to Z, \\quad g_n \\to \\phi_\\infty.\r\n$$ Where $\\phi_\\infty$ is Non-dominant meromorphic map. Note that $Z\\setminus U$ is connected and Zarinski open. Since $g_n = \\text{exp}(t_n\\xi),$ it follows $\\phi_\\infty (Z\\setminus U) \\subset Z^{\\xi^\\#}$ where\r\n$$\r\nZ^{\\xi^\\#} = \\{x\\in Z : \\xi^\\#(x) = 0\\}.\r\n$$\r\nThe vector $J\\xi$ is a Killing vector field and so $Z^{\\xi^\\#}$ is smooth, possibly disconnected, submanifold of $Z.$ Since $Z\\setminus U$ s connected, $\\phi_\\infty(Z\\setminus U)$ is contained in a connected components of $Z^{\\xi^\\#}$ which we denoted by $Z_0.$\r\n\r\nSet $\\mu_\\mathfrak{a}^\\xi := \\langle \\mu_\\mathfrak{a}, \\xi \\rangle.$ $\\mu_\\mathfrak{a}^\\xi(Z_0)$ is constant. We claim that\r\n$$\r\n\\mu_\\mathfrak{a}^\\xi(Z_0) = \\text{max}_{z\\in Z}\\mu_\\mathfrak{a}^\\xi.\r\n$$\r\nIndeed, let $x_0\\in Z:$\r\n$$\r\n\\mu_\\mathfrak{a}^\\xi(x_0) = c = \\text{max}_{z\\in Z}\\mu_\\mathfrak{a}^\\xi.\r\n$$ Let $\\epsilon > 0.$ There exist neighbourhood $N$ of $x_0$ such that\r\n$$\r\n\\mu_\\mathfrak{a}^\\xi(N) \\subset (c-\\epsilon, c].\r\n$$ $(Z\\setminus U)\\cap N \\neq \\emptyset.$ Pick $p\\in (Z\\setminus U)\\cap N.$ Then\r\n$$\r\nc-\\epsilon \\leq \\mu_\\mathfrak{a}^\\xi(p) \\leq \\mu_\\mathfrak{a}^\\xi(\\phi_\\infty (p)).\r\n$$\r\nTherefore\r\n$$\r\n\\mu_\\mathfrak{a}^\\xi(Z_0) = c = \\text{max}_{z\\in Z}\\mu_\\mathfrak{a}^\\xi.\r\n$$\r\n\r\nLet $P = \\text{Conv}(\\mu_\\mathfrak{a}(Z))$. For any $p\\in Z,$ we have by \\cite{heinzner-stoetzel} and \\cite{LG}\r\n$$\r\n\\mu_\\mathfrak{a}(\\overline{A\\cdot p}) = \\text{Conv}(\\mu_\\mathfrak{a}(Z^A \\cap \\overline{A\\cdot p})),\r\n$$ where\r\n$$Z^A = \\{z\\in Z : A\\cdot z = z\\}$$\r\nIt is well known that $\\mu_\\mathfrak{a}(Z^A)$ is finite. Therefore $P = \\text{Conv}(\\mu_\\mathfrak{a}(Z))$ is a polytope and $P = \\text{Conv}(\\mu_\\mathfrak{a}(Z^A)).$\r\n\r\nLet $x_0, x_1, \\cdots, x_n \\in P$ be verticies. Since $P$ is a polytope any face is exposed. Then there exist $\\xi_0, \\xi_1, \\cdots, \\xi_n \\in \\mathfrak{a}$ such that\r\n$$\r\nx_i = \\{\\theta \\in P: \\langle\\theta, \\xi_i\\rangle = \\text{max}_{y\\in P}\\langle y, \\xi_i\\rangle, i = 0, 1, \\cdots, n  \\}.\r\n$$\r\nDenote $c_i = \\langle x_i, \\xi_i\\rangle$. There exists $U_0, \\cdots, U_n$ proper analytic subset and $t_N \\to +\\infty$ such that\r\n$\r\n\\lim_{N \\to \\infty}\\text{exp}(t_N\\xi_i)\r\n$ exists in $Z\\setminus U_i.$ Moreover, if $z_i\\in Z\\setminus U_i,$ then\r\n\r\n$$\r\n\\lim_{N \\to \\infty}\\text{exp}(t_N\\xi_i)\\cdot z_i \\in (\\mu_\\mathfrak{a}^{\\xi_i})^{-1}(c_i) = \\mu_\\mathfrak{a}^{-1}(X_i).\r\n$$\r\nNow,\r\n\r\n$$\r\n(Z\\setminus U_0) \\cap (Z\\setminus U_1) \\cap \\cdots \\cap (Z\\setminus U_n) = Z\\setminus (U_0 \\cup  U_1 \\cup \\cdots \\cup  U_n)\\neq \\emptyset,\r\n$$ then\r\n$$\r\n\\overline{A\\cdot p}\\cap \\mu_\\mathfrak{a}^{-1}(x_i) \\neq \\emptyset,\r\n$$ whenever $p\\in Z\\setminus (U_0 \\cup  U_1 \\cup \\cdots \\cup  U_n)$ for any $i = 0, \\cdots, n.$ By Atiyah-Theorem, see \\cite{LG, heinzner-schwarz-stoetzel},\r\n\r\n$$\r\n\\mu_\\mathfrak{a}(\\overline{A\\cdot p}) = \\text{Conv}(\\mu_\\mathfrak{a}(Z^A)\\cap \\overline{A\\cdot p}) = P.\r\n$$ Therefore\r\n$$\r\n\\mu_\\mathfrak{a}(Z) = P.\r\n$$\r\n\\end{proof}\r\n\r\n\\begin{corollary}\r\nIn the above setting, the following hold true:\r\n\\begin{enumerate}\r\n    \\item $\\{p\\in Z: \\mu_\\mathfrak{a}(\\overline{A\\cdot p}) = \\mu_\\mathfrak{a}(Z) \\}$ contains an open and dense subset of $Z.$\r\n\r\n    \\item Any local maximal of $\\mu_\\mathfrak{a}^\\xi$ is a global maximal. Indeed, we have proved that the unstable manifold of the critical component $c_0$ corresponding to the maximum is Zarinski open.\r\n\\end{enumerate}\r\n\\end{corollary}\r\n\r\nWe now prove the convexity property of the gradient map when $X$ is a connected, compact coisotropic submanifold of $(Z,\\omega).$\r\n\\begin{definition}\\label{coisotropic}\r\nA submanifold $X\\subset (Z, \\omega)$ is coisotropic if for any $p\\in X,$ we have\r\n$$\r\n(T_pX)^{\\bot_\\omega} \\subset T_pX.\r\n$$\r\n\\end{definition}\r\n\r\nSince $(Z, \\omega)$ is \\keler,\r\n\r\n$$\r\n(T_pX)^{\\bot_\\omega} = J((T_pX)^{\\bot}).\r\n$$\r\n\r\n\\begin{lemma}\\label{lemaa}\r\nIf $X$ is coisotropic, then for any $p\\in X$, we have\r\n$$\r\nT_pX + J(T_pX) = T_pZ.\r\n$$\r\n\\end{lemma}\r\n\\begin{proof}\r\n$$\r\nJ((T_pX)^{\\bot}) \\subset T_pX.\r\n$$\r\nApplying $J$ we have\r\n$$\r\n(T_pX)^{\\bot} \\subset J(T_pX).\r\n$$\r\nAnd so\r\n$$\r\nT_pX + J(T_pX) = T_pZ.\r\n$$\r\n\\end{proof}\r\n\\begin{lemma}\\label{open} Let $X$ be a $A$-invariant compact connected coistropic submanifold of $(Z,\\omega).$\r\nLet $\\xi\\in \\mathfrak{a}.$ Then\r\n$$\r\n\\text{max}_{p\\in X}\\mu_\\mathfrak{a}^\\xi = \\text{max}_{z\\in Z}\\mu_\\mathfrak{a}^\\xi.\r\n$$ Moreover, the unstable manifold associated to the maximum of $\\mu_\\mathfrak{a}^\\xi$ is open and dense.\r\n\\end{lemma}\r\n\\begin{proof}\r\nLet $W_0^\\xi$ be the unstable manifold of the critical component $C_0$ satisfying $\\mu_\\mathfrak{a}^\\xi(C_0) = c_0.$ Assume that $C_0$ is a Local maximum. Since\r\n$$\r\n\\nabla \\mu_\\mathfrak{a}^\\xi|_X = \\nabla \\mu_\\mathfrak{a}^\\xi,\r\n$$ it follows that\r\n\r\n$$\r\nW_0^\\xi = \\bar{W_0^\\xi} \\cap X,\r\n$$ where $\\bar{W_0^\\xi}$ is the unstable manifold in $Z$ of the critical components $\\bar{C}_0$ such that\r\n$$\r\n\\mu_\\mathfrak{a}^\\xi(\\bar{C}_0) = \\mu_\\mathfrak{a}^\\xi(C_0) = c_0.\r\n$$\r\nBy a Linearization theorem in \\cite{PG}, $\\bar{W_0^\\xi}$ is a complex manifold and $W_0^\\xi$ is open in $X$. Let $p\\in W_0^\\xi.$ Since\r\n$$\r\nT_pW_0^\\xi = T_pX \\subset T_p\\bar{W_0^\\xi},\r\n$$\r\nit follows that\r\n$$\r\nT_pX + J(T_pX) \\subset T_p\\bar{W_0^\\xi}.\r\n$$ By Lemma \\ref{lemaa}, $\\bar{W_0^\\xi}$ is open and so $C_0$ is the maximum of\r\n$\\mu_\\mathfrak{a}^\\xi : Z \\to \\mathbb{R}.$\r\n\r\nThis also implies that $W_0^\\xi$ is open and dense in $X.$ Indeed, we have proved that a local maximum of $\\mu_\\mathfrak{a}^\\xi : X \\to \\mathbb{R}$ is a global maximum. Since $\\mu_\\mathfrak{a}^\\xi$ is Morse-Bott, it follows that the unstable manifold associated to the maximum of $\\mu_\\mathfrak{a}^\\xi$ is open and dense.\r\n\\end{proof}\r\n\\begin{theorem}\\label{convexity-coisotropic}\r\nIf $X$ is a $A$-invariant compact connected coistropic submanifold of $(Z,\\omega).$ Then\r\n$$\r\n\\mu_\\mathfrak{a}(X) = \\mu_\\mathfrak{a}(Z),\r\n$$ and so a polytope. Moreover, there exists a subset an open and dense subset $W$ of $X$  such that for any $p\\in W$, we have\r\n\\[\r\n\\mu_\\mathfrak{a}(X) =\\overline{\\mu_{\\mathfrak a} (A\\cdot p)}.\r\n\\]\r\n\\end{theorem}\r\n\\begin{proof}\r\nLet $\\xi\\in \\mathfrak{a},$ by Lemma \\ref{open},\r\n$$\r\n\\text{max}_{p\\in X}\\mu_\\mathfrak{a}^\\xi = \\text{max}_{z\\in Z}\\mu_\\mathfrak{a}^\\xi,\r\n$$ and the unstable manifold associated to the maximum of $\\mu_\\mathfrak{a}^\\xi$ is open and dense. By Proposition \\ref{convex-criterium},\r\n$$\r\n\\mu_\\mathfrak{a}(X) = \\mu_\\mathfrak{a}(Z).\r\n$$\r\nBy Proposition 3.1 in \\cite{LG}, the set\r\n\\[\r\n\\left\\{p\\in X : \\mu_\\mathfrak{a}(\\overline{A\\cdot p}) = \\mu_\\mathfrak{a}(X)\\right\\}\r\n\\] is open and dense, concluding the proof.\r\n\\end{proof}\r\n\\section{Two orbits variety}\r\nIn this section we investigate two orbits variety.\r\n\\begin{definition}\r\nLet $X$ be a compact and connected $G$-stable submanifold of $(Z,\\omega)$. We say that $X$ is a two orbit variety if $G$-action on $X$ has two orbits.\r\n\\end{definition}\r\nS. Cupit-Foutou obtained the classification of a complex algebraic varieties on which a reductive complex algebraic group acts with two orbits \\cite{Cupit-Foutou}.\r\n\r\nThe norm square $f$ has a maximum and a minimum. By the stratification theorem, keeping in mind that the strata are $G$-invariant, $X$ is the union of a closed $G$-orbit $S_{\\beta_{max}}$, where the norm square achieves the maximum, and an open $G$-orbit $S_{\\beta_{min}}$, the stratum relative to the minimum of the norm square. We then show that $f$ is a Morse-Bott function.\r\n\\begin{theorem}\\label{two-orbit-Morse} If $G$ acts on $X$ with two orbits, then\r\n\\begin{enumerate}\r\n    \\item the function $f : X \\rightarrow \\mathbb{R}$ given by\r\n\\begin{equation*}\r\n    f(x) := \\frac{1}{2}\\parallel \\mu_\\mathfrak{p}(x)\\parallel^2 \\qquad \\text{for}\\quad x \\in X.\r\n\\end{equation*} is Morse-Bott; It has only two connected critical submanifolds given by the closed $G$-orbit $S_{\\beta_{max}}$, which realizes the maximum of $f$ and by a $K$-orbit $S_{\\beta_{min}}$ which realizes the minimum.\r\n\r\n    \\item The Poincar\\'{e} polynomial $P_X(t)$ of $X$ satisfies\r\n    $$\r\n    P_X(t) = t^k\\cdot P_{S_{\\beta_{max}}}(t) +P_{S_{\\beta_{min}}}(t) - (1+t)R(t),\r\n    $$\r\n    where $k$ is the real codimension of $S_{\\beta_{max}}$ in $X$ and $R(t)$ is a polynomial with positive integer coefficients. In particular $\\chi(X) = \\chi(S_{\\beta_{max}}) + \\chi(S_{\\beta_{min}});$\r\n\r\n    \\item The $K$-equivariant Poincar\\'{e} series of $X$ is given by\r\n\r\n    $$\r\n    P^K_X(t) = t^k\\cdot P^K_{S_{\\beta_{max}}}(t) + P^K_{S_{\\beta_{min}}}(t).\r\n    $$\r\n\\end{enumerate}\r\n\\end{theorem}\r\n\\begin{proof}\r\nWe first proof (a). Consider the function $f$ and its critical set $C.$ $f$ is non constant on $X;$ in fact if $f$ is constant, then every point of $X$ is a maximum point and in view of proposition \\ref{lemm}, all $G$-orbit would be closed.\r\n\r\n\r\nBy Theorem \\ref{critt}, we have that $S_{\\beta_{min}}$ consist of a single $K$-orbit, and so it is connected.\r\n\r\nSince $f$ realizes its maximum value at any critical point $x$ belonging to $S_{\\beta_{max}},$ then by Proposition \\ref{Hessian comp}d\r\n$$\r\nH_x(f) < 0\\quad \\text{on} \\quad T_x(S_{\\beta_{max}})^\\bot.\r\n$$\r\nNow we show that the Hessian of $f$ at a critical point $x$ belonging to $S_{\\beta_{min}}$ is non degenerate in the normal direction. Set $\\mu_\\mathfrak{p} (x) = \\beta_{min} = \\beta.$\r\n\r\nSuppose $\\beta \\neq 0.$ By Remark \\ref{Hessian comp1},\r\n\r\n$$T_xS_{\\beta_{min}} = T_x(G\\cdot x) = T_x(K\\cdot x) \\oplus \\mathfrak{p}^\\beta \\cdot x \\oplus \\lier^{\\beta+}\\cdot x,\r\n$$\r\nwhere $\\lier^{\\beta+}$ is the Lie algebra of $R^{\\beta+}.$ By Proposition \\ref{Hessian comp}b,\r\n\r\n$$\r\nH_x(f) > 0\\quad \\text{on} \\quad \\mathfrak{p}^\\beta \\cdot x \\oplus \\lier^{\\beta+}\\cdot x.\r\n$$ Since $H_x(f)\\geq 0,$ it follows that\r\n$$\r\nT_x(G\\cdot x) = T_x(K\\cdot x) \\oplus^\\bot (\\mathfrak{p}^\\beta \\cdot x \\oplus \\lier^{\\beta+}\\cdot x)\r\n$$\r\n\r\nSuppose $\\beta = 0$. Let $x = x_{min}.$ By Theorem \\ref{critt}, $\\mu_\\mathfrak{p}^{-1}(0) = K\\cdot x.$ \n$$\r\n\\text{ker}\\;\\;d\\mu_\\mathfrak{p}(x) = (\\mathfrak{p} \\cdot x)^\\bot\r\n$$\r\nBy Proposition \\ref{Hessian}, $$H_x(f)|_{(\\mathfrak{p}\\cdot x)} > 0.\r\n$$\r\n$$\r\nT_{x}X = T_{x}(G\\cdot x).\r\n$$\r\n\r\n\r\n\r\nSince $K\\cdot x_{min} \\subset \\text{ker}\\;\\;d\\mu_\\mathfrak{p}(x_{min}),$\r\nit follows that\r\n\r\n$$\r\nT_{x}(G\\cdot x) = K\\cdot x + \\mathfrak{p}\\cdot x = T_{x}X\r\n$$\r\n$$\r\nH_x(f)|_{(K\\cdot x)} = 0.\r\n$$\r\n$$\r\nT_{x}X = K\\cdot x \\oplus^\\bot \\mathfrak{p}\\cdot x.\r\n$$ By dimensional reason, $H_x(f)$ is non degenerate.\r\n\r\n\r\n\r\n\r\nThese show that $H_x(f)$ is non degenerate. Hence $f$ is Morse-Bott. The statements in (b) and (c) follow from the general theory in \\cite{Kirwan}.\r\n\\end{proof}\r\n\r\n\r\nNow, we investigate\nthe convexity property of gradient map of a two orbit variety. Since $X$ has a unique closed orbit, we derive the following result.\r\n\\begin{theorem}\r\nIf $X$ is a compact and connected two orbits variety, then $\\mu_\\mathfrak{a}(X)$ is a polytope.\r\n\\end{theorem}\r\n\\begin{proof}\r\nA two orbits variety have one closed orbit while the other is open. Hence, it has a unique closed orbit and the result follows from Theorem \\ref{convexity property}.\r\n\\end{proof}\r\n\r\n We now prove the nonAbelian convexity Theorem for a compact and connected two orbit variety $X$. This result was proved by Kirwan for the case when $X$ is a symplectic manifold. In his case, the proof strongly rely on the fact that the strata are symplectic submanifolds with even dimension. This allows to show that the set of points in $X$ on which the function $f$ takes its minimal value is connected. But this does not hold if $X$ is a real submanifold of a \\Keler manifold $Z$ which is our case. In fact, a strata can be of codimension one. However, we can still show that the distance square function (Lemma \\ref{connected}) takes it's minimal value on a connected subset of $X$ when the $G$-action on $X$ has two orbits with a unique open orbit.\r\n\r\n\\begin{theorem}\\label{NACT}\r\nSuppose the $G$-action $X$ has two orbits with a unique open orbit. Let $\\mathfrak{a}_+$ be a closed Weyl chamber. Then $\\mu(X) \\cap \\mathfrak{a}_+$ is a convex polytope.\r\n\\end{theorem}\r\n\r\nThe proof of this theorem rely on the following Lemma.\r\n\r\n\\begin{lemma}\\label{connected}\r\nLet $\\beta \\in \\mathfrak{a}_+.$ Then the function $\\varphi: X \\to \\mathbb{R}$ defined by\r\n\\begin{equation}\r\n    \\varphi(x) = \\parallel \\mu_\\mathfrak{p}(x) - \\beta\\parallel^2\r\n\\end{equation} takes its minimum value on a connected subset of $X$.\r\n\\end{lemma}\r\n\r\n\\begin{proof}\r\nLet $X_{-\\beta}$ be the real flag of $K$ through $-\\beta.$ The action of $K$ on $X \\times X_{-\\beta}$ has a gradient map given as\r\n\\begin{equation}\r\n\\mu_\\mathfrak{p}(x, \\xi) =  \\mu_\\mathfrak{p}(x) + \\xi.\r\n\\end{equation}\r\nDefine $\\bar{\\varphi}: X \\times X_{-\\beta} \\to \\mathbb{R}$ by\r\n$$\r\n\\bar{\\varphi}(x, \\xi) = \\parallel \\mu_\\mathfrak{p}(x, \\xi)\\parallel^2 = \\parallel \\mu_\\mathfrak{p}(x) + \\xi\\parallel^2\r\n$$\r\n\r\n\r\n\\textbf{Claim 1:} The function $\\bar{\\varphi}$ takes it's minimum value on a connected subset Min($\\bar{\\varphi}$) of\r\n$X \\times X_{-\\beta}$. In fact, since $G$ act on $X$ with two orbits and the Min($\\parallel \\mu_\\mathfrak{p}(x)\\parallel^2$) is associated with the strata\r\n$S_{\\beta_{min}}$ relative to the minimum which is a $G$-invariant open subset. Since $X$ is a two orbit variety, it follows that $S_{\\beta_{min}}$ is connected.\r\n$S_{\\beta_{min}}$ flows to Min($\\bar{\\varphi}$) and so Min($\\bar{\\varphi}$) is connected. \n\r\n\r\n\r\nNow, let $Y :=$ Min($\\bar{\\varphi}) \\cap (\\{-\\beta\\} \\times M).$\r\n\r\n\\textbf{Claim 2:} $Y = \\{-\\beta\\} \\times$ Min(${\\varphi})$. In fact,\r\n\r\n\\begin{align*}\r\n\\bar{\\varphi}(-Ad^*_k\\beta, z) & = \\parallel \\mu_\\mathfrak{p}(z)-Ad^*_k\\beta\\parallel^2\\\\\r\n& = \\parallel k^{-1}\\mu_\\mathfrak{p}(z)k-\\beta\\parallel^2 \\qquad \\text{Since $|\\cdot|$ is $K$-invariant}\\\\\r\n&=  \\parallel \\mu_\\mathfrak{p}(k^{-1}z)-\\beta\\parallel^2 \\qquad \\text{Since $\\mu_\\mathfrak{p}$ is $K$-equivariant}\\\\\r\n& = \\varphi(k^{-1}z),  \\qquad \\text{for all $z\\in X$ and $k\\in K$}.\r\n\\end{align*}\r\nSuppose $x\\in$ Min(${\\varphi})$, then\r\n$$\\bar{\\varphi}(-Ad^*_k\\beta, z) = \\varphi(k^{-1}z) \\geq \\varphi(x) = \\bar{\\varphi}(-\\beta, x)\r\n$$ for all $z\\in X$ and $k\\in K.$ Hence, $(-\\beta, x)\\in$Min($\\bar{\\varphi}).$ And if $(-\\beta, x)\\in$Min($\\bar{\\varphi}),$\r\n$$\\varphi(z) = \\bar{\\varphi}(-\\beta, z) \\geq \\bar{\\varphi}(-\\beta, x) = \\varphi(x).$$\r\n\r\n\\textbf{Claim 3:} $Y$ is connected. In fact,from claim 1, there is a curve $\\alpha: [0,1] \\to$ Min($\\bar{\\varphi}$) connecting two points $(-\\beta, x_0)$ and $(-\\beta, x_1)$ of $Y$ such that $\\alpha(0) = (-\\beta, _0)$ and $\\alpha(1) = (-\\beta, x_1).$ Let $\\alpha(t) = (\\xi_t, x_t).$ There is a curve $k_t$, $t\\in [0,1],$ such that for all $t:$\r\n$$\\xi_t = -Ad^*_{k_t}\\beta,$$ and $k_o = e.$\r\n\r\nDefine the curve $\\delta : [0,1] \\to Y,$ by $\\delta(t) = (-\\beta, k_t^{-1})x_t).$ for all $t,$\r\n$$\r\n\\bar{\\varphi}(\\delta(t)) = \\parallel \\mu_\\mathfrak{p}(k_t^{-1})x_t) - \\beta\\parallel^2 = \\parallel \\mu_\\mathfrak{p}(x_t) -Ad^*_{k_t}\\beta\\parallel^2 = \\parallel \\mu_\\mathfrak{p}(x_t) + \\xi_t\\parallel^2 = \\bar{\\varphi}(\\xi_t, x_t) = \\bar{\\varphi}(\\alpha(t)).\r\n$$Which shows that $\\delta(t) \\in Y$ for all $t.$\r\n\r\n$\\delta(0) = (-\\beta, x_0)$ and $\\delta(1) = (-\\beta, k_1^{-1})x_1).$\r\nHence, the curve $\\delta$ can be extended to a curve in $Y$ connecting $(-\\beta, x_0)$ and $(-\\beta, x_1).$\r\n\\end{proof}\r\n\\begin{proof}\\textit{Proof of Theorem \\ref{NACT}:} In \\cite{heinzner-schuetzdeller}, the author proved that $\\mu_\\mathfrak{p}(X) \\cap \\mathfrak{a}_+$ is a polyhedron. We only show that it is convex. Suppose it is not convex. There is a point $\\beta \\in \\mathfrak{a}_+$ and $r > 0,$ such that $B_r(\\beta)$ is tangent to the boundary of $\\mu_\\mathfrak{p}(X) \\cap \\mathfrak{a}_+$ in more than one point. Then, the function $\\parallel \\mu_\\mathfrak{p}(x) - \\beta\\parallel^2$ attains it's minimum value in two or more distinct points. Which contradict Lemma \\ref{connected}.\r\n\r\n\\end{proof}\r\n", "meta": {"timestamp": "2021-06-25T02:20:54", "yymm": "2106", "arxiv_id": "2106.13074", "language": "en", "url": "https://arxiv.org/abs/2106.13074"}}
{"text": "\\section{Introduction}\n Nuclear physics plays a vital role in many important phenomena occurring in the core to the crust of astrophysical objects, this leads to the development of this ever-hot discipline of physics called nuclear astrophysics. It was Bethe who in 1939 first worked on to describe various cycles of stellar evolution with a prime focus on the mechanisms of production of light elements like $^{2}$H, $^{3}$He, $\\alpha$, etc in the Big-Bang nucleosynthesis \\cite{bethe-1939}. Works of Bethe was later advanced by Burbidge 1957 \\cite{burbidge-1957}. Primordial nucleosynthesis is found to be limited up to the production of the isotopes of iron (Fe). Elements heavier than iron are found to be formed through various other processes like neutron capture- Reifarth et al. 2018 \\cite{reifarth-2018}, neutrino-induced reactions- Alvarez-Ruso et al.  2018 \\cite{alvarezruso-2018}, explosive events in supernovae- Wiescher et al. 2012 \\cite{wiescher-2012}, and the rapid-neutron process in neutron-star mergers- Thielemann et al. 2017 \\cite{thielemann-2017}. A systematic review of the most critical nuclear reactions under several nucleosynthesis environments and the status of those reactions giving insight into the speci\ufb01c uncertainties associated with their reaction rates has been presented by Wiescher et al. 2012\\cite{wiescher-2012}. The review reports a large quantity of data on nuclear reactions involving chargeless particles (neutrons) and charged particles (protons and alphas) relevant to nucleosynthesis networks. Processes that play a major role in nuclear reactions involving charged particles are the transfer and capture processes. The transfer process is solely controlled by the strong nuclear interaction while electromagnetic interaction takes the credit for the capture process. When both channels are open the capture cross-section is always smaller than the transfer cross-section. A standard theoretical model for the evaluation of the reaction rates involves energy-dependent reaction cross-section ($\\sigma(E)$) as a part of the integral expression and one of the major challenges nuclear physicists ever faces is the evaluation of the cross-section at stellar energies which are usually much smaller than the height of the mutual Coulomb barrier of the interacting nuclei. As the cross-sections are too small to be determined in the laboratory, direct measurement in the low energy regime ($E\\sim$ 1eV to few keV) is not possible. And theoretical evaluation of astrophysical S-function ($S(E)$) is also model-dependent, hence the uncertainties in the computed S-factor can be significant \\cite{yakovlev-2010} in nuclear physics. As discussed by Broggini et al. 2010 \\cite{broggini-2010}, there exist few experimental techniques developed during the last few decades, and a sound theoretical justification is often needed for a successful explanation of the experimental findings at the stellar energies. Several indirect techniques have also been developed during the last couple of decades like the Trojan Horse method of Baur et al. 1986 \\cite{baur-1986}, Tumino et al 2013 \\cite{tumino-2013}, Spitalery et al. 2019 \\cite{spitaleri-2019}, the Coulomb breakup method of Baur et al. 1986 \\cite{baur-1986}, and the Asymptotic Normalization Coefficient method of Mukhamedzhanov et al. 2001 \\cite{mukhamedzhanov-2001}. However, the methods referred above require a standard theoretical scheme to estimate the reaction cross-section from the captured data.\n \nIn the present work, we studied the energy dependence of nuclear fusion cross-section and astrophysical S-function for the fusion reaction of few light nuclei following an elegant theoretical model. A complex nuclear potential function is considered which facilitates the description of absorption inside the nuclear potential well. Here we adopt the selective resonant tunneling model (SRTM) of Li et al. 2000 \\cite{li-2000} instead of the conventional compound nucleus model. Li et al was the first group to propose the SRTM model to compute data on fusion cross-section of D+T reaction using complex square-well and to compare their findings with the available experimental data.  Later in 2002, Li showed that this model also works well for D+T reaction up to 100 KeV \\cite{li-2002}. Li et al again in 2004 \\cite{li-2004} used the SRTM model to compute fusion cross-sections for D+D and D+$^{3}$He fusion reactions. Recently, Singh et al. 2019 \\cite{sing-2019} applied this model considering a complex square-well nuclear potential to study fusion cross-section and astrophysical S-function for some light nuclear reactions like D+D, D+T, D+ $^{3}$He, p+$^6$Li, p+$^7$Li etc. \n\nIn this work, we will use the complex Gaussian type potential function instead of the complex square-well potential as used by Li et al \\cite{li-2000} to investigate some light nuclear fusion reactions like the D+D and p+$^{11}$B fusion at the deep sub-barrier energy region. The data obtained are to be checked with the well-known three-parameter and five-parameter fitting formula's from the NRL Plasma Formulary \\cite{huba-2013} for reactions having fusion cross-section data available in the literature. The improved values of the astrophysical S-factor for light nuclei is expected to give a better and more clear picture of the elemental abundance in nucleosynthesis.\n\nIn section II, we briefly discuss the theoretical method involved in the selective resonant tunneling model and its application to a light nuclear fusion reaction. Results and discussions will be presented in Section III, and finally, we will present the summary and conclusions in section V.\n \n\\section[]{Theoretical method}\nThe nuclear fusion process is supposed to occur via two independent processes: the first involves the penetration of the incoming projectile through the Coulomb barrier and the second involves the actualization of fusion reaction as described by Clayton et al. 1990 \\cite{clayton-1990}. Resonant tunneling in the light nuclear fusion reaction is sequentially followed by tunneling and decay. A theoretical model based on the assumption of the fact that \\enquote{decay is independent of tunneling} does not give a clear picture of the fusion process pointed out by Gamow in 1938 \\cite{gamow-1938}. Selective resonance tunneling model (SRTM) is different from the well-known compound nucleus model (CNM) because, in the former one, the penetrating particle may still remember its phase unlike in the latter one in which the penetrating particle loses memory of its formation as described in the volume by Feshbach 1992 \\cite{feshbach-1992}. The uniqueness of this model comes from the keyword 'selectivity'. This can be understood if we visualize absorption to act like damping in a resonance. The energy absorbed by a damping process is proportional to the product of the damping coefficient and the square of the amplitude of the oscillation. For a vanishing damping coefficient, the energy absorbed by the damping process is zero even if the resonance develops fully. On the other hand, for a very large value of damping coefficient, the damping process will destroy the resonance before it is fully developed. Thus, the energy absorbed by the damping process is still very small. Hence,\nthere must be some specific damping that makes the absorbed energy maximized. In a similar understanding, the fusion cross-section is proportional to the product of the depth of the imaginary component of nuclear potential and the square of the amplitude of the wave function inside the nuclear well \\cite{frobrich-1996}; thus, there should be suitable damping represented by the depth parameter of the imaginary component of the nuclear potential to make the fusion cross-section maximized. We may call this suitable damping as the matching damping \\cite{li-2000}. Hopefully, it is understood that SRTM selects both of the frequency of oscillation in the energy level and the damping corresponding to the resonance absorption. In the low energy regime, the selectivity becomes very prominent at the resonance energy \\cite{li-2008}, causing a complete suppression of neutron-emission in the SRTM process.\n\nThe Maxwellian-averaged thermonuclear reaction rate $<\\sigma v >$ at some temperature, T is given by the following integral \\cite{boyd-2008}:\n\\begin{equation}\\label{eq01}\n<\\sigma v> = \\sqrt{\\frac{8}{\\pi\\mu (K_BT)^3}}\\int\\sigma(E)E\\exp\\left(-\\frac{E}{K_BT}\\right) dE\n\\end{equation}\nwhere $E$ is the center-of-mass energy, $v$ is the relative velocity and $\\mu$ is the reduced mass of\nreactants. At low energies (far below the Coulomb barrier) where the lassical turning point is much larger than the nuclear radius, barrier penetrability can be approximated by $\\exp(-2\\pi\\zeta)$ so that\nthe charge induced cross section can be decomposed into \n\\begin{equation} \\label{eq02}\n\\sigma(E) = \\frac{S(E)}{E} \\exp(-2\\pi\\zeta)\n\\end{equation}\nwhere $S(E)$ is the astrophysical S-factor and $\\zeta$ is the Sommerfeld parameter, defined by \n\\begin{equation} \\label{eq03}\n\\zeta = \\frac{Z_1Z_2e^2}{\\hbar v}\n\\end{equation}\nwhere $Z_1$ and $Z_2$ are the charges of the reacting nuclei in units of elementary charge $e$. Except for narrow resonances, the S-factor S(E) is a smooth function of energy, which is\nconvenient for extrapolating measured cross sections down to astrophysical energies.\nNuclear fusion reaction in the low energy range ($E\\sim$ 1eV to few keV) can be explained successfully by the phenomenon of quantum mechanical tunneling through the mutual Coulomb barrier of interacting nuclide. For two approaching nuclei of charge numbers $Z_1$, $Z_2$, mass numbers $A_1$ and $A_2$, height of the Coulomb barrier is given by\n\n\\begin{eqnarray}\\label{eq04}\nV_{CB}&=&\\left(\\frac{e^2}{4\\pi\\epsilon_0}\\right)\\left(\\frac{Z_1Z_2}{R}\\right)=\n(1.44)\\left(\\frac{Z_1Z_2}{R_0(A_1^{1/3}+A_2^{1/3})}\\right) MeV\n\\end{eqnarray}\nwhere $R_0$ is the nuclear radius parameter and $R$ is the touching distance between the centers of the interacting nuclei. The cross section for sub-barrier fusion for light nuclei can be calculated using the SRTM assuming a complex nuclear potential (including Coulomb term) of the form\n\\begin{equation}\\label{eq05}\nV(r)= V_{N}(r) +V_{C}(r)\n\\end{equation}\nwhere\n\\begin{equation}\\label{eq06}\nV_{N}(r) = -V_{r}\\exp\\left[  -\\left(\\frac{r}{\\beta_r}\\right)^{2}\\right] + iV_{i}\\exp\\left[  -\\left(\\frac{r}{\\beta_i}\\right)^{2}\\right]\n\\end{equation}\nand\n\\begin{equation}\\label{eq07}\nV_C(r) =\\left\\{ \\begin{array}{l}\n 1.44 \\frac{Z_1Z_2}{2R}\\left(3 - \\frac{r^2}{R^2}\\right), \\: \\textbf{for}\\: r \\leq R\\\\ \n 1.44 \\frac{Z_1Z_2}{r}, \\: \\textbf{for} \\:r > R\\\\ \n\\end{array} \\right.\n\\end{equation}\nThe imaginary component introduced in the nuclear potential facilitates the absorption phenomenon and description of its effect on the associated wave function. The Schor\\\"{o}dinger equation for the nuclear plus Coulomb potential is given by\n\\begin{equation}\\label{eq08}\n\\left(-\\nabla^2 + \\frac{2\\mu}{\\hbar^2}\\left[  V_{N}(r) +V_{C}(r) \\right]-k^2\n\\right)\\psi({\\bf r})=0\n\\end{equation}\nwhere $k^2=\\frac{2\\mu E}{\\hbar^2}$ and $\\psi({\\bf r})$ represents the sum of the nuclear and Coulomb wave function i.e.,\n\\begin{equation}\\label{eq09}\n\\psi({\\bf r}) = \\psi_N({\\bf r})+\\psi_C({\\bf r})\n\\end{equation}\nThe Coulomb wave function contains the incoming wave while the nuclear wave function represents only the outgoing wave in the asymptotic range. \nFor numerical solution of Eq.(\\ref{eq08}), it is reduced to the one-dimensional equation in $r$ given by\n\\begin{equation}\\label{eq10}\n\\left(-\\frac{d^2}{dr^2} +\\frac{l(l+1)}{r^2}+ \\frac{2\\mu}{\\hbar^2}\\left[  V_{N}(r) +V_{C}(r) \\right]-k^2\n\\right)\\psi({r})=0\n\\end{equation}\nThus, when a light nucleus is injected into another light nucleus, the relative motion can be described in terms of the radial wave function $\\psi(r)$ connected to the general solution $\\psi(r,t)$ of Schr$\\ddot{o}$dinger equation for the interacting nuclei as \n\\begin{equation}\\label{eq11}\n\\psi(r,t)=\\frac{1}{\\sqrt{4\\pi}r}\\psi(r) \\exp\\left(-i\\frac{E}{\\hbar}t\\right)\n\\end{equation}\nNow, the reaction cross-section in terms of the phase shift, $\\delta_0$ introduced by the nuclear potential in the wave function at the low energy limit (where only S-wave contributes) is given by\n\\begin{equation}\\label{eq12}\n\\sigma=\\frac{\\pi}{k^2}(1- |\\eta|^2)\n\\end{equation}\nwhere $\\eta=e^{2i\\delta_0}$ and k is the wave number corresponding to the relative motion.\nSince, the chosen nuclear potential is a complex one, the corresponding phase shift $\\delta_0$ will also be a complex number and can be expressed as \n\\begin{eqnarray}\\label{eq13}\n\\cot(\\delta_0)=W_r+i W_i\n\\end{eqnarray}\nwhere $W_r$ and $W_i$ are two parameters connected to the real and imaginary components of the complex wavenumber corresponding the complex nuclear potential. The wavenumber (K) corresponding to the complex nuclear potential can be expressed as\n\\begin{equation}\\label{eq14}\n\\left. \\begin{array}{lcl}\nK &=& \\sqrt{\\frac{2\\mu}{\\hbar^2}[V_r(r)+iV_i(r) -E]}\\\\\n  &=& \\sqrt{(\\frac{2\\mu}{\\hbar^2})[V_r(r)-E]}[1+i\\frac{V_i}{V_r(r) -E}]^{1/2}\\\\\n&=& \\sqrt{\\frac{2\\mu}{\\hbar^2}[V_r(r)-E)]} +i \\sqrt{\\frac{2\\mu}{\\hbar^2}}\\frac{V_i(r)}{2\\sqrt{V_r(r)-E}}\\\\\n&=&K_r+iK_i\\\\\nor, \\: KR &=& K_rR+iK_iR \\: [\\textit{Let}, Z = KR; Z_r = K_rR; Z_i = K_iR]\\\\\n\\Rightarrow Z&=&Z_r+iZ_i\\\\\n\\end{array} \\right\\}\n\\end{equation} \nIn terms of real and imaginary parts of $Z$ the parameters $W_r$ and $W_i$ are defined as\n\\begin{equation}\\label{eq15}\n\\left. \\begin{array}{lcl}\nW_r &=& \\chi^2 \\left[\\left(\\frac{R_C}{R}\\right)\\frac{Z_r\\sin(2Z_r)+Z_i\\sinh(2Z_i)}{2[\\sin^2(Z_r)+\\sinh^2(Z_i)]}\\right] \\\\\n&&-2\\chi^2 \\left[\\ln\\left(\\frac{2R}{R_C}\\right)+2C+h(kR_C)\\right]\\\\\nW_i &=& \\chi^2{\\it Im}\\left[\\frac{R_C}{R} (KR) \\cot(KR)    \\right]\\\\\n&=& \\chi^2\\left[\\frac{R_C}{R}\\frac{Z_i\\sin(2Z_r)-Z_r\\sinh(2Z_i)}{2[\\sin^2(Z_r)+\\sinh^2(Z_i)]}    \\right]\n\\end{array}  \\right\\}\n\\end{equation}\nwhere $R_C = \\frac{\\hbar^2}{Z_1Z_2\\mu e^2}$ is the Coulomb unit of length and $C = 0.577$ is Euler's constant. The function $h(kR_C)$ is connected to the logarithmic derivative of $\\Gamma$ function\n\\begin{equation}\\label{eq16}\nh(y) = \\frac{1}{y^2}\\sum_{i=1}^{\\infty}\\frac{1}{i(i^2+y^{-2})}-C+\\ln(y)\n\\end{equation}\nThe fusion cross-section can then be expressed as\n\\begin{equation}\\label{eq17}\n\\left. \\begin{array}{lcl}\n\\sigma&=&\\left(\\frac{\\pi}{k^2}\\right)\\left(-\\frac{4W_i}{(1-W_i)^2+W_r^2}\\right)\\\\\n&=&\\left(\\frac{\\pi}{k^2}\\right)\\left(\\frac{1}{\\chi^2}\\right)\\left(-\\frac{4\\omega_i}{\\omega_r^2+ (\\omega_i-\\frac{1}{\\chi^2})^2}\\right)\\\\\n\\end{array} \\right\\}\n\\end{equation}\nwhere the quantity\n$\\chi^2=\\left\\{\\frac{\\exp\\left(\\frac{2\\pi}{kR_C}\\right)-1}{2\\pi}\\right\\}$ is related to the Gamow penetration factor. The last factor in Eq. (\\ref{eq17}) within curly braces$\\{\\}$ is called the astrophysical S-factor, which depends on the projectile energy, E and the cross-section $\\sigma(E)$. Thus we have\n\\begin{equation}\\label{eq18}\n\\left. \\begin{array}{lcl}\nS(E)&=&\\left(\\frac{k^2}{\\pi}\\right) \n\\left( \\chi^2 \\right) \\sigma(E)\\\\\n&=&\\left(-\\frac{4\\omega_i}{\\omega_r^2+ (\\omega_i-\\frac{1}{\\chi^2})^2}\\right)\n\\end{array}  \\right\\}\n\\end{equation}\nwhere $\\omega=\\omega_r+i\\omega_i=W/\\chi^2=(W_r+iW_i)/\\chi^2.$\nThe wave function inside the nuclear well (ie., in the region r$ < $R) is determined by two depth parameters- the depths of the real and imaginary components of the nuclear potential ($V_{r}$ and $V_{i}$) and corresponding range parameter $\\beta_r$ and $\\beta_i$ respectively. The Coulomb wave\nfunction outside the nuclear well (r$ > $ R) is determined by two other parameters: the real and the imaginary part of the complex phase shift  $(\\delta_{0r})$ and $(\\delta_{0i})$. A pair of convenient parameters, $W_r$ and $W_i$, are introduced to make a linkage\nbetween the cross section and the nuclear potential. This facilitates a clear understanding of the resonance and the selectivity in damping.\nThe continuity of the wave function at the boundary ($r = R$) can be expressed by the matching of the logarithmic derivative of the wave function.\nIn the above relations, k represents the wavenumber outside the nuclear well and $R_C$ is the Coulomb unit of length.\n\nThe cross-section $\\sigma$ can also be computed using the following three- and five parameter fitting formulae \\cite{li-2008,huba-2013}.\n\\begin{equation}\\label{eq19}\n\\left. \\begin{array}{lcl}\n\\sigma_3(E_{lab})&=&\\left( \\frac{\\pi}{({\\frac{2\\mu}{\\hbar^2})E_{lab}(\\frac{m_2}{m_1+m_2}})}\\right)\\left(\\frac{1}{\\chi^2} \\right)\\\\\n&&\\times \\left(\\frac{(-4C_3)}{(C_1+C_2E_{lab})^2+(C_3-\\frac{1}{\\chi^2})^2}\\right)\\\\\n\\end{array} \\right\\}\n\\end{equation}\n\n\\begin{equation}\\label{eq20}\n\\left. \\begin{array}{lcl}\n\\sigma_5(E_{lab})&=&\\frac{A_5+(A_2/((A_4-A_3E_{lab})^2+1)}{E_{lab}[\\exp(A_1/\\sqrt{E_{lab}})-1]};\\\\\nE_{lab}&=&(1+\\frac{m_1}{m_2})E\\\\\n\\end{array} \\right\\}\n\\end{equation}\n\n\n\\section{Results and discussion}\nIn the present SRTM scheme, we  have four prime tuning parameters $V_{r}$ , $V_{i}$, $\\beta_{r} $, $ \\beta_{i}$ i.e. depths and ranges of real and imaginary components of the chosen nuclear potential. These are adjusted to search the resonance energy. In this case, we fine-tuned these parameters for different nuclear systems to obtain better resonating behavior. Here we also adjust the radius parameter, $R_{0}$ for finding the sharp resonance nature. The fusion cross-sections and astrophysical S-factor are calculated using Eqs. (\\ref{eq16}) \\& (\\ref{eq17}) respectively.\n\n\n\\begin{table}[htbp]\n\\caption[]{Numerical values of the adjustable potential parameters involved in Eqs. (\\ref{eq04}) \\& (\\ref{eq06}).}\\small\\smallskip\n\\tabcolsep=4.6pt\n\\begin{tabular}{@{}ccccccc@{}}\n\\hline\n&&&&&\\\\[-10pt]\n\\textbf{Reactions}&\\textbf{$V_{r}$}   &\\textbf{$V_{i}$}&\\textbf{$\\beta_{r}$}   &\\textbf{$\\beta_{i}$}&\\textbf{$R_{0}$}&Reference \\\\\n\\textbf{}    &\\textbf{(MeV)}&\\textbf{(MeV)}&\\textbf{(fm)}&\\textbf{(fm)}  &\\textbf{(fm)}&label \\\\\n\\hline\n&&&&&\\\\\n[-10pt]D(D, n)$^{3} $He &-36.135 &-0.1330 &1.25&1.50& 2.77795&data-set-0 \\\\\n&&&\\\\\n[-10pt] &-36.135 &-25.2825 &0.42&1.50& 2.30795&data-set-1  \\\\\n&&&\\\\\n[-10pt]&-36.135 &-0.2625 &0.50&1.50& 2.29795&data-set-2 \\\\\n&&&\\\\\n[-10pt]$^{11}$B(p, $\\alpha)^8$Be &-35.700&-0.0129&0.50&1.50& 1.7979&data-set-0   \\\\\n&&&\\\\\n[-10pt]&-35.600&-0.05825&0.50&1.50& 1.88620&data-set-1  \\\\\n\\hline\n\\end{tabular}\n\\label{t01}\n\\end{table}\n\nThe fusion cross-section, $\\sigma$ and the astrophysical S- function are computed by the use of Eqs. (\\ref{eq17}) \\&  (\\ref{eq18}) respectively at energies below the height of the barrier. Plot of the astrophysical S-factor and fusion cross-sections against energy of the projectile are shown in Figures \\ref{fig01}, \\ref{fig02}, \\ref{fig03}, \\ref{fig04} \\& \\ref{fig05} respectively for five different sets of parameters listed in Table \\ref{t01}. \n\n\\begin{figure}\n\t\\centering\n\t\\fbox{\\includegraphics[width=0.45\\linewidth, height=0.35\\linewidth]{khan-fig1a.pdf}}\n\t\\fbox{\\includegraphics[width=0.45\\linewidth, height=0.35\\linewidth]{khan-fig1b.pdf}}\n\t\\caption{(Left) S-factor S(E) in \\textbf{KeV-barn} calculated for D-D fusion reaction using Eq. (\\ref{eq18}), (Right) Comparison of Fusion cross section, $\\sigma$ in \\textbf{barn} calculated by the use of Eqs. (\\ref{eq17}), (\\ref{eq19}) \\& (\\ref{eq20}).}\n\t\\label{fig01}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\fbox{\\includegraphics[width=0.46\\linewidth, height=0.35\\linewidth]{khan-fig2a.pdf}}\n\t\\fbox{\\includegraphics[width=0.46\\linewidth, height=0.35\\linewidth]{khan-fig2b.pdf}}\n\t\\caption{(Left) S-factor S(E) in \\textbf{KeV-barn} calculated for D-D fusion reaction using Eq. (\\ref{eq18}), (Right) Comparison of Fusion cross section, $\\sigma$ in \\textbf{barn} calculated by the use of Eqs. (\\ref{eq17}), (\\ref{eq19}) \\& (\\ref{eq20}).}\n\t\\label{fig02}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\fbox{\\includegraphics[width=0.45\\linewidth, height=0.35\\linewidth]{khan-fig3a.pdf}}\n\t\\fbox{\\includegraphics[width=0.45\\linewidth, height=0.35\\linewidth]{khan-fig3b.pdf}}\n\t\\caption{(Left) S-factor S(E) in \\textbf{KeV-barn} calculated for D-D fusion reaction using Eq. (\\ref{eq18}), (Right) Comparison of Fusion cross section, $\\sigma$ in \\textbf{barn} calculated by the use of Eqs. (\\ref{eq17}), (\\ref{eq19}) \\& (\\ref{eq20}).}\n\t\\label{fig03}\n\\end{figure}\n\n\\begin{figure}\n\t\\center\n\t\\fbox{\\includegraphics[width=.45\\linewidth, height=0.35\\linewidth]{khan-fig4a.pdf}}\n\t\\fbox{\\includegraphics[width=.45\\linewidth, height=0.35\\linewidth]{khan-fig4b.pdf}}\n\t\\caption{(Left) S-factor S(E) in \\textbf{KeV-barn} calculated for p-$^{11}$B fusion reaction using Eq. (\\ref{eq18}), (Right)  Comparison of Fusion cross section, $ \\sigma $ in \\textbf{barn} obtained by Eq.(\\ref{eq17}) (solid line) with those reported by Nevins and Swain 2000 \\cite{nevins-2000}, Becker et al 1987 \\cite{becker-1987} (dashed line).}\n\t\\label{fig04}\n\\end{figure}\n\n\\begin{figure}\n\t\\center\n\t\\fbox{\\includegraphics[width=.45\\linewidth, height=0.35\\linewidth]{khan-fig5a.pdf}}\n\t\\fbox{\\includegraphics[width=.45\\linewidth, height=0.35\\linewidth]{khan-fig5b.pdf}}\n\t\\caption{(Left) S-factor S(E) in \\textbf{KeV-barn} calculated for p-$^{11}$B fusion reaction using Eq. (\\ref{eq18}), (Right)  Comparison of Fusion cross section, $ \\sigma $ in \\textbf{barn} obtained by Eq. (\\ref{eq17}) (solid line) with those reported by Beckman et al 1953 \\cite{beckman-1953} (dashed line) and Kamke and Krug 1967 \\cite{kamke-1967}(filled square).} \n    \\label{fig05}\n\\end{figure}\n\nThe first three-rows of the parameter values in Table  \\ref{t01} are used for the $D-D$ fusion reaction while the fourth row is used for the p-$^{11}$B reaction. The second and third sets of parameters (namely, data-set-1 and data-set-2) for the $D-D$ fusion reaction are chosen to obtain data that are comparable to those obtained by the use of 3-parameter and 5-parameter formulae (see Eqs. (\\ref{eq19}) \\&  (\\ref{eq20})) found in the literature \\cite{li-2008,huba-2013}. Results of the $D-D$ fusion obtained with the set of parameters presented in the second row of Table \\ref{t01} agrees nicely with the result of the 3-parameter formula (see Eq. (\\ref{eq19})) as depicted in the right panel of Figure \\ref{fig02}. Similarly, the same obtained with the set of parameters presented in the third row of Table \\ref{t01} agrees excellently with the result of the 5-parameter formula (see Eq. (\\ref{eq20})) as depicted in the right panel of Figure \\ref{fig03}. However, those two sets of data do not exhibit resonance behavior clearly in the cross-section versus energy graph. Hence, we fine-tuned the adjustable parameters in Eqs.(\\ref{eq04}) \\& (\\ref{eq06}) to obtain the set of data (labelled as \\enquote{data-set-0}) presented in the first row of Table \\ref{t01}, which indicate a prominent resonance absorption near 120 keV with an approximate fusion cross-section of 0.174 barn. We used two sets of parameters (namely, data-set-0 \\& data-set-1) for p-$^{11}$B cross-section and s-function calculation which are respectively presented in the fourth and fifth rows of Table \\ref{t01}.\nCross-section versus energy graph for p-$^{11}$B obtained with the set of parameters presented in the fourth row of Table \\ref{t01} exhibits a sharp peak at around 650 keV indicating a clear resonance with a cross-section of about 1.1 barn which is in good agreement with the results reported by Nevins and Swain 2000 \\cite{nevins-2000}, Becker 1987 \\cite{becker-1987} in which they obtained a peak cross-section of about 1.2 barn near 654 keV as shown in Figure \\ref{fig04}. In Figure \\ref{fig05} plot of cross-section versus energy for p-$^{11}$B is obtained with the set of parameters presented in the fifth row of Table \\ref{t01}. The plot exhibits a sharp resonance peak near 760 keV with a peak fusion cross-section value of about 0.62 barn which in excellent agreement with the results reported by Beckman et al 1953 \\cite{beckman-1953} who reported a peak cross-section of about 0.623 barn near 757 keV. The astrophysical S-function depicted in the left panels of Figures \\ref{fig04} and \\ref{fig05} indicate a clear resonance behavior for both the chosen data sets. The quantum-mechanical computation agrees well with the experimental findings. The experimental data for both of the D-D and p-$^{11}$B fusion are well reproduced by our chosen interaction. For D-D fusion our potential model indicates clear selective resonance around 120 keV (\\textbf{119.98 keV, 0.174 barn}) for data-set-0 (Figure \\ref{fig01}). And for p-$^{11}$B, a more prominent selective resonance around 650 KeV (\\textbf{647.22 keV, 1.09 barn}) for the data-set-0 (Figure \\ref{fig04}) and around 760 keV (\\textbf{760.024 keV, 0.635 barn}) for data-set-1 (Figure \\ref{fig05}) respectively. Yet calculations for fusion cross-section involving heavy and medium mass nucleus-nucleus systems, need a different treatment \\cite{atta-2014}.\n\n\\section{Summary and conclusion}\nThe compound nuclear model fails to describe the process of fusion reaction of light nuclei at very low energies, since the fusing nuclei may still\nremember the phase factor of the wave function describing the system.\nIn the compound nuclear model, the reaction is assumed to proceed in two\nsteps: first fusing to form the compound nucleus followed by its decay.\nHere we consider the selective resonant tunneling model according to which the tunneling probability itself depends upon the decay lifetime, hence it is a single-step process. The agreement with the experimental data for the deep sub-barrier fusion of light nuclei also suggests that the tunneling proceeds in a single step.\n\nThe present study successfully reproduces the expected result as evident from the sharp resonance peak in the calculated cross-sections (see Figures \\ref{fig01}, \\ref{fig04} \\& \\ref{fig05}). This method (SRTM) also indicates the soundness in explaining light nuclear fusion reactions by the present tunneling model using a complex nuclear potential function. As the fusion cross-section obtained from thermonuclear reaction rate calculation is found to be too small to measure in a laboratory, the present method could be more fruitful to achieve the desired goal.\n\n\\section*{Acknowledgements} Authors acknowledge a fruitful discussion with D. N. Basu, V. Singh, and D. Atta of VECC Kolkata. And also Aliah University for providing computational facilities.\n\n\n\n\n", "meta": {"timestamp": "2021-06-25T02:21:34", "yymm": "2106", "arxiv_id": "2106.13091", "language": "en", "url": "https://arxiv.org/abs/2106.13091"}}
{"text": "\\section{Introduction}\r\n\r\nWe use standard set-theoretic notation. In particular, a collection $\\mathcal{I}$ of subsets of a set $X$ is called an ideal on $X$ if it is closed under subsets and finite unions of its elements. We assume additionally that $\\mathcal{P}(X)$ (i.e., the power set of $X$) is not an ideal, and that every ideal contains all finite subsets of $X$ (hence, $X=\\bigcup\\I$). All ideals considered in this paper are defined on infinite countable sets. \r\n\r\nWe treat the power set $\\mathcal{P}(X)$ as the space $2^X$ of all functions $f:X\\rightarrow 2$ (equipped with the product topology, where each space $2=\\left\\{0,1\\right\\}$ carries the discrete topology) by identifying subsets of $X$ with their characteristic functions. Thus, we can talk about descriptive complexity of subsets of $\\mathcal{P}(X)$ (in particular, of ideals on $X$). \r\n\r\nThis article is motivated by a problem of M. Hru\\v{s}\\'ak concerning characterization of Borel ideals that can be extended to a $\\bf{\\Sigma^0_2}$ ideal (i.e., such Borel ideals $\\I$ that there is a $\\bf{\\Sigma^0_2}$ ideal $\\J$ with $\\I\\subseteq\\J$). In order to formulate this question is a precise way, we need to recall two definitions:\r\n\\begin{itemize}\r\n\\item if $\\I$ and $\\J$ are ideals then we say that $\\I$ is below $\\J$ in the Kat\\v{e}tov preorder (and write $\\I\\leq_K\\J$), if there is $f:\\bigcup\\J\\to \\bigcup\\I$ such that $f^{-1}[A]\\in\\J$ for each $A\\in\\I$ (cf. \\cite[SUbsection 1.3]{Hrusak} or \\cite[Subsection 1.5]{Meza});\r\n\\item by $\\conv$ we denote the ideal on $\\mathbb{Q}\\cap[0,1]$ generated by sequences in $\\mathbb{Q}\\cap[0,1]$ that are convergent in $[0,1]$, i.e., $A\\subseteq\\mathbb{Q}\\cap[0,1]$ belongs to the ideal $\\conv$ if it can be covered by finitely many such sequences (cf. \\cite[Subsection 3.4]{Hrusak} or \\cite[Subsection 1.6]{Meza}).\r\n\\end{itemize}\r\nA property of ideals can often be expressed by finding a critical ideal (in sense of the Kat\\v{e}tov preorder) with respect to this property. This approach proved to be especially effective in many papers including \\cite{Hrusak}, \\cite{Hrusak2}, \\cite{WR}, \\cite{EUvsSD} and \\cite{Meza}. The above mentioned question of M. Hru\\v{s}\\'ak is the following: \r\n\r\n\\begin{question}\r\nIs it true that, if $\\I$ is a Borel ideal then either $\\conv\\leq_K\\I$ or there is a $\\bf{\\Sigma^0_2}$ ideal containing $\\I$?\r\n\\end{question}\r\n\r\nThis problem has been asked in \\cite[Question 5.16]{Hrusak} and repeated in \\cite[Question 5.8]{Hrusak2} (see also \\cite[Question 4.4.6]{Meza}, where a similar problem is posed by D. Meza-Alc\\'antara). It is known that $\\conv$ is a $\\bf{\\Sigma^0_4}$ ideal that cannot be extended to any $\\bf{\\Sigma^0_2}$ ideal (by \\cite[Propositions 3.4 and 4.1]{Gdansk} and \\cite[Subsection 2.7]{Meza}). Thus, a positive answer to M. Hru\\v{s}\\'ak's question would establish a combinatorial characterization of Borel ideals extendable to $\\bf{\\Sigma^0_2}$ ideals. However, in this paper we answer it in negative. \r\n\r\n$\\bf{\\Sigma^0_2}$ ideals are closely related to the notion of P$^+$-ideals. We say that an ideal $\\I$ on $X$ is a P$^+$-ideal if for each decreasing sequence $(A_n)$ of sets not belonging to $\\I$ one can find $B\\notin\\J$ such that $B\\setminus A_n$ is finite for all $n\\in\\omega$ (\\cite[Definition 2.2.3]{Meza}). This notion has been studied by for instance in \\cite[Subsection 1.1]{Hrusak}. By \\cite[Theorem 3.2.7]{Meza}, a Borel ideal is extendable to a $\\bf{\\Sigma^0_2}$ ideal if and only if it is extendable to a P$^+$-ideal. Therefore, original question of M. Hru\\v{s}\\'ak can be reformulated in the following way: Is is true that a Borel ideal $\\I$ is extendable to a P$^+$-ideal if and only if $\\conv\\not\\leq_K\\I$?\r\n\r\nIt is worth mentioning that M. Hru\\v{s}\\'ak's conjecture holds for all analytic P-ideals (an ideal $\\I$ is called a P-ideal if for every $(A_n)\\subseteq\\I$ there is $A\\in\\I$ with $A\\setminus A_n$ finite for all $n$), i.e., an analytic P-ideal $\\I$ is extendable to a $\\bf{\\Sigma^0_2}$ ideal if and only if $\\conv\\not\\leq_K\\I$ (cf. \\cite[Theorem 4.2]{Gdansk} and \\cite[Subsection 2.7]{Meza}). Moreover, by \\cite[Theorem 3.2.14]{Meza}, if $\\I$ is a Borel ideal such that the forcing $\\cP(\\omega)/\\I$ is proper then either it is extendable to a $\\bf{\\Sigma^0_2}$ ideal or there is $A\\notin\\I$ with $\\conv\\leq\\I|A$ (here $\\I|A=\\{B\\subseteq A:\\ B\\in\\I\\}$).\r\n\r\nIn Section 2 we introduce some necessary notions. Section 3 contains the solution of M. Hru\\v{s}\\'ak's question. Section 4 is devoted to some concluding remarks.\r\n\r\n\r\n\\section{Preliminaries}\r\n\r\nIf $\\I$ and $\\J$ are ideals on $X$ and $Y$, respectively, then the ideal:\r\n$$\\I\\oplus\\J=\\{A\\subseteq (\\{0\\}\\times X)\\cup(\\{1\\}\\times Y):\\ A_{(0)}\\in\\I\\text{ and }A_{(1)}\\in\\J\\}$$\r\nis their disjoint sum (see \\cite{Farah}). Two ideals $\\I$ and $\\J$ are isomorphic if there is a bijection $f:\\bigcup\\J\\to \\bigcup\\I$ such that \r\n$$f^{-1}[A]\\in\\J\\ \\Longleftrightarrow\\ A\\in\\I$$\r\nfor all $A\\subseteq\\bigcup\\I$. Isomorphisms of ideals have been deeply studied for instance in \\cite{Tryba} (see also \\cite{Farah} and \\cite{EUvsSD}). \r\n\r\nRecall the definition of the classical ideal of asymptotic density zero sets: \r\n$$\\I_{d}=\\left\\{A\\subseteq\\omega:\\ \\lim_{n\\to\\infty}\\frac{|A\\cap[0,n]|}{n+1}=0\\right\\},$$\r\nwhere by $[0,n]$ we denote the set $\\{0,1,\\ldots,n\\}$. This ideal has been deeply investigated in the past in the context of convergence (see e.g. \\cite{Fast}, \\cite{Fridy}, \\cite{Salat} and \\cite{Steinhaus}) as well as from the set-theoretic point of view (see e.g. \\cite{Farah}, \\cite{Just} and \\cite{EUvsSD}).\r\n\r\nIn our considerations we will need some new notions which we introduce below. \r\n\r\nIn this paper we denote by $\\cS$ the family of all sequences in $(0,\\frac{1}{2}]$ decreasing to zero, i.e.:\r\n$$\\cS=\\left\\{(\\alpha_n)\\in\\left(0,\\frac{1}{2}\\right]^\\omega:\\ \\lim_n\\alpha_n=0\\text{ and }\\alpha_{n+1}<\\alpha_n\\text{ for all }n\\in\\omega\\right\\}.$$\r\nFor $x\\in[0,1]$ and $r>0$ by $B(x,r)$ we denote the ball of radius $r$ centered at $x$, i.e., $B(x,r)=(x-r,x+r)$. Note that for every $(\\alpha_n)\\in\\cS$ and every $x\\in[0,1]$ we have $(B(x,\\alpha_0)\\setminus B(x,\\alpha_1))\\cap[0,1]\\neq\\emptyset$.\r\n\r\n\\begin{definition}\r\nIf $(\\alpha_n)\\in\\cS$ and $\\I$ is an ideal on $\\omega$ then we say that a sequence $(x_k)\\in[0,1]^\\omega$ converges $\\I$-quickly with respect to $(\\alpha_n)$ if there is $x\\in[0,1]$ such that $\\lim_k x_k=x$ and:\r\n$$\\left\\{n\\in\\omega:\\ \\{x_k:\\ k\\in\\omega\\}\\cap (B(x,\\alpha_n)\\setminus B(x,\\alpha_{n+1}))\\neq\\emptyset\\right\\}\\in\\I.$$\r\n\\end{definition}\r\n\r\n\\begin{definition}\r\nIf $(\\alpha_n)\\in\\cS$ and $\\I$ is an ideal on $\\omega$ then $\\conv(\\I,(\\alpha_n))$ is the ideal on $[0,1]\\cap\\mathbb{Q}$ generated by all sequences converging $\\I$-quickly with respect to $(\\alpha_n)$, i.e., $A\\subseteq\\mathbb{Q}\\cap[0,1]$ belongs to $\\conv(\\I,(\\alpha_n))$ if it can be covered by finitely many sequences converging $\\I$-quickly with respect to $(\\alpha_n)$.\r\n\\end{definition}\r\n\r\nOur counterexample will be of the form $\\conv(\\I,(\\alpha_n))$. However, we start with a simple general result concerning such ideals.\r\n\r\n\\begin{proposition}\r\n\\label{analytic}\r\nIf $\\I$ is an analytic ideal on $\\omega$ and $(\\alpha_n)\\in\\cS$ then $\\conv(\\I,(\\alpha_n))$ is also analytic.\r\n\\end{proposition}\r\n\r\n\\begin{proof}\r\nFix an ideal $\\I$ on $\\omega$ and $(\\alpha_n)\\in\\cS$. Observe that:\r\n$$\\conv(\\I,(\\alpha_n))=\\left\\{A\\subseteq[0,1]\\cap\\mathbb{Q}:\\ \\exists_{k\\in\\omega}\\ \\exists_{x_0,\\ldots,x_{k-1}\\in[0,1]}\\ \\left(\\forall_{i<k}\\ A\\in f_{x_i}^{-1}[\\I]\\right)\\ \\wedge\\right.$$\r\n$$\\left. \\left(\\forall_{m\\in\\omega}\\ \\exists_{F\\in[\\mathbb{Q}\\cap[0,1]]^{<\\omega}}\\ A\\subseteq F\\cup\\bigcup_{i<k}B\\left(x_i,\\frac{1}{m+1}\\right)\\right)\\ \\right\\},$$\r\nwhere $f_x:\\mathcal{P}([0,1]\\cap\\mathbb{Q})\\to\\mathcal{P}(\\omega)$, for each $x\\in[0,1]$, is given by:\r\n$$f_{x}(A)=\\left\\{n\\in\\omega:\\ A\\cap \\left(B(x,\\alpha_n)\\setminus B(x,\\alpha_{n+1})\\right)\\neq\\emptyset\\right\\},$$\r\nfor all $A\\subseteq[0,1]\\cap\\mathbb{Q}$.\r\n\r\nNotice that if $x\\in[0,1]$ and $l\\in\\omega$ then $f_x^{-1}[\\{A\\subseteq\\omega:\\ l\\in A\\}]$ is open and $f_x^{-1}[\\{A\\subseteq\\omega:\\ l\\notin A\\}]$ is closed. Hence, $f_x$ is of Baire class $1$, for every $x\\in[0,1]$, and $f_{x}^{-1}[\\I]$ is analytic, for each analytic ideal $\\I$. It follows that $\\conv(\\I,(\\alpha_n))$ is analytic for every analytic ideal $\\I$.\r\n\\end{proof}\r\n\r\n\r\n\r\n\\section{The counterexample}\r\n\r\nThe following sequence of lemmas will lead us to the solution of M. Hru\\v{s}\\'ak's problem.\r\n\r\n\\begin{lemma}\r\n\\label{1}\r\nThe following are equivalent for every ideal $\\I$ on $\\omega$ and every sequence $(\\alpha_n)\\in\\cS$:\r\n\\begin{itemize} \r\n\\item[(a)] $\\I$ can be extended to a $P^+$-ideal;\r\n\\item[(b)] $\\conv(\\I,(\\alpha_n))$ can be extended to a $P^+$-ideal.\r\n\\end{itemize}\r\n\\end{lemma}\r\n\r\n\\begin{proof}\r\n(b)$\\implies$(a): Suppose that $\\conv(\\I,(\\alpha_n))\\subseteq\\J$ for some $P^+$-ideal $\\J$ (on $[0,1]\\cap\\mathbb{Q}$). If each convergent sequence is in $\\J$, then $\\conv\\subseteq\\J$, which contradict the choice of $\\J$ (as $\\conv$ cannot be extended to any $P^+$-ideal -- see the Introduction for details). Thus, there is a convergent sequence $A\\in\\conv\\setminus\\J$ (so also $A\\notin\\conv(\\I,(\\alpha_n))$). Let $x\\in[0,1]$ be the limit of $A$. Without loss of generality we may assume that $A\\subseteq B(x,\\alpha_0)$. Note that $\\I\\leq_K\\conv(\\I,(\\alpha_n))|A\\subseteq\\J|A$ as witnessed by the function $f:A\\to\\omega$ given by: \r\n$$f(i)=n\\ \\Leftrightarrow\\ i\\in B(x,\\alpha_n)\\setminus B(x,\\alpha_{n+1})$$ \r\nfor all $i\\in A$.\r\n\r\nObserve that $\\I$ is a subset of $\\J'=\\{C\\subseteq\\omega:\\ f^{-1}[C]\\in\\J|A\\}$ as for each $C\\in\\I$ we have $f^{-1}[C]\\in\\J|A$. To finish the proof, we will show that $\\J'$ is a $P^+$-ideal. It is easy to check that $\\J'$ is indeed an ideal, so let $(C_n)$ be a decreasing sequence of sets not belonging to $\\J'$. Define $B_n=f^{-1}[C_n]\\subseteq A$ for all $n\\in\\omega$. Then each $B_n$ does not belong to $\\J|A$ (as $C_n\\notin\\J'$) and $(B_n)$ is decreasing. Since $\\J$ is a $P^+$-ideal, so is $\\J|A$. Hence, there is $X\\subseteq A$, $X\\notin\\J|A$ such that $X\\setminus B_n$ is finite for each $n$. Then $f[X]\\setminus C_n\\in\\fin$ and $f[X]\\notin\\J'$ (as $f^{-1}[f[X]]\\supseteq X\\notin\\J|A$). Thus, $\\J'$ is a $P^+$-ideal. \r\n\r\n(a)$\\implies$(b): Suppose that $\\J$ is a $P^+$-ideal such that $\\I\\subseteq\\J$. Consider the sequence $X=\\{\\alpha_n:\\ n\\in\\omega\\}$. Clearly, $\\lim X=0$ and $X\\notin\\conv(\\I,(\\alpha_n))$. Let $f:\\omega\\to X$ be given by $f(n)=\\alpha_n$ for all $n$. Observe that $f$ witnesses that $\\I$ and $\\conv(\\I,(\\alpha_n))|X$ are isomorphic. Consider the ideal $\\J'=\\{A\\subseteq\\mathbb{Q}\\cap[0,1]:\\ f^{-1}[A\\cap X]\\in\\J\\}$ (which is isomorphic with $\\J\\oplus\\cP(\\omega)$). Note that $\\conv(\\I,(\\alpha_n))\\subseteq\\J'$. Moreover, similarly as above it can be shown that $\\J'$ is a $P^+$-ideal. This ends the proof.\r\n\\end{proof}\r\n\r\n\\begin{lemma}\r\n\\label{2}\r\nWe have $\\conv\\not\\leq_K\\conv(\\I,(\\alpha_n))$, for every $(\\alpha_n)\\in\\cS$ and every ideal $\\I$ on $\\omega$.\r\n\\end{lemma}\r\n\r\n\\begin{proof}\r\nWe will use the following characterization: $\\conv\\leq_K\\J$ if and only if there is a countable family $\\{X_n:\\ n\\in\\omega\\}\\subseteq[\\bigcup\\J]^\\omega$ such that for every $A\\notin\\J$ there is $n\\in\\omega$ such that both $A\\cap X_n$ and $A\\setminus X_n$ are infinite (cf. \\cite[Theorem 2.4.3]{Meza}).\r\n\r\nLet $\\{X_n:\\ n\\in\\omega\\}\\subseteq[[0,1]\\cap\\mathbb{Q}]^\\omega$. We will inductively define $(i_n)\\in 2^\\omega$, $(Y_n)\\subseteq[[0,1]\\cap\\mathbb{Q}]^\\omega$ and a sequence $(I_n)$ of closed subintervals of $[0,1]$ such that:\r\n\\begin{itemize}\r\n\\item[(a)] $I_{n+1}\\subseteq \\text{int}(I_n)$ and the length of $I_n$ is at most $\\frac{1}{2^{n+1}}$, for all $n\\in\\omega$;\r\n\\item[(b)] $Y_{n+1}\\subseteq Y_n$ and $\\overline{Y_n}=I_n$, for all $n\\in\\omega$;\r\n\\item[(c)] if $i_n=0$ then $Y_n\\cap X_n=\\emptyset$ and if $i_n=1$ then $Y_n\\subseteq X_n$.\r\n\\end{itemize}\r\nAt first, since $[0,1]=\\overline{[0,1]\\cap\\mathbb{Q}}=\\overline{Y_0^0}\\cup\\overline{Y_0^1}$, where $Y_0^0=([0,1]\\cap\\mathbb{Q})\\setminus X_0$ and $Y_0^1=X_0$, there is $i_0\\in\\{0,1\\}$ such that $Y_0^{i_0}$ is dense in some open subinterval of $[0,1]$. Find a closed interval $I_0\\subseteq[0,1]$ of length at most $\\frac{1}{2}$ such that $Y_0^{i_0}$ is dense in $I_0$ and define $Y_0=I_0\\cap Y_0^{i_0}$. Note that $\\overline{Y_0}=I_0$. At step $n+1$, if all $i_j$, $I_j$ and $Y_j$ for $j\\leq n$ are already defined, observe that $I_n=\\overline{Y_n}=\\overline{Y_{n+1}^0}\\cup\\overline{Y_{n+1}^1}$, where $Y_{n+1}^0=Y_n\\setminus X_{n+1}$ and $Y_{n+1}^1=Y_n\\cap X_{n+1}$. Then there is $i_{n+1}\\in\\{0,1\\}$ such that $Y_{n+1}^{i_{n+1}}$ is dense in some open subinterval of $I_n$. Find a closed interval $I_{n+1}\\subseteq \\text{int}(I_n)$ of length at most $\\frac{1}{2^{n+2}}$ such that $Y_{n+1}^{i_{n+1}}$ is dense in $I_{n+1}$ and define $Y_{n+1}=I_{n+1}\\cap Y_{n+1}^{i_{n+1}}$. Then $\\overline{Y_{n+1}}=I_{n+1}$ and $Y_{n+1}\\subseteq Y_n$.\r\n\r\nOnce the induction is completed, let $x\\in[0,1]$ be the unique point such that $\\{x\\}=\\bigcap_n I_n$. Denote: \r\n$$k_0=\\min\\{k\\in\\omega:\\ \\text{int} (I_0)\\cap(B(x,\\alpha_{k})\\setminus B(x,\\alpha_{k+1}))\\neq\\emptyset\\}.$$ \r\nPick $x_k$, for each $k\\geq k_0$, such that $x_k\\in Y_{m_k}\\cap(B(x,\\alpha_{k})\\setminus B(x,\\alpha_{k+1}))$, where: \r\n$$m_k=\\max\\{n\\in\\omega:\\ \\text{int} (I_n)\\cap(B(x,\\alpha_{k})\\setminus B(x,\\alpha_{k+1}))\\neq\\emptyset\\}$$\r\n(item (a) guarantees that each $m_k$ is well-defined). This is possible as each $Y_n$ is dense in $I_n$ (by (b)). Note that $\\lim_k m_k=\\infty$ (by (a) and the choice of $x$). \r\n\r\nObserve that $X=\\{x_k:\\ k\\in\\omega\\}\\notin\\conv(\\I,(\\alpha_n))$. Indeed, let $B_0,\\ldots,B_m$ be sequences converging $\\I$-quickly with respect to $(\\alpha_n)$ and assume to the contrary that $X\\subseteq\\bigcup_{i\\leq m}B_i$. Since $\\I$ is an ideal, without loss of generality we may assume that $\\lim B_i\\neq\\lim B_j$ whenever $i,j\\leq m$ are distinct (as a union of finitely many sequences converging $\\I$-quickly with respect to $(\\alpha_n)$ to the same limit is a sequence converging $\\I$-quickly with respect to $(\\alpha_n)$). Since $\\lim X=x$, there is $i_0\\leq m$ such that $x=\\lim B_{i_0}$. Then $X\\cap\\bigcup_{i\\leq m,i\\neq i_0}B_i$ is finite and $X\\setminus\\bigcup_{i\\leq m,i\\neq i_0}B_i$ does not converge $\\I$-quickly with respect to $(\\alpha_n)$ (as it intersects almost all $B(x,\\alpha_{k})\\setminus B(x,\\alpha_{k+1})$). Hence, $X\\setminus\\bigcup_{i\\leq m,i\\neq i_0}B_i$ cannot be covered by $B_{i_0}$. This shows that $X\\notin\\conv(\\I,(\\alpha_n))$.\r\n\r\nWe claim that for each $n\\in\\omega$ either $X\\cap X_n$ or $X\\setminus X_n$ is finite. Indeed, fix any $n\\in\\omega$. If $i_n=0$ then $Y_j\\cap X_n=\\emptyset$ for all $j\\geq n$ (by (b) and (c)). Thus, by (b) and $\\lim_k m_k=\\infty$ we get $X\\cap X_n\\subseteq X\\setminus Y_n\\in[\\mathbb{Q}\\cap[0,1]]^{<\\omega}$. On the other hand, if $i_n=1$ then (b) and (c) give us $Y_j\\subseteq X_n$ for all $j\\geq n$. Therefore, similarly as before, $X\\setminus X_n\\subseteq X\\setminus Y_n\\in[\\mathbb{Q}\\cap[0,1]]^{<\\omega}$. This finishes the proof.\r\n\\end{proof}\r\n\r\n\\begin{lemma}\r\n\\label{3}\r\nThe ideal $\\conv(\\I_d,(\\frac{1}{2^{n+1}}))$ is $\\bf{\\Sigma^0_6}$.\r\n\\end{lemma}\r\n\r\n\\begin{proof}\r\nRecall that: \r\n$$\\I_d=\\Exh(\\phi)=\\left\\{A\\subseteq\\omega:\\ \\lim_n\\phi(A\\setminus[0,n])=0\\right\\},$$ \r\nwhere $\\phi:\\mathcal{P}(\\omega)\\to[0,1]$ given by: \r\n$$\\phi(A)=\\sup_{n\\in\\omega}\\frac{|A\\cap[0,n]|}{n+1},$$ \r\nfor all $A\\subseteq\\omega$, is a lower semicontinuous submeasure, i.e., it satisfies $\\phi(\\emptyset)=0$, $\\phi(A)\\leq\\phi(A\\cup B)\\leq\\phi(A)+\\phi(B)$ and $\\phi(A)=\\lim_n\\phi(A\\cap[0,n])$ (lower semicontinuity) for all $A,B\\subseteq\\omega$ (see \\cite[Example 1.2.3.(d)]{Farah}). \r\n\r\nWe will need the following observation: if $G\\in[\\omega]^{<\\omega}$ and $\\min G>0$ then $\\phi((G-1)\\cup G\\cup(G+1))\\leq 4\\phi(G)$ (here $B+d=\\{b+d:\\ b\\in B\\}$ for $d\\in\\mathbb{Z}$ and $B\\subseteq\\omega$). Indeed, it is obvious that $\\phi(G+1)\\leq\\phi(G)$. Moreover, since $G$ is finite, there is $d\\in\\omega$ such that $\\phi(G-1)=\\frac{|(G-1)\\cap[0,d]|}{d+1}$. If $|(G-1)\\cap[0,d]|=1$ then $\\phi(G-1)=\\frac{1}{d+1}\\leq\\frac{2}{d+2}=2\\phi(G)$. If $|(G-1)\\cap[0,d]|\\neq 1$ then we have:\r\n$$\\phi(G-1)=\\frac{|(G-1)\\cap[0,d]|}{|(G-1)\\cap[0,d]|-1}\\cdot\\frac{|(G-1)\\cap[0,d]|-1}{d+1}\\leq$$\r\n$$2\\cdot\\frac{|(G-1)\\cap[0,d]|-1}{d+1}\\leq 2\\phi(G).$$\r\nThus, $\\phi((G-1)\\cup G\\cup(G+1))\\leq \\phi(G-1)+\\phi(G)+\\phi(G+1)\\leq 4\\phi(G)$.\r\n\r\nDenote $\\alpha_n=\\frac{1}{2^{n+1}}$ for all $n$. We will show that $A\\in\\conv(\\I,(\\alpha_n))$ is equivalent to:\r\n$$\\exists_{k\\in\\omega}\\ \\forall_{m\\in\\omega}\\ \\exists_{n\\in\\omega}\\ \\exists_{H\\in[\\mathbb{Q}\\cap[0,1]]^{<\\omega}}\\ \\forall_{l\\in\\omega}\\ \\exists_{x_0,\\ldots,x_k\\in\\mathbb{Q}\\cap[0,1]}\\ \\exists_{G_0,\\ldots,G_k\\subseteq[n,l]}$$\r\n$$\\left(\\left(\\forall_{i\\leq k}\\ \\phi(G_i)<\\frac{1}{m+1}\\right)\\ \\wedge\\ \\left(\\forall_{i,j\\leq k,i\\neq j}\\ B(x_i,\\alpha_n)\\cap B(x_j,\\alpha_n)=\\emptyset\\right)\\ \\wedge\\ \\right.$$\r\n$$\\left. A\\subseteq H\\cup\\bigcup_{i\\leq k}\\left(B(x_i,\\alpha_{l+1})\\cup\\bigcup_{j\\in G_i}\\left(B(x_i,\\alpha_j)\\setminus B(x_i,\\alpha_{j+1})\\right)\\right)\\ \\wedge \\right.$$\r\n$$\\left.\\left(\\forall_{i\\leq k}\\ \\forall_{j\\in G_i}\\ A\\cap \\left(B(x_i,\\alpha_j)\\setminus B(x_i,\\alpha_{j+1})\\right)\\text{ is finite}\\right)\\right).$$\r\nThis will finish the proof as the right-hand side condition is clearly $\\bf{\\Sigma^0_6}$.\r\n\r\n($\\Rightarrow$): Let $A\\in\\conv(\\I,(\\alpha_n))$. Then there is $k\\in\\omega$ and sequences $A_0,\\ldots,A_k$ converging $\\I$-quickly with respect to $(\\frac{1}{2^{n+1}})$ such that $A\\subseteq A_0\\cup\\ldots\\cup A_k$. Let $m\\in\\omega$ be arbitrary. For each $i\\leq k$ there is $n_i\\in\\omega$ such that\r\n$$\\phi\\left(\\left\\{j\\in\\omega\\setminus[0,n_i):\\ A_i\\cap (B(\\lim A_i,\\alpha_j)\\setminus B(\\lim A_i,\\alpha_{j+1}))\\neq\\emptyset\\right\\}\\right)<\\frac{1}{4(m+1)}.$$\r\nThere is also $n'\\in\\omega$ such that $|\\lim A_i-\\lim A_j|>2\\alpha_{n'}$ for all $i,j\\leq k$, $i\\neq j$. Let $n=\\max(\\{n_i:\\ i\\leq k\\}\\cup\\{1,n'\\})$ and $H=A\\setminus\\bigcup_{i\\leq k}B(\\lim A_i,\\alpha_{n+1})\\in[\\mathbb{Q}\\cap[0,1]]^{<\\omega}$. Let $l\\in\\omega$ be arbitrary and put $G_i=((G'_i-1)\\cup G'_i\\cup(G'_i+1))\\cap[n,l]$ for all $i\\leq k$, where\r\n$$G'_i=\\left\\{j\\in[n,l+1]:\\ A_i\\cap (B(\\lim A_i,\\alpha_j)\\setminus B(\\lim A_i,\\alpha_{j+1}))\\neq\\emptyset\\right\\}.$$\r\nNote that $\\phi(G_i)<\\frac{1}{m+1}$ (by the observation from the first paragraph of this proof). For each $i\\leq k$ find $x_i\\in[0,1]\\cap\\mathbb{Q}$ close to $\\lim A_i$ such that:\r\n\\begin{itemize}\r\n\\item $\\forall_{i,j\\leq k,i\\neq j}\\ B(x_i,\\alpha_n)\\cap B(x_j,\\alpha_n)=\\emptyset$ (this is possible as $|\\lim A_i-\\lim A_j|>2\\alpha_{n'}$);\r\n\\item $\\forall_{i\\leq k}\\ B(\\lim A_i,\\alpha_{l+2})\\subseteq B(x_i,\\alpha_{l+1})$;\r\n\\item if $i\\leq k$ and $n+1\\leq j\\leq l+1$ then $$B(\\lim A_i,\\alpha_j)\\setminus B(\\lim A_i,\\alpha_{j+1})\\subseteq\\bigcup_{j-1\\leq p\\leq j+1} B(x_i,\\alpha_p)\\setminus B(x_i,\\alpha_{p+1}).$$\r\n\\end{itemize}\r\nThen the first two items above guarantee that $A\\cap (B(x_i,\\alpha_j)\\setminus B(x_i,\\alpha_{j+1}))$ is finite for each $i\\leq k$ and $j\\in G_i\\subseteq[0,l]$. Moreover, we have: \r\n$$A\\subseteq H\\cup\\bigcup_{i\\leq k}\\left(B(x_i,\\alpha_{l+1})\\cup\\bigcup_{j\\in G_i}\\left(B(x_i,\\alpha_j)\\setminus B(x_i,\\alpha_{j+1})\\right)\\right).$$\r\nHence, $A$ satisfies the right-hand side condition.\r\n\r\n($\\Leftarrow$): Firstly, observe that if $A$ satisfies the right-hand side condition then there is $k\\in\\omega$ such that for each $l\\in\\omega$ there are $x_0,\\ldots,x_k\\in[0,1]\\cap\\mathbb{Q}$ such that $A\\setminus\\bigcup_{i\\leq k}B(x_i,\\alpha_{l+1})$ is finite. Thus, $A\\in\\conv$ (see \\cite[Subsection 1.6]{Meza}). We need to show that each $A\\in\\conv\\setminus\\conv(\\I,(\\alpha_n))$ does not satisfy the right-hand side condition. \r\n\r\nNotice that each $A\\in\\conv$ is contained in finitely many convergent sequences and if $A\\notin\\conv(\\I,(\\alpha_n))$ then at least one of those sequences does not belong to $\\conv(\\I,(\\alpha_n))$. Hence, as the right-hand side condition is closed under subsets, it suffices to show that each convergent sequence $A\\notin\\conv(\\I,(\\alpha_n))$ does not satisfy the right-hand side condition.\r\n\r\nSuppose that $A$ is a convergent sequence not belonging to $\\conv(\\I,(\\alpha_n))$. Let $k\\in\\omega$ be arbitrary and using $A\\notin\\conv(\\I,(\\alpha_n))$ find $m\\in\\omega$ such that for every $n\\in\\omega$ we have:\r\n$$\\phi\\left(\\left\\{j\\in\\omega\\setminus[0,n]:\\ A\\cap (B(\\lim A,\\alpha_j)\\setminus B(\\lim A,\\alpha_{j+1}))\\neq\\emptyset\\right\\}\\right)>\\frac{4}{m+1}.$$\r\nFix arbitrary $n\\in\\omega$ and $H\\in[\\mathbb{Q}\\cap[0,1]]^{<\\omega}$. Using lower semicontinuity of $\\phi$, pick $l\\in\\omega$ such that: \r\n$$\\phi\\left(\\left\\{j\\in(n,l-1):\\ (A\\setminus H)\\cap (B(\\lim A,\\alpha_j)\\setminus B(\\lim A,\\alpha_{j+1}))\\neq\\emptyset\\right\\}\\right)>\\frac{4}{m+1}.$$\r\nObserve that the above implies that $l>n+1$. \r\n\r\nLet $x_0,\\ldots,x_k\\in[0,1]\\cap\\mathbb{Q}$ and $G_0,\\ldots,G_k\\subseteq[n,l]$ be arbitrary such that:\r\n\\begin{itemize}\r\n\\item[(i)] $\\phi(G_i)<\\frac{1}{m+1}$ for all $i\\leq k$;\r\n\\item[(ii)] $B(x_i,\\alpha_n)\\cap B(x_j,\\alpha_n)=\\emptyset$ for all $i,j\\leq k$, $i\\neq j$;\r\n\\item[(iii)] $A\\cap B(x_i,\\alpha_j)\\setminus B(x_i,\\alpha_{j+1})$ is finite for all $i\\leq k$ and $j\\in G_i$. \r\n\\end{itemize}\r\nAssume to the contrary that \r\n$$A\\subseteq H\\cup\\bigcup_{i\\leq k}\\left(B(x_i,\\alpha_{l+1})\\cup\\bigcup_{j\\in G_i}\\left(B(x_i,\\alpha_j)\\setminus B(x_i,\\alpha_{j+1})\\right)\\right).$$ \r\nBy (ii), (iii) and the fact that $A$ is convergent, there is $i_0\\leq k$ such that $A\\setminus B(x_{i_0},\\alpha_{l+1})$ is finite. Then $|x_{i_0}-\\lim A|\\leq\\alpha_{l+1}=\\frac{1}{2^{l+2}}<\\frac{1}{2^{n+2}}$ (by $l>n+1$) and using (ii) we get: \r\n$$(A\\setminus H)\\cap B(\\lim A,\\alpha_{n+1})\\subseteq (A\\setminus H)\\cap B(x_{i_0},\\alpha_{n})\\subseteq $$\r\n$$B(x_{i_0},\\alpha_{l+1})\\cup\\bigcup_{j\\in G_{i_0}}\\left(B(x_{i_0},\\alpha_j)\\setminus B(x_{i_0},\\alpha_{j+1})\\right)\\subseteq$$\r\n$$B(x_{i_0},\\alpha_{l})\\cup\\bigcup_{j\\in G_{i_0}\\setminus\\{l\\}}\\left(B(x_{i_0},\\alpha_j)\\setminus B(x_{i_0},\\alpha_{j+1})\\right)\\subseteq$$\r\n$$B(\\lim A,\\alpha_{l-1})\\cup\\bigcup_{j\\in G_{i_0}\\setminus\\{l\\}}\\left(B(x_{i_0},\\alpha_j)\\setminus B(x_{i_0},\\alpha_{j+1})\\right).$$\r\n\r\nNote that if $x\\in B(x_{i_0},\\alpha_j)\\setminus B(x_{i_0},\\alpha_{j+1})$ for some $n\\leq j<l$ then: \r\n$$x\\in \\bigcup_{p\\in\\{j-1,j,j+1\\}}\\left(B(\\lim A,\\alpha_p)\\setminus B(\\lim A,\\alpha_{p+1})\\right)=B(\\lim A,\\alpha_{j-1})\\setminus B(\\lim A,\\alpha_{j+2}).$$\r\nIndeed, $|x-x_{i_0}|<\\alpha_j=\\frac{1}{2^{j+1}}$ implies:\r\n$$|x-\\lim A|\\leq |x-x_{i_0}|+|x_{i_0}-\\lim A|<\\frac{1}{2^{j+1}}+\\frac{1}{2^{l+2}}\\leq\\frac{1}{2^{j+1}}+\\frac{1}{2^{j+3}}<\\frac{1}{2^{j}}=\\alpha_{j-1}.$$ \r\nAnalogously, \r\n$|x-x_{i_0}|\\geq\\alpha_{j+1}=\\frac{1}{2^{j+2}}$ implies:\r\n$$|x-\\lim A|\\geq |x-x_{i_0}|-|x_{i_0}-\\lim A|>\\frac{1}{2^{j+2}}-\\frac{1}{2^{l+2}}\\geq\\frac{1}{2^{j+2}}-\\frac{1}{2^{j+3}}=\\frac{1}{2^{j+3}}=\\alpha_{j+2}.$$ \r\n\r\nTherefore, we obtain:\r\n$$(A\\setminus H)\\cap B(\\lim A,\\alpha_{n+1})\\subseteq B(\\lim A,\\alpha_{l-1})\\cup$$\r\n$$\\bigcup_{j\\in (G_{i_0}-1)\\cup G_{i_0}\\cup(G_{i_0}+1)}B(\\lim A,\\alpha_j)\\setminus B(\\lim A,\\alpha_{j+1})$$\r\nHowever, \r\n$$\\phi(((G_{i_0}-1)\\cup G_{i_0}\\cup(G_{i_0}+1))\\cap(n,l-1))<4\\phi(G_{i_0})<\\frac{4}{m+1}$$ \r\n(by the observation from the first paragraph of this proof). This contradicts the choice of $l$ and finishes the proof.\r\n\\end{proof}\r\n\r\nWe are ready to answer the M. Hru\\v{s}\\'ak's question formulated in the Introduction.\r\n\r\n\\begin{theorem}\r\nThere is a $\\bf{\\Sigma^0_6}$ ideal $\\I$ not extendable to a $P^+$-ideal and such that $\\conv\\not\\leq_K\\I$.\r\n\\end{theorem}\r\n\r\n\\begin{proof}\r\nConsider the ideal $\\conv(\\I_d,(\\frac{1}{2^{n+1}}))$. By Lemmas \\ref{2} and \\ref{3}, it is Borel and such that $\\conv\\not\\leq_K\\conv(\\I_d,(\\frac{1}{2^{n+1}}))$. Moreover, $\\conv(\\I_d,(\\frac{1}{2^{n+1}}))$ cannot be extended to a $P^+$-ideal by Lemma \\ref{1}, since $\\I_d$ is not extendable to a $P^+$-ideal (cf. \\cite[Theorem 4.2 and the discussion below Proposition 3.3]{Gdansk}).\r\n\\end{proof}\r\n\r\n\r\n\\section{Concluding remarks}\r\n\r\nHowever Proposition \\ref{analytic} shows that for every Borel ideal $\\I$ (and every $(\\alpha_n)\\in\\cS$) the family $\\conv(\\I,(\\alpha_n))$ is analytic, in the general case we were not able to show that it is Borel. In Lemma \\ref{3} we have done it only in one special case. Thus, we have produced only one counterexample for the M. Hru\\v{s}\\'ak's question. It should be expected that Lemma \\ref{3} can be generalized for a broader class of ideals. \r\n\r\nIt seems that our method cannot give any counterexample for the M. Hru\\v{s}\\'ak's question of Borel class lower than $\\bf{\\Sigma^0_6}$. Indeed, if we want to apply Lemma \\ref{1}, we cannot take $\\I$ of class lower than $\\bf{\\Pi^0_3}$ (as there are no $\\bf{\\Pi^0_2}$ ideals, all $\\bf{\\Sigma^0_2}$ ideals are clearly extendable to a $\\bf{\\Sigma^0_2}$ ideal and there are no examples of $\\bf{\\Sigma^0_3}$ ideals not extendable to a $\\bf{\\Sigma^0_2}$ ideal). Hence, there is still a chance that the original M. Hru\\v{s}\\'ak's  conjecture works for instance in the case of all $\\bf{\\Sigma^0_4}$ ideals.\r\n\r\nFInally, it is also worth noticing that the proof of Lemma \\ref{1} heavily uses the fact that $\\conv$ cannot be extended to a $\\bf{\\Sigma^0_2}$ ideal. This suggests that $\\conv$ is indeed a critical ideal for the property of extendability to a $\\bf{\\Sigma^0_2}$ ideal. However, the potential characterization of this property with the use of $\\conv$ has to be more complicated than just the condition \"$\\conv\\not\\leq_K\\I$\".\r\n\r\n\r\n\r\n\r\n\\bibliographystyle{amsplain}\r\n", "meta": {"timestamp": "2021-06-25T02:20:03", "yymm": "2106", "arxiv_id": "2106.13053", "language": "en", "url": "https://arxiv.org/abs/2106.13053"}}
{"text": "\\section{Introduction}\r\n\\IEEEPARstart{T}{he} interference alignment (IA) technique is an elegant way towards improving the degree of freedom (DoF) of MIMO systems. With the help of IA technique, the $K$-user time-varying interference channel almost surely has ${K \\mathord{\\left/{\\vphantom {K 2}} \\right. \\kern-\\nulldelimiterspace} 2}$ DoF \\cite{2008Interference}. However, the DoF gain caused by IA technique comes from the cost of perfect instantaneous channel state information at the transmitter (CSIT) acquisition \\cite{2008Interference2,Shin2011On,Lee2013Uplink,2013Degrees}, which is a critical challenge, especially for distributed cellular networks \\cite{Love2008An,2011Downlink,2011Interference}. That is, in distributed networks, the CSI feedback from receivers to transmitters experiences unavoidable delay, and meanwhile CSIT sharing among multiple transmitters will take up lots of feedback resources and bring burden to transmitters. To handle these issues, different kinds of approaches have been proposed, such as the blind interference alignment (BIA), the space-time interference alignment (STIA) and the retrospective interference alignment (RIA).\r\n\r\nConcerning the broadcast channel (BC), a novel approach is to take IA technique without CSIT, which is the BIA  \\cite{2012Blind}. Under certain heterogeneous block fading models, the BIA makes use of channel characteristics to keep channel gain constant in adjacent slots, and meanwhile alters the values of signals to reduce the dimension of interference signals. However, since the real channel fading model is somewhat between independent distributed fading model and block fading model, the scheme still cannot work in practical application. Therefore, for the $K$-user MISO channel systems with reconfigurable antennas, a new blind interference alignment is presented in \\cite{2011Aiming,5962838}. Owing to the circulant and non-circulant structures of the scheme, the channels over different users keep correlated, which means that the inter-user interference cancellation and inter-subblock interference cancellation can be used to reduce the dimension of interference signals. Subsequently, the BIA gets extended to other systems, i.e., the cellular interference systems \\cite{2013Blind} and heterogeneous systems \\cite{Zhou2012On}. Nevertheless, the BIA still has the defect that the complexity of precoding matrix increases exponentially with the number of user antennas, which makes it hard to be used in MIMO systems.\r\n\r\nTo make IA technique practical and applicable in MIMO systems, the idea of making use of the moderate delayed CSIT is taken into account. Specifically, \\cite{2013Space} firstly proposes STIA scheme for the $K$-user MISO BC. The core idea of the STIA is that, in block fading model, the transmitter combines IA technique with physical network coding (PNC) to eliminate interference in space-time domain. Subsequent studies achieve extensions of the STIA to different application scenarios \\cite{Lee2014Distributed,2013CSI,2013Space2}. In particular, \\cite{Lee2014Distributed} puts forward distributed STIA (DSTIA) for $2 \\times 2$ user SISO $X$-channel (XC) and $3 \\times 3$ user SISO interference channel (IC) which achieves ${4 \\mathord{\\left/{\\vphantom {4 3}} \\right. \\kern-\\nulldelimiterspace} 3}$ and ${6 \\mathord{\\left/{\\vphantom {6 5}} \\right. \\kern-\\nulldelimiterspace} 5}$ DoF respectively. \\cite{2013CSI} introduces DSTIA into the $K$-user MISO IC and achieves $K-1$ DoF. For $K$-user MIMO BC and $K$-user MIMO IC, \\cite{2013Space2} analyzes the trade-off strategy between achievable DoF and the range of CSI delay. However, the DoF gain of the STIA is restricted to the time span of the feedback delay, i.e., the time span of the feedback delay cannot exceed the time span of the coherent time. At the same time, owing to the inverse operation of the precoding matrix, the scheme brings burden to the transceivers.\r\n\r\nTo handle the issue of feedback delay, the delayed CSIT is studied. The RIA is firstly proposed \\cite{Maddah2012Completely} for MISO networks and gets extended to $K$-user MIMO networks \\cite{2014Linear}. The core idea of the RIA is adopting the repetition coding, i.e., at the transmitter, partial data is repeatedly transmitted $R$ times during the slots, and then the received signals are jointly decoded at the receivers' side at last. Subsequently, the scheme is extended to the IC scenarios, i.e., SISO IC \\cite{2011Achieving}, MISO IC \\cite{2014On}, MIMO IC \\cite{Hao2016Achievable}, MIMO BC \\cite{2015Retrospective,2015Retrospective2}, and meanwhile the achievable DoF gets further studied. However, with different channels, the upper bounds of the DoF are quite different. On the one hand, \\cite{6179997} explores the upper bounds of the DoF for 2-user MIMO BC and 3-user MIMO BC, where the bounds are tight to the DoF obtained by RIA. On the other hand, by taking RIA scheme, the obtained DoF is relaxed to the upper bounds in the $K$-user MIMO IBC \\cite{6205390} and $2 \\times 2$-user SISO XC \\cite{6341083} for the sake of the distributed transmitter loss. Therefore, the TDMA groups (TG) scheme and 3-user PSR scheme are introduced to maintain the DoF gain made by RIA, under which the system can be adaptive with different kinds of system configurations \\cite{7588140}. After making tradeoff among three schemes, unfortunately, the obtained DoF is still not optimal in the cellular system, because it cannot properly handle the issue of inter-cell interference (ICI) \\cite{6388347} by the middle way.\r\n\r\nFrom the above illustration, we know that, when the number of user antennas is large, the BIA suffers from high computational complexity so that its application is not practical in MIMO systems. The STIA is adopted in MIMO systems and gets DoF gain, but its performance is restricted to the time span of the feedback delay. The RIA scheme solves the issue of feedback delay, but the effect of DoF gain gets affected in cellular networks. Therefore, an interference alignment scheme suitable for cellular networks is desired. In this paper, two new interference alignment schemes, i.e., the retrospective interference regeneration (RIR) scheme and the beamforming based distributed retrospective interference alignment (B-DRIA) scheme, are proposed for K-user cellular MIMO downlink systems. The main contribution of this paper is three-fold as follows:\r\n\\begin{itemize}\r\n\\vspace{-0.3em}\r\n\\item The paper proposes the RIR scheme which adopts interference elimination algorithm to erase redundant symbols in inter-cell interference (ICI) signals, and then uses interference regeneration algorithm to avoid secondary ICI interference. The scheme achieves higher DoF gain than the RIA scheme, i.e., \r\n${{LM} \\mathord{\\left/\r\n {\\vphantom {{LM} {(L + {1 \\mathord{\\left/\r\n {\\vphantom {1 {\\left\\lfloor {{1 \\mathord{\\left/\r\n {\\vphantom {1 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }}} \\right.\r\n \\kern-\\nulldelimiterspace} {\\left\\lfloor {{1 \\mathord{\\left/\r\n {\\vphantom {1 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }})}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(L + {1 \\mathord{\\left/\r\n {\\vphantom {1 {\\left\\lfloor {{1 \\mathord{\\left/\r\n {\\vphantom {1 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }}} \\right.\r\n \\kern-\\nulldelimiterspace} {\\left\\lfloor {{1 \\mathord{\\left/\r\n {\\vphantom {1 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }})}}$\r\n, but incurs performance degradation when transceiver antennas ratio approaches 1.\r\n\r\n\\item To avoid the performance degradation of the RIR scheme under certain circumstances, the paper further proposes the B-DRIA scheme which adopts the cellular beamforming matrix to eliminate the ICI, and meanwhile utilizes distributed retrospective interference alignment algorithm to align inter-user interference (IUI). The scheme maintains the same performance improvement for the DoF, i.e.,\r\n${{L\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil K(\\bar \\varphi { + }1)} \\mathord{\\left/\r\n {\\vphantom {{L\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil K(\\bar \\varphi { + }1)} {(L{ + }\\bar \\varphi { + }}1)}} \\right.\r\n \\kern-\\nulldelimiterspace} {(L{ + }\\bar \\varphi { + }1)}}$, and in the meantime avoids performance degradation of the RIR scheme.\r\n\r\n\\item To further analyze the performance of the two proposed schemes effected by transceiver antennas ratio, we perform numerical evaluations via simulations. The simulation results show that, when the transceiver antennas ratio approaches 2, both RIR scheme and B-DRIA scheme obtain greater DoF gain than the RIA scheme, while as the transceiver antennas ratio approaches 1, B-DRIA scheme achieves greater DoF gain than RIR scheme.\r\n\\vspace{-0.3em}\r\n\\end{itemize}\r\n\r\nThe rest of this paper is organized as follows. The cellular distributed multi-user MIMO system is presented in Section II, which includes the description of the network in time domain and angular domain, and meanwhile the CSIT feedback model and the performance criteria are introduced. Then, two novel interference alignment schemes, i.e., the RIR scheme and the B-DRIA scheme, are proposed and the corresponding typical applications are given in Section III and Section IV, respectively. The performances of the proposed schemes are evaluated in Section V and we conclude at last.\r\n\r\n\\section{System Model}\r\nConsider cellular distributed $K$-user MIMO downlink network \\cite{7464855} as illustrated in Fig. 1. Define the cells set as ${\\cal L}{ = }\\{ 1,2, \\cdots ,L\\}$, and for each cell $i,\\forall i \\in {\\cal L}$, the base station is configured with $M$ antennas to provide service to the users in set \r\n${{\\textbf{K}}_i}{ = }\\left\\{ {1, \\cdots ,K} \\right\\}$. In addition, each served user $k,\\forall k \\in {{\\textbf{K}}_i}$ in the cell $i$ is equipped with $N$ antennas where $M>N$, which means that no user can decode its message without additional information.\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig1}\r\n\\caption{Cellular distributed multi-user MIMO network.}\r\n\\label{Fig1}\r\n\\end{figure}\r\n\r\nIn the following, for the simplicity, we use $i,\\forall i \\in {\\cal L}$ and $j,\\forall j \\in {\\cal L}$ to denote the cell which the transmitter and the receiver belong to, respectively. Then, we further classify the cells into two categories: if $j = i$, the cell $i$ is called target cell, otherwise denotes  interference cell. Assume that the network takes ${\\textbf{T}}{ = }\\{ 1,2, \\cdots ,T\\}$ slots to transmit signals and for each slot $t,\\forall t \\in {\\textbf{T}}$, the base station $i$ sends messages ${{\\bf{S}}_i}[t] = \\left[ {{{\\bf{S}}_{i,1}}[t] \\cdots {{\\bf{S}}_{i,k}}[t]} \\right] $ to its served users. Due to the broadcast nature of the wireless communication, on the one hand, the received signals of the user $k,\\forall k \\in{\\textbf{K}}$ are composed of two parts, i.e., the disred signals ${{\\bf{S}}_i}[t]{ = }\\left[ {{{\\bf{S}}_{i,1}}[t] \\cdots {{\\bf{S}}_{i,k}}[t]} \\right]$ from the cell $i$ and undesired signals  ${{\\bf{S}}_{ - i}}[t]{ = }\\left[ {{{\\bf{S}}_{ - i,1}}[t] \\cdots {{\\bf{S}}_{ - i,k}}[t]} \\right]$ from the interference cells $ - i = {\\cal L}\\backslash i$. On the other hand, the desired signals is made up of desired symbols ${{\\bf{S}}_{i,k}}[t]{ = }\\left[ {{s_{k,1}}[t] \\cdots {s_{k,n}}[t]} \\right]$ where $0\\!\\! <\\!n \\!\\!\\le\\! N$ and undesired symbols ${{\\bf{S}}_{i, - k}}[t]{ = }\\left[ {{s_{ - k,1}}[t] \\cdots {s_{ - k,n}}[t]} \\right]$ where $ - k ={\\textbf{K}} \\backslash k$.\r\n\r\nHerein, for the cellular distributed K-user MIMO downlink network, since all users share the same channel resources, i.e., the IUI and the ICI coexist in each slot. In order to implement our schemes to align IUI and ICI, the CSI should be further differentiated. Specifically, the component of the CSI should be expanded into two parts, the channel gain information and the angel information. Thus, the system model should be described in time domain \\cite{5358700} and angular domain \\cite{1143830}, as that presented in the section II.A and II.B, respectively. Meanwhile, based on the relationship between the coherent time and the time delay, the types of the CSI should be classified into three categories, the instantaneous CSI, the moderate delayed CSI and the delayed CSI. In specific, the CSIT feedback model is presented in the section II.C \\cite{6926832}.\r\n\r\n\\subsection{Time Domain Model}\r\nTo acquire the channel gain information of CSI, the system model mentioned above should be described as the time domain model \\cite{5358700}. Taking the transmission of one slot $t,\\forall t \\in {\\textbf{T}} $ as example, the base station $i$ transmits messages ${\\boldsymbol{{S}}_{i,k}}[t]$ to the user $k$ in the cell $i$. Then the received signal of the user $k$ in cell $j,\\forall j \\in {\\cal L}$ can be written by \r\n\\begin{equation}\r\n\\label{eq1}\r\n{\\boldsymbol{{y}}_{j,k}}[t]{ = }\\sum\\limits_{i = 1}^L {\\sum\\limits_{k = 1}^K {{\\bf{H}}_i^{j,k}[t]{{\\boldsymbol{{S}}}_{i,k}}[t]} } { + }\\overline {\\boldsymbol{{N}}}.\r\n\\end{equation}\r\nwhere ${\\bf{H}}_i^{j,k}[t] \\in{\\textbf{C}}{^{M \\times N}}$ denotes the channel matrix between the base station $i$ and the user $k$ of the cell $j$, and it is subject to independent and identically distributed $\\left( {i.i.d.} \\right)$ according to ${\\cal C}{\\cal N}(0,1)$. $\\overline {\\boldsymbol{N}}  \\in{\\textbf{C}}{^{N \\times 1}}$ denotes the additive white Gaussian noise (AWGN) term at the user $k$ of the cell $j$.\r\n\r\nIf the transmitted messages ${{\\boldsymbol{S}}_{i,k}}[t]$ is known to the user $k$ in the cell $i$, the channel matrix ${\\bf{H}}_i^{j,k}[t]$ can be gotten in the form of the time domain, which denotes the channel gain information of CSI.\r\n\r\n\\subsection{Angular Domain Model}\r\nSimilarly, to acquire the angel information of CSI, the system model of the network should be described in angular domain \\cite{1143830}. The application scenario is the same as it is assumed in the section II.A. The received signals are given by\r\n\\begin{equation}\r\n\\label{eq2}\r\n{{\\boldsymbol{y}}_{j,k}}[t]{ = }\\sum\\limits_{i = 1}^L {\\sum\\limits_{k = 1}^K {{\\bf{A}}_i^{j,k}[{\\theta _i}]{{\\boldsymbol{S}}_{i,k}}[t]} } { + }\\overline {\\boldsymbol{N}}.\r\n\\end{equation}\r\nwhere ${\\bf{A}}_i^{j,k}[{\\theta _i}] \\in{\\textbf{C}}{^{N \\times M}}$  is the direction of arrival (DoA) matrix between the base station $i$ and the user $k$ in the cell $j$ and meanwhile, $\\overline {\\boldsymbol{N}}\\in{\\textbf{C}}{^{N \\times 1}}$ denotes AWGN. By analyzing the ingredient of ${\\bf{A}}_i^{j,k}[{\\theta _i}]$, it is composed of $M$ direction vectors,\r\n\\begin{equation}\r\n\\label{eq3}\r\n{\\bf{A}}_i^{j,k}[{\\theta _i}]{ = }\\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\boldsymbol{a}}}_{i,1}^{j,k}[{\\theta _i}]}& \\cdots &{{{\\boldsymbol{a}}}_{i,M}^{j,k}[{\\theta _i}]}\r\n\\end{array}} \\right],\r\n\\end{equation}\r\nwhere the direction vector ${{\\boldsymbol{a}}}_{i,m}^{j,k}[{\\theta _i}]$ can be regarded as the function of the DoA angular ${\\theta _i}$. As for each ${{\\boldsymbol{a}}}_{i,m}^{j,k}[{\\theta _i}]$, it represents the direction from the antenna $m \\in [1,M]$ of the cell i to the user $k$ in the cell $j$ and is written as\r\n\\begin{equation}\r\n\\label{eq4}\r\n{\\boldsymbol{a}}_{i,m}^{j,k}[{\\theta _i}] = {\\left[ {1,{e^{ - j{\\mu _{m}}}}, \\cdots ,{e^{ - j(N - 1){\\mu _m}}}} \\right]^T}.\r\n\\end{equation}\r\nNote that ${\\mu _m} = ({{2\\pi d} \\mathord{\\left/{\\vphantom {{2\\pi d} \\lambda }} \\right.\\kern-\\nulldelimiterspace} \\lambda })\\sin {\\theta _{i}}$, where $d$ is the array spacing and $\\lambda $ is the carrier wavelength. The DoA angular ${\\theta _i}$ denotes the angel information of CSI.\r\n\r\n\\subsection{CSIT Feedback Model}\r\nDue to the unavoidable feedback delay from the receiver to the transmitter, after making expansion of the component of CSI, the obtained CSIT is a delayed version. It means that, at the transmitter side, the obtained CSIT cannot be used to instruct the design of precoding matrix at the current slot. Therefore, the types of the CSI should be further classified by CSIT feedback model \\cite{6926832} which is summarized in Fig. 2. \r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig2}\r\n\\caption{CSIT feedback model.}\r\n\\label{Fig2}\r\n\\end{figure}\r\n\r\nWe assume that the receiver can perfectly estimate the CSI and then send it back to the corresponding transmitter through an error-free but delayed feedback link. Therefore, the transmitter can continuously track the variation of the channel matrix over different slots. To distinguish different versions of CSIT, we define the channel feedback delay and the period of coherence time as ${T_{fb}}$ and ${T_c}$, respectively. Then, the definition of the ratio $\\lambda$ is given by\r\n\\begin{equation}\r\n\\label{eq5}\r\n\\lambda { = }{{{T_{fb}}} \\mathord{\\left/\r\n {\\vphantom {{{T_{fb}}} {{T_c}}}} \\right.\r\n \\kern-\\nulldelimiterspace} {{T_c}}},\r\n\\end{equation}\r\nwhich is naturally divided into three types and each type represents one kind of CSI:\r\n\\begin{itemize}\r\n\\vspace{-0.3em}\r\n\\item $\\lambda  = 0$, the obtained CSI is an instantaneous CSI, that is the transmitters acquire the CSI feedback from the receivers immediately.\r\n\r\n\\item $0 < \\lambda  < 1$, this situation should be subdivided into two subcases, i.e., the moment of getting CSI belongs to $\\left[ {{T_{fb}} ,{T_c}- {T_{fb}}} \\right]$ and $\\left[ {{T_c} - {T_{fb}},{T_c}} \\right]$. In the former interval, the obtained CSI is equivalent to the instantaneous CSI, while in the later interval, the obtained CSI represents the moderate delayed CSI, which means that, after feedback delay, the transmitters can also make use of the CSI in the rest time of the current slot.\r\n\r\n\\item $\\lambda  \\ge 1$, the obtained CSI is a delayed version, that is the CSI is absolutely outdated. Thus, the channel gain for the current slot is unknown and the CSI is no longer limited by the feedback delay.\r\n\\vspace{-0.3em}\r\n\\end{itemize}\r\nThe type of CSI adopted in Section III and Section IV should be a comprehensive combination of the latter two types, i.e., the moderate delayed CSI and the delayed CSI.\r\n\r\n\\subsection{Sum Degrees of Freedom}\r\nFor the considered system, a measurement criterion is desired to compare the performance of the proposed schemes with existing IA schemes. Herein, the sum degrees of freedom \\cite{7442513} is extended to the cellular distributed $K$-user MIMO downlink network. Specifically, in each slot $t,\\forall t \\!\\in\\!{\\textbf{T}}$, the base stations simultaneously transmit independent messages. Taking the base station $i,\\forall i \\in {\\cal L}$ for example, the messages ${{\\bf{S}}_{i,1}}[t], \\cdots ,{{\\bf{S}}_{i,k}}[t]$ are sent with the rates of ${R_{i,1}}, \\cdots ,{R_{i,k}}$  bits/s/Hz, respectively. For the user $k,\\forall k \\in {\\textbf{K}}$, its achievable rate is characterized by \r\n\\begin{equation}\r\n\\label{eq6}\r\n{R_{i,k}}{ = }{d_{i,k}}\\log (1{ + }SNR) + o\\left( {\\log (SNR)} \\right),\r\n\\end{equation}\r\nwhere ${d_{i,k}}$ denotes the degree of freedom belonging to the user $k$ in the cell $i$ and can be expressed as\r\n\\begin{equation}\r\n\\label{eq7}\r\n{d_{i,k}} = \\mathop {\\lim }\\limits_{SNR \\to \\infty } {{{R_{i,k}}} \\mathord{\\left/\r\n {\\vphantom {{{R_{i,k}}} {{{\\log }_2}(SNR)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {{{\\log }_2}(SNR)}}.\r\n\\end{equation}\r\nThe formula (7) measures the channel capacity of a single cell per user, i.e., the number of data streams that can be transmitted reliably by one user within one slot. Then, we extend it to the case of cellar $K$-user, given by\r\n\\begin{equation}\r\n\\label{eq8}\r\nDoF = \\mathop {\\lim }\\limits_{SNR \\to \\infty } \\mathop {\\max }\\limits_{\\boldsymbol{R}} \\sum\\limits_{i = 1}^L {\\sum\\limits_{k = 1}^K {{{{R_{i,k}}} \\mathord{\\left/\r\n {\\vphantom {{{R_{i,k}}} {{{\\log }_2}(SNR)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {{{\\log }_2}(SNR)}}} },\r\n\\end{equation}\r\nwhere ${\\boldsymbol{R}}{ = }\\left[ {{R_{i,1}}, \\cdots ,{R_{i,k}}} \\right] \\in {{\\textbf{R}}^K}$ denotes the vector of achievable rates for the users in the cell $i$. Thus, the DoF measures the channel capacity of the whole network.\r\n\r\n\\section{Retrospective Interference Regeneration Scheme}\r\nAlthough the RIA scheme achieves greater DoF than TDMA scheme \r\n\\cite{Maddah2012Completely,2014Linear,2011Achieving,2014On,Hao2016Achievable,2015Retrospective,2015Retrospective2,6179997,6205390,6341083,7588140,6388347}, when the numbers of cells and users increase, some space resources are wasted and the DoF gain will be decreased. To handle this issue, we propose the retrospective interference regeneration (RIR) scheme. The core idea of the scheme is making use of the interference signals from interference cells to align ICI, i.e., when the receivers feed interference signals back, the base stations eliminate redundant symbols in the interference signals to extract desired symbols and meanwhile, provide desired symbols to the target cell. In the following, we firstly illustrate the details of the proposed scheme which includes three phases, i.e.,  signal transmission, interference elimination and interference regeneration, and  interference retransmission. Then, the DoF of the RIR scheme is analyzed. To make it understandable, we also present an example of the scheme at last. \r\n\r\n\\subsection{Signal Transmission}\r\nThe \ufb01rst phase spans ${\\cal L}$ groups of $\\varphi  = \\left\\lfloor {{N \\mathord{\\left/ {\\vphantom {N {\\left( {M - N} \\right)}}} \\right. \\kern-\\nulldelimiterspace} {\\left( {M - N} \\right)}}} \\right\\rfloor $ slots where $\\left\\lfloor {*} \\right\\rfloor $ denotes the round down operation. In each slot $t \\in \\left\\{ {1, \\cdots ,\\varphi } \\right\\}$ of period $i \\in {\\cal L}$, the base station $i$ transmits signals while the other base stations keep silent. The transmitted signal is characterized by\r\n\\begin{equation}\r\n\\label{eq9}\r\n{\\boldsymbol{S}}_{i}^{}[(i - 1)\\varphi  + t] = {\\left[ {{s_{i,1}}[(i - 1)\\varphi  + t], \\cdots ,{s_{i,M}}[(i - 1)\\varphi  + t]} \\right]^T}.\r\n\\end{equation}\r\nThen the signals received by user $k$ of the cell $j$ are \r\n\\begin{equation}\r\n\\label{eq10}\r\n{\\boldsymbol{y}}_{j,k}^{}[(i - 1)\\varphi  + t] = {\\bf{H}}_i^{j,k}[(i - 1)\\varphi  + t]{{\\boldsymbol{S}}_i}[(i - 1)\\varphi  + t],j \\in {\\cal L}.\r\n\\end{equation}\r\nAs mentioned earlier, if $i \\ne j$, the cell $i$ is the interference cell and the corresponding received signals are ICI. If $i = j$ , the cell $i$ is the target cell but, for the user $k$, only part of ${{\\boldsymbol{S}}_i}[(i - 1)\\varphi  + t]$ are desired symbols, i.e., \r\n\\begin{small}\r\n$\\left\\{ {{s_{i,(k - 1)\\left\\lceil {{M \\mathord{\\left/ {\\vphantom {M K}} \\right. \\kern-\\nulldelimiterspace} K}} \\right\\rceil  + 1}}\\left[ {(i - 1)\\varphi  + t} \\right], \\cdots ,{s_{i,k\\left\\lceil {{M \\mathord{\\left/ {\\vphantom {M K}} \\right. \\kern-\\nulldelimiterspace} K}} \\right\\rceil }}\\left[ {(i - 1)\\varphi  + t} \\right]} \\right\\}$\r\n\\end{small}\r\n and the left are the interference symbols. According to whether this user belong to the target cell, the user feedback different information to the transmitter, i.e., if $i = j$, the users feed the channel gain information back to the target cell\u2019s base station by delayed feedback links. While if $i \\ne j$, the users feed the ICI back to the interference cell\u2019s base station, i.e., ${\\boldsymbol{y}}_{j,k}^{}[(i - 1)\\varphi  + t]$. This process is illustrated by Fig. 3.\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig3}\r\n\\caption{Signal transmission in 1st phase.}\r\n\\label{Fig3}\r\n\\end{figure}\r\n\r\nAs mentioned earlier, since $M > N$, no user can decode the messages with only one-slot transmitted signals. To handle this issue, the additional interference alignment process is developed to the system, i.e., interference elimination and interference regeneration.\r\n\r\n\\subsection{Interference Elimination and Interference Regeneration}\r\nBy analyzing the components of ICI, we find that, for the user $k$, part of the information in ICI can be utilized, while the remainder is still the interference. Therefore, to distinguish these two parts, the ICI can be written as \r\n\\begin{align}\r\n\\label{eq11}\r\n{{\\textbf{L}}_j}[(i\\! -\\! 1)\\varphi  \\!+ \\!t]{ = }&{{\\textbf{L}}_j}\\left( {{s_{i,1}}[(i\\! -\\! 1)\\varphi \\! + \\!t],\\! \\cdots\\! ,{s_{i,M}}[(i\\! - \\!1)\\varphi \\! + \\!t]} \\right)\\nonumber\\\\\r\n{ = }&{\\bf{H}}_i^{j,k}[(i - 1)\\varphi  + t]{{\\boldsymbol{S}}_i}[(i - 1)\\varphi  + t],\\nonumber\\\\\r\n          &{\\cal L},j \\ne i,k \\in {\\textbf{K}}\\nonumber\\\\\r\n{ = }&{\\boldsymbol{L}}_j^{i,k}[(i - 1)\\varphi  + t]{ + }{\\boldsymbol{I}}_j^{i,k}[(i - 1)\\varphi  + t],\r\n\\end{align}\r\nwhere ${\\boldsymbol{L}}_j^{i,k}[(i - 1)\\varphi  + t]$ represents the useful part and ${\\boldsymbol{I}}_j^{i,k}[(i - 1)\\varphi  + t]$ represents the rest, i.e., the interference. Then, we can define these two parts as partial desired signals and partial interference signals as follows, respectively,\r\n\\begin{align}\r\n\\label{eq12}\r\n{\\boldsymbol{L}}_j^{i,k}[(i - 1)\\varphi  + t]\r\n{ = }& {\\boldsymbol{L}}_j^{i,k}({s_{i,(k - 1)\\left\\lceil {{M \\mathord{\\left/{\\vphantom {M K}} \\right.\r\n            \\kern-\\nulldelimiterspace} K}} \\right\\rceil  + 1}}[(i - 1)\\varphi  + t], \\nonumber\\\\\r\n          &\\cdots ,{s_{i,k\\left\\lceil {{M \\mathord{\\left/{\\vphantom {M K}} \\right.\\kern-\\nulldelimiterspace} K}} \\right\\rceil }}[(i - 1)\\varphi  + t])\\nonumber\\\\\r\n{=}  &{\\bf{H}}_i^{j,k}[(i - 1)\\varphi  + t]{{\\boldsymbol{S}}_i}[(i - 1)\\varphi  + t], \\nonumber\\\\\r\n          &j \\in {\\cal L},j \\ne i,\r\n\\end{align}\r\n\\begin{align}\r\n\\label{eq13}\r\n{\\boldsymbol{I}}_j^{i,k}[(i - 1)\\varphi  + t]\r\n{ = }&{\\boldsymbol{I}}_j^{i,k}({s_{i,( - k - 1)\\left\\lceil {{M \\mathord{\\left/{\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil  + 1}}[(i - 1)\\varphi  + t],\\nonumber\\\\\r\n          &\\cdots ,{s_{i, - k\\left\\lceil {{M \\mathord{\\left/{\\vphantom {M K}} \\right.\\kern-\\nulldelimiterspace} K}} \\right\\rceil }}[(i - 1)\\varphi  + t])\\nonumber\\\\\r\n{ = }&{\\bf{H}}_i^{j,k}[(i - 1)\\varphi  + t]{{\\boldsymbol{S}}_i}[(i - 1)\\varphi  + t],\\nonumber\\\\\r\n          &j \\in {\\cal L},j \\ne i, - k ={\\textbf{K}} \\backslash k,\r\n\\end{align}\r\nFor the interference cell, the interference space of ${\\boldsymbol{L}}_j^{i,k}[(i - 1)\\varphi  + t]$ is a subspace of ${{\\textbf{L}}_j}[(i - 1)\\varphi  + t]$. Hence, according to the feedback interference signals, the interference cell\u2019s base station can design a precoding matrix ${{\\bf{U}}_j}[(i - 1)\\varphi  + t]$ to achieve partial interference elimination, i.e.,\r\n\\begin{equation}\r\n\\label{eq14}\r\n{{\\bf{U}}_j}[(i - 1)\\varphi  + t]{{\\textbf{L}}_j}[(i - 1)\\varphi  + t] = {\\boldsymbol{L}}_j^{i,k}[(i - 1)\\varphi  + t].\r\n\\end{equation}\r\nFor (14), according to the extension theorem of linear subspace \\cite{1998Matrix}, the precoding matrix ${{\\bf{U}}_j}[t]$ must exist, and the solution of the matrix can be obtained by cyclic-zero-padding precoding matrix \\cite{7820128}.\r\n\r\nAfter the interference elimination, the partial desired signal ${\\boldsymbol{L}}_j^{i,k}[(i - 1)\\varphi  + t]$ can be transmitted in the interference retransmission phase. Unfortunately, the interference retransmission will cause additional interference to other cells except the target cell, as illustrated in Fig. 4.\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig4}\r\n\\caption{Additional ICI in 2nd phase.}\r\n\\label{Fig4}\r\n\\end{figure}\r\n To handle this issue, the base station should design interference regeneration matrices to satisfy the following condition\r\n\\begin{equation}\r\n\\label{eq15}\r\n\\begin{array}{l}\r\n{{\\bf{V}}_p}[(i - 1)\\varphi  + t]{\\boldsymbol{L}}_p^{i,k}[(i - 1)\\varphi  + t] \\\\\r\n{ = }{{\\bf{V}}_q}[(i - 1)\\varphi  + t]{\\boldsymbol{L}}_q^{i,k}[(i - 1)\\varphi  + t],\\forall p,q \\in {\\cal L}\\backslash i,\r\n\\end{array}\r\n\\end{equation}\r\nwhere each interference regeneration matrix ${{\\bf{V}}_p}[(i - 1)\\varphi  + t]$ is an $\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil  \\times \\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil $\r\n matrix that can align the interference signals into the same space. Since the vectors ${\\boldsymbol{L}}_p^{i,k}[(i - 1)\\varphi  + t]$ and ${\\boldsymbol{L}}_q^{i,k}[(i - 1)\\varphi  + t]$ are irrelevant with same dimension $\\left\\lceil {{M \\mathord{\\left/{\\vphantom {M K}} \\right.\\kern-\\nulldelimiterspace} K}} \\right\\rceil $, the precoding matrices ${{\\bf{V}}_p}[(i - 1)\\varphi  + t]$  and ${{\\bf{V}}_q}[(i - 1)\\varphi  + t]$ can be regarded as row transformation matrices. Thus, from the property of equivalence matrix \\cite{1998Matrix},  the interference regeneration matrices are always exist, and cyclic-zero-padding precoding \\cite{7820128} can be used to obtain the particular solution. The details are summarized in Fig. 5.\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig5}\r\n\\caption{Interference regeneration in 2nd phase.}\r\n\\label{Fig5}\r\n\\end{figure}\r\n\r\n\\subsection{Interference Retransmission}\r\nAfter interference elimination and interference regeneration, base stations are capable to simultaneously transmit partial desired signals as \r\n\\begin{align}\r\n\\label{eq16}\r\n{{\\boldsymbol{X}}_j}[L\\varphi  + 1] &=\\! \\sum\\nolimits_{i \\in {\\cal L}\\backslash j}\\! {{\\boldsymbol{X}}_j^i[L\\varphi  + 1]}\\nonumber \\\\\r\n                            &= \\! \\sum\\nolimits_{i \\in {\\cal L}\\backslash j} \\! \\! {\\sum\\limits_{t = \\left( {i - 1} \\right)\\varphi  + 1}^{i\\varphi } \\! {{{\\bf{V}}_j}[t]{{\\bf{U}}_j}[t]({\\bf{H}}_i^{j,k}[t]{{\\boldsymbol{S}}_i}[t])} }\\nonumber \\\\\r\n                            &= \\sum\\nolimits_{i \\in {\\cal L}\\backslash j} {\\sum\\limits_{t = \\left( {i - 1} \\right)\\varphi  + 1}^{i\\varphi } {{{\\bf{V}}_j}[t]{\\boldsymbol{L}}_j^{i,k}[t]} }.\r\n\\end{align}\r\nWith the preprocessing of ${{\\bf{V}}_j}[t]$ and ${{\\bf{U}}_j}[t]$, for the whole cells, the additional signals coming from the interference retransmission phase are convert to the partial desired signals, which means that the retransmitted process will not bring about ICI and IUI, as illustrated in Fig. 6. Herein, for the user $k$ in the target cell $i$, the received signals are written as,\r\n\\begin{equation}\r\n\\label{eq17}\r\n{{\\bf{y}}_{i,k}}[L\\varphi  + 1] = \\sum\\nolimits_{j \\in {\\cal L}} {{\\bf{H}}_j^{i,k}[L\\varphi  + 1]{{\\bf{X}}_j}[L\\varphi  + 1]},\r\n\\end{equation}\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig6}\r\n\\caption{Interference retransmission in 3rd phase.}\r\n\\label{Fig6}\r\n\\end{figure}\r\n\\!and the whole process can be summarized as the equation (18),\r\n\\newcounter{mytempeqncnt}\r\n\\begin{figure*}[!t]\r\n\\normalsize\r\n\\setcounter{mytempeqncnt}{\\value{equation}}\r\n\\setcounter{equation}{17}\r\n\\begin {small}\r\n\\begin{equation}\r\n\\setlength{\\arraycolsep}{0.5pt}\r\n\\label{eq18}\r\n\\underbrace {\\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\bf{y}}_{i,k}}[(i - 1)\\varphi  + 1]}\\\\\r\n{{{\\bf{y}}_{i,k}}[(i - 1)\\varphi  + 2]}\\\\\r\n \\vdots \\\\\r\n{{{\\bf{y}}_{i,k}}[i\\varphi ]}\\\\\r\n{{{\\bf{y}}_{i,k}}[L\\varphi  + 1]}\r\n\\end{array}} \\right]}_{\\bf{Y}}\\! { = }\\! \\underbrace {\\left[ {\\begin{array}{*{20}{c}}\r\n{{\\bf{H}}_i^{i,k}[(i - 1)\\varphi  + 1]}&0& \\cdots &0\\\\\r\n0& \\ddots & \\ddots & \\vdots \\\\\r\n \\vdots & \\ddots & \\ddots &0\\\\\r\n0& \\cdots &0&{{\\bf{H}}_i^{i,k}[i\\varphi ]}\\\\\r\n{\\sum\\limits_{j \\in {\\cal L}} {{\\bf{H}}_j^{i,k}[L\\varphi  + 1]} {{\\boldsymbol{\\beta }}_{(i - 1)\\varphi  + 1}}}&{\\sum\\limits_{j \\in {\\cal L}} {{\\bf{H}}_j^{i,k}[L\\varphi  + 1]{{\\boldsymbol{\\beta }}_{(i - 1)\\varphi  + 2}}} }& \\cdots &{\\sum\\limits_{j \\in {\\cal L}} {{\\bf{H}}_j^{i,k}[L\\varphi  + 1]{{\\boldsymbol{\\beta }}_{i\\varphi }}} }\r\n\\end{array}} \\right]}_{\\bf{H}}\\underbrace {\\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\bf{S}}_i}[(i - 1)\\varphi  + 1]}\\\\\r\n \\vdots \\\\\r\n \\vdots \\\\\r\n \\vdots \\\\\r\n{{{\\bf{S}}_i}[i\\varphi ]}\r\n\\end{array}} \\right]}_{\\bf{S}}\r\n\\end{equation}\r\n\\end {small}\r\n\\hrulefill\r\n\\vspace*{4pt}\r\n\\end{figure*}\r\n where ${{\\boldsymbol{\\beta}}_t}$ is the combination precoding of the interference cells. The precoding matrix is composed of interference elimination matrix, interference regeneration matrix and channel matrix in slot $t$, denoted by\r\n\\begin{equation}\r\n\\label{eq19}\r\n{{\\boldsymbol{\\beta} }_t} = \\sum\\nolimits_{j \\in {\\cal L}{\\backslash }i} {{{\\bf{V}}_j}[t]{{\\bf{U}}_j}[t]{\\bf{H}}_i^{j,k}[t]}.\r\n\\end{equation}\r\n\r\nFor our proposed scheme, ${{\\bf{V}}_j}[t]$ , ${{\\bf{U}}_j}[t]$ and ${\\bf{H}}_j^{i,k}[L\\varphi  + 1]$ are non-singular matrix which means that they don't change the rank of ${\\bf{H}}_i^{j,k}[t]$, and in the meantime, for any two subchannel matrices, they either  come from different time slots, e.g., ${\\bf{H}}_i^{i,k}[(i - 1)\\varphi  + 1]$ and ${\\bf{H}}_i^{i,k}[i\\varphi ]$, or come from different spaces, e.g., ${\\bf{H}}_i^{i,k}[(i - 1)\\varphi  + 1]$ and ${\\bf{H}}_j^{i,k}[(i - 1)\\varphi  + 1], j \\ne i$, are independent. The former is for the sake of the delayed CSIT and the latter is on account of the irrelevance of the channels among different cells. Herein, the elements of $\\bf{H}$ are independent so that the matrix itself is a $\\left\\lfloor {{N \\mathord{\\left/{\\vphantom {N {(M - N}}} \\right. \\kern-\\nulldelimiterspace} {(M - N}})} \\right\\rfloor M$ dimensional non-singular matrix. Therefore, at the user side, the Least Square (LS) method \\cite{2004Discussion} can be used to decode the messages of the whole slots. \r\n\r\n\\subsection{Degrees of Freedom}\r\nIn this subsection, the DoF of the proposed RIR scheme is analyzed. In specific, since the base stations spend $L\\varphi { + }1$ incoherent slots to transmit $L\\left\\lceil {{M \\mathord{\\left/ {\\vphantom {M K}} \\right. \\kern-\\nulldelimiterspace} K}} \\right\\rceil K\\varphi $ independent symbols in all. We assume that the transmission is reliable, i.e., the value of signal-to-noise ratio approaches infinity. Therefore, its achievable DoF is \r\n\\begin{equation}\r\n\\label{eq20}\r\nDoF{ = }{{(L\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil K\\varphi )} \\mathord{\\left/\r\n {\\vphantom {{(L\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil K\\varphi )} {(L\\varphi { + }1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(L\\varphi { + }1)}}.\r\n\\end{equation}\r\nTo facilitate the analysis, we assume that $M$ can be divided by $K$. Then the DoF can be simplified as\r\n\\begin{equation}\r\n\\label{eq21}\r\nDoF{{{ = }LM} \\mathord{\\left/\r\n {\\vphantom { {(L + {1 \\mathord{\\left/\r\n {\\vphantom {1 \\varphi }} \\right.\r\n \\kern-\\nulldelimiterspace} \\varphi })}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(L + {1 \\mathord{\\left/\r\n {\\vphantom {1 \\varphi }} \\right.\r\n \\kern-\\nulldelimiterspace} \\varphi })}}.\r\n\\end{equation}\r\nIn practice, the number of cells $L$ is fixed and $\\varphi $ is a function of $M$ and $N$, thus the value of DoF mainly depends on the relationship between $M$ and $N$. Define $\\rho  = {M \\mathord{\\left/{\\vphantom {M N}} \\right.\\kern-\\nulldelimiterspace} N}$ as the transceiver antennas ratio  \\cite{Maddah2012Completely} and then, the DoF can be simplified as\r\n\\begin{equation}\r\n\\label{eq22}\r\nDoF{{{ = }LM} \\mathord{\\left/\r\n {\\vphantom {{{ = }LM} {(L + {1 \\mathord{\\left/\r\n {\\vphantom {1 {\\left\\lfloor {{1 \\mathord{\\left/\r\n {\\vphantom {1 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }}} \\right.\r\n \\kern-\\nulldelimiterspace} {\\left\\lfloor {{1 \\mathord{\\left/\r\n {\\vphantom {1 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }})}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(L + {1 \\mathord{\\left/\r\n {\\vphantom {1 {\\left\\lfloor {{1 \\mathord{\\left/\r\n {\\vphantom {1 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }}} \\right.\r\n \\kern-\\nulldelimiterspace} {\\left\\lfloor {{1 \\mathord{\\left/\r\n {\\vphantom {1 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }})}}.\r\n\\end{equation}\r\nTo analyze the formula (22), we can get the conclusion that, with the increase of the ratio $\\rho$, the DoF should be distinct. That is, when $M$ is not divisible by $ M -N $, the value of $\\varphi = \\left\\lfloor {{N \\mathord{\\left/{\\vphantom {N {\\left( {M - N} \\right)}}} \\right.\\kern-\\nulldelimiterspace} {\\left( {M - N} \\right)}}} \\right\\rfloor $ will not change owing to the round down operation $\\left\\lfloor {*} \\right\\rfloor$ and the DoF keeps constant. While if $M$ is divisible by $ M -N $, the value of the DoF will grow sharply. Therefore, the value of $\\varphi$ is divided into three cases, i.e., $\\varphi > 2$, $\\varphi=2$, $\\varphi=1$, and the corresponding DoF of each situation is further discussed in Section V.\r\n\r\n\\subsection{Typical Application}\r\nIn this subsection, we take a typical application as example to demonstrate the whole process of the RIR scheme. In particular, the configuration of the network is set as $(L,M,K,N) = (3,4,2,3)$, i.e., the network is composed of three cells and in each cell, the base station with four antennas provides services to two users, where each user has three antennas. For this scenario, the details about the application of the RIR scheme are as follows.\r\n\r\nThe \ufb01rst phase spans ${\\cal L} = 3$ groups of $\\varphi  = 3$ slots. From slot 1 to slot 3 of the first group, the symbols sent by the base station 1 are  as follows\r\n\\begin{equation}\r\n\\label{eq23}\r\n\\begin{array}{l}\r\n{{\\boldsymbol{S}}_1}[1] = \\left( {\\begin{array}{*{20}{c}}\r\n{{a_1}},\r\n{{a_2}},\r\n{{b_1}},\r\n{{b_2}}\r\n\\end{array}} \\right)^T,\\\\\r\n{{\\boldsymbol{S}}_1}[2] = \\left( {\\begin{array}{*{20}{c}}\r\n{{a_3}},\r\n{{a_4}},\r\n{{b_3}},\r\n{{b_4}}\r\n\\end{array}} \\right)^T, \\\\\r\n{{\\boldsymbol{S}}_1}[3] = \\left( {\\begin{array}{*{20}{c}}\r\n{{a_5}},\r\n{{a_6}},\r\n{{b_5}},\r\n{{b_6}}\r\n\\end{array}} \\right)^T,\r\n\\end{array}\r\n\\end{equation}\r\nwhere the sets ${\\bf{A}} = \\left\\{ {{a_1}, \\cdots ,{a_6}} \\right\\}$ and ${\\bf{B}} = \\left\\{ {{b_1}, \\cdots ,{b_6}} \\right\\}$ are the desired symbols for the users 1 and 2 in the target cell 1, respectively. Then, the received signals at user $k \\in \\left\\{ {1,2} \\right\\}$ can be characterized as\r\n\\begin{equation}\r\n\\label{eq24}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\begin{array}{l}\r\n{{\\bf{y}}_{1,k}}[t] = {\\bf{H}}_1^{1,k}[t]{{\\boldsymbol{S}}_1}[t]\\\\\r\n\\;\\;\\;\\;\\;\\;\\;\\quad{   = }\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{k,1}[t]}&{h_{1,2}^{k,1}[t]}&{h_{1,3}^{k,1}[t]}&{h_{1,4}^{k,1}[t]}\\\\\r\n{h_{1,1}^{k,2}[t]}&{h_{1,2}^{k,2}[t]}&{h_{1,3}^{k,2}[t]}&{h_{1,4}^{k,2}[t]}\\\\\r\n{h_{1,1}^{k,3}[t]}&{h_{1,2}^{k,3}[t]}&{h_{1,3}^{k,3}[t]}&{h_{1,4}^{k,3}[t]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_{2t - 1}}}\\\\\r\n{{a_{2t}}}\\\\\r\n{{b_{2t - 1}}}\\\\\r\n{{b_{2t}}}\r\n\\end{array}} \\right]\\\\\r\n\\;\\;\\;\\;\\;\\;\\;\\quad{   = }{{\\boldsymbol{L}}_1}[t]\\left( {{a_{2t - 1}},{a_{2t}},{b_{2t - 1}},{b_{2t}}} \\right),t \\in \\{ 1,2,3\\}.\r\n\\end{array}\r\n\\end{equation}\r\nSince $M > N$, no users can decode the messages within one slot. From (24), we know that, in each slot, there are 4 independent values but we only have 3 equations. Thus, for this group, additional $\\varphi  \\times (M - N) = 3$ equations are needed. The users feed the channel estimation ${\\bf{H}}_1^{1,k}[t]$ back to the base station 1. Similarly, in the second and third group, the transmitted symbols are defined as (25) and (26) respectively,\r\n\\begin{equation}\r\n\\label{eq25}\r\n\\begin{array}{l}\r\n{{\\boldsymbol{S}}_2}[4]\\! =\\! \\left( {\\begin{array}{*{20}{c}}\r\n{{c_1}},\r\n{{c_2}},\r\n{{d_1}},\r\n{{d_2}}\r\n\\end{array}} \\right)^T,\\\\\r\n{{\\boldsymbol{S}}_2}[5]\\! =\\! \\left( {\\begin{array}{*{20}{c}}\r\n{{c_3}},\r\n{{c_4}},\r\n{{d_3}},\r\n{{d_4}}\r\n\\end{array}} \\right)^T,\\!\\\\\r\n{{\\boldsymbol{S}}_2}[6]\\! = \\!\\left( {\\begin{array}{*{20}{c}}\r\n{{c_5}},\r\n{{c_6}},\r\n{{d_5}},\r\n{{d_6}}\r\n\\end{array}} \\right)^T,\r\n\\end{array}\r\n\\end{equation}\r\n\\begin{equation}\r\n\\label{eq26}\r\n\\begin{array}{l}\r\n{{\\boldsymbol{S}}_3}[7]\\! =\\! \\left( {\\begin{array}{*{20}{c}}\r\n{{e_1}},\r\n{{e_2}},\r\n{{f_1}},\r\n{{f_2}}\r\n\\end{array}} \\right)^T,\\\\\r\n{{\\boldsymbol{S}}_3}[8]\\! = \\!\\left( {\\begin{array}{*{20}{c}}\r\n{{e_3}},\r\n{{e_4}},\r\n{{f_3}},\r\n{{f_4}}\r\n\\end{array}} \\right)^T,\\\\\r\n{{\\boldsymbol{S}}_3}[9]\\! =\\! \\left( {\\begin{array}{*{20}{c}}\r\n{{e_5}},\r\n{{e_6}},\r\n{{f_5}},\r\n{{f_6}}\r\n\\end{array}} \\right)^T.\r\n\\end{array}\r\n\\end{equation}\r\nDuring the second group and the third group of the first phase, the base stations of the interference cells transmit signals to their served users in turn, but at the same time, the users in cell 1 will receive ICI inevitably, that is\r\n\\begin{equation}\r\n\\label{eq27}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\begin{array}{l}\r\n{{\\bf{y}}_{1,k}}[t] = {\\bf{H}}_2^{1,k}[t]{{\\boldsymbol{S}}_2}[t]\\\\\r\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\,{   = }\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{2,1}^{k,1}[t]}&{h_{2,2}^{k,1}[t]}&{h_{2,3}^{k,1}[t]}&{h_{2,4}^{k,1}[t]}\\\\\r\n{h_{2,1}^{k,2}[t]}&{h_{2,2}^{k,2}[t]}&{h_{2,3}^{k,2}[t]}&{h_{2,4}^{k,2}[t]}\\\\\r\n{h_{2,1}^{k,3}[t]}&{h_{2,2}^{k,3}[t]}&{h_{2,3}^{k,3}[t]}&{h_{2,4}^{k,3}[t]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{{c}_{2t - 7}}}\\\\\r\n{{{c}_{2t - 6}}}\\\\\r\n{{d_{2t - 7}}}\\\\\r\n{{d_{2t - 6}}}\r\n\\end{array}} \\right]\\\\\r\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;{   = }{{\\textbf{L}}_2}\\left( {{{c}_{2t - 7}},{{c}_{2t - 6}},{d_{2t - 7}},{d_{2t - 6}}} \\right),t \\in \\{ 4,5,6\\},\r\n\\end{array}\r\n\\end{equation}\r\n\\begin{equation}\r\n\\label{eq28}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\begin{array}{l}\r\n{{\\bf{y}}_{1,k}}[t] = {\\bf{H}}_3^{1,k}[t]{{\\boldsymbol{S}}_3}[t]\\\\\r\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\,{   = }\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{3,1}^{k,1}[t]}&{h_{3,2}^{k,1}[t]}&{h_{3,3}^{k,1}[t]}&{h_{3,4}^{k,1}[t]}\\\\\r\n{h_{3,1}^{k,2}[t]}&{h_{3,2}^{k,2}[t]}&{h_{3,3}^{k,2}[t]}&{h_{3,4}^{k,2}[t]}\\\\\r\n{h_{3,1}^{k,3}[t]}&{h_{3,2}^{k,3}[t]}&{h_{3,3}^{k,3}[t]}&{h_{3,4}^{k,3}[t]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{e_{2t - 13}}}\\\\\r\n{{e_{2t{ - 12}}}}\\\\\r\n{{f_{2t - 13}}}\\\\\r\n{{f_{2t{ - }12}}}\r\n\\end{array}} \\right]\\\\\r\n\\;\\;\\;\\;\\;\\;\\;\\;\\;{=}{{\\textbf{L}}_3}\\left( \\!{{e_{2t - 13}},{e_{2t{ - 12}}},{f_{2t - 13}},{f_{2t{ - }12}}} \\right)\\!,\\!t\\! \\in\\! \\{ 7,8,9\\}\r\n\\end{array}\r\n\\end{equation}\r\nFrom (27), we know that ${{\\textbf{L}}_2}\\left( {{{c}_{2t - 7}},{{c}_{2t - 6}}} \\right),t \\!\\in\\! \\{4,5,6\\} $ are the partical desired signals for the user 1 in cell 2 but the partical interference signals for the user 2 in cell 2, and ${{\\textbf{L}}_2}\\left( {{d_{2t - 7}},{d_{2t - 6}}} \\right),t \\!\\in\\! \\{4,5,6\\}$ is opposite to the former. In the meantime, the similar conclusion can be gotten from (28). Therefore, to utilize partial desired signals, partial interference signals should be eliminated with precoding matrix  which is desined at base station side. Therefore, the users feed interference signals ${{\\textbf{L}}_2}\\left( {{{c}_{2t - 7}},{{c}_{2t - 6}},{d_{2t - 7}},{d_{2t - 6}}} \\right),t \\!\\in\\! \\{ 4,5,6\\}$ and ${{\\textbf{L}}_3}\\left( {{e_{2t - 13}},{e_{2t{ - 12}}},{f_{2t - 13}},{f_{2t{ - }12}}} \\right),t \\!\\in\\! \\{ 7,8,9\\}$ back to the base station 1 to achieve interference elimination by ${{\\bf{U}}_1}[t]$.\r\n\r\nIn the second phase, as the analysis before, \\begin{small}$\\left({{c}_{2t - 7}},{{c}_{2t - 6}}\\right),t \\!\\in\\! \\{4,5,6\\} $\\end{small} and \\begin{small}$\\left( {{e_{2t - 13}},{e_{2t{ - }12}}} \\right),t \\!\\in\\! \\{7,8,9\\}$\\end{small} are the desidered symbols for the user 1 of cell 2 and the user 1 of cell 3, respectively. After receiving feedback information, i.e., \\begin{small}${\\boldsymbol{y}}_{1,k}^{}[t],t \\!\\in\\! \\{4,5,6,7,8,9\\} $\\end{small}, the precoding matrix \\begin{small}${{\\bf{U}}_1}[t],t \\!\\in\\! \\{ 4,5,6\\} $\\end{small} and \\begin{small}${{\\bf{U}}_1}[t],t \\!\\in\\! \\{ 7,8,9\\} $\\end{small} are designed as (29) and (30), respectively.\r\n\\newcounter{mytempeqncnt2}\r\n\\begin{figure*}[!t]\r\n\\normalsize\r\n\\setcounter{mytempeqncnt2}{\\value{equation}}\r\n\\setcounter{equation}{28}\r\n\\begin{equation}\r\n\\label{eq29}\r\n\\begin{array}{l}\r\n{{\\bf{U}}_1}[t]{{\\textbf{L}}_2}[t] = {{\\bf{U}}_1}[t]\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{2,1}^{k,1}[t]}&{h_{2,2}^{k,1}[t]}&{h_{2,3}^{k,1}[t]}&{h_{2,4}^{k,1}[t]}\\\\\r\n{h_{2,1}^{k,2}[t]}&{h_{2,2}^{k,2}[t]}&{h_{2,3}^{k,2}[t]}&{h_{2,4}^{k,2}[t]}\\\\\r\n{h_{2,1}^{k,3}[t]}&{h_{2,2}^{k,3}[t]}&{h_{2,3}^{k,3}[t]}&{h_{2,4}^{k,3}[t]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{{c}_{2t - 7}}}\\\\\r\n{{{c}_{2t - 6}}}\\\\\r\n{{d_{2t - 7}}}\\\\\r\n{{d_{2t - 6}}}\r\n\\end{array}} \\right]\\\\\r\n\\qquad\\qquad\\quad\\!\\! = {{\\bf{U}}_1}[t]{\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{2,1}^{k,1}[t]}&{h_{2,1}^{k,2}[t]}&{h_{2,1}^{k,3}[t]}\\\\\r\n{h_{2,2}^{k,1}[t]}&{h_{2,2}^{k,2}[t]}&{h_{2,2}^{k,3}[t]}\r\n\\end{array}} \\right]^T}\\left[ {\\begin{array}{*{20}{c}}\r\n{{{c}_{2t - 7}}}\\\\\r\n{{{c}_{2t - 6}}}\r\n\\end{array}} \\right]{ + }{{\\bf{U}}_1}[t]{\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{2,3}^{k,1}[t]}&{h_{2,3}^{k,2}[t]}&{h_{2,3}^{k,3}[t]}\\\\\r\n{h_{2,4}^{k,1}[t]}&{h_{2,4}^{k,2}[t]}&{h_{2,4}^{k,3}[t]}\r\n\\end{array}} \\right]^T}\\left[ {\\begin{array}{*{20}{c}}\r\n{{d_{2t - 7}}}\\\\\r\n{{d_{2t - 6}}}\r\n\\end{array}} \\right]\\\\\r\n\\qquad\\qquad\\quad\\!\\!= {{\\bf{U}}_1}[t]{\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{2,1}^{k,1}[t]}&{h_{2,1}^{k,2}[t]}&{h_{2,1}^{k,3}[t]}\\\\\r\n{h_{2,2}^{k,1}[t]}&{h_{2,2}^{k,2}[t]}&{h_{2,2}^{k,3}[t]}\r\n\\end{array}} \\right]^T}\\left[ {\\begin{array}{*{20}{c}}\r\n{{{c}_{2t - 7}}}\\\\\r\n{{{c}_{2t - 6}}}\r\n\\end{array}} \\right] + {\\bf{0}}\\\\\r\n\\qquad\\qquad\\quad\\!\\!= {\\bf{L}}_1^{2,1}({{c}_{2t - 7}},{{c}_{2t - 6}})[t],t \\in \\{ 4,5,6\\} \r\n\\end{array}\r\n\\end{equation}\r\n\\begin{equation}\r\n\\label{eq30}\r\n{{\\bf{U}}_1}[t]{_3}[t] = {\\bf{L}}_1^{3,1}\\left( {{e_{2t - 13}},{e_{2t{ - }12}}} \\right)[t],t \\in \\{ 7,8,9\\}.\r\n\\end{equation}\r\n\\hrulefill\r\n\\vspace*{4pt}\r\n\\end{figure*}\r\nNote that for the user 2 in the interference cells, the partial interference signals ${\\boldsymbol{L}}_1^{2,1}({{c}_{2t - 7}},{{c}_{2t - 6}})[t]$ and ${\\boldsymbol{L}}_1^{3,1}\\left( {{e_{2t - 13}},{e_{2t{ - }12}}} \\right)[t]$ can also be used to eliminate IUI so that they can achieve equivalent functions for the user 2. Then the interference regeneration matrix is used at the base station 1 to avoid the additional interference in the third phases, i.e.\r\n\\begin{align}\r\n\\label{eq31-32}\r\n&{{\\bf{V}}_1}[t]{\\boldsymbol{L}}_1^{2,1}({{c}_{2t - 7}},{{c}_{2t - 6}})[t]\\nonumber\\\\\r\n&={{\\bf{V}}_3}[t]{\\boldsymbol{L}}_3^{2,1}({{c}_{2t - 7}},{{c}_{2t - 6}})[t],\\\\\r\n&{{\\bf{V}}_1}[t]{\\boldsymbol{L}}_1^{3,1}\\left( {{e_{2t - 13}},{e_{2t{ - }12}}} \\right)[t]\\nonumber\\\\\r\n&={{\\bf{V}}_2}[t]{\\boldsymbol{L}}_2^{3,1}\\left( {{e_{2t - 13}},{e_{2t{ - }12}}} \\right)[t].\r\n\\end{align} \r\nHerein, for the users in the cell 3, the additional partial interference signals ${{\\bf{V}}_1}[t]\\boldsymbol{L}_1^{2,1}({{c}_{2t - 7}},{{c}_{2t - 6}})[t]$ is convert to the partial interference signals, i.e., ${{\\bf{V}}_3}[t]{\\boldsymbol{L}}_3^{2,1}({{c}_{2t - 7}},{{c}_{2t - 6}})[t]$ that the later one is known by the users in the cell 3. Meanwhile, for the users in the cell 2, the signals are processed in the same way. Henceforth,  the additional interference caused by interference retransmission process can be avoid.\r\n\r\nIn the third phase, all the base stations simutanously retransmit partial desired signals as follows\r\n\\begin{align}\r\n\\label{eq33-35}\r\n{{\\bf{X}}_1}[10] &= \\sum\\limits_{t = 4}^6 {{{\\bf{V}}_1}[t]} {\\boldsymbol{L}}_1^{2,1}[t]{ + }\\sum\\limits_{t = 7}^9 {{{\\bf{V}}_1}[t]} {\\boldsymbol{L}}_1^{3,1}[t],\\\\\r\n{{\\bf{X}}_2}[10] &= \\sum\\limits_{t = 1}^3 {{{\\bf{V}}_2}[t]} {\\boldsymbol{L}}_2^{1,1}[t]{ + }\\sum\\limits_{t = 7}^9 {{{\\bf{V}}_2}[t]} {\\boldsymbol{L}}_2^{3,1}[t],\\\\\r\n{{\\bf{X}}_3}[10] &= \\sum\\limits_{t = 1}^3 {{{\\bf{V}}_3}[t]} {\\boldsymbol{L}}_3^{1,1}[t]{ + }\\sum\\limits_{t = 4}^6 {{{\\bf{V}}_3}[t]} {\\boldsymbol{L}}_3^{2,1}[t].\r\n\\end{align}\r\nFor the user $k$ in the cell 1, received signals can be written as\r\n\\begin{equation}\r\n\\label{eq36}\r\n\\begin{array}{l}\r\n{{\\bf{y}}_{1,k}}[10] = {\\bf{H}}_1^{[1,k]}[10]\\sum\\limits_{t = 4}^6 {{{\\bf{V}}_1}[t]\\boldsymbol{L}_1^{2,1}[t]\\left( {{c_{2t - 7}},{c_{2t - 6}}} \\right)} \\\\\r\n + {\\bf{H}}_1^{[1,k]}[10]\\sum\\limits_{t = 7}^9 {{{\\bf{V}}_1}[t]{\\boldsymbol{L}}_1^{3,1}[t]\\left( {{e_{2t - 13}},{e_{2t - 12}}} \\right)} \\\\\r\n + {\\bf{H}}_2^{[1,k]}[10]\\sum\\limits_{t = 1}^3 {{{\\bf{V}}_2}[t]{\\boldsymbol{L}}_2^{1,1}[t]\\left( {{a_{2t - 1}},{a_{2t}}} \\right)} \\\\\r\n + {\\bf{H}}_2^{[1,k]}[10]\\sum\\limits_{t = 7}^9 {{{\\bf{V}}_2}[t]{\\boldsymbol{L}}_2^{3,1}[t]\\left( {{e_{2t - 13}},{e_{2t - 12}}} \\right)} \\\\\r\n + {\\bf{H}}_3^{[1,k]}[10]\\sum\\limits_{t = 1}^3 {{{\\bf{V}}_3}[t]{\\boldsymbol{L}}_3^{1,1}[t]\\left( {{a_{2t - 1}},{a_{2t}}} \\right)} \\\\\r\n + {\\bf{H}}_3^{[1,k]}[10]\\sum\\limits_{t = 4}^6 {{{\\bf{V}}_3}[t]{\\boldsymbol{L}}_3^{2,1}[t]\\left( {{c_{2t - 7}},{c_{2t - 6}}} \\right)}.\r\n\\end{array}\r\n\\end{equation}\r\n\r\nOwing to the existence of interference regeneration matrix, the partial interference signals can be converted to the partial interference signals known by the user $k$ in the cell 1. Thus, (36) can be simplified as\r\n\\begin{equation}\r\n\\label{eq37}\r\n\\begin{small}\r\n\\begin{array}{l}\r\n{{\\bf{y}}^{i,k}}[10]\\;{ = }\r\n\\left( {{\\bf{H}}_2^{[1,k]}[10]{ + }{\\bf{H}}_3^{[1,k]}[10]} \\right) \\sum\\limits_{t = 1}^3 {{{\\bf{V}}_2}[t]{\\boldsymbol{L}}_2^{1,1}[t]\\left( {{a_{2t - 1}},{a_{2t}}} \\right)} {}\\\\\r\n{ + }\\left( {{\\bf{H}}_1^{[1,k]}[10]{ + }{\\bf{H}}_3^{[1,k]}[10]} \\right) \\sum\\limits_{t = 4}^6 {{{\\bf{V}}_1}[t]{\\boldsymbol{L}}_1^{2,1}[t]\\left( {{{c}_{2t - 7}},{c_{2t{ - 6}}}} \\right)} \\\\\r\n{ + }\\left( {{\\bf{H}}_1^{[1,k]}[10]{ + }{\\bf{H}}_2^{[1,k]}[10]} \\right) \\sum\\limits_{t = 7}^9 {{{\\bf{V}}_1}[t]{\\boldsymbol{L}}_1^{3,1}[t]\\left( {{e_{2t - 13}},{e_{2t{ - 12}}}} \\right)}.\r\n\\end{array}\r\n\\end{small}\r\n\\end{equation}\r\nFor (37), we know that \r\n\\begin{small}$({\\bf{H}}_2^{[1,k]}[10]{ + }{\\bf{H}}_3^{[1,k]}[10])\\sum\\limits_{t = 1}^3 {{{\\bf{V}}_2}[t]{\\boldsymbol{L}}_2^{1,1}[t]} \\\\({a_{2t - 1}},{a_{2t}})$\\end{small} is the partial desired signals and the rest is the known partial interference signals. Therefore, for the user 1 in the target cell 1, the whole process can be summarized as the following equation (38) \r\n\\newcounter{mytempeqncnt3}\r\n\\begin{figure*}[!t]\r\n\\normalsize\r\n\\setcounter{mytempeqncnt3}{\\value{equation}}\r\n\\setcounter{equation}{37}\r\n\\begin{equation}\r\n\\label{eq38_1}\r\n\\hspace{-36.2em}\r\n\\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\boldsymbol{y}}_{1,1}}[1]},\r\n{{{\\boldsymbol{y}}_{1,1}}[2]},\r\n{{{\\boldsymbol{y}}_{1,1}}[3]},\r\n{{{\\boldsymbol{y}}_{1,1}}[10]}\r\n\\end{array}} \\right]^T\r\n{\\rm{ = }}\\nonumber\r\n\\end{equation}\r\n\\begin{small}\r\n\\begin{equation}\r\n\\label{eq38_2}\r\n\\hspace{-2mm}\r\n\\setlength{\\arraycolsep}{0.5pt}\r\n\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{1,1}[1]}&{h_{1,2}^{1,1}[1]}&{h_{1,3}^{1,1}[1]}&{h_{1,4}^{1,1}[1]}& \\cdots &{}&{}&{}&{}\\\\\r\n{h_{1,1}^{1,2}[1]}&{h_{1,2}^{1,2}[1]}&{h_{1,3}^{1,2}[1]}&{h_{1,4}^{1,2}[1]}& \\cdots &{}&{}&{}&{}\\\\\r\n{h_{1,1}^{1,3}[1]}&{h_{1,2}^{1,3}[1]}&{h_{1,3}^{1,3}[1]}&{h_{1,4}^{1,3}[1]}& \\cdots &{}&{}&{}&{}\\\\\r\n \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\r\n{}&{}&{}&{}& \\cdots &{h_{1,1}^{1,1}[3]}&{h_{1,2}^{1,1}[3]}&{h_{1,3}^{1,1}[3]}&{h_{1,4}^{1,1}[3]}\\\\\r\n{}&{}&{}&{}& \\cdots &{h_{1,1}^{1,2}[3]}&{h_{1,2}^{1,2}[3]}&{h_{1,3}^{1,2}[3]}&{h_{1,4}^{1,2}[3]}\\\\\r\n{}&{}&{}&{}& \\cdots &{h_{1,1}^{1,3}[3]}&{h_{1,2}^{1,3}[3]}&{h_{1,3}^{1,3}[3]}&{h_{1,4}^{1,3}[3]}\\\\\r\n{\\sum\\limits_{j = 2,3} {h_{j,1}^{1,1}[10]} {\\boldsymbol\\beta _1}}&{\\sum\\limits_{j = 2,3} {h_{j,2}^{1,1}[10]} {\\boldsymbol\\beta _1}}&{\\sum\\limits_{j = 2,3} {h_{j,3}^{1,1}[10]} {\\boldsymbol\\beta _1}}&{\\sum\\limits_{j = 2,3} {h_{j,4}^{1,1}[10]} {\\boldsymbol\\beta _1}}& \\cdots &{\\sum\\limits_{j = 2,3} {h_{j,1}^{1,1}[10]} {\\boldsymbol\\beta _3}}&{\\sum\\limits_{j = 2,3} {h_{j,2}^{1,1}[10]} {\\boldsymbol\\beta _3}}&{\\sum\\limits_{j = 2,3} {h_{j,3}^{1,1}[10]} {\\boldsymbol\\beta _3}}&{\\sum\\limits_{j = 2,3} {h_{j,4}^{1,1}[10]} {\\boldsymbol\\beta _3}}\\\\\r\n{\\sum\\limits_{j = 2,3} {h_{j,1}^{1,2}[10]} {\\boldsymbol\\beta _1}}&{\\sum\\limits_{j = 2,3} {h_{j,2}^{1,2}[10]} {\\boldsymbol\\beta _1}}&{\\sum\\limits_{j = 2,3} {h_{j,3}^{1,2}[10]} {\\boldsymbol\\beta _1}}&{\\sum\\limits_{j = 2,3} {h_{j,4}^{1,2}[10]} {\\boldsymbol\\beta _1}}& \\cdots &{\\sum\\limits_{j = 2,3} {h_{j,1}^{1,2}[10]} {\\boldsymbol\\beta _3}}&{\\sum\\limits_{j = 2,3} {h_{j,2}^{1,2}[10]} {\\boldsymbol\\beta _3}}&{\\sum\\limits_{j = 2,3} {h_{j,3}^{1,2}[10]} {\\boldsymbol\\beta _3}}&{\\sum\\limits_{j = 2,3} {h_{j,4}^{1,2}[10]} {\\boldsymbol\\beta _3}}\\\\\r\n{\\sum\\limits_{j = 2,3} {h_{j,1}^{1,3}[10]} {\\boldsymbol\\beta _1}}&{\\sum\\limits_{j = 2,3} {h_{j,2}^{1,3}[10]} {\\boldsymbol\\beta _1}}&{\\sum\\limits_{j = 2,3} {h_{j,3}^{1,3}[10]} {\\boldsymbol\\beta _1}}&{\\sum\\limits_{j = 2,3} {h_{j,4}^{1,3}[10]} {\\boldsymbol\\beta _1}}& \\cdots &{\\sum\\limits_{j = 2,3} {h_{j,1}^{1,3}[10]} {\\boldsymbol\\beta _3}}&{\\sum\\limits_{j = 2,3} {h_{j,2}^{1,3}[10]} {\\boldsymbol\\beta _3}}&{\\sum\\limits_{j = 2,3} {h_{j,3}^{1,3}[10]} {\\boldsymbol\\beta _3}}&{\\sum\\limits_{j = 2,3} {h_{j,4}^{1,3}[10]} {\\boldsymbol\\beta _3}}\r\n\\end{array}} \\right]\r\n\\!\\!\\!\\!  \\left[ {\\begin{array}{*{20}{c}}\r\n{{a_1}}\\\\\r\n{{a_2}}\\\\\r\n{{b_1}}\\\\\r\n{{b_2}}\\\\\r\n \\vdots \\\\\r\n{{a_5}}\\\\\r\n{{a_6}}\\\\\r\n{{b_5}}\\\\\r\n{{b_6}}\r\n\\end{array}} \\right]\r\n\\end{equation}\r\n\\end{small}\r\n\\hrulefill\r\n\\vspace*{4pt}\r\n\\end{figure*}\r\nwhere ${{\\boldsymbol{\\beta}} _t}{ = }{v_2}[t]{u_2}[t]h_1^{2,1}[t] + {v_3}[t]{u_3}[t]h_1^{3,1}[t]$. It is obvious that the channel matrix is a $12 \\times 12$ non-singular matrix. Herein, user 1 in the target cell 1 can decode its messages with delayed CSI.\r\n\r\nFrom the above illustration, 36 independent symbols are transmitted by three base stations over 10 coherent slots. Therefore, the DoF of the proposed RIR scheme is 3.6. While under the same conditions, i.e., $(L,M,K,N) = (3,4,2,3)$, the DoF for the TDMA scheme and RIA scheme are only 3 and 3.28, respectively.\r\n\r\n\\section{Beamforming Based Distributed Retrospective Interference Alignment Scheme}\r\nFor the proposed RIR scheme, though it can achieve greater DoF than the RIA scheme, the scheme has the problem of performance degradation as the transceiver antennas ratio $\\rho$ approaches 1. To handle this issue, a new beamforming based distributed retrospective interference alignment (B-DRIA) scheme is proposed. The core idea of this scheme is that the beamforming technique is adopted to eliminate ICI and then, several additional slots are taken for the base stations to simultaneously transmit desired signals. Finally, we design distributed retrospective interference matrices to align the IUI at the user side and decode the messages separately. In the following, we firstly illustrate the details of the proposed scheme which includes three phases, i.e., CSIT acquisition process, cellular beamforming and signal transmission, and distributed retrospective interference alignment. Then, the DoF of the B-DRIA scheme is analyzed. To make it understandable, we also present an example of the scheme at last.\r\n\r\n\\subsection{CSIT Acquisition Process}\r\nThe first phase takes $L$ slots and in which the base stations take turns to send messages to its served users. Since $M > N$, these desired messages cannot be decoded within only one slot and additional transmissions are required. Therefore, the users do not complete decoding in the current slot but feedback the estimated CSI to the base stations. The CSI  contains both the channel gain estimation and the DoA estimation where the former part is used for retrospective interference alignment and the latter part is taken in cellular  beamforming. Taking the cell 1 for example, the process of the first phase is illustrated as Fig. 7.\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig7}\r\n\\caption{CSIT Acquisition process in 1st phase.}\r\n\\label{Fig7}\r\n\\end{figure}\r\nTo make it universal applicable, assume that in the slot $i \\in {\\cal L}$, the base station $i \\in {\\cal L}$ transmits messages as\r\n\\begin{equation}\r\n\\label{eq39}\r\n{\\boldsymbol{S}}_{i}^{}[i] = \\left[ {{s_{i,1}}[i], \\cdots {s_{i,\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil }}[i], \\cdots ,{s_{i,K\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil }}[i]} \\right].\r\n\\end{equation}\r\nThen for user $k$ in the target cell $j$, its received signals can be expressed in time domain and angular domain as follows\r\n\\begin{align}\r\n\\label{eq40-41}\r\n&{\\boldsymbol{y}}_{j,k}^{}[i] = {\\bf{H}}_i^{j,k}[t]{{\\boldsymbol{S}}_i}[t],i \\in {\\cal L},j = i,\\\\\r\n&{\\boldsymbol{y}}_{j,k}^{}[t] = {\\bf{A}}_i^{j,k}[t]{{\\boldsymbol{S}}_i}[t],i \\in {\\cal L},j = i.\r\n\\end{align} \r\nBased on the received signals, the LS algorithm \\cite{2004Discussion} and the multiple signal classification algorithm algorithm (MUSIC) \\cite{6422415} can be used at the user side to estimate the channel gain and DoA, respectively. With channel estimation, the feedback information from user $k$ is extended into two parts, i.e., the channel gain information and the angel information, and its form can be characterized by\r\n\\begin{equation}\r\n\\label{eq42}\r\nCSIT{ \\!=\\! }\\left\\{ {\\left( {{\\bf{H}}_i^{i,1}[i]\\!,\\!{\\theta _1}[i]} \\right)\\!,\\! \\!\\cdots\\! \\!,\\!\\left( {{\\bf{H}}_i^{i,k}[i]\\!,\\!{\\theta _k}[i]} \\right)} \\right\\}\\!,\\!i \\in {\\cal L},\r\n\\end{equation}\r\nwhere $\\left( {{\\bf{H}}_i^{i,k}[i],{\\theta _k}[i]} \\right)$ denotes the channel matrix from base station $i$ to user $k$ in the target cell $i$ and the corresponding DoA of that user.\r\n\r\n\\subsection{Cellular Beamforming and Signal Transmission}\r\nFollowing the CSIT acquisition process, we can further perform the process of cellular beamforming and signal transmission. While this phase spans $\\bar \\varphi  = \\left\\lfloor {{{2N - M} \\mathord{\\left/{\\vphantom {{2N - M} {M - N}}} \\right.\\kern-\\nulldelimiterspace} {M - N}}} \\right\\rfloor $ slots, the base stations simultaneously transmit independent messages to their served users. However, the desired signals received at the user sides are contaminated by both ICI and IUI. Therefore, additional operations are needed to eliminate  interference signals.\r\n\r\nIn order to cancel the ICI, beamforming is adopted at the base stations. Define the slots set of this phase as $\\Psi { = }\\left\\{ {L + 1,\\cdots ,L{ + }\\bar \\varphi } \\right\\}$ and the beamforming matrices are\r\n\\begin{align}\r\n\\label{eq43-44}\r\n&{\\bf{W}}_i^{j,k}[t]{ = }{{\\bf{0}}_{M \\times M}},j \\in {\\cal L}\\backslash i,k \\in {\\textbf{K}},t \\in \\Psi,\\\\\r\n&{\\bf{W}}_i^{j,k}[t]{ = }{{\\bf{I}}_{M \\times M}},i = j,k \\in {\\textbf{K}},t \\in \\Psi.\r\n\\end{align} \r\nwhere ${{\\bf{0}}_{M \\times M}}$ is a zero matrix and ${{\\bf{I}}_{M \\times M}}$ is an $M$-dimensional identity matrix. Then, for one slot $t,\\forall t \\in \\Psi $, the signals sent by base station $i$ to user $k$ in the target cell $j$ is \r\n\\begin{equation}\r\n\\begin{array}{l}\r\n\\label{eq45}\r\n{\\boldsymbol{X}}_i^{j,k}[t] = {\\bf{W}}_i^{j,k}[t]{\\boldsymbol{S}}_{i}^{}[t]\\\\\r\n\\qquad\\quad\\, = {{\\bf{I}}_{M \\times M}}{\\left[ {{s_{i,1}}[t], \\cdots ,{s_{i,M}}[t]} \\right]^T}\\\\\r\n\\qquad\\quad\\, = {\\left[ {{s_{i,1}}[t], \\cdots ,{s_{i,M}}[t]} \\right]^T}.\r\n\\end{array}\r\n\\end{equation}\r\nAfter performing the beamforming with ${\\bf{W}}_i^{j,k}$, the ICI is eliminated which means that all the users will not receive signals from the base stations of the interference cells. This process is shown in Fig. 8.\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig8}\r\n\\caption{Signals transmission with beamforming in 2nd phase.}\r\n\\label{Fig8}\r\n\\end{figure}\r\nThus, the received signals of the user $k$ in the cell $j$ can be expressed as \r\n\\begin{equation}\r\n\\label{eq46}\r\n{\\boldsymbol{y}}_{j,k}^{}[t] = {\\bf{H}}_i^{i,k}[t]{\\boldsymbol{X}}_i^{j,k}[t].\r\n\\end{equation}\r\nDue to the presence of the IUI, no users can decode the received message in one slot. By analyzing the components of the received signals, we find that, for user $k$, the IUI contained desired information for the other users, i.e., the received signals of the user $k$ can be written as\r\n\\begin{equation}\r\n\\begin{array}{l} \r\n\\label{eq47}\r\n{\\boldsymbol{y}}_{j,k}^{}[t] = {\\bf{H}}_i^{i,k}[t]{{\\boldsymbol{S}}_i}[t] + {\\bf{H}}_i^{i, - k}[t]{{\\boldsymbol{S}}_i}[t]\\\\\r\n\\qquad\\quad\\!= \\boldsymbol{L}\\left( {{s_{i,(k - 1)\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil  + 1}}[L + t], \\cdots ,{s_{i,k\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil }}[L + t]} \\right)\\\\\r\n\\qquad\\quad\\! + \\boldsymbol{I} \\left( {{s_{i,(k - 1)\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil  + 1}}[L + t], \\cdots ,{s_{i,k\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil }}[L + t]} \\right)\\\\\r\n\\qquad\\quad\\!= \\boldsymbol{L}_i^{i,k}[t]{ + }\\boldsymbol{I}_i^{i,k}[t], - k = {\\textbf{K}}\\backslash k,\r\n\\end{array}\r\n\\end{equation}\r\nwhere $\\boldsymbol{L}_i^{i,k}[t]$ represents the desired signals and $ \\boldsymbol{I}_i^{i,k}[t]$ represents IUI. Therefore, through $L{ + }\\bar \\varphi$ slots, there are $\\bar \\varphi M + M$ messages needed to be decoded for each user and we only have $\\bar \\varphi N + N$ equations. In order to decode these messages, some extra information is desired, i.e., to formulate $N$ additional equations of each user.\r\n\r\n\\subsection{Distributed Retrospective Interference Alignment}\r\nBased on the first phase and the second phase, especially the CSI estimation and feedback, each base station can obtain the knowledge of the CSI and the IUI feedback, i.e., $ \\boldsymbol{I}_i^{i,k}[t]$. Therefore, the distributed retrospective interference alignment is developed to make use of the combination of the IUI from the previous slots. Part of which can be extracted by the users to supplement $N$ additional equations. In specific, the designed  vector from the base station in the cell $i$ is shown as follows\r\n\\begin{equation}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\label{eq48}\r\n{{\\boldsymbol{S}}_i}[L\\! + \\!\\bar \\varphi {\\! +\\! }1]\r\n{ = }\\left[ {\\begin{array}{*{20}{c}}\r\n{\\sum\\limits_{k = 1}^K {\\left( { I_i^{i,k}[i]\\! + \\! I_i^{i,k}[L\\! + \\!1] \\!+ \\! \\cdots \\! + \\! I_i^{i,k}[L\\! + \\!\\bar \\varphi ]} \\right)} }\\\\\r\n0\\\\\r\n \\vdots \\\\\r\n0\r\n\\end{array}} \\right],\r\n\\end{equation}\r\nwhere ${\\sum\\limits_{k = 1}^K {\\left( { \\boldsymbol{I}_i^{i,k}[i]\\! + \\! \\boldsymbol{I}_i^{i,k}[L\\! + \\!1] \\!+ \\! \\cdots \\! + \\! \\boldsymbol{I}_i^{i,k}[L\\! + \\!\\bar \\varphi ]} \\right)} } $ is a $N$-dimensional vector composed of the whole past slots\u2019 IUI. By the pre-mentioned beamforming process and the distributed interference alignment scheme, the base stations are capable of transmitting the vector ${{\\boldsymbol{S}}_i}[L\\! + \\!\\bar \\varphi {\\! +\\! }1]$ without introducing new ICI or IUI, as shown in Fig. 9.\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig9}\r\n\\caption{Regenerated interference retransmission in 3rd phase.}\r\n\\label{Fig9}\r\n\\end{figure}\r\nThen, the received signals of the user $k$ in the cell $j$ can be expressed as\r\n\\begin{equation}\r\n\\label{eq49}\r\n\\begin{array}{l}\r\n{\\boldsymbol{y}}_{j,k}^{}[L + \\bar \\varphi { + }1]\\\\\r\n= {\\bf{H}}_i^{j,k}[L + \\bar \\varphi { + }1]{\\bf{W}}_i^{j,k}[L + \\bar \\varphi { + }1]{{\\boldsymbol{S}}_i}[L + \\bar \\varphi { + }1].\r\n\\end{array}\r\n\\end{equation}\r\nFrom (49), the desired signals can be extracted from the combination of the IUI by the interference elimination process\r\n\\begin{equation}\r\n\\begin{array}{l}\r\n\\label{eq50}\r\n{\\boldsymbol{y}}_{j,k}^{}[L + \\bar \\varphi { + }1] - {\\bf{H}}_i^{j,k}[L + \\bar \\varphi { + }1]\\sum\\limits_{t = 1}^{L + \\bar \\varphi } { \\boldsymbol{I}_i^{i, - k}[t]}\\\\\r\n= {\\bf{H}}_i^{j,k}[L + \\bar \\varphi { + }1]( \\boldsymbol{I}_i^{i,k}[i] + \\sum\\limits_{t = L + 1}^{L + \\bar \\varphi } { \\boldsymbol{I}_i^{i,k}[t]} ),\r\n\\end{array}\r\n\\end{equation}\r\nwhere $\\boldsymbol{I}_i^{i,k}[i] + \\sum\\limits_{t = L + 1}^{L + \\bar \\varphi } { \\boldsymbol{I}_i^{i,k}[t]}$ is the supplementary of the $N$ additional equations. Herein, the whole process of the proposed B-DRIA scheme can be summarized in (51), \r\n\\newcounter{mytempeqncnt4}\r\n\\begin{figure*}[!t]\r\n\\normalsize\r\n\\setcounter{mytempeqncnt4}{\\value{equation}}\r\n\\setcounter{equation}{50} \r\n\\begin{equation}\r\n\\hspace{-2mm}\r\n\\label{eq50}\r\n\\underbrace {\\left[ {\\begin{array}{*{20}{c}}\r\n{{\\bf{y}}_{j,k}^{}[i]}\\\\\r\n{{\\bf{y}}_{j,k}^{}[L + 1]}\\\\\r\n \\vdots \\\\\r\n{{\\bf{y}}_{j,k}^{}[L + \\bar \\varphi ]}\\\\\r\n{{\\bf{y}}_{j,k}^{}[L + \\bar \\varphi  + 1]}\r\n\\end{array}} \\right]}_{\\bf{Y}}{ = }\\underbrace {\\left[ {\\begin{array}{*{20}{c}}\r\n{{\\bf{H}}_i^{i,k}[i]}&0& \\cdots &0\\\\\r\n0&{{\\bf{H}}_i^{i,k}[L + 1]}& \\cdots & \\vdots \\\\\r\n \\vdots & \\ddots & \\ddots &0\\\\\r\n0& \\cdots &0&{{\\bf{H}}_i^{i,k}[L + \\bar \\varphi ]}\\\\\r\n{{\\bf{H}}_i^{i,k}[L + \\bar \\varphi { + }1]{{\\bf{\\beta }}_i}}&{{\\bf{H}}_i^{i,k}[L + \\bar \\varphi { + }1]{{\\bf{\\beta }}_{L + 1}}}& \\cdots &{{\\bf{H}}_i^{i,k}[L + \\bar \\varphi { + }1]{{\\bf{\\beta }}_{L + \\bar \\varphi }}}\r\n\\end{array}} \\right]}_{\\bf{H}}\\underbrace {\\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\bf{S}}_i}[i]}\\\\\r\n{{{\\bf{S}}_i}[L + 1]}\\\\\r\n \\vdots \\\\\r\n{{{\\bf{S}}_i}[L + \\bar \\varphi ]}\r\n\\end{array}} \\right]}_{\\bf{S}}\r\n\\end{equation}\r\n\\hrulefill\r\n\\vspace*{4pt}\r\n\\end{figure*}\r\nwhere ${{\\boldsymbol{\\beta}} _t}$ is the precoding matrix defined as follows\r\n\\begin{equation}\r\n\\label{eq52}\r\n{{\\boldsymbol{\\beta}} _t} = {\\bf{W}}_i^{j,k}[t]\\sum\\nolimits_{k ={\\textbf{K}} \\backslash k} {{\\bf{H}}_i^{i,k}[t]}.\r\n\\end{equation}\r\nSince the delayed CSIT is adopted in the transmission process, which means that, for any two different slots, the time span exceeds the coherence time, the channels are independent with each other over different slots, e.g., ${\\bf{H}}_i^{i,k}[L + 1]$ and ${\\bf{H}}_i^{i,k}[L + \\bar \\varphi ]$. In addition, in each slot, the channel matrix is non-singular, which means that each submatrix has a full rank. Herein, the matrix $\\bf{H}$ is a $\\left\\lfloor {{N \\mathord{\\left/\r\n {\\vphantom {N {M - N}}} \\right.\\kern-\\nulldelimiterspace} {M - N}}} \\right\\rfloor M$ dimensional non-singular matrix, and the LS method \\cite{2004Discussion} can be used to decode the desired messages.\r\n\r\n\\subsection{Degrees of Freedom}\r\nFollowing the above illustration, we analyze the DoF of the proposed B-DRIA scheme. Since the base stations spend $L{ + }\\bar \\varphi { + }1$  incoherent slots and transmit $L\\left\\lceil {{M \\mathord{\\left/ {\\vphantom {M K}} \\right.\\kern-\\nulldelimiterspace} K}} \\right\\rceil K(\\bar \\varphi { + }1)$  independent symbols, we assume that the transmission is reliable, i.e., the value of signal-to-noise ratio (SNR) is approximated to infinity. Hence the DoF of the B-DRIA scheme can be calculated as\r\n\\begin{equation}\r\n\\label{eq53}\r\nDoF{ = }{{L\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil K(\\bar \\varphi { + }1)} \\mathord{\\left/\r\n {\\vphantom {{L\\left\\lceil {{M \\mathord{\\left/\r\n {\\vphantom {M K}} \\right.\r\n \\kern-\\nulldelimiterspace} K}} \\right\\rceil K(\\bar \\varphi { + }1)} {(L{ + }\\bar \\varphi { + }1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(L{ + }\\bar \\varphi { + }1)}}.\r\n\\end{equation}\r\nTo facilitate the analysis, we assume that $M$ is divisible by $K$. Then, the above equation can be simplified as\r\n\\begin{equation}\r\n\\label{eq54}\r\n\\begin{array}{l}\r\nDoF{{{ = }LM} \\mathord{\\left/\r\n {\\vphantom {{{ = }LM} {(1 + {L \\mathord{\\left/\r\n {\\vphantom {L {(\\bar \\varphi { + }1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\bar \\varphi { + }1)}})}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(1 + {L \\mathord{\\left/\r\n {\\vphantom {L {(\\bar \\varphi { + }1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\bar \\varphi { + }1)}})}}.\r\n\\end{array}\r\n\\end{equation}\r\nAs mentioned earlier, the number of cells $L$ is constant and the signal transmission slots in the second phase $\\bar \\varphi$ is a function of $M$ and $N$. Hence, the value of DoF mainly depends on transceiver $\\rho$ and $M$, and the DoF is rewritten as\r\n\\begin{equation}\r\n\\label{eq55}\r\nDoF{{{ = }LM} \\mathord{\\left/\r\n {\\vphantom {{{ = }LM} {(1 + {L \\mathord{\\left/\r\n {\\vphantom {L {\\left\\lfloor {{3 \\mathord{\\left/\r\n {\\vphantom {3 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }}} \\right.\r\n \\kern-\\nulldelimiterspace} {\\left\\lfloor {{3 \\mathord{\\left/\r\n {\\vphantom {3 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }})}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(1 + {L \\mathord{\\left/\r\n {\\vphantom {L {\\left\\lfloor {{3 \\mathord{\\left/\r\n {\\vphantom {3 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }}} \\right.\r\n \\kern-\\nulldelimiterspace} {\\left\\lfloor {{3 \\mathord{\\left/\r\n {\\vphantom {3 {(\\rho  - 1)}}} \\right.\r\n \\kern-\\nulldelimiterspace} {(\\rho  - 1)}}} \\right\\rfloor }})}}.\r\n\\end{equation}\r\nWith the increase of $\\rho$, the valued of the DoF can be separately discussed under two cases/conditions. That is, when $M$ is not divisible by $ M -N $, the value of $\\bar \\varphi = \\left\\lfloor {{{2N - M} \\mathord{\\left/{\\vphantom {{2N - M} {M - N}}} \\right.\\kern-\\nulldelimiterspace} {M - N}}} \\right\\rfloor$ will not change owing to the round down operation $\\left\\lfloor {*} \\right\\rfloor$ and the DoF keeps constant. While if $M$ is divisible by $ M -N $, the value of the DoF will grow sharply. Therefore, the value of $\\bar \\varphi $ is divided into three cases, i.e., $\\bar \\varphi > 1$, $\\bar \\varphi =1$, $\\bar \\varphi =0$, and the corresponding DoF gains of each situation are further discussed in Section V.\r\n\r\n\\subsection{Typical Application}\r\nFor the comparison purpose, the condition used herein is the same as that adopted for the RIR scheme, that is, $(L,M,K,N) = (3,4,2,3)$, which means that the network is composed of 3 cells and in each cell, the base station with 4 antennas provides services to 2 users, where each user has 3 antennas. For this scenario, the B-DRIA scheme is adopted and the whole process is illustrated as follows.\r\n\r\nThe \ufb01rst phase takes 3 slots, in which the base stations take turns to send messages. In each slot, the symbols sent by the base stations are shown as follows\r\n\\begin{equation}\r\n\\label{eq56}\r\n\\begin{array}{l}\r\n{{\\boldsymbol{S}}_1}[1] = \\left( {\\begin{array}{*{20}{c}}\r\n{{a_1}},\r\n{{a_2}},\r\n{{b_1}},\r\n{{b_2}}\r\n\\end{array}} \\right)^T,\\\\\r\n{{\\boldsymbol{S}}_2}[2] = \\left( {\\begin{array}{*{20}{c}}\r\n{{c_1}},\r\n{{c_2}},\r\n{{d_1}},\r\n{{d_2}}\r\n\\end{array}} \\right)^T,\\\\\r\n{{\\boldsymbol{S}}_3}[3] = \\left( {\\begin{array}{*{20}{c}}\r\n{{e_1}},\r\n{{e_2}},\r\n{{f_1}},\r\n{{f_2}}\r\n\\end{array}} \\right)^T.\r\n\\end{array}\r\n\\end{equation}\r\nTake the user $k$ in the target cell 1 for example, in the slot 1, the received signals can be expressed as\r\n\\begin{equation}\r\n\\label{eq57}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\begin{array}{l}\r\n{\\boldsymbol{y}}_{1,k}^{}[1]\\! =\\!  {\\bf{H}}_1^{1,k}[1]{{\\boldsymbol{S}}_1}[1]\\\\  \r\n\\qquad\\quad\\!\\! =\\!  \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{k,1}[1]}&{h_{1,2}^{k,1}[1]}&{h_{1,3}^{k,1}[1]}&{h_{1,4}^{k,1}[1]}\\\\\r\n{h_{1,1}^{k,2}[1]}&{h_{1,2}^{k,2}[1]}&{h_{1,3}^{k,2}[1]}&{h_{1,4}^{k,2}[1]}\\\\\r\n{h_{1,1}^{k,3}[1]}&{h_{1,2}^{k,3}[1]}&{h_{1,3}^{k,3}[1]}&{h_{1,4}^{k,3}[1]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_1}}\\\\\r\n{{a_2}}\\\\\r\n{{b_1}}\\\\\r\n{{b_2}}\r\n\\end{array}} \\right]\r\n\\end{array},\r\n\\end{equation}\r\n\\begin{equation}\r\n\\label{eq58}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\begin{array}{l}\r\n{\\boldsymbol{y}}_{1,k}^{}[1]\\!  = \\! {\\bf{A}}_1^{1,k}[1]{{\\boldsymbol{S}}_1}[1]\\\\\r\n\\qquad\\quad\\!\\! = \\! \\left[ {\\begin{array}{*{20}{c}}\r\n1&1&1&1\\\\\r\n{{e^{ - j{\\mu _1}}}}&{{e^{ - j{\\mu _2}}}}&{{e^{ - j{\\mu _3}}}}&{{e^{ - j{\\mu _4}}}}\\\\\r\n{{e^{ - j2{\\mu _1}}}}&{{e^{ - j2{\\mu _2}}}}&{{e^{ - j2{\\mu _3}}}}&{{e^{ - j2{\\mu _4}}}}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_1}}\\\\\r\n{{a_2}}\\\\\r\n{{b_1}}\\\\\r\n{{b_2}}\r\n\\end{array}} \\right]\r\n\\end{array}.\r\n\\end{equation}\r\nThe former is the time domain expression where the channel gain matrix is estimated by the LS algorithm\\cite{2004Discussion}, and the latter is the angular domain expression where the DoA matrix is obtained by the MUSIC algorithm \\cite{6422415}. Therefore, the feedback estimation contains two parts, i.e., the channel gain information ${\\bf{H}}_1^{1,k}[1]$ and the angel information ${\\theta _1}[1]$. Since $M > N$, user $k$ cannot decode the messages ${{\\bf{S}}_1}[1]$ within one slot and meanwhile, the estimated CSI is fed back to the base station 1. Similarly, the user $k$ of cell 2 receive the signals in slot 2 and the user $k$ of cell 3 receive signals in slot 3 as\r\n\\begin{equation}\r\n\\label{eq59}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\begin{array}{l}\r\n{\\bf{y}}_{2,k}^{}[2] = {\\bf{H}}_2^{2,k}[2]{{\\boldsymbol{S}}_2}[2]\\\\\r\n \\;\\;\\;\\;\\;\\;\\;\\;= \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{2,1}^{k,1}[2]}&{h_{2,2}^{k,1}[2]}&{h_{2,3}^{k,1}[2]}&{h_{2,4}^{k,1}[2]}\\\\\r\n{h_{2,1}^{k,2}[2]}&{h_{2,2}^{k,2}[2]}&{h_{2,3}^{k,2}[2]}&{h_{2,4}^{k,2}[2]}\\\\\r\n{h_{2,1}^{k,3}[2]}&{h_{2,2}^{k,3}[2]}&{h_{2,3}^{k,3}[2]}&{h_{2,4}^{k,3}[2]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{c_1}}\\\\\r\n{{c_2}}\\\\\r\n{{d_1}}\\\\\r\n{{d_2}}\r\n\\end{array}} \\right]\r\n\\end{array},\r\n\\end{equation}\r\n\\vspace{-0.5em}\r\n\\begin{equation}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\label{eq60}\r\n\\begin{array}{l}\r\n{\\bf{y}}_{3,k}^{}[t] = {\\bf{H}}_3^{3,k}[t]{{\\boldsymbol{S}}_3}[t]\\\\\r\n\\;\\;\\;\\;\\;\\;\\;\\;{   = }\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{3,1}^{k,1}[3]}&{h_{3,2}^{k,1}[3]}&{h_{3,3}^{k,1}[3]}&{h_{3,4}^{k,1}[3]}\\\\\r\n{h_{3,1}^{k,2}[3]}&{h_{3,2}^{k,2}[3]}&{h_{3,3}^{k,2}[3]}&{h_{3,4}^{k,2}[3]}\\\\\r\n{h_{3,1}^{k,3}[3]}&{h_{3,2}^{k,3}[3]}&{h_{3,3}^{k,3}[3]}&{h_{3,4}^{k,3}[3]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{e_1}}\\\\\r\n{{e_2}}\\\\\r\n{{f_1}}\\\\\r\n{{f_2}}\r\n\\end{array}} \\right],\r\n\\end{array}\r\n\\end{equation}\r\nBy channel estimation, both channel gain information ${\\bf{H}}_2^{2,k}[2]$ and angel information ${\\theta _2}[2]$ are obtained and fed back to the base station 2, and it is the same for ${\\bf{H}}_3^{3,k}[3]$ and ${\\theta _3}[3]$.\r\n\r\nIn the second phase, after obtaining CSI, the beamforming is used by the base station to eliminate the ICI. Taking base station 1  for example,  beamforming matrices are written as\r\n\\begin{equation}\r\n\\label{eq61}\r\n\\begin{array}{l}\r\n{\\bf{W}}_1^{j,k}[t]{ = }\\left[ {\\begin{array}{*{20}{c}}\r\n0&0&0&0\\\\\r\n0&0&0&0\\\\\r\n0&0&0&0\\\\\r\n0&0&0&0\r\n\\end{array}} \\right]\\\\\r\n{ = }{{\\sl\\bf{0}}_{4 \\times 4}},j \\in {\\cal L}\\backslash 1,t \\in \\Psi ,\r\n\\end{array}\r\n\\end{equation}\r\n\\begin{equation}\r\n\\label{eq62}\r\n\\begin{array}{l}\r\n{\\bf{W}}_1^{1,k}[t]{ = }\\left[ {\\begin{array}{*{20}{c}}\r\n1&0&0&0\\\\\r\n0&1&0&0\\\\\r\n0&0&1&0\\\\\r\n0&0&0&1\r\n\\end{array}} \\right]\\\\\r\n{ = }{{\\bf{I}}_{4 \\times 4}},t \\in \\Psi ,\r\n\\end{array}\r\n\\end{equation}\r\nwhere $\\Psi$ denotes the slots set of the second phase. For base station 1, the transmitted signals are changed to\r\n\\begin{equation}\r\n\\label{eq63}\r\n\\begin{array}{l}\r\n{{\\bf{X}}_1}[t] = {{\\bf{W}}_1}[t]{{\\boldsymbol{S}}_1}[t]\\\\\r\n = {\\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\bf{I}}_{4 \\times 4}}}&{{{\\bf{0}}_{4 \\times 4}}}&{{{\\bf{0}}_{4 \\times 4}}}\r\n\\end{array}} \\right]^T}{\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_1}}&{{a_2}}&{{b_1}}&{{b_2}}\r\n\\end{array}} \\right]^T}.\r\n\\end{array}\r\n\\end{equation}\r\nAfter the process of the beamforming, the base stations are able to simultaneously transmit signals without introducing additional ICI. However, the unsolvable issue which caused by the condition $M > N$ is still exist. While from (59) and (60), it is not difficult to infer that, for the case  $M=4$ and $N =3$, only 1 ($=M - N$) additional equation is desired in each slot. In order to make full use of the time resources, 2 more slots are taken to transmit signals, i.e., $\\Psi { = }\\left\\{ {4,5} \\right\\}$. Herein, for user $k$ in cell 1, in slot 4 and slot 5, the received signals can be written as follows, respectively,\r\n\\begin{equation}\r\n\\label{eq64}\r\n\\begin{small}\r\n\\begin{array}{l}\r\n{\\bf{y}}_{1,k}^{}[4] \\\\\r\n={\\bf{H}}_1^{1,k}[4]{\\bf{W}}_1^{1,k}[4]{{\\boldsymbol{S}}_1}[4]\\\\\r\n= \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{k,1}[4]}&{h_{1,2}^{k,1}[4]}&{h_{1,3}^{k,1}[4]}&{h_{1,4}^{k,1}[4]}\\\\\r\n{h_{1,1}^{k,2}[4]}&{h_{1,2}^{k,2}[4]}&{h_{1,3}^{k,2}[4]}&{h_{1,4}^{k,2}[4]}\\\\\r\n{h_{1,1}^{k,3}[4]}&{h_{1,2}^{k,3}[4]}&{h_{1,3}^{k,3}[4]}&{h_{1,4}^{k,3}[4]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_3}}\\\\\r\n{{a_4}}\\\\\r\n{{b_3}}\\\\\r\n{{b_4}}\r\n\\end{array}} \\right]\\\\\r\n=\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{k,1}[4]}&{h_{1,2}^{k,1}[4]}\\\\\r\n{h_{1,1}^{k,2}[4]}&{h_{1,2}^{k,2}[4]}\\\\\r\n{h_{1,1}^{k,3}[4]}&{h_{1,2}^{k,3}[4]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_3}}\\\\\r\n{{a_4}}\r\n\\end{array}} \\right]\\vspace{0.5ex}\\\\\r\n+\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^{k,1}[4]}&{h_{1,4}^{k,1}[4]}\\\\\r\n{h_{1,3}^{k,2}[4]}&{h_{1,4}^{k,2}[4]}\\\\\r\n{h_{1,3}^{k,3}[4]}&{h_{1,4}^{k,3}[4]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{b_3}}\\\\\r\n{{b_4}}\r\n\\end{array}} \\right]\\\\\r\n={\\bf{L}}_1^{1,k}[4]{ + }{\\bf{I}}_1^{1,k}[4],\r\n\\end{array}\r\n\\end{small}\r\n\\end{equation}\r\n\\begin{equation}\r\n\\label{eq65}\r\n\\begin{small}\r\n\\begin{array}{l}\r\n{\\bf{y}}_{1,k}^{}[5] \\\\\r\n={\\bf{H}}_1^{1,k}[5]{\\bf{W}}_1^{1,k}[5]{{\\boldsymbol{S}}_1}[5]\\\\\r\n= \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{k,1}[5]}&{h_{1,2}^{k,1}[5]}&{h_{1,3}^{k,1}[5]}&{h_{1,4}^{k,1}[5]}\\\\\r\n{h_{1,1}^{k,2}[5]}&{h_{1,2}^{k,2}[5]}&{h_{1,3}^{k,2}[5]}&{h_{1,4}^{k,2}[5]}\\\\\r\n{h_{1,1}^{k,3}[5]}&{h_{1,2}^{k,3}[5]}&{h_{1,3}^{k,3}[5]}&{h_{1,4}^{k,3}[5]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_5}}\\\\\r\n{{a_6}}\\\\\r\n{{b_5}}\\\\\r\n{{b_6}}\\\\\r\n\\end{array}} \\right]\\\\\r\n=\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{k,1}[5]}&{h_{1,2}^{k,1}[5]}\\\\\r\n{h_{1,1}^{k,2}[5]}&{h_{1,2}^{k,2}[5]}\\\\\r\n{h_{1,1}^{k,3}[5]}&{h_{1,2}^{k,3}[5]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_5}}\\\\\r\n{{a_6}}\\\\\r\n\\end{array}} \\right] \\vspace{0.5ex} \\\\\r\n+\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^{k,1}[5]}&{h_{1,4}^{k,1}[5]}\\\\\r\n{h_{1,3}^{k,2}[5]}&{h_{1,4}^{k,2}[5]}\\\\\r\n{h_{1,3}^{k,3}[5]}&{h_{1,4}^{k,3}[5]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{b_5}}\\\\\r\n{{b_6}}\\\\\r\n\\end{array}} \\right]\\\\\r\n= {\\boldsymbol{L}}_1^{1,k}[5]{ + }{\\boldsymbol{I}}_1^{1,k}[5].\r\n\\end{array}\r\n\\end{small}\r\n\\end{equation}\r\nNote that $\\boldsymbol{L}_i^{i,k}[t]$ represents the desired signals and $\\boldsymbol{I}_i^{i,k}[t]$ represents the IUI and which means that, if $k=1$, the former is the desired signals while the later is the IUI, that is\r\n\\begin{equation}\r\n\\label{eq66}\r\n{\\boldsymbol{L}}_1^{1,1}[5]{ = }\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{1,1}[5]}&{h_{1,2}^{1,1}[5]}\\\\\r\n{h_{1,1}^{1,2}[5]}&{h_{1,2}^{1,2}[5]}\\\\\r\n{h_{1,1}^{1,3}[5]}&{h_{1,2}^{1,3}[5]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_5}}\\\\\r\n{{a_6}}\r\n\\end{array}} \\right],\r\n\\end{equation}\r\n\\begin{equation}\r\n\\vspace{-0.5em}\r\n\\label{eq67}\r\n{\\boldsymbol{I}}_1^{1,1}[5]{ = }\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^{1,1}[5]}&{h_{1,4}^{1,1}[5]}\\\\\r\n{h_{1,3}^{1,2}[5]}&{h_{1,4}^{1,2}[5]}\\\\\r\n{h_{1,3}^{1,3}[5]}&{h_{1,4}^{1,3}[5]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{b_5}}\\\\\r\n{{b_6}}\r\n\\end{array}} \\right],\r\n\\end{equation}\r\nand this is the same for the case $k=2$. Now, the signals from different cells are independent, thus they are able to align the interference separately and the DRIA algorithm is adopted.\r\n\r\nIn the third phase, with the knowledge of delayed CSIT, base station 1 regenerates interference signals of  previous slots as\r\n\\begin{small}\r\n\\begin{equation}\r\n\\label{eq68}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\begin{array}{l}\r\n{{\\boldsymbol{ I}}_1^{1,1}}[1] + {{\\boldsymbol{ I}}_1^{1,2}}[1]\\\\\r\n\\!=\\! \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^{1,1}[1]}&{h_{1,4}^{1,1}[1]}\\\\\r\n{h_{1,3}^{1,2}[1]}&{h_{1,4}^{1,2}[1]}\\\\\r\n{h_{1,3}^{1,3}[1]}&{h_{1,4}^{1,3}[1]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{b_1}}\\\\\r\n{{b_2}}\r\n\\end{array}} \\right]\\!+\\! \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^{2,1}[1]}&{h_{1,4}^{2,1}[1]}\\\\\r\n{h_{1,3}^{2,2}[1]}&{h_{1,4}^{2,2}[1]}\\\\\r\n{h_{1,3}^{2,3}[1]}&{h_{1,4}^{2,3}[1]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_1}}\\\\\r\n{{a_2}}\r\n\\end{array}} \\right]\\\\\r\n\\!=\\! \\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\boldsymbol{h}}_{1,3}^2}[1]}&{{{\\boldsymbol{h}}_{1,4}^2}[1]}&{{{\\boldsymbol{h}}_{1,3}^1}[1]}&{{{\\boldsymbol{h}}_{1,4}^1[1]}}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_1}}\\\\\r\n{{a_2}}\\\\\r\n{{b_1}}\\\\\r\n{{b_2}}\r\n\\end{array}} \\right],\r\n\\end{array}\r\n\\end{equation}\r\n\\vspace{-0.5em}\r\n\\begin{equation}\r\n\\label{eq69}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\begin{array}{l}\r\n{{\\boldsymbol{I}}_1^{1,1}}[4] + {{\\boldsymbol{I}}_1^{1,2}}[4] \\\\\r\n\\!=\\! \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^{1,1}[4]}&{h_{1,4}^{1,1}[4]}\\\\\r\n{h_{1,3}^{1,2}[4]}&{h_{1,4}^{1,2}[4]}\\\\\r\n{h_{1,3}^{1,3}[4]}&{h_{1,4}^{1,3}[4]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{b_3}}\\\\\r\n{{b_4}}\r\n\\end{array}} \\right]\\!+\\!\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^{2,1}[4]}&{h_{1,4}^{2,1}[4]}\\\\\r\n{h_{1,3}^{2,2}[4]}&{h_{1,4}^{2,2}[4]}\\\\\r\n{h_{1,3}^{2,3}[4]}&{h_{1,4}^{2,3}[4]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_3}}\\\\\r\n{{a_4}}\r\n\\end{array}} \\right]\\\\\r\n\\!= \\!\\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\boldsymbol{h}}_{1,3}^2}[4]}&{{{\\boldsymbol{h}}_{1,4}^2}[4]}&{{{\\boldsymbol{h}}_{1,3}^1}[4]}&{{{\\boldsymbol{h}}_{1,4}^1}[4]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_3}}\\\\\r\n{{a_4}}\\\\\r\n{{b_3}}\\\\\r\n{{b_4}}\r\n\\end{array}} \\right],\r\n\\end{array}\r\n\\end{equation}\r\n\\begin{equation}\r\n\\vspace{-0.5em}\r\n\\label{eq70}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\begin{array}{l}\r\n\\boldsymbol{I_1^{1,1}}[5] +\\boldsymbol{ I_1^{1,2}}[5]\\\\ \r\n\\!=\\! \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^{1,1}[5]}&{h_{1,4}^{1,1}[5]}\\\\\r\n{h_{1,3}^{1,2}[5]}&{h_{1,4}^{1,2}[5]}\\\\\r\n{h_{1,3}^{1,3}[5]}&{h_{1,4}^{1,3}[5]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{b_5}}\\\\\r\n{{b_6}}\r\n\\end{array}} \\right]\\!+ \\!\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^{2,1}[5]}&{h_{1,4}^{2,1}[5]}\\\\\r\n{h_{1,3}^{2,2}[5]}&{h_{1,4}^{2,2}[5]}\\\\\r\n{h_{1,3}^{2,3}[5]}&{h_{1,4}^{2,3}[5]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_5}}\\\\\r\n{{a_6}}\r\n\\end{array}} \\right]\\\\\r\n\\!= \\!\\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\boldsymbol{h}}_{1,3}^2}[5]}&{{{\\boldsymbol{h}}_{1,4}^2}[5]}&{{{\\boldsymbol{h}}_{1,3}^1}[5]}&{{{\\boldsymbol{h}}_{1,4}^1}[5]}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{a_5}}\\\\\r\n{{a_6}}\\\\\r\n{{b_5}}\\\\\r\n{{b_6}}\r\n\\end{array}} \\right],\r\n\\end{array}\r\n\\end{equation}\r\n\\end{small}\r\nwhere ${\\boldsymbol{h}}_{i,M}^j[t]$ denotes the channel vector from antenna $M$ of the base station $i$ to user $k$ in the t-th slot and it has the form as below\r\n\\begin{equation}\r\n\\label{eq71}\r\n{\\boldsymbol{h}}_{i,M}^j[t]{ = }{\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{i,M}^{j,1}[t]}&{h_{i,M}^{j,2}[t]}&{h_{i,M}^{j,3}[t]}\r\n\\end{array}} \\right]^T}.\r\n\\end{equation}\r\nThen the transmitted symbols can be characterized as\r\n\\begin{small}\r\n\\begin{equation}\\\r\n\\hspace{-1em}\r\n\\label{eq72}\r\n\\setlength{\\arraycolsep}{2pt}\r\n\\begin{array}{l}\r\n{{\\boldsymbol{S}}_1}[6]{ \\!=\\! } \\\\\r\n\\left[ {\\begin{array}{*{20}{c}}\r\n{({\\boldsymbol{I}}_1^{1,1}[1]\\! + \\! {\\boldsymbol{I}}_1^{1,2}[1])\\! +\\! ( {\\boldsymbol{I}}_1^{1,1}[4]\\! + \\! {\\boldsymbol{I}}_1^{1,2}[4])\\! + \\!( {\\boldsymbol{I}}_1^{1,1}[5] \\!+\\!  {\\boldsymbol{I}}_1^{1,2}[5])}\\\\\r\n{{0_{1 \\times 1}}}\r\n\\end{array}} \\right].\r\n\\end{array}\r\n\\end{equation}\r\n\\end{small}\r\n\\!\\!For user 1 in the target cell 1, the received signals are\r\n\\begin{equation}\r\n\\label{eq73}\r\n\\begin{array}{l}\r\n{{\\boldsymbol{y}}_{1,1}}[6] = {\\bf{H}}_1^{1,1}[6]{\\bf{W}}_1^{1,1}[6]{{\\boldsymbol{S}}_1}[6]\\\\\r\n\\quad\\quad\\quad = \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{1,1}[6]}&{h_{1,2}^{1,1}[6]}&{h_{1,3}^{1,1}[6]}\\\\\r\n{h_{1,1}^{1,2}[6]}&{h_{1,2}^{1,2}[6]}&{h_{1,3}^{1,2}[6]}\\\\\r\n{h_{1,1}^{1,3}[6]}&{h_{1,2}^{1,3}[6]}&{h_{1,3}^{1,3}[6]}\r\n\\end{array}} \\right]{{\\boldsymbol{S}}_1}[6]\\\\\r\n\\quad\\quad\\quad= \\left[ {\\begin{array}{*{20}{c}}\r\n{\\boldsymbol{h}_1^{1,1}[6]}\\\\\r\n{\\boldsymbol{h}_1^{1,2}[6]}\\\\\r\n{\\boldsymbol{h}_1^{1,3}[6]}\r\n\\end{array}} \\right]{{\\boldsymbol{S}}_1}[6],\r\n\\end{array}\r\n\\end{equation}\r\nwhere ${\\boldsymbol{h}}_i^{k,N}[t]$ denotes the channel vector which is consist of the channel gains from the whole antennas of the base station $i$ to the $N$th antenna of the user $k$, and the vector is given by\r\n\\begin{equation}\r\n\\label{eq74}\r\n{\\boldsymbol{h}}_i^{k,N}[t]{ = }\\left[ {\\begin{array}{*{20}{c}}\r\n{h_{i,1}^{k,N}[t]}&{h_{i,2}^{k,N}[t]}&{h_{i,3}^{k,N}[t]}\r\n\\end{array}} \\right].\r\n\\end{equation}\r\nNow, we have $(\\bar \\varphi  + 1) \\times (M - N) = 3$ additional equations and for user 1 in the target cell 1, the whole process can be summarized by the equation (75). \r\n\\newcounter{mytempeqncnt5}\r\n\\begin{figure*}[!t]\r\n\\normalsize\r\n\\setcounter{mytempeqncnt5}{\\value{equation}}\r\n\\setcounter{equation}{74}\r\n\\begin{small}\r\n\\begin{equation}\r\n\\label{75}\r\n\\begin{array}{l}\r\n\\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\bf{y}}_{1,1}}[1]}\\\\\r\n{{{\\bf{y}}_{1,1}}[4]}\\\\\r\n{{{\\bf{y}}_{1,1}}[5]}\\\\\r\n{{{\\bf{y}}_{1,1}}[6]}\r\n\\end{array}} \\right]{\\rm{ = }}\\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\bf{A}}_{3 \\times 4}}}&{{{\\bf{0}}_{3 \\times 4}}}&{{{\\bf{0}}_{3 \\times 4}}}\\\\\r\n{{{\\bf{0}}_{3 \\times 4}}}&{{{\\bf{B}}_{3 \\times 4}}}&{{{\\bf{0}}_{3 \\times 4}}}\\\\\r\n{{{\\bf{0}}_{3 \\times 4}}}&{{{\\bf{0}}_{3 \\times 4}}}&{{{\\bf{C}}_{3 \\times 4}}}\\\\\r\n{{{\\bf{D}}_{3 \\times 4}}}&{{{\\bf{E}}_{3 \\times 4}}}&{{{\\bf{F}}_{3 \\times 4}}}\r\n\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}\r\n{{{\\bf{X}}_{4 \\times 1}}}\\\\\r\n{{{\\bf{Y}}_{4 \\times 1}}}\\\\\r\n{{{\\bf{Z}}_{4 \\times 1}}}\r\n\\end{array}} \\right] \\quad where\\\\\r\n{\\rm{  }}{{\\bf{A}}_{3 \\times 4}} = \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{1,1}[1]}&{h_{1,2}^{1,1}[1]}&{h_{1,3}^{1,1}[1]}&{h_{1,4}^{1,1}[1]}\\\\\r\n{h_{1,1}^{1,2}[1]}&{h_{1,2}^{1,2}[1]}&{h_{1,3}^{1,2}[1]}&{h_{1,4}^{1,2}[1]}\\\\\r\n{h_{1,1}^{1,3}[1]}&{h_{1,2}^{1,3}[1]}&{h_{1,3}^{1,3}[1]}&{h_{1,4}^{1,3}[1]}\r\n\\end{array}} \\right],{{\\bf{B}}_{3 \\times 4}} = \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{1,1}[4]}&{h_{1,2}^{1,1}[4]}&{h_{1,3}^{1,1}[4]}&{h_{1,4}^{1,1}[4]}\\\\\r\n{h_{1,1}^{1,2}[4]}&{h_{1,2}^{1,2}[4]}&{h_{1,3}^{1,2}[4]}&{h_{1,4}^{1,2}[4]}\\\\\r\n{h_{1,1}^{1,3}[4]}&{h_{1,2}^{1,3}[4]}&{h_{1,3}^{1,3}[4]}&{h_{1,4}^{1,3}[4]}\r\n\\end{array}} \\right],\\\\\r\n{{\\bf{C}}_{3 \\times 4}} = \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,1}^{1,1}[5]}&{h_{1,2}^{1,1}[5]}&{h_{1,3}^{1,1}[5]}&{h_{1,4}^{1,1}[5]}\\\\\r\n{h_{1,1}^{1,2}[5]}&{h_{1,2}^{1,2}[5]}&{h_{1,3}^{1,2}[5]}&{h_{1,4}^{1,2}[5]}\\\\\r\n{h_{1,1}^{1,3}[5]}&{h_{1,2}^{1,3}[5]}&{h_{1,3}^{1,3}[5]}&{h_{1,4}^{1,3}[5]}\r\n\\end{array}} \\right],\r\n{\\rm{           }}{{\\bf{X}}_{4 \\times 1}} = \\left[ {\\begin{array}{*{20}{c}}\r\n{{a_1}}\\\\\r\n{{a_2}}\\\\\r\n{{b_1}}\\\\\r\n{{b_2}}\r\n\\end{array}} \\right],{{\\bf{Y}}_{4 \\times 1}} = \\left[ {\\begin{array}{*{20}{c}}\r\n{{a_3}}\\\\\r\n{{a_4}}\\\\\r\n{{b_3}}\\\\\r\n{{b_4}}\r\n\\end{array}} \\right],{{\\bf{Z}}_{4 \\times 1}} = \\left[ {\\begin{array}{*{20}{c}}\r\n{{a_5}}\\\\\r\n{{a_6}}\\\\\r\n{{b_5}}\\\\\r\n{{b_6}}\r\n\\end{array}} \\right],\\\\\r\n{\\rm{           }}{{\\bf{D}}_{3 \\times 4}} = \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^2[1]h_1^{1,1}[6]}&{h_{1,4}^2[1]h_1^{1,1}[6]}&{h_{1,3}^1[1]h_1^{1,1}[6]}&{h_{1,4}^1[1]h_1^{1,1}[6]}\\\\\r\n{h_{1,3}^2[1]h_1^{1,2}[6]}&{h_{1,4}^2[1]h_1^{1,2}[6]}&{h_{1,3}^1[1]h_1^{1,2}[6]}&{h_{1,4}^1[1]h_1^{1,2}[6]}\\\\\r\n{h_{1,3}^2[1]h_1^{1,3}[6]}&{h_{1,4}^2[1]h_1^{1,3}[6]}&{h_{1,3}^1[1]h_1^{1,3}[6]}&{h_{1,4}^1[1]h_1^{1,3}[6]}\r\n\\end{array}} \\right],\\\\\r\n{\\rm{          }}{{\\bf{E}}_{3 \\times 4}} = \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^2[4]h_1^{1,1}[6]}&{h_{1,4}^2[4]h_1^{1,1}[6]}&{h_{1,3}^1[4]h_1^{1,1}[6]}&{h_{1,4}^1[4]h_1^{1,1}[6]}\\\\\r\n{h_{1,3}^2[4]h_1^{1,2}[6]}&{h_{1,4}^2[4]h_1^{1,2}[6]}&{h_{1,3}^1[4]h_1^{1,2}[6]}&{h_{1,4}^1[4]h_1^{1,2}[6]}\\\\\r\n{h_{1,3}^2[4]h_1^{1,3}[6]}&{h_{1,4}^2[4]h_1^{1,3}[6]}&{h_{1,3}^1[4]h_1^{1,3}[6]}&{h_{1,4}^1[4]h_1^{1,3}[6]}\r\n\\end{array}} \\right],\\\\\r\n{\\rm{           }}{{\\bf{F}}_{3 \\times 4}} = \\left[ {\\begin{array}{*{20}{c}}\r\n{h_{1,3}^2[5]h_1^{1,1}[6]}&{h_{1,4}^2[5]h_1^{1,1}[6]}&{h_{1,3}^1[5]h_1^{1,1}[6]}&{h_{1,4}^1[5]h_1^{1,1}[6]}\\\\\r\n{h_{1,3}^2[5]h_1^{1,2}[6]}&{h_{1,4}^2[5]h_1^{1,2}[6]}&{h_{1,3}^1[5]h_1^{1,2}[6]}&{h_{1,4}^1[5]h_1^{1,2}[6]}\\\\\r\n{h_{1,3}^2[5]h_1^{1,3}[6]}&{h_{1,4}^2[5]h_1^{1,3}[6]}&{h_{1,3}^1[5]h_1^{1,3}[6]}&{h_{1,4}^1[5]h_1^{1,3}[6]}\r\n\\end{array}} \\right].\r\n\\end{array}\r\n\\end{equation}\r\n\\end{small}\r\n\\hrulefill\r\n\\vspace*{4pt}\r\n\\end{figure*}\r\nIt is obvious that the channel matrix $\\bf{H}$ is a $12 \\times 12$ non-singular matrix and 36 independent symbols can be decoded over 6 slots. Thus, the DoF of the B-DRIA is 6, which is better than both the RIA scheme and the RIR scheme.\r\n\r\n\\section{Numerical Results}\r\nIn this section, numerical results are presented to characterize the performance of the proposed two IA schemes. In specific, to analyze the performance of the proposed schemes, two benchmark schemes are introduced:\r\n\\begin{itemize}\r\n\\vspace{-0.3em}\r\n\\item The TDMA scheme \\cite{2006Ameliorated}, that is at each slot, only one base station is selected to transmit signals while the other base stations keep silent so that the negative effect of the ICI is cancelled. However, owing to the pattern of single base station transmission, both space resources and time resources are wasted inevitably. Hence, the obtained DoF is relaxed to the outer bound \\cite{5074376}.\r\n\r\n\\item The RIA scheme \\cite{7065324}, that is in the previous $t$ slots, only one base station is selected to transmit signals while the other base stations keep silent, and in the last slot, all the base stations make use of the combination of IUI in the previous slots to achieve simultaneous transmission. Therefore, the RIA scheme gets DoF gain while the improvement is still limited. The main reason is that the scheme only improves  the space and time resources' utilization in the last slot.\r\n\\vspace{-0.3em}\r\n\\end{itemize}\r\nHerein, we develop the RIR scheme and B-DRIA scheme to further improve the utilization of time and space resources. As mentioned in Section III and Section IV, in the specific scenario, both RIR scheme and B-DRIA scheme achieve DoF gain better than RIR scheme, but the degree of the improvement of the DoF mainly depends on the relationship between transceiver antennas ratio $\\rho$ and transmitter antennas $M$, whereas the relationship is nonlinear correlated. Therefore, the relationship should be further discussed in two cases:\r\n\\begin{itemize}\r\n\\vspace{-0.3em}\r\n\\item Fix the value of $\\rho$ and change the value of $M$, and for this case, the interval of $\\rho$ is from 1 to 2. In specific, owing to the assumption that no users can decode the messages with only one-slot transmitted signals, the lower bound of $\\rho$ should be greater than 1. Meanwhile, in order to make comparison with the RIA scheme, the upper bound of $\\rho$ should be no more than 2. Therefore, we fix the value of $\\rho$ at 1, 3/2 or 2 and change the value of $N$ from 5 to 25. The effects of $M$ on the DoF are shown in Fig. 10, Fig. 11 and Fig. 12.\r\n\r\n\\item Fix the value of $M$ and change the value of $N$, and for this case, when $N$ increases and $M$ can be divided by $M-N$, the DoF grows sharply, otherwise the value of DoF keeps constant. Therefore, we fix the value of $M$ at 72 to make it divisible by as many integers as possible, and then change the value of $N$ from 36 to 72. The effects of $\\rho$ on the DoF are shown in Fig. 13 and Fig. 14.\r\n\\vspace{-0.5em}\r\n\\end{itemize}\r\n\r\n\\subsection{Performance Analysis with Fixed $\\rho$}\r\nAt first, we analyze how does the DoF is affected by the parameter $M$ under the condition that the parameter $\\rho$ is fixed, i.e., $\\rho  = {3 \\mathord{\\left/\r\n {\\vphantom {3 2}} \\right. \\kern-\\nulldelimiterspace} 2}$. In specific, the configuration of the network is set to $[L,M,K,N = (2,\\left\\lfloor {{3 \\mathord{\\left/ {\\vphantom {3 2}} \\right.\\kern-\\nulldelimiterspace} 2}N} \\right\\rfloor ,3,N)$, and meanwhile the value of the $N$ is varying from 5 to 25. Under this configuration, for the RIR scheme, $\\varphi  = 2$ slots per group are used for signal transmission and at the same time, for the B-DRIA scheme, $\\bar \\varphi  = 1$ slot is used for signal transmission, respectively. From the result shown in Fig. 10, we observe that, for all the value of the $M$, B-DRIA scheme always has the best DoF performance, then it is the RIR scheme, and the RIA scheme obtains the least DoF performance. This phenomenon can be explained as that, on the one hand, the RIR scheme can aligns both ICI and IUI of which the RIA scheme only can align the later one, so that the RIR scheme gets higher DoF gain than RIA scheme. On the other hand, the B-DRIA scheme aligns both IUI and ICI, and meanwhile takes less slots to achieve IA, hence it keeps the highest DoF gain. In addition, we can note that, as the number of user antennas reaches 25, compared with the TDMA scheme, the DoF gain of the RIA scheme, RIR scheme and B-DRIA scheme are 11.54\\%, 24.8\\% and 56\\%, respectively. Furthermore, it is worth noting that some points of the DoF keep constant locally when $N$ increases. This comes from the fact that, when the base station sends the same amount of information $\\left\\lceil {{M \\mathord{\\left/{\\vphantom {M K}} \\right.\\kern-\\nulldelimiterspace} K}} \\right\\rceil$ to each user, if $M$ cannot be divided to $K$, the base station does not send redundant symbols $[{M \\mathord{\\left/ {\\vphantom {M K}} \\right. \\kern-\\nulldelimiterspace} K} - \\left\\lceil {{M \\mathord{\\left/ {\\vphantom {M K}} \\right. \\kern-\\nulldelimiterspace} K}} \\right\\rceil $ separately. This measurement leads to no increase of the DoF and we call this operation as round down operation loss.\r\n\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig10}\r\n\\caption{DoF versus $N$ with fixed $\\rho  = {3 \\mathord{\\left/{\\vphantom {3 2}} \\right.\\kern-\\nulldelimiterspace} 2}$ of the schemes.}\r\n\\label{Fig10}\r\n\\end{figure}\r\n\r\nThen we further analyze how does the DoF is affected by the parameter $M$ under two special values of $\\rho$, i.e., $\\rho  = 1$ and $\\rho  = 2$. For the former, we set the value of $\\rho$ approaching to the critical point 2 and the other parameters used herein are the same as that used for the Fig. 10. Under this configuration, for the RIR scheme, $\\varphi  = 1$ slots per group are used for signal transmission, and for the B-DRIA scheme, $\\bar \\varphi  = 0$ slot is used for signal transmission, in other words, B-DRIA scheme does not implement the second phase. It can be witnessed from the Fig. 11 that, the proposed two schemes achieve 36\\% DoF gain than the benchmark schemes and meanwhile, both RIR scheme and B-DRIA scheme get the same DoF. The former confirms that the proposed schemes have a better effect on the improvement of DoF, and later is lies in the absence of the second phase so that the spatial gain is lost. It is also worth noting  that the RIA scheme has the same performance as the TDMA scheme. It can be explained as that RIA scheme transmits signals in the first slot and then transmits interference in the second slots where the whole process can be equivalent to the TDMA scheme with two slots. Notice that the third reason can also be used to explain why does the B-DRIA scheme not get the DoF gain from the past interference.\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig11}\r\n\\caption{DoF versus $N$ with fixed $\\rho  = 2$ of the schemes.}\r\n\\label{Fig11}\r\n\\end{figure}\r\n\r\nFinally, we set the value of $\\rho$ approaching to the critical point 1 and the other parameters used herein are the same as that used for the Fig. 10. Under this configuration, for the RIR scheme and B-DRIA scheme, more than 2 slots per group and 1 slot per group are used for signal transmission, respectively. The Fig. 12 shows that the RIA scheme obtains 19.5\\% DoF gain than the TDMA scheme, and meanwhile the performance of the two proposed schemes is polarized, i.e., the B-DRIA scheme acquires 100\\% DoF gain than TDMA scheme while the performance of RIR scheme is degraded which is worse than RIA scheme. The former result can be explained as  that, for the B-DRIA scheme, with the multi-slots\u2019 simultaneous transmission, the space resources have been fully utilized and the obtained DoF is tight to the upper bound \\cite{5074376}. The later result is mainly because that, when   approaches 1, too many slots of each group are taken. In specific, when the number of user antennas reaches 25, there are $\\bar \\varphi {{ = }}26$ slots should be used for the single cell transmission which leads to serious waste of the time and space resources.\r\n\r\nIt is worth noting that, although the performance of the B-DRIA scheme is completely superior to the RIR scheme, in practice, the application scenario of the former is restricted. In specific, for the sake of DoA estimation, the location of the users should be stationary or moves within the current cell, otherwise the estimation should be reacquired which leads to the poor mobility of the network. Therefore, for the selection of the two proposed schemes, the choice should be made according to specific application scenario.\r\n\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig12}\r\n\\caption{DoF versus N with fixed $\\rho  = 1$ of the schemes.}\r\n\\label{Fig12}\r\n\\end{figure}\r\n\r\n\\subsection{Performance Analysis with Fixed M}\r\nWe analyze how does the DoF is affected by the parameter $\\rho$ under the condition that the parameter $M$ is fixed, i.e., $M = 72$. In specific, the configuration of the network is set to $L,M,K,N = (2,72,3,N)$, and meanwhile the value of the $N$ ranges from 36 to the 72. Aiming to this configuration the parameter goes through three intervals, i.e., ${3 \\mathord{\\left/{\\vphantom {3 2}} \\right. \\kern-\\nulldelimiterspace} 2} < \\rho  \\le 2$, ${4 \\mathord{\\left/\r\n {\\vphantom {4 3}} \\right. \\kern-\\nulldelimiterspace} 3} < \\rho  \\le {3 \\mathord{\\left/ {\\vphantom {3 2}} \\right. \\kern-\\nulldelimiterspace} 2}$ and $1 < \\rho  \\le {4 \\mathord{\\left/ {\\vphantom {4 3}} \\right. \\kern-\\nulldelimiterspace} 3}$ where the corresponding intervals measured by the parameter $N$ is $36 \\le N < 48$ $48 \\le N < 54$ and $54 \\le N < 72$, respectively.\r\n\r\nFrom the result shown in the Fig. 13, we find that, for the RIR scheme, the DoF keeps constant in each interval, while increasing sharply when the value of $N$ exceeds one critical point. This phenomenon is caused by round down operation loss mentioned before. Meanwhile, when the value of $N$ reaches 36, 48 and 54, the obtained DoF gains are 20\\%, 14.3\\% and 10.3\\%, respectively. It indicates that, with the increase of $N$, the improvement made by RIR scheme becomes weaken. In another word, the DoF gain is inversely related to the critical points of $\\rho$. Herein, the local optimum of DoF gain is obtained at critical points of $\\rho$, and the global optimum of DoF gain is acquired when $\\rho$ approaches 2.\r\n\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig13}\r\n\\caption{DoF versus $N$ with fixed $M=72$ of RIR scheme.}\r\n\\label{Fig13}\r\n\\end{figure}\r\n\r\nFrom the result shown in the Fig. 14, we discover that, for the B-DRIA scheme, the DoF keeps constant in each interval, while increasing sharply when the value of $N$ exceeds one critical point. The reason for this phenomenon is the same as that for the RIR scheme. Meanwhile, when the value of $N$ reaches 36, 48, and 54, the obtained DoF gains are 33.3\\%, 50\\% and 60\\%, respectively. That is to say, the DoF gain is positively related to the critical points of $\\rho$. Herein, the local optimum of DoF gain is obtained at the critical points of $\\rho$, and the global optimum of DoF gain is acquired when $\\rho$ approaches 1.\r\n\r\n\\begin{figure}[!t]\r\n\\centering\r\n\\includegraphics[width=2.5in]{Fig14}\r\n\\caption{DoF versus $\\!N\\!$ with fixed $M\\!=\\!72$ of B-DRIA scheme.}\r\n\\label{Fig14}\r\n\\end{figure}\r\n\r\n\\section{Conclusion}\r\nIn this paper, two new IA schemes were proposed for cellular $K$-user MIMO downlink networks, i.e., the RIR scheme and the B-DRIA scheme. For the former, the redundant symbols of the interference signals are eliminated with delayed CSIT and meanwhile, the desired symbols are provided to the target cell\u2019s users without causing secondary interference. The analysis indicates that it achieves better DoF gain than the RIA scheme but, when the transceiver antennas ratio $\\rho$ approaches 1, the performance improvement becomes weakened. Therefore, we further proposed the B-DRIA scheme, in which the beamforming technique is adopted to eliminate ICI and meanwhile, the distributed retrospective interference alignment is developed to use the past interference signals. Simulation results show that the B-DRIA scheme obtains larger DoF than the RIR scheme locally. Specifically, when $\\rho$ approaches 1, two schemes obtain the same DoF. While as $\\rho$ approaches 2, the DoF of the B-DRIA scheme is superior than the later.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\\ifCLASSOPTIONcaptionsoff\r\n  \\newpage\r\n\\fi\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "meta": {"timestamp": "2021-06-25T02:19:11", "yymm": "2106", "arxiv_id": "2106.13022", "language": "en", "url": "https://arxiv.org/abs/2106.13022"}}
{"text": "\\section{Introduction}\n\nDiffusive transport at interfaces is an ubiquitous process in nature. Prototypical examples involve proteins on a phospholipid membrane of a living cell \\cite{franosh13}, nanoparticles in liquid-filled solid pores \\cite{alvaro16}, or molecules interacting with growing or functional materials interfaces \\cite{kumar18}. The characteristic of this transport is that it takes place on a two-dimensional surface,  which by its structure and molecular composition may be very complex. Often transport takes place at an appreciable concentration of diffusing species which furthermore interact with defects on the surface or specifically incorporated functional moieties such as proteins on the membranes \\cite{smith18}. These local interactions trap the diffusing species, intermittently arresting the particle, which in turn reflects on the effective diffusion coefficient.  \n\nDiffusion on interfaces has been theoretically studied as a simplified problem of a stochastic motion on a lattice \\cite{saxton87,saxton94}. In these models, steric hindrance between diffusing particles is typically introduced, which prevents double occupancy of the same lattice site. For a gas of diffusing particles in these conditions, long-time diffusion coefficient has been determined by calculating many-particle correlation functions \\cite{nakazato80,tahir83,beijeren85}, as well as by considering memory effects of diffusing particles \\cite{halpern96}. These different approximations were recently complemented with the exact calculation of the tracer particles' probability distribution function on a crowded lattice \\cite{pigeon17}, yielding the diffusion coefficient as a function of gas density or surface coverage. \n\\begin{figure}\n\\includegraphics[scale=0.4]{fig1.pdf}\n\\caption{ Depiction of a 2-D lattice gas (solid spheres) in the field of traps (red cones).  If the particle and trap position coincide, then the particle can bind to the trap with a probability $ P_{on} $ and unbind with a probability $ P_{off}$. Length and time traversed by the particle during diffusion from one trap to another is referred as $ l_{t}^{eff} $ and $ \\tau_{t-t} $, respectively, while the time in the trap is denoted as $\\tau_{off}$}. \n\\label{fig_model}\n\\end{figure}\n\nWhile the increased density of diffusing species rescales the diffusion constant at long times \\cite{dix08}, interactions with defects, however, yield a rich dynamic behavior\\cite{jeon11,jeon16}, which was extensively studied using theoretical \\cite{sokolov12,yann18,metzler19} and computational means \\cite{evers13,samanta16,schnyder18}. Particular efforts were focused on understanding the effect of interaction types on the diffusion of a tracer particle, which lead to the random trap model \\cite{bouchaud90}, Havlin\u2013Weiss comb model \\cite{weiss86}, or the quenched trap model \\cite{burov07}. Computational approaches provided insight on the effects of surface heterogeneity using the kinetic lattice gas model \\cite{mak88}, or the bivariate trap model \\cite{viljoen96}. Furthermore, kinetic Monte-Carlo simulations were used to study the diffusion of a tracer on a surface with binding sites characterized by multiple energy levels \\cite{saxton96}. It has been established that the tracer diffusion in the field of traps exhibits a sub-diffusive motion on short time scales and normal diffusion on long time scales \\cite{franosh13,saxton96,saxton07,kusumi05}. These works provided significant impact on the role of the trap energetics on the diffusivity of a tracer \\cite{bouchaud90}, however, the combined effects of crowding and trapping were not addressed so far in full depth to our knowledge. \n\nIn this letter, we aim to rectify this situation by providing an approximate theory for surface diffusion of crowded particles in a field of simple traps.  We propose an expression for the long-time effective diffusion coefficient of the crowding particles using the scaling argument as well as the master equation approach. The model is validated by a favorable comparison with Monte Carlo (MC) simulations \\cite{bihr15} (methodological details in Supplementary Information). In agreement with previous works \\cite{franosh13,saxton96,saxton07,kusumi05}, we find that an intermittent sub-diffusive regime is inherited to the system as soon as the traps are introduced. More surprisingly, however, we find that in the long time limit the diffusion coefficient may be enhanced by crowding effects for any concentration of traps, at significant range of densities of lattice gas.\n\n\nWe start with placing $ N_{w} $  particles, mutually interacting with a hard wall repulsive potential, on a 2D  square lattice with $ N_{g} $  lattice sites of a length $ a_{0} $. The concentration of gas particles on the lattice is $ c_{w}=N_{w}/N_{g} $.  A random walking particle will traverse a distance $ a_{0} $ during time $ \\tau=a_{0}^{2}/4D_{0} $, where $ D_{0} $ is the diffusion coefficient of a particle at infinite dilution. The concentration-dependent diffusion coefficient $ D_{cr}$ is determined as $ D_{cr}(c_{w},D_{0})=D_{0}p(c_{w}) $ where $ p(c_{w}) $ is the concentration-dependent  probability of a jump, whereby the larger the concentration, the smaller the effective diffusion coefficient.\n $p(c_{w})$ can also be seen as the factor normalizing the characteristic time to make a step in the crowded environment $\\tau_{cr}=\\tau/p$. It has been calculated using different approaches,  \\cite{nakazato80,tahir83,beijeren85}, while here we estimate it from backward correlations as in the anti-persistent random walk (APRW) model \\cite{halpern96}. The appropriateness of this choice, which is a compromise between accuracy over the entire density range and simplicity, as demonstrated by comparison with MC simulations and with other models (see Supplementary Information, section I).\n\nThe lattice is furthermore decorated by $ N_{t} $ randomly placed traps, the concentration of which is denoted as $c_{t}= N_{t}/N_{g} $. Upon hopping onto a site with a trap, a diffusing particle binds with the probability $ P_{on} $ ( Fig.\\ref{fig_model}), and unbinds with the probability $P_{off}$.  Following the detailed balance condition, the binding energy is $ \\Delta E_{b}=-k_{B}T ln(P_{on}/P_{off})$. The latter sets the concentration of bound particles $\\langle c_{b}\\rangle $, which can be calculated analytically from the partition function of the system, and its exact form is given in the SI \\cite{mislav17}.\n\nTo estimate the diffusion constant in the long time limit  for the lattice gas in the field of traps $D_{eff}$, we aim at coarsening the diffusion process and finding the scaling function $f$ \n\\begin{equation}\nD_{eff}(c_{w},c_{t},P_{on},P_{off})=f(c_{w},c_{t},P_{on},P_{off})D_{cr}(c_{w}).\n\\label{eq-D_eff}\n\\end{equation}\nSpecifically, we presume that the effective diffusion coefficient can be related to the square of the average distance between two traps at which binding actually occurs $ \\langle  l_{t}^{2}\\rangle ^{eff}$ , and  the time it takes to make this coarsened diffusion step $\\tau^{eff}$, i.e.  $4D_{eff} = \\langle   l_{t}^{2}\\rangle ^{eff}/\\tau^{eff}$.  \nHere, the average square distance between two efficient traps can be related to the effective concentration of traps  $\\rho^{eff}$ as $ \\langle l_{t}^{2} \\rangle =a_{0}^{2}/\\rho^{eff}$. The latter can be estimated from the density of free traps $c_{t}-\\langle c_{b}\\rangle $, to which a diffusing particle can bind with the probability $P_{on}$ such that  $ \\rho^{eff}=(c_{t}-\\langle c_{b} \\rangle )P_{on}$. \n\n\\begin{figure}. \n\\includegraphics[scale=0.43]{fig2.pdf}\n\\caption{ Effective diffusion coefficient, normalized by the diffusion coefficient at infinite dilution,  as a function of the lattice gas concentration $ c_{w} $ for various trap densities $c_t$ ( $ c_{t}=0, 0.1,0.2,0.3 $ and $ 0.4 $) with $ P_{on}=0.5 $ and $ P_{off}=0.001 $. Results of MC simulations are shown with symbols, and theory (eq.(\\ref{eq-D_eff2})) with lines.}\n\\label{fig_Deff}\n\\end{figure} \n\nThe characteristic time $\\tau^{eff}$ comprises of the time that it takes to leave a bounded trap $ \\tau_{off} $, and the time to diffuse to the next trap where it binds again $ \\tau_{t-t} $, which is simply $\\tau_{t-t}=\\langle l_{t}^{2}\\rangle ^{eff}/4D_{cr}=\\tau_{cr}/\\rho^{eff} $. To leave the trap, a particle must unbind  with the probability $ P_{off} $ and make a step with the probability $ p(c_{w}) $, and hence, $ \\tau_{off}=\\tau_{cr} /[P_{off}p(c_{w})]$. With $\\tau^{eff}= \\tau_{off}+\\tau_{t-t}$, it is straightforward to estimate the effective diffusion coefficient as\n\\begin{equation}\nD_{eff}=D_{cr}\\bigg(1+\\frac{P_{on}}{P_{off}}\\frac{c_{t}-\\langle c_{b}\\rangle }{p(c_{w})}\\bigg)^{-1}.\n\\label{eq-D_eff2}\n\\end{equation}\n\nThe same result can be derived by considering the master equation approach by calculating the probability of finding a particle $ P(r,t)$, at position $ r $ at time $ t $. Since the later emerges as a sum of the probability of finding a particle unbound on that site $ P_{cr}(r,t) $ and the probability of finding the particle trapped on the same site $ P_{tr}(r,t) $, we can write $ P(r,t)=P_{cr}(r,t)+P_{tr}(r,t) $. The time evolution of both probabilities is given by:\n\\begin{eqnarray}\n\\frac{\\partial P_{cr}(r,t)}{\\partial t}=\\frac{\\gamma}{2d}\\sum_{i=1}^{d}(P_{cr}(r-e_{i},t)+P_{cr}(r+e_{i},t)\\nonumber\\\\\n-2P_{cr}(r,t))-k_{on}^{e}P_{cr}(r,t)+k_{off}^{e}P_{tr}(r,t) ,\n\\label{eq-master1}\n\\end{eqnarray}\nand\n\\begin{equation}\n\\frac{\\partial P_{tr}(r,t)}{\\partial t}=k_{on}^{eff}P_{cr}(r,t)-k_{off}^{eff}P_{tr}(r,t).\n\\label{eq-master2}\n\\end{equation}\nHere, $\\gamma=p(c_{w})/\\tau $ is the hopping rate of the particle and $ k_{on}^{eff} $, $ k_{off}^{eff} $ are the effective binding and unbinding rates of any particle.\nBy introducing Fourier transform $ S(\\textbf{k},w)=1/\\pi \\int_{\\Omega}dr\\int_{0}^{\\infty}dt P(r,t)e^{i(wt-\\textbf{k}.r)} $ of the eq.(\\ref{eq-master1}) and eq.(\\ref{eq-master2}) we can calculate the relation between dynamic structure factors $ S_{cr}(\\textbf{k},w) $ and $ S_{tr}(\\textbf{k},w) $. By calculating real part of total Fourier transform $ Re(S(\\textbf{k},w)) $ and using the relation\n $ D_{eff} = \\lim_{w \\to 0}(\\lim_{\\textbf{k} \\to 0}\\pi w^{2}/\\textbf{k}^{2}Re(S(\\textbf{k},w)))$ we obtain\n\\begin{equation}\nD_{eff}=D_{cr}\\frac{k_{off}^{eff}}{k_{on}^{eff}+k_{off}^{eff}}\n\\label{eq-D_effM}\n\\end{equation}\nWe note that for $ k_{on}^{eff}=0 $ the effective diffusion constant $ D_{eff} $ reduces to $ D_{cr} $.\n \nA particle can only bind to a trap if the trap is empty. Hence, the effective binding rate is $ k_{on}^{eff}=P_{on}(c_{t}-\\langle c_{b} \\rangle )/\\tau $. Similarly, effective unbinding rate of a bounded particle depends on the availability of a free site to jump, so $ k_{off}^{eff}=P_{off}p(c_{w})/\\tau $. Substituting these expression for effective rates in eq.(\\ref{eq-D_eff}) yields $ D_{eff} $ equal to that in eq.(\\ref{eq-D_eff2}) (see SI for details of the calculation).\n\n\\begin{figure}\n\\includegraphics[scale=0.43]{fig3.pdf}\n\\caption{\nThe probability distribution of the path length of a particle between two successive trapping events ($ l_{d}^{MC} $) is shown for $ c_{w}=0.1, 0.3 $ and $ 0.9 $ for fixed $ c_{t} = 0.1 $, $ P_{on}=0.5 $ and $ P_{off}=0.001 $ . In the inset, corresponding distribution of time between two successive trapping events of a particle ($ \\tau_{d}^{MC} $) is shown.\nCorresponding $\\langle l_{d}^{MC}\\rangle $ and $\\langle \\tau_{d}^{MC} \\rangle $ is presented in the inset. The data is obtained from kinetic Monte-Carlo simulations as presented in lattice units.  \n}\n\\label{fig_histogram}\n\\end{figure}\n\n\\begin{figure}\n\\includegraphics[scale=0.4]{fig4.pdf}\n\\caption{Contour plot of the mean concentration of bound particles $ \\langle c_{b}\\rangle $  in the $ c_{w} $-$ c_{t} $ plane. The intersection points between solid black lines and red dashed lines represent the inflection points, calculated from our theory. Two red dashed inflection lines divide the plot into three regions, namely \n\\textit{trapping-dominated diffusion} region (TDD), \\textit{crowding enhanced diffusion} region (CED) and \\textit{crowding-dominated diffusion} region (CDD). The black dashed line and points represent the critical concentration of crowding, at which the effective diffusion coefficient is maximized. The lines have been calculated from our theory, whereas the discrete data points have been obtained from MC simulation using $ P_{on}=0.5 $ and $ P_{off}=0.001 $.\n}\n\\label{fig_cb}\n\\end{figure}\nIntuitively, one expects a monotonic decay of the effective diffusion coefficient with increasing the concentration of particles $ c_{w}$, which indeed is the result observed in the absence of traps ($c_t=0$ (brown line and points in Fig.\\ref{fig_Deff})). However, as soon as traps are introduced ($c_t>0$), the effective diffusion coefficient starts to non-monotonically vary with the gas density and a maximum in $D_{eff}(c_w)$ is predicted by eq.(\\ref{eq-D_eff2}). This surprising property of lattice gas diffusion in the field of traps is confirmed by MC simulations (symbols in Fig. \\ref{fig_Deff}). Particularly good agreement between theory and simulations is obtained at low $ c_{w} $ and $ c_{t} $. The strongest discrepancies occur for intermediate to high  $ c_{w} $ and $ c_{t} $, which suggest that higher-order correlations play an important role in this range of parameters.\n\nIn order to understand the underlying mechanism of for this behaviour, we extract the probability distribution of the actual path length traversed by the particle between two successive trapping events  $l_{d}^{MC}$ (Fig.\\ref{fig_histogram}), and the distribution of time between two trapping events $\\tau_{d}^{MC}$ (inset of Fig.\\ref{fig_histogram}). We clearly see that at low concentrations of $c_{w}$, both typical lengths of the path, and the actual time between two trapping events is in average short, which means that the particles spend most time in the traps, this being detrimental to the effective diffusion coefficient. We denote this regime as \"trapping-dominated diffusion\".  For moderate $ c_{w}$, the characteristic $\\tau_{d}^{MC}$ becomes significant while the tails of the distribution of $l_{d}^{MC}$ are the thickest. This means that at these concentrations the particles spend extended time meandering through the system. In this regime, a high level of trap occupancy is achieved by the significant concentration of walkers, but the crowding effects are not sufficient to prevent the diffusion - i.e. the walkers move away from occupied traps before they interact with a free trap. This yields \"crowding enhanced diffusion\". At high concentrations, nearly all traps are occupied, hence the walkers survive the longest between two trapping events, but they make significantly shorter paths than in the intermediate regime. This is because the likelihood for making a step onto a next site decreases significantly due to the high concentration of the particles, and therefore, there is enough time to actually interact of the trap. We denote this regime as \"the crowding dominated diffusion\".  \n\nThe three regimes are clearly denoted in the diffusivity phase diagram (Fig.\\ref{fig_cb}) which highlights the importance of the mean number of occupied traps $ \\langle c_{b} \\rangle $. For a given $ c_{t} $, the latter increases with increasing $ c_{w} $  until saturation, which in the case of reasonable large binding affinities presumes that either nearly all walkers are bound, or that nearly all traps are occupied, depending on their relative total number.  In both cases, the particles are still diffusing and binding-unbinding kinetics are still ongoing. The boundaries between the three regimes are determined from the inflection points in $ D_{eff}/D_{0} $ vs $ c_{w} $ for a given $ c_{t} $ (red dashed lines in Fig.\\ref{fig_cb}), with $ D_{eff}$ maximized for the particular concentration of particles $ c_{w} $ as determined analytically (black dotted line) and fully supported by simulations (black symbols).\n\n\n\nIn summary, we discussed the simultaneous effect of crowding and trapping on surface diffusion. Using scaling arguments and from the master equation approach, we show that the diffusion is directly related to the density of free traps and not to the absolute density of traps, the occupancy of which is defined by the density of the gas and affinity of the gas for the traps.  We find the so-called \\textit{trapping-dominated diffusion} as long as there are more traps than walkers in the system allowing significant interactions of the two, and impeding the capacity of walkers to explore the system. When the number of traps and walkers are comparable \\textit{crowding-enhanced diffusion} takes place. In this regime, a large number of traps are occupied but there is  a significant fraction of particles still able to diffuse in an environment that is not overly crowded, which optimizes the diffusion constant. Finally, if the number of walkers dominates, than crowding becomes significant even at relatively low $ c_{w} $. The traps are, by and large occupied, but the particles have a smaller likelihood to move to the next site as it is likely already occupied. Consequently the system displays \\textit{crowding-dominated diffusion} where the effective diffusion constant continuously decays with the density of walkers. These results are confirmed by scaling arguments, analytical modeling and kMC simulations.\n\nRecent theoretical studies on diffusion of rod-shaped active particles \\cite{mandal20}, tracer diffusion inside active particles bath \\cite{abbaspour21} and external force driven tracer diffusion \\cite{illien18} diffusion is enhanced either by the energetics of active particles or by external force driven dynamics. However, in our minimalistic model of lattice gas in the presence of traps \\textit{crowding-enhanced diffusion} arises solely because of the interplay between crowding and trapping as part of equilibrium thermodynamics. \n\nOur findings naturally still require a direct experimental confirmation. However, few recent experimental studies suggested that crowding can favorably affect diffusion. One example is the facilitated diffusion of DNA-recognizing protein during specific target search over a long DNA strand\\cite{krepel16,brackley13,bauer13}, which is a system that shows similar features as our model. Non-monotonous behavior was also observed in simulations of a tracer diffusing in a field of particles crowding the environment, as a function of the density of crowder and the depth of the minimum of the tracer-particle interaction potential, which is a result consistent with our findings albeit in three dimensions \\cite{putzel14}. In three dimension both experimental and theoretical studies on polymer transport in a crowded medium suggested that crowding enhances long-term diffusivity \\cite{chien16,chien17}. It would be therefore interesting to extend our work from surface to volume diffusion and account for more complex behavior and properties of traps, a task that we plan to address in future. \n\n\nWe acknowledge the funding by ERC StG 2013\u2013337283 of the European Research Council in the early stages of the project, which was later supported by the German Science Foundation program SFB 1411 Design of Particulate Systems and the Institute of Ru\u0111er Bo{\\v s}kovi{\\'c} support funds. \n\n", "meta": {"timestamp": "2021-06-25T02:17:24", "yymm": "2106", "arxiv_id": "2106.12965", "language": "en", "url": "https://arxiv.org/abs/2106.12965"}}
{"text": "\\section{Introduction}\n\\label{sec:intro}\nAn appropriate dynamics model is at the core of many learning-based control methods.\nThe problem of learning dynamical systems can be formulated as a function approximation task in numerical analysis.\nMathematically, we wish to solve the optimization problem\n$\n\\min_{f\\in \\mathcal H} \\| f - f^*\\|,\n$\nwhere $f^*$ is the (unknown) true dynamics model of the underlying system and $\\|\\cdot\\|$ some criteria such as function norms.\nIn this problem, the function class $\\mathcal H$ plays a central role.\nIt can be chosen as, e.g., deep neural networks (DNN), Gaussian processes (GP).\nAs an example, let us consider a simplified optimal control problem (OCP)\n\\begin{equation}\n\t\\label{eq:ocpIntro}  \n\t\\min_{u_0,\\dots,u_N} \\sum_{t=1}^N c(x_t, u_t)\\ \\  \\text{ subject to }{x}_{n+1} =  f({x}_n, {u}_n),x_0 = \\hat{p} ,\n\\end{equation}\nwhere $n=0, ..., N-1$.\nIdeally, a learned dynamics model $f$ should allows us to reason about the worst-case objective value of $ \\sum_{t=1}^N c(x_t, u_t)$ under model uncertainty (i.e., the highest plausible cost).\n\nIn the current literature, DNN are often celebrated as the most expressive \\emph{universal function approximator}. Numerous works have considered DNN dynamics models in the learning-based control tasks, \\textsf{}e.g., \\cite{claveraLearningAdaptDynamic2019,picheNonlinearModelPredictive2000}.\nHowever, despite efforts to propagate uncertainty under DNN dynamics (see \\cite{chuaDeepReinforcementLearning2018} and references therein),\ntypical methods still rely on exhaustive simulations which struggle to be used with reliable numerical methods for robust and stochastic control.\n\nOn the other hand, GP dynamics models have enjoyed great success in learning-based control due to its uncertainty-aware nature.\nFor example,\nmany authors such as \\cite{deisenrothPILCOModelbasedDataefficient2011,hewingCautiousModelPredictive2020} exploited the moment-matching of Gaussian distributions for uncertainty propagation.\nSince moment-matching inevitably weakens any robustness guarantees,\nthe authors of \\cite{kollerLearningbasedModelPredictive2018} proposed an over-approximation scheme.\nHowever, it was pointed out that their method leads to \\emph{over-conservatism}; see, e.g., \\cite{lewSamplingbasedReachabilityAnalysis2020}.\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{fig/vdp_small_and_large_data.pdf}\n    \\caption{Learning the (nonlinear) Van der Pol oscillator with different amounts of training data. Our uncertainty-aware random feature model accurately learns the nonlinear dynamics with adequate data (\\textbf{right}), while predicting with uncertainty under limited training data (\\textbf{left}). Plausible sample trajectories (given a predefined confidence level) are depicted in light blue. Note that although the true trajectory sometimes deviates from the model (mean) prediction, it remains close to the sampled trajectories.}\n    %\n    \\label{fig:time_evo}\n\\end{figure}\n\nIn providing safety guarantees with GP dynamics, works such as \\cite{kollerLearningbasedModelPredictive2018} assumed the true dynamics to live in an RKHS.\nFrom a numerical analysis perspective, this is a reasonable assumption since the RKHS is dense in the space of continuous functions. \nIntuitively, RKHS functions can be used as powerful universal function approximators analogous to the celebrated \\emph{Weierstrass approximation theorem}.\nMotivated by that insight and the well-known connection between GP and kernel methods~\\cite{kanagawaGaussianProcessesKernel2018}, this paper proposes to directly approximate the uncertain dynamics function using \\emph{random features} (RF), a function approximation tool originated from large-scale kernel machines~\\cite{rahimiRandomFeaturesLargescale2008}.\nDue to its simple \\emph{linear-in-parameter\\xspace} structure, RF have been extensively used in analyzing the theoretical foundation of statistical and deep learning, see, e.g., \\cite{bachBreakingCurseDimensionality2017,belkinUnderstandDeepLearning2018,carratinoLearningSGDRandom2018,eeMachineLearningContinuous2020,hastieSurprisesHighDimensionalRidgeless2020,meiGeneralizationErrorRandom2020,rudiGeneralizationPropertiesLearning2017,sunApproximationPropertiesRandom2019}, and \\cite{liuRandomFeaturesKernel2021} for a recent survey. Significant attention, such as the \\emph{NeurIPS 2017 Test-of-Time Award}~\\cite{rahimiRandomFeaturesLargescale2008} and \\emph{ICML 2019 Honorable mentions}~\\cite{liUnifiedAnalysisRandom2019}, have been dedicated to studies of random features.\n\nIn this paper, we exploit the function approximation capacity of RF for learning uncertainty-aware dynamics.\nWe summarize our \\emph{contributions} and sketch the main results.\n\\begin{enumerate}[noitemsep,topsep=0pt]\n\t\\item We propose the set-valued uncertainty-aware random feature (URF) dynamics model. Our model lifts the dynamics function to a reproducing kernel Hilbert space, allowing it to learn general nonlinear systems while quantifying the uncertainty due to limited observations (See Fig. \\ref{fig:time_evo}).\n\t\\item Exploiting the lifting structure of URF, we propose an indirect method to find the worst-case realization of the learned dynamics using Pontryagin's minimum principle\\xspace, where the Hamiltonian is linear in the URF dynamics parameter.\n\t\\item By viewing the one-step dynamics model as a shallow neural network, we take the perspective that the whole dynamical system can be viewed as a DNN (as well as a Bayesian net).\n\tWe then show that, thanks to the URF model, our Pontryagin's minimum principle\\xspace-based method is equivalent to the conditional gradient method\\ (also known as the Frank-Wolfe\\space algorithm) on the DNN.\n\t\\item Finally, we show that the eigenvalue-decay structure of common RKHSs allow us to learn a low dimensional representation of the dynamics model.\n\\end{enumerate}\n\nThe rest of the paper is organized as follows. \nIn Section~\\ref{sec:bg}, we briefly visit the background of kernel methods and random features.\nIn Section~\\ref{sec:model}, we propose our main dynamics model --- the uncertainty-aware random feature model, and propose to find the worst-case dynamics via Pontryagin's minimum principle\\xspace.\nAll proofs are deferred to Section~\\ref{sec:pf}.\nWe then detail the concrete learning algorithms to estimate such models in Section~\\ref{sec:learnSet}.\nOur methodology is validated through numerical experiments on learning various dynamical systems in Section~\\ref{sec:exp}.\nSection~\\ref{sec:related} contains other related works in the literature.\nWe conclude with a discussion in Section~\\ref{sec:conclude}.\n\n\\section{Background}\n\\label{sec:bg}\n\nIn this paper, we assume a partially-known discrete-time dynamics according to\\footnote{\\textbf{Notation}: lower/upper case symbols denote vectors/matrices. We write scalars in both lower and upper case. Curly symbols (e.g., \\wset) denote sets. $\\nabla_x f(w)$ is the gradient of $f$ with respect to $x$ evaluated at $w$. $I_p$ denotes the $p \\times p$ identity matrix. A normally distributed vector $x$ with mean $\\mu$ and covariance $\\Sigma$ is written as $x \\sim \\mathcal{N}(\\mu, \\Sigma)$. We use $\\mathcal{U}(a,b)$ to denote the uniform distribution over the interval $(a,b)$. The euclidean norm of $x$ is denoted by $\\| x \\|_2$. We use $p(x|y)$ to refer to the conditional probability density of $x$ given $y$. $[M]_{i,j}$ denotes a specific entry of the matrix $M$.}\n\\begin{equation}\n    \\mathbf{x}_{t+1} = h(\\mathbf{x}_t, \\mathbf{u}_t) + f(\\mathbf{x}_t, \\mathbf{u}_t),\n\\label{eq:dynamics}\n\\end{equation}\nwhere $\\mathbf{x}_t \\in \\mathbb{R}^p, \\mathbf{u}_t \\in \\mathbb{R}^q$ denote the system state and input action, respectively; $h(\\cdot, \\cdot)\\colon \\mathbb{R}^{p+q} \\to \\mathbb{R}^p$ represents a (known) nominal model and $f(\\cdot, \\cdot)\\colon \\mathbb{R}^{p+q} \\to \\mathbb{R}^p$ accounts for uncertain deviations from the nominal component. We are interested in learning the unknown term from collected transitions of a system governed by \\eqref{eq:dynamics}.\nSince this work focuses on dynamics learning instead of control design, we will suppress the control input $u_t$ henceforth.\nAlso, without loss of generality, we consider a single-output system $p=1$ throughout this work.\nNote that we can easily handle multi-output systems by independently applying the proposed representation to each output dimension.\n\n\\subsection*{Learning with kernels and random features}\n\\newcommand{\\ensuremath{\\mathcal H}\\xspace}{\\ensuremath{\\mathcal H}\\xspace}\nKernel-based machine learning uses smooth function spaces to learn the true data-generating functions in nature based on empirical data.\nWe refer interested readers to \\cite{scholkopfLearningKernelsSupport2002,steinwartSupportVectorMachines2008}\nfor full coverage.\nIn the context of this paper, we focus on the regression setting, which can be viewed as the following approximation problem, also referred to as the risk minimization problem\n$$\n    \\min_{f\\in \\mathcal H}\\mathcal R (f):= \\int (f(x) - f^*(x))^2 \\mu(d x),\n$$\nfor some data-generating distribution $\\mu$.\nGiven a data set $\\mathcal{D} = \\{(x_i,y_i)\\}_{i=1}^T$, one can show using the representer theorem that the solution to the above variational problem admits a finite representation of the form\n$\n\\hat f=\\sum_{i=1}^T\\alpha_i k(x_i,\\cdot)\n$,\nwhere $k$ is a positive (semi-)definite kernel defined as a symmetric (real-valued) function, i.e., $\\sum_{i=1}^T \\sum_{j=1}^T a_i a_j k(x_i, x_j)\\ge 0$ for any $T \\in \\mathbb{N}$, $\\{ x_i \\}_{i=1}^T \\subset \\mathcal{X} \\subset \\mathbb{R}^p$, and $\\{a_i\\}_{i=1}^T \\subset \\mathbb{R}$. \n\nOne can also show that every positive definite kernel $k$ is associated with a Hilbert space \\ensuremath{\\mathcal H}\\xspace and a feature map $\\phi\\colon\n\\mathcal{X} \\to \\ensuremath{\\mathcal H}\\xspace$, for which $k(x,y) = \\langle \\phi(x), \\phi(y)\n\\rangle_\\ensuremath{\\mathcal H}\\xspace$ defines an inner product on $\\ensuremath{\\mathcal H}\\xspace$, where $\\ensuremath{\\mathcal H}\\xspace$ is a space of real-valued functions on $\\mathcal{X}$. The space $\\ensuremath{\\mathcal H}\\xspace$ is called a reproducing kernel Hilbert space (RKHS), equipped with the \\emph{reproducing property}: \n$f(x) = \\langle f, \\phi(x) \\rangle_\\ensuremath{\\mathcal H}\\xspace$ for any $f\\in \\ensuremath{\\mathcal H}\\xspace, x \\in\n\\mathcal{X}$.\nWe denote the canonical feature map as $\\phi(x):=k(x, \\cdot)$.\nOne computational concern of kernel methods is that forming the Gram matrix $K$ ($[K]_{i,j} := k(x_i, x_j)$) and performing operations with it (e.g., matrix inversion) is expensive. To alleviate that burden, we now overview the RF approach.\n\nThe RF framework seeks to approximate the kernel function $k(\\cdot, \\cdot)$ (and the corresponding RKHS functions) through a randomized finite feature map $\\hat{\\phi}(\\cdot)$ such that\n\\begin{equation}\n\t\\label{eq:rf}\nf(x) \\approx {f}_\\text{RF}(x)= w^\\top \\hat\\phi(x), \\ \\ k(x, x')\\approx\n\\sum_{i=1}^L \\hat\\phi_i(x)\\hat\\phi_i(x'),\n\\end{equation}\nwhere $\\{\\hat\\phi_i(x)\\}_{i=1}^L$ are the random features.\nConsider, for instance, random Fourier features (RFF)~\\cite{rahimiRandomFeaturesLargescale2008} to approximate stationary (i.e., shift-invariant) kernels, where\n$\n\\hat\\phi_i(x) = \\sqrt{2/L} \\cos(a_i^{\\top} x+b_i),\n$\nand the vector $a_i$ is sampled proportional to the kernel's spectral density and the offset as $b_i\\sim \\mathrm{Uniform}[0,2\\pi]$. Interestingly, we can obtain an approximation to the popular Gaussian RBF kernel\n$k (x,x')=e^{-{{\\|x-x'\\|}^2_2}/2l^2}$ by sampling\n$a_i\\sim \\mathcal{N}(0, l^{-2}\\mathbf{I}_p)$.\n\nOne strength of modern kernel methods is the richness of certain RKHSs.\nConcretely, \\emph{universal RKHSs}, e.g., that associated with the Gaussian RBF kernel\\xspace, are dense in the space of continuous functions that vanish at infinity, cf. \\cite{steinwartSupportVectorMachines2008,sriperumbudurUniversalityCharacteristicKernels2011}.\nThis density makes RKHSs an ideal choice for approximating functions.\nSince RFs approximate features in the RKHS, they are also universal function approximators suitable for learning dynamics functions.\n\nOne prominent use of RF in recent machine learning literature is in studying the properties of DNN, such as in analyzing the so-called \\emph{double descent} phenomenon~\\cite{belkinUnderstandDeepLearning2018}.\nThis is due to the fact that RF can be seen as a two-layer shallow neural network. For example, instead of the Fourier features, one may choose to construct the random ReLU feature (see, e.g., \\cite{belkinUnderstandDeepLearning2018,sunApproximationPropertiesRandom2019}) by choosing the feature as\n$\n\\hat\\phi_i(x) = \\max (0, a_i  x + b_i).\n$\nLater in this paper, we consider the neural-network perspective by viewing the dynamical system across multiple time steps as a deep neural network.\nWe refer to the references in Section~\\ref{sec:intro} and a recent survey \\cite{liuRandomFeaturesKernel2021} for a detailed coverage of RF.\n\n\\section{Uncertainty-aware random feature dynamics model}\n\\label{sec:model}\nWe now propose our main set-valued  dynamics model, the \\emph{uncertainty-aware random feature} (URF) dynamics model\n\\begin{multline}\n    \\mathbf{x}_{n+1} \\in \\mathcal F_\\textrm{URF}(\\mathbf{x}_n,\\wset):=\\\\\n    \\bigg\\{ h(\\mathbf{x}_n) + \\ensuremath{f(\\mathbf{x}_n, \\mathbf{w}_n)}\\xspace:\\mathbf{w}_n\\in\\wset\\bigg\\}.\n\\label{eq:dynamicsRF}\n\\end{multline}\nUsing the terminology of robust optimization, $\\wset$ is an uncertainty set, which can be data-driven and learned from data, or set by practitioners as a robustness tuning parameter.\nURF considers the parameter to lie within some ellipsoid given by\n\\begin{equation}\n\t\\wset := \\{\\mathbf{w} : (\\mathbf{w} - \\boldsymbol\\mu)^\\top \\boldsymbol\\Sigma^{-1} (\\mathbf{w} - \\boldsymbol\\mu) \\leq 1\\},\n\t\\label{eq:credible_interval_ddro}\n\\end{equation}\nfor some mean $\\mu$ and covariance matrix $\\Sigma$.\nAlternatively, we also discuss how to learn a lower-dimensional representation of \\wset in Section~\\ref{sec:pca}.\nIn the case that the uncertainty set shrinks to a singleton estimate $\\wset=\\{\\mu\\}$, we refer to the resulting model as \\emph{certainty-equivalent random feature} (CERF) dynamics model.\n\nPreviously, the authors of \\cite{mohammadiEstimationUncertainARX2015} considered ellipsoidal parameter uncertainty sets with linear dynamics.\nBy contrast, we lift the dynamics into an approximate RKHS by modeling the uncertain (nonlinear) dynamics part using RF\n\\begin{equation}\n\t\\label{eq:rfDef}\nf(x,w) := \\hat{\\boldsymbol{\\phi}}(\\mathbf{x})^\\top \\mathbf{w},\n\\end{equation}\nwhere $\\hat\\phi(\\cdot)$ is the random feature map; e.g., random ReLU feature, random Fourier features.\n\n\\begin{remark}\n\tOur URF model is equivalent to a two-layer Bayesian neural network (BNN) with an ellipsoidal uncertainty set for the weights.\n\tHowever, compared with typical BNN designs such as those using dropout, our ellipsoidal uncertainty set has the advantage of tractable optimization as we shall show.\n\t\n\tOur model~\\eqref{eq:dynamicsRF} can also be seen as a distributional robustness model~\\cite{delageDistributionallyRobustOptimization2010}, by constraining the future state distribution in an ambiguity set \n\t$$\n\tP_{x_{n+1}} \\in \\bigg\\{ f(\\cdot, w_n)_\\sharp P_{x_n}\\ :\\ {w}_n\\in\\wset\\bigg\\},\n\t$$\n\twhere $f(\\cdot, w_n)_\\sharp$ is a push-forward operator.\n\\end{remark}\n\n\\subsection{Worst-case dynamics via Pontryagin's minimum principle}\nIn this section, instead of deriving approximate uncertainty propagation for nonlinear dynamics, we focus on finding the worst-case (or analogously best-case) scenario with respect to a given cost function and its propagation through the system.\nSince, after all, the worst-case scenario is the key to robust control design.\nTo that end, we now explain how to characterize the condition for the worst-case realizations of the URF dynamics.\nGiven some overall cost objective $J$ of interest, e.g., accumulated cost in an OCP, and a horizon $N$ (see \\eqref{eq:ocpIntro}) we wish to find the worst-case realization of the dynamics by solving\n\\begin{multline}\n\t\\label{eg:argmaxJ}\n\\ensuremath{(w_0^*,w_1^*,\\dots ,w_{N-1}^*)}\\xspace =\\\\ \\argmax_{w_n\\in\\wset, n=0\\dots N-1} \\ensuremath{ J(w_0, w_1, \\dots, w_{N-1})}\\xspace.\n\\end{multline}\nIn the next section, we propose the framework to find the worst-case URF dynamics by characterizing the optimality condition via Pontryagin's minimum principle (PMP).\nNaturally, the worst-case cost ${ J(w^*_0, w^*_1, \\dots, w^*_{N-1})}$ can be used to stress test a system or certify certain controller design.\n\nDue to the nonlinear dynamics, solving for the worst-case\\xspace dynamics under URF model~\\eqref{eq:dynamicsRF} is not a convex optimization problem. However, the shallow structure of URF, and the reproducing property in general, is to view the dynamics as linear in a lifted space, e.g., an RKHS.\nWe now characterize the optimality condition for the worst-case dynamics in \\eqref{eg:argmaxJ} via the Pontryagin's minimum principle.\n\nLet us denote, with a slight abuse of notation, the total \\emph{negative} cost as $\\hat{J}(w_0, w_1, \\dots, w_{N-1})=\\sum_{n=1}^N \\hat{c}(x_n)$, where $\\hat{c}(x_n) = - c(x_n)$ denotes the negative of certain stage cost $c(\\cdot)$. We note that the maximization in \\eqref{eg:argmaxJ} can equivalently be formulated as the minimization of $\\hat{J}$. For what follows, we now define the control Hamiltonian function as\n\\[\nH(x,p,w) := \\hat{c}(x) +  p^\\top f(x,w),\n\\]\nwith $p \\in \\mathbb{R}^p$. For simplicity of the derivation, we omit the known part $h(x)$ of the model since it is independent of the variable $w$.\n\\begin{prop}[PMP for worst-case dynamics]\n\t\\label{thm:pmp}\n    Suppose the \\ensuremath{(w_0^*,w_1^*,\\dots ,w_{N-1}^*)}\\xspace is the set of worst-case realizations of the uncertainty set \\wset and \\ensuremath{(x_0^*,x_1^*,\\dots ,x_{N}^*)}\\xspace is the corresponding state trajectory.\n    Then there exist the co-state variables $\\ensuremath{(p_0^*,p_1^*,\\dots ,p_{N}^*)}\\xspace$ that satisfy the adjoint equations\n    \\begin{align*}\n        &p^*_{n} = \\nabla_x H(x^*_n,p^*_{n+1},w^*_n), & &p^*_N = \\nabla_x \\hat{c}(x^*_N).\n    \\end{align*}\n    Furthermore, the worst-case dynamics parameter \\ensuremath{(w_0^*,w_1^*,\\dots ,w_{N-1}^*)}\\xspace minimizes the Hamiltonian\n    \\begin{equation}\n        \\label{eq:maxHam}\n        w_n^* = \\argmin_{w\\in\\wset} H(x^*_n,p^*_{n+1},w), \\text{ for } n=0,\\dots,N-1.\n    \\end{equation}\n\\end{prop}\nThe PMP for worst-case\\xspace dynamics motivates us to find the worst-case\\xspace URF dynamics via an \\emph{indirect method} of optimal control, as illustrated in Algorithm~\\ref{alg:pmp}\\xspace. \n\\begin{algorithm}\n\t\\SetAlgoLined\n\tInitialize $w_0, w_1, \\dots, w_{N-1}$ (e.g., $w_i = \\boldsymbol{\\mu}$, see \\eqref{eq:credible_interval_ddro})\\\\\n\t\\For{$k = 0, 1, \\dots$}{\n\t\t\\textbf{Forward pass: shooting dynamics}\\\\\n        Initialize $x_0$\n        \\\\\n\t\t\\For{$n = 0$ \\KwTo $N-1$}{\n\t\t\t$x_{n+1} =  f(x_n,w_n)$ \\\\\n\t\t}\n\t\t\\textbf{Backward pass: adjoint equation}\\\\\n        Initialize $p_{N}=\\nabla \\hat{c}(x_{N})$\\\\\n\t\t\\For{$n = N-1$ \\KwTo $0$}{\n\t\t\t$p_{n} = \\nabla_x H(x_n, p_{n+1}, w_n)$ \\\\\n\t\t}\n        \\textbf{Update worst-case\\xspace dynamics}\\\\\n\t\t\\For{$n = 0$ \\KwTo $N-1$}{\n\t\t\tSet\n\t\t\t$w_n = \\argmin_{w\\in\\wset}\n\t\t\t{H}(x_n,p_{n+1},w)\n\t\t\t$\\label{step:ham}\\\\\n\t\t}\n\t}\n\t\\caption{PMP for worst-case\\xspace dynamics}\\label{alg:pmp}\n\\end{algorithm} \n\nA few comments are in order.\n    The Hamiltonian under URF dynamics is linear-in-parameter\\xspace, hence the objective of \\eqref{eq:maxHam} is linear.\n    Furthermore, Step~\\ref{step:ham} of Algorithm~\\ref{alg:pmp}\\xspace can be performed incrementally.\n    We will study the case where a conditional gradient descent step is used.\n    Under URF, the state distribution is never explicitly propagated through the nonlinear dynamics, which would be generally intractable.\n\nWe now study the updating formula for the worst-case\\xspace dynamics in Step~\\ref{step:ham} of Algorithm~\\ref{alg:pmp}\\xspace.\nWe write down the form of Hamiltonian minimization under URF dynamics\n\\begin{equation}\n\\label{eq:qclp}\n\\min_{w\\in\\wset}\\Big\\{H(x_n,p_{n+1},w) = \\hat{c}(x_n) +  p_{n+1}^\\top  \\hat\\phi(x_n)^\\top w\\Big\\},\n\\end{equation}\nwhere \\wset is an ellipsoid defined by some $\\mu$ and $\\Sigma$, as in \\eqref{eq:credible_interval_ddro}.\nThis optimization problem has a linear objective, a quadratic constraint in $w$, and can be efficiently solved by the following result.\n\\begin{prop}\n\t\\label{thm:minHam}\n\tUnder URF dynamics, the minimizer of the Hamiltonian in \\eqref{eq:qclp} is given by\n\t$$\n\tw^*_n= \\mu - \\frac{\\Sigma\\hat\\phi(x_n) p_{n+1} }{\\sqrt{p_{n+1}^\\top\\hat\\phi^\\top(x_n) \\Sigma\\hat\\phi(x_n) p_{n+1}}}, n=0,...,N-1.\n\t$$\n\\end{prop}\n\n\\begin{remark}\n    We can take the above argument beyond random features that are linear in parameters.\n    By merit of the reproducing property of RKHSs, the Hamiltonian can be written as \n    \\begin{equation}\n    \t\\label{eq:hamRKHS}\n        H(x,p,f) = \\hat{c}(x) +  p^\\top \\hip{f}{\\phi(x)},\n    \\end{equation}\n    which is linear in the RKHS function $f$, see Fig.~\\ref{fig:fw}.\n    Hence, our approach can be also viewed as a (constrained) functional gradient approach where the decision variable is a RKHS function. Note that such functional gradients have been used in the recent kernel methods literature \\cite{daiScalableKernelMethods2014,zhuKernelDistributionallyRobust2021} to optimize w.r.t. RKHS functions.\n\\end{remark}\n\n\\subsubsection*{Incremental update of Hamiltonian dynamics: equivalence to deep learning optimization}\nAn alternative way to perform the update in Step~\\ref{step:ham} of Algorithm~\\ref{alg:pmp}\\xspace is to only incrementally optimize the Hamiltonian. For example, recent works such as \\cite{liMaximumPrincipleBased2018} advocated for the incremental updates to avoid incurring large errors in the Hamiltonian dynamics for training DNN.\nEnabled by the linear-in-parameter\\xspace structure of URF dynamics, we now propose to perform incremental Hamiltonian minimization\n\\begin{equation}\n    \\label{eq:gradHam}  \n    \\begin{aligned}\n        \\bar{w_{n}} &=& &\\min_{w\\in\\wset} H(x_n,p_{n+1},w),\\\\\n        {w_{n}^+} &=& &w_n + \\gamma_k(\\bar{w_{n}} - w_n),\n    \\end{aligned}\n\\end{equation}\nfor some step size schedule $\\gamma_k$ and $n=0,...,N-1$.\nWe will refer to Algorithm~\\ref{alg:pmp}\\xspace with the incremental update~\\eqref{eq:gradHam} as the \\emph{inexact PMP}.\n\nWe now show that, the incremental minimization of Hamiltonian under URF dynamics is equivalent to performing the conditional gradient method, also known as the Frank-Wolfe\\space algorithm, on the weights of a DNN.\n\\begin{prop}[Equivalence of inexact PMP and Frank-Wolfe for deep learning]\n    \\label{thm:eqFrank}\n    Inexact PMP,\n    i.e.,\n    Algorithm~\\ref{alg:pmp}\\xspace with update step replaced by \\eqref{eq:gradHam},\n    is equivalent to performing Frank-Wolfe\\space algorithm on the total negative cost $\\hat{J}$\n    \\begin{equation}\n        \\label{eq:fwDnn}\n        \\begin{aligned}\n            %\n            %\n            \\bar{w_{n}} &=& &\\min_{w\\in\\wset} \\ensuremath{\\nabla_{w_n} \\hat{J}(w_0, w_1, \\dots, w_{N-1})}\\xspace ^\\top w,\\\\\n            {w_{n}^+} &=& &w_n + \\gamma_k(\\bar{w_{n}} - w_n),\n        \\end{aligned}\n    \\end{equation}\nFurthermore, Frank-Wolfe\\space algorithm with $\\gamma_k=1$ recovers the exact PMP solution.\n\\end{prop}\n\n\\begin{remark}\n\tNote that, if the dynamics is not linear-in-parameter\\xspace, the last statement of Proposition~\\ref{thm:eqFrank} does not hold.\n\tThat is the motivation of using PMP in the context of URF dynamics as a shallow representation.\n\tThis can also be seen as a manifestation of the reproducing property in Hamiltonian dynamics~\\eqref{eq:hamRKHS}.\n\\end{remark}\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.6\\columnwidth]{fig/fw.pdf}\n    \\put(-35,80){\\small{${H}(x_n,p_{n+1},w_n)$}} \n    \\put(-60,10){\\small{\\wset}} \n    \\caption{Illustration of one step of the Frank-Wolfe\\space algorithm, equivalent to inexact PMP. The Hamiltonian is depicted as a (salmon) plane considering its (affine) linear dependence on $w_n$. A particular uncertain set \\wset (turquoise) is shown together with its induced constraint on the Hamiltonian values (grey). The black arrow goes from the current parameter value $w_n$ (cross) to the next value $w_n^+$ (blue dot) in the Frank-Wolfe\\space minimization, as defined in \\eqref{eq:gradHam} and \\emph{equivalently} in \\eqref{eq:fwDnn}. }\n    \\label{fig:fw}\n\\vspace{-0.5cm}\n\\end{figure}\n    To the best of our knowledge, Proposition~\\ref{thm:eqFrank}\\xspace also constitutes a contribution to the current deep learning literature, aside from this paper's context of learning dynamics. \n    It characterizes the equivalence of constrained minimization of Hamiltonian via PMP and optimizing deep models via conditional gradient method, which generalizes the results in \\cite{liMaximumPrincipleBased2018,zhangYouOnlyPropagate2019}.\n    Compared with those works, our focus is on finding the worst-case\\xspace dynamics and our result applies to constrained optimization.\n\nIn summary, Proposition~\\ref{thm:eqFrank} tells us that, to compute the worst-case\\xspace dynamics, it suffices to perform conditional gradient method\\ on the DNN induced by the URF dynamics.\nNote that we can likewise compute the best-case dynamics.\nFurthermore, if we use code libraries based on computational graphs, such as PyTorch, the backward pass does not need to be implemented explicitly. We remark that the previous results assume a given uncertainty set $\\wset$. Next we describe an approach to estimate $\\wset$ in a data-driven fashion.\n\n\\section{Dynamics learning algorithms}\n\\label{sec:learnSet}\nIn this section we specify the details of learning the URF model~\\eqref{eq:dynamicsRF} proposed in the previous section.\nIn addition, we provide a complementary dimensionality-reduction procedure based on nonlinear principal component analysis (PCA).\n\n\\subsection{Learning uncertainty set \\wset via Bayesian linear regression (BLR)}\n\\label{ssec:blr}\nWe estimate the uncertainty set \\wset in a data-driven fashion via BLR and ellipsoidal credible bounded regions. We assume access to observations of the full state of the system in $\\eqref{eq:dynamics}$ together with a noisy version of the consecutive state, following common practice \\cite{kollerLearningbasedModelPredictive2018,hewingCautiousModelPredictive2020}.\nThese observations form a dataset\n$\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^T$\nwith\n\\begin{equation}\ny_i = h(x_i) + f(x_i) + \\epsilon_i,\\quad \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\n\\label{eq:original_dataset}\n\\end{equation}\nwhere $\\epsilon_i$ accounts for iid Gaussian noise realizations. Using the proposed RF dynamics in \\eqref{eq:rfDef}, we can rewrite \\eqref{eq:original_dataset} as\n\\begin{equation}\n\\hat{y}_i = \\hat{\\boldsymbol{\\phi}}(\\mathbf{x}_i)^\\top w + \\epsilon_i,\n\\label{eq:rf_dataset}\n\\end{equation}\nwhere in the l.h.s. we have now the residuals $\\hat{y}_i = y_i - h(x_i)$ instead.\nWe highlight that a particular realization of the weights $w' \\in \\mathbb{R}^L$ induces in turn a \\emph{deterministic} dynamics model $f(\\cdot, w')$. \nTaking a Bayesian perspective, we assume a standard Gaussian prior over the feature weights $w \\sim \\mathcal{N}(0, \\mathbf{I}_L)$, which can be shown to yield a $L$-dimensional GP approximation~\\cite{lazaro-gredillaSparseSpectrumGaussian2010, rasmussenGaussianProcessesMachine2005}. \nThen \\eqref{eq:rf_dataset} becomes a linear-Gaussian model in which posterior inference over $w$ is computationally tractable. In fact, conditioned on the observed data $\\mathcal{D}$, the posterior over weights $p(w|\\mathcal{D})$ is also Gaussian with parameters\n\\begin{equation}\n\\begin{split}\n    \\boldsymbol\\mu_{\\boldsymbol\\omega | \\mathcal{D}} &:= (\\boldsymbol\\Phi(X)^\\top \\boldsymbol\\Phi(X) + \\sigma^2 \\mathbf{I}_L)^{-1} \\boldsymbol\\Phi(X)^\\top \\mathbf{y},\\\\\n    \\boldsymbol\\Sigma_{\\boldsymbol\\omega | \\mathcal{D}} &:= (\\boldsymbol\\Phi(X)^\\top \\boldsymbol\\Phi(X) + \\sigma^2 \\mathbf{I}_L)^{-1} \\sigma^2,\n\\end{split}\n\\label{eq:blr_posterior}\n\\end{equation}\nwhere $\\boldsymbol\\Phi(X) \\coloneqq [\\hat{\\boldsymbol\\phi}(\\mathbf{x}_1), \\hat{\\boldsymbol\\phi}(\\mathbf{x}_2),\\dots, \\hat{\\boldsymbol\\phi}(\\mathbf{x}_T)]^\\top$ denotes the RF evaluated at the training inputs.\nNote that it is also possible to perform the posterior update incrementally as new data becomes available, i.e., with streaming data.\n\n\nTo construct the data-driven ellipsoidal set we rely on credible bounded regions,\nwithin which the unknown $w$ value falls with high probability after seeing the data set $\\mathcal{D}$. Formally, we define a posterior credible region for $w$ given a probability level $\\alpha_w$ as\n\\begin{equation}\n\\wset =  \\{\\mathbf{w} \\in \\mathbb{R}^L : (\\mathbf{w} - \\boldsymbol\\mu_{\\boldsymbol\\omega | \\mathcal{D}})^\\top \\boldsymbol\\Sigma_{\\boldsymbol\\omega | \\mathcal{D}}^{-1} (\\mathbf{w} - \\boldsymbol\\mu_{\\boldsymbol\\omega | \\mathcal{D}}) \\leq \\chi_L^2(\\alpha_{\\boldsymbol\\omega})\\},\n\\label{eq:credible_interval}\n\\end{equation}\nwhere $\\chi_L^2(\\cdot)$ is the quantile function of the chi-squared distribution with $L$ degrees of freedom.\nIntuitively, \\wset captures a high-density ellipsoidal region of the posterior $p(w|\\mathcal{D})$ whose size is controlled through $\\alpha_w$.\nWe have thus obtained a data-driven characterization of the uncertainty set \\wset for the URF dynamics~\\eqref{eq:dynamicsRF}.\n\n\\subsection{Lower-dimensional representation using random feature nonlinear component analysis}\n\\label{sec:pca}\n\\newcommand{\\ensuremath{\\Phi(X) \\Phi(X)^\\top}\\xspace}{\\ensuremath{\\Phi(X) \\Phi(X)^\\top}\\xspace}\nIt is well-known that, for some common kernels, the kernel Gram matrix $K$ has special eigenspectrum structure (see \\cite{zhuGaussianRegressionOptimal1997,rasmussenGaussianProcessesMachine2005} for the classical results and \\cite{belkinApproximationBeatsConcentration2018} for alternative characterizations).\nFor example, the eigenvalues of Gaussian RBF kernel\\xspace Gram matrices decay at an exponential rate.\nAs the RFF approximate the feature maps associated with the RKHS of the Gaussian RBF kernel\\xspace, we can expect the data Gram matrix \\ensuremath{\\Phi(X) \\Phi(X)^\\top}\\xspace to have rapidly decaying eigenvalues.\nIntuitively, this gives us the power to capture high-dimensional data in its lower dimensional (shallow) representation, such as by using kernel PCA.\nWe now show how to exploit this structure to learn a \\emph{lower-dimensional representation} of the URF dynamics.\n\nGiven the RF representation of the data, we perform PCA on the Gram matrix \\ensuremath{\\Phi(X) \\Phi(X)^\\top}\\xspace to obtain the lower-dimensional representation, which we denote as\n$\n\\hat{\\psi}(x): = P \\hat{\\phi}(x)\n$,\nwhere $P$ is a PCA projection matrix of size $\\hat{L} \\times L$ obtained, e.g., by performing singular value decomposition of the  Gram matrix. Note that $\\hat{L}$ denotes the new dimension of the feature-based representation of the dynamics and therefore we choose it such that $\\hat{L} \\ll L$.\nAs a consequence, our URF dynamics in \\eqref{eq:rfDef} can be alternatively constructed using the \\emph{lower dimensional representation} as\n$$\nf_{\\text{PCA}}(x,w) \\coloneqq  \\hat{\\psi}(x)^\\top w = \\hat{\\phi}(x)^\\top P^\\top w,\n$$\nwhere $w \\in \\mathbb{R}^{\\hat{L}}$.\nWe highlight that the BLR algorithm described in section \\ref{ssec:blr} can seamlessly incorporate the lower dimensional representation by using $\\boldsymbol\\Psi(X) \\coloneqq [\\hat{\\boldsymbol\\psi}(\\mathbf{x}_1), \\hat{\\boldsymbol\\psi}(\\mathbf{x}_2),\\dots, \\hat{\\boldsymbol\\psi}(\\mathbf{x}_T)]^\\top$ instead of $\\boldsymbol\\Phi(X)$ in \\eqref{eq:blr_posterior}.\nA rigorous statistical analysis of the approximation error is beyond our current scope, for which we refer to \\cite{lopez-pazRandomizedNonlinearComponent2014} and a comprehensive survey \\cite[Section~5.3]{liuRandomFeaturesKernel2021}.\nNote that it is also possible to perform PCA without explicitly forming the Gram matrix via tailored numerical methods.\n\n\\begin{figure}\n    \\subfloat[Source-spiral]{\\includegraphics[width=0.33\\linewidth]{fig/cost_spiral.pdf}}\n    \\subfloat[Van der Pol]{\\includegraphics[width=0.33\\linewidth]{fig/cost_vdp.pdf}}\n    \\subfloat[Pendulum]{\\includegraphics[width=0.33\\linewidth]{fig/cost_pend.pdf}}\n    \\caption{Cost estimation of a fixed test trajectory (i.e., under a fixed initial condition) with the proposed URF model as a function of the number of training trajectories for three different systems. In addition to the mean model's prediction (blue) we depict the computed best (green) and worst (red) cost yielded by plausible dynamics within the inferred uncertainty sets (Algorithm \\ref{alg:pmp}). Note that in the small-data regime the mean estimate might be biased, however the true cost is still contained in the range induced by the found best and worst values. The latter suggests that our URF model successfully captures the uncertainty due to limited training data. Both the uncertainty set and its induced best-worst range shrink as more training trajectories are used, converging to the true cost for large enough training datasets.}\n    \\label{fig:cost_fig}\n\\end{figure}\n\n\\section{Numerical experiment}\n\\label{sec:exp}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{fig/spiral_independent_w.pdf}\n    \\caption{Worst-case dynamics optimization for different training data regimes using a nonlinear plant of the form $\\mathbf{x}_{t+1} = \\mathbf{A} \\mathbf{x}_t + \\cos (\\mathbf{B}\\mathbf{x}_t + \\mathbf{c})$. The left column depicts the scarce training data setting ($N=5$ rollouts of length $T=50$ each) and the right column accounts for the large training data regime ($N=200$). \\textbf{First row}: the predicted state mean trajectory (blue) is plotted alongside the true trajectory (black). Plausible state trajectories under the uncertainty set are also sampled uniformly and shown in light blue forming an \\textit{uncertainty tube} around the mean prediction. \\textbf{Bottom rows}: worst-case dynamics optimization for different schedulings of the learning rate $\\gamma_k$ in \\eqref{eq:gradHam}. Lighter green denotes earlier iterations of the optimization with the final result being shown in dark green. Remarkably, the \\textit{uncertainty tube} shrinks as more training data is used for learning and the obtained worst-case trajectory is therefore closer to the mean trajectory for larger training datasets. Also note that the optimization is strongly dependent on the learning rate scheduling with the standard FW scheduling yielding better optimization performance in our experiments.\n    }\n    \\label{fig:worst_dyn_opt}\n\\end{figure}\n\nWe illustrate the learning of URF dynamics in multiple autonomous discrete systems and showcase the possibility of finding the worst-case (and best-case) dynamics given a certain cost function. These examples enable us to study the influence of the size of the training sets in our data-driven characterization of the uncertainty. Furthermore, we explore different instances of the worst-case optimization procedure described in Algorithm \\ref{alg:pmp}.\n\nIn all experiments we assume limited prior information about the underlying system and therefore our model consists of an identity nominal component $h(\\mathbf{x}_t) = \\mathbf{x}_t$. The residual $\\mathbf{x}_{t+1} - h(\\mathbf{x}_t)$ is then learned using the proposed URF model using as training data a varying number of randomly sampled trajectories ($N$) of fixed length ($T$). After model learning we can (i) predict future state trajectories given initial conditions, (ii) get an uncertainty \\textit{tube} induced by the posterior distribution over the feature weights $\\omega$ and (iii) find an adversarial realization of the state trajectories under such uncertainty for a particular task at hand; i.e., given a cost function $c(x_t)$ and a confidence level $\\alpha_{\\boldsymbol\\omega}$. We depict graphically these capabilities in Figs. \\ref{fig:worst_dyn_opt}, \\ref{fig:pend_vdp} for three different nonlinear systems. In all our experiments we use $L=1000$ Fourier RF and apply RF nonlinear component analysis (Section \\ref{sec:pca}) to obtain a lower-dimensional representation ($\\hat{L}=100$).\n\nFirst we consider a nonlinear plant of the form $\\mathbf{x}_{t+1} = \\mathbf{A} \\mathbf{x}_t + \\cos (\\mathbf{B}\\mathbf{x}_t + \\mathbf{c})$, where $\\mathbf{A}$ is fixed such that the resulting plot exhibits a source-spiral pattern as shown in Fig. \\ref{fig:worst_dyn_opt}, and the affine mapping ($\\mathbf{B}$,$\\mathbf{c}$) is randomly sampled. We assume as cost function the quadratic form $c(x_t) = x_t^{\\top} x_t$ and sample initial conditions according to $\\mathbf{x}_0 \\sim \\mathcal{N}(0, \\mathbf{I}_2)$.\n\nWe also learn a Van der Pol oscillator governed by the second-order differential equation $\\dot{x}_1=(1 - x_2^2)x_1 - x_2, \\dot{x}_2=x_1$ and use an explicit Runge-Kutta integrator for its discrete-time simulation. We sample initial conditions from $\\mathcal{U}(-1,1)$ for training data generation. The URF model's prediction and obtained worst-case trajectory are shown in the top row of Fig. \\ref{fig:pend_vdp} for the quadratic cost $c(x_t) = x_t^{\\top} x_t$.\n\nWe finally consider a pendulum system under friction whose continuous-time dynamics is $\\dot{x}_1=x_2, \\dot{x}_2= -(g/l) \\sin{x_1} -(\\beta/(ml^2))x_2$, where the gravity is set to $g=9.81$; and the mass $m$, the rod length $l$ and the friction parameter $\\beta$ are all set to 1. In training trajectories we sample the initial pendulum's angle from $\\mathcal{U}(-\\pi, \\pi)$ and the initial angular velocity from $\\mathcal{U}(-1, 1)$. We use semi-implicit Euler integration in order to get a discrete-time approximation of this system. To ease implementation we map the pendulum's state to a representation $\\hat{x} = (a, b, c) = (l \\cos x_1, l \\sin x_1, x_2)$, where the pendulum's position is expressed in Cartesian coordinates, before learning the URF dynamics. The cost function is then defined as $c(\\hat{x}) = b^2 -a + 0.1 c^2$, which encourages the pendulum to stay upright. Results are likewise graphically depicted in the bottom row of Fig. \\ref{fig:pend_vdp}.\n\nMoreover, we explore the role of the learning rate in the Hamiltonian maximization \\eqref{eq:gradHam} for finding the worst plausible dynamics under a certain cost. In Fig. \\ref{fig:worst_dyn_opt} we consider three variations in the scheduling of said learning rate in the Frank-Wolfe algorithm, namely the standard Frank-Wolfe scheduling $\\gamma_k = 2 / (k + 2)$, $\\gamma_k = 1$ (i.e., full steps) which implies that the optimization successively moves along the uncertainty set boundary and $\\gamma_k = 1/F$, with $F$ being the total number of optimization steps. The optimization behavior is strongly dependent on the used learning rate schedule, with the standard Frank-Wolfe scheduling yielding the highest cost (i.e., the worse) dynamics for the performed experiments. \n\nWe highlight that in the absence of large amounts of training data, the computed uncertainty set contains dynamics that could drive the system to high cost regions, as shown in the left columns of Figs. \\ref{fig:worst_dyn_opt}, \\ref{fig:pend_vdp}. However, once the inferred uncertainty set is reduced as a result of a larger training data set, the obtained worst-case dynamics optimization cost is closer to the cost incurred by the certainty-equivalent model (i.e., the mean model), which is consistent with the fact that in the limit of infinite data both worst and mean cost should converge to the same value since the uncertainty set will shrink to a singleton.\n\nA finer-grained analysis of the previous point is presented in Fig. \\ref{fig:cost_fig}, where we see that both the computed worst and best costs define a region (i.e., a tube) containing the true cost across different training data sizes and systems. Note that the gap between the worst, best, mean and true cost vanishes after enough training rollouts have been used. However, we emphasize that although we might get poor cost (mean) predictions under scarce training data (e.g., right column of Fig. \\ref{fig:cost_fig}), the true cost is always within the region defined by the best- and worst-case curves. This hints that our method might be useful in robust control designs where the uncertainty from limited training data is taken into account.\n\n\\begin{figure}\n    \\centering\\subfloat[Van der Pol oscillator]{\\includegraphics[width=0.8\\linewidth]{fig/vdp_independent_w.pdf}}\\\\\n    \\centering\\subfloat[Damped Pendulum]{\\includegraphics[width=0.73\\linewidth]{fig/pendulum_independent_w.pdf}}\n    \\caption{Mean URF model prediction (blue) along with computed worst-case trajectory (green) for two nonlinear systems: the Van der Pol oscillator (\\textbf{top row}) and the damped pendulum (\\textbf{bottom row}). We consider both the scarce (\\textbf{left column}) and large (\\textbf{right column}) training data regimes and use the standard Frank-Wolfe learning rate scheduling for worst-case optimization. Note that under high uncertainty the worst-case trajectory might yield much higher cost than the true trajectory, whereas in the case of less uncertain model fits (i.e., large-data regime) even the worst case approaches the true trajectory.}\n    \\label{fig:pend_vdp}\n\\end{figure}\n\n\\section{Other related works}\n\\label{sec:related}\nOur use of Pontryagin's minimum principle\\xspace for finding the worst-case\\xspace URF dynamics is similar to the use of adjoint sensitivity in robust nonlinear optimization \\cite{diehlApproximationTechniqueRobust2006}.\n\nOur shallow URF model is in contrast with a large body of deep learning literature that employ DNN as dynamics model, for which we refer to \\cite{chuaDeepReinforcementLearning2018} for references.\nAs we have demonstrated, shallow URF models are rich enough to approximate general dynamics function while allowing the linear Hamiltonian structure.\nOur perspective, which views the whole dynamical system as a DNN instead of the one-step dynamics model, is also consistent with recent mathematically principled analysis for deep learning via gradient flows, e.g., \\cite{eeMachineLearningContinuous2020}.\n\nThe authors of \\cite{zhangYouOnlyPropagate2019} used PMP for deep adversarial learning, but with no equivalence theorems for the constrained optimization.\nMoreover, they used gradient projection algorithms which typically perform poorly in nonlinear programs. In \\cite{kamtheDataefficientReinforcementLearning2018} a PMP-based analysis is presented for MPC with GP dynamics, however they rely on heuristic uncertainty propagation.\n\nRF is closely related to the GP literature for dynamics learning.\nThe use of Fourier RF for general GP regression is known as Sparse Spectrum GP Regression \\cite{lazaro-gredillaSparseSpectrumGaussian2010} and has been previously considered to learn dynamical system for filtering and control problems \\cite{panPredictionUncertaintySparse2017}.\nThe authors of \\cite{arcariMetaLearningMPC2020} used finite functional basis expansion for meta-learning, which is in the same spirit as RF.\nRF has been also recently used to sample from GP dynamics \\cite{hewingSimulationTrajectoryPrediction2020} and to devise more efficient general-purpose GP sampling algorithms~\\cite{wilsonEfficientlySamplingFunctions2020}.\n\n\\section{Discussion and future work}\n\\label{sec:conclude}\n\nIn this paper we have proposed the uncertainty-aware random feature (URF) model for learning dynamical systems.\nOur idea is to learn the one-step dynamics model as a shallow neural net, while the whole system can be seen as a deep net.\nExploiting the lifting structure of RKHS functions and RF, we propose a Pontryagin's minimum principle\\xspace-based numerical algorithm to find the worst-case scenario of the dynamics model.\nWe further show that our approach is equivalent to performing the conditional gradient method~(Franke-Wolfe) on a DNN.\nVarious numerical experiments validate the power of the proposed URF model.\n\nIn the future, we look forward to applying our dynamics model in data-driven robust control design, such as learning-based MPC. Another direction is to explore RF models in the direction of distributionally robust optimization and control. This is indeed possible as universal RKHSs have been shown to be effective tools for enforcing distributional robustness in optimization~\\cite{zhuKernelDistributionallyRobust2021}.\n\n\\section{Proofs}\n\\label{sec:pf}\n\\subsection{Proof of Proposition~\\ref{thm:pmp}}\n\\begin{proof}\nThe starting point is by viewing the dynamics as an \\emph{adversarial player} in a two-player zero-sum game.\nThen, the worst-case dynamics parameter $w_t$ is seen as the control input to the system dynamics $f(x_t,w_t)$.\nConsequently , the proposition follows from the standard discrete-time Pontryagin's minimum principle; see, e.g., \\cite[Volume~I, 4th Edition]{bertsekasDynamicProgrammingOptimal1995}.\n\\end{proof}\n\\subsection{Proof of Proposition~\\ref{thm:minHam}}\nIn the following, we use $\\Sigma^{1/2}$ to denote the Cholesky factor of the positive (semi-)definite matrix $\\Sigma$.\n\\begin{proof}\nWe restate \\eqref{eq:qclp} for convenience.\n\\begin{equation*}\n    \\begin{aligned}\n        \\min_{{(w-\\mu)}^T\\boldsymbol\\Sigma^{-1}{(w-\\mu)} \\leq 1} \n        \\hat{c}(x_n) +  p_{n+1}^\\top  \\hat{\\phi}(x_n)^\\top w.\n    \\end{aligned}\n\\end{equation*}\nWe use a change of variable $\\Sigma^{1/2}v = w - \\mu$, resulting in the optimization problem\n\\begin{equation*}\n    \\begin{aligned}\n        \\min_{{v}^Tv \\leq 1} \n        \\hat{c}(x_n) + \\ensuremath{p_{n+1}^\\top  \\hat{\\phi}(x_n)^\\top }\\xspace \\mu+ \\ensuremath{p_{n+1}^\\top  \\hat{\\phi}(x_n)^\\top \\Sigma^{1/2}}\\xspace  v,\n    \\end{aligned}\n\\end{equation*}\nwhere the first two terms are independent of the decision variable.\nThen, we use the Cauchy-Schwarz inequality and the fact that ${v}^Tv \\leq 1$ to write\n$$\n\\ensuremath{p_{n+1}^\\top  \\hat{\\phi}(x_n)^\\top \\Sigma^{1/2}}\\xspace   v\\geq - \\|\\ensuremath{(\\Sigma^{1/2})^\\top   \\hat{\\phi}(x_n) p_{n+1}}\\xspace \\|_2,\n$$\nwhere the minimum is attained at\n$$\nv^* = - \\frac{\\ensuremath{(\\Sigma^{1/2})^\\top   \\hat{\\phi}(x_n) p_{n+1}}\\xspace }{\\|\\ensuremath{(\\Sigma^{1/2})^\\top   \\hat{\\phi}(x_n) p_{n+1}}\\xspace \\|_2}.\n$$\nHence, the minimizer of the Hamiltonian is given by\n$$\nw^* = \\mu + \\Sigma^{1/2}v^*  = \\mu - \\frac{\\Sigma \\hat{\\phi}(x_n) p_{n+1} }{\\|\\ensuremath{(\\Sigma^{1/2})^\\top   \\hat{\\phi}(x_n) p_{n+1}}\\xspace \\|_2}.\n$$\n\\end{proof}\n\\subsection{Proof of Proposition~\\ref{thm:eqFrank}}\n\\begin{proof}\n    It is an exercise (e.g., using \\cite[Proposition~5]{liMaximumPrincipleBased2018}) to show\n    \\begin{equation}\n        \\label{eq:JeqH}\n        \\begin{aligned}\n            \\ensuremath{\\nabla_{w_n} \\hat{J}(w_0, w_1, \\dots, w_{N-1})}\\xspace &= \\ensuremath{\\nabla_{w_n} H(x_n,p_{n+1},w_n)}\\xspace\\\\\n            &= \\ensuremath{\\nabla_{w_n} f(x_n, w_n)}\\xspace p_{n+1}\\\\\n            &= \\hat{\\phi}(x_n) p_{n+1}.\n        \\end{aligned}\n    \\end{equation}\nThe first step of \\eqref{eq:fwDnn} has the closed-form solution\n$$\n\\bar w_n = \\mu - \\frac{\\Sigma\\ensuremath{\\nabla_{w_n} \\hat{J}(w_0, w_1, \\dots, w_{N-1})}\\xspace }{\\|(\\Sigma^{1/2})^\\top \\ensuremath{\\nabla_{w_n} \\hat{J}(w_0, w_1, \\dots, w_{N-1})}\\xspace\\|_2}.\n$$\nPlugging \\eqref{eq:JeqH} into the above expression and noting Equation~\\ref{eq:qclp}, we exactly recover \\eqref{eq:gradHam}, which is the inexact PMP.\n\nIf $\\gamma_k=1$, then the exact minimization of Hamiltonian is performed under linear dynamics. Hence, we recover exact PMP.\n\\end{proof}\n\n\\section*{Acknowledgment}\nThe authors thank Alexandra Gessner and Wittawat Jitkrittum for the helpful discussions. This work was supported by the German Federal Ministry of Education and Research (BMBF) through the T\u00fcbingen AI Center (FKZ: 01IS18039B).\n\n\n\n\\bibliographystyle{IEEEtran}\n", "meta": {"timestamp": "2021-06-25T02:20:44", "yymm": "2106", "arxiv_id": "2106.13066", "language": "en", "url": "https://arxiv.org/abs/2106.13066"}}
{"text": "\\section{Review of previous work: Approximating gradients with approximate predictive coding}\n\n\n\n The backpropagation algorithm and its variants are widely used to train artificial neural networks. While artificial and biological neural networks share some common features, a direct implementation of backpropagation in the brain is often considered biologically implausible in part because of the nonlocal nature of parameter updates: The update to a parameters in one layer depends on activity in all deeper layers. In contrast, biological neural networks are believed to learn largely through local synaptic plasticity rules for which changes to a synaptic weight depend on neural activity local to that synapse. Backpropagation can be performed using local updates if gradients of neurons' activations are passed upstream through feedback connections, but this interpretation implies other biologically implausible properties of the network, like symmetric feedforward and feedback weights. See previous work~\\cite{lillicrap2020backpropagation,whittington2019theories} for a more complete review of the biological plausibility of backpropagation.\n\nSeveral approaches have been proposed for achieving or approximating backpropagation with more biologically realistic learning rules~\\cite{urbanczik2014learning,lillicrap2016random,scellier2017equilibrium,lillicrap2020backpropagation,whittington2019theories,aljadeff2019cortical,kunin2020two,payeur2021burst,clark2021credit,millidge2020predictive,whittington2017approximation,salvatori2021predictive,song2020can}. One such approach~\\cite{millidge2020predictive,whittington2017approximation,salvatori2021predictive,song2020can} is derived from the theory of ``predictive coding'' or ``predictive processing''~\\cite{rao1999predictive,friston2010free,huang2011predictive,bastos2012canonical,clark2015surfing,buckley2017free,bogacz2017tutorial,spratling2017review,keller2018predictive}. When applied to artificial neural networks trained on supervised learning tasks, predictive coding can produce weight updates that are similar to or the same as the exact gradients computed by backpropagation~\\cite{millidge2020predictive,whittington2017approximation,salvatori2021predictive,song2020can}. \n\nIn this manuscript, I first extensively review this previous work~\\cite{millidge2020predictive,whittington2017approximation,salvatori2021predictive,song2020can} from an algorithmic perspective and explore its implications on empirical examples. I next prove a theorem that modestly extends results from this previous work and discuss some implications of these results on the interpretation of predictive coding and artificial neural networks as models of biological learning. Finally, I introduce a repository of Python functions, \\texttt{Torch2PC}, that can be used to perform predictive coding on any PyTorch Sequential model. \\texttt{Torch2PC} can be found at\\\\\n\\url{https://github.com/RobertRosenbaum/Torch2PC}\\\\\nA Google Drive folder with Colab notebooks that produce all figures in this text can be found at\\\\ \\url{https://drive.google.com/drive/folders/1m_y0G_sTF-pV9pd2_sysWt1nvRvHYzX0}\\\\ \nA copy of the same code is also stored at\\\\ \\url{https://github.com/RobertRosenbaum/PredictiveCodingVsBackProp}\n\n\n\n\n\n\n\\section{A review of the relationship between backpropagation and predictive coding from previous work}\n\n\nFor completeness, let us first review the backpropagation algorithm. Consider a feedforward deep neural network (DNN) defined by \n\\begin{equation}\\label{E:fwdpass}\n\\begin{aligned}\n\\hat v_0&=x\\\\\n\\hat v_{\\ell}&=f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell),\\;\\; \\ell=1,\\ldots,L\n\\end{aligned}\n\\end{equation}\nwhere each $\\hat v_\\ell$ is a vector or tensor of activations, each $\\theta_\\ell$ is a set of parameters for layer $\\ell$, and $L$ is the network's depth. \nIn supervised learning, we seek to minimize a loss function ${\\mathcal L}(\\hat y,y)$ where $y$ is a label associated with input, $x$, and \n\\[\n\\hat y=f(x;\\theta)=\\hat v_L\n\\]\nis the network's output, which depends on parameters $\\theta=\\{\\theta_\\ell\\}_\\ell$. \nThe loss is typically minimized using gradient-based optimization methods with gradients computed using automatic differentiation tools based on the backpropagation algorithm. For completeness,  backpropagation  is reviewed in the pseudocode below.\n\n\\begin{algorithm}[H]\n\\caption{A standard implementation of backpropagation. }\\label{A:backprop}\n \\vspace{.04in}\n\\begin{algorithmic\n\\State  {\\bf Given:} Input ($x$) and label ($y$)\n\\vspace{.04in} \n\\State {\\color{gray}\\# forward pass}\n\\State $\\hat v_0=x$\n\\For{$\\ell=1,\\ldots,L$}\n            \\State $\\hat v_\\ell=f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)$\n\\EndFor\n\n\\vspace{.04in} \n\\State {\\color{gray}\\# backward pass}\n\\State $\\delta_L=\\tfrac{\\partial {\\mathcal L}(\\hat v_L,y)}{\\partial  \\hat v_L}$\n\n\\For{$\\ell=L-1,\\ldots,1$}\n\t\t\\State $\\delta_\\ell=\\delta_{\\ell+1}\\tfrac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_\\ell}$\t\t\n\t\t\\State $d\\theta_\\ell=-\\delta_\\ell\\tfrac{\\partial f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)}{\\partial \\theta_\\ell}$\\vspace{.05in}\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\n\nA direct application of the chain rule and mathematical induction shows that backpropagation computes the gradients,\n\\[\n\\delta_\\ell=\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\hat v_\\ell}\\;\\textrm{ and }\\;\nd\\theta_\\ell=-\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial  \\theta_\\ell}.\n\\]\nThe negative gradients, $d\\theta_\\ell$, are then used to update parameters, either directly for stochastic gradient descent or indirectly for other gradient-based learning methods~\\cite{goodfellow2016deep}. \n\n\n\nI next review algorithms derived from the theory of predictive coding and their relationship to backpropagation. \nI focus on the approach described in~\\cite{millidge2020predictive}, but the approach in other work is similar~\\cite{whittington2017approximation,salvatori2021predictive,song2020can}. For clarity, I consider the special case of a sequential, feedforward neural network (as in Eq.~\\eqref{E:fwdpass} and Algorithm~\\ref{A:backprop}), whereas the approach in~\\cite{millidge2020predictive} applies to the more general case of networks that can be represented by directed acyclic graphs.\n\n\n\\subsection{A strict interpretation of predictive coding does not accurately compute gradients.}\n\nPredictive coding can be derived from a hierarchical, Gaussian probabilistic model in which each layer, $\\ell$, is associated with  a Gaussian random variable, $V_\\ell$, satisfying\n\\[\n\\begin{aligned}\np(V_\\ell=v_\\ell\\,|\\,V_{\\ell-1}= v_{\\ell-1})=\\mathcal N(v_\\ell\\, ; \\,\\,f_\\ell( v_{\\ell-1};\\theta_\\ell),\\;\\Sigma_\\ell)\n\\end{aligned}\n\\]\nwhere $\\mathcal N(v;\\,\\mu,\\Sigma)\\propto \\exp(-[v-\\mu]^T\\Sigma^{-1}[v-\\mu]/2)$ is the multivariate Gaussian distribution with mean, $\\mu$, and covariance matrix, $\\Sigma$, evaluated at $v$. Following previous work~\\cite{whittington2017approximation,millidge2020predictive,salvatori2021predictive,song2020can}, I take $\\Sigma=I$ to be the identity matrix, but later discuss the potential implications of relaxing this assumption~\\cite{bogacz2017tutorial}. \n\nIf we condition on an observed input, $V_0=x$, then a forward pass through the network described by Eq.~\\eqref{E:fwdpass} corresponds to setting $\\hat v_0=x$ and then sequentially computing the conditional expectations or, equivalently, maximizing conditional probabilities, \n\\[\n\\begin{aligned}\n\\hat v_\\ell&=E[V_\\ell\\,|\\,V_{\\ell-1}=\\hat v_{\\ell-1}]\\\\\n&=\\operatornamewithlimits{argmax}_{v_\\ell} p(V_\\ell=v_\\ell\\,|\\,V_{\\ell-1}=\\hat v_{\\ell-1})\\\\\n&=f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)\n\\end{aligned}\n\\]\nuntil reaching an inferred output, $\\hat y=\\hat v_L$. \nNote that this forward pass does not necessarily maximize the global conditional probability, $p(V_L=\\hat y\\,|\\,v_0=x)$ and it does not account for any prior belief about $V_L$. \n\n\n\n\nOne interpretation of a forward pass is that each $\\hat v_\\ell$ is the network's ``belief'' about the state of $V_\\ell$, when only $V_0=x$ has been observed. \nNow suppose that we condition on both an observed input, $V_0=x$, {\\it and} its label, $V_L=y$. In this case, generating beliefs about the hidden states, $V_\\ell$, is more difficult because we need to account for potentially conflicting information at each end of the network. We can proceed by initializing a set of beliefs, $v_\\ell$, about the state of each $V_\\ell$, and then updating our initial beliefs to be more consistent with the observations, $x$ and $y$, and  parameters, $\\theta_\\ell$. \n\nThe error made by a set of beliefs, $\\{v_\\ell\\}_\\ell$, under parameters, $\\{\\theta_\\ell\\}_\\ell$, can be quantified by \n\\[\n\\epsilon_\\ell=f_\\ell(v_{\\ell-1};\\theta_\\ell)-v_\\ell\n\\]\nfor $\\ell=1,\\ldots,L-1$ where $v_0=V_0=x$ is observed. It is not so simple to quantify the error, $\\epsilon_L$, made at the last layer in a way that accounts for arbitrary loss functions. In the special case of a squared-Euclidean loss function,\n\\[\n{\\mathcal L}(\\hat y,y)=\\frac{1}{2}\\|\\hat y-y\\|^2,\n\\]\nwhere $\\|u\\|^2=u^Tu$. \nStandard formulations of predictive coding~\\cite{bogacz2017tutorial,buckley2017free} use\n\\begin{equation}\\label{E:epsLPC}\n\\epsilon_L=f_L(v_{L-1};\\theta_L)-y\n\\end{equation}\nwhere recall that $y$ is the label. In this case, $\\epsilon_L$ satisfies\n\\begin{equation}\\label{E:epsLPC2}\n\\epsilon_L=\\frac{\\partial {\\mathcal L}(\\tilde v_L,y)}{\\partial \\tilde v_L}\n\\end{equation}\nwhere \n\\[\n\\tilde v_L=f_L(v_{L-1};\\theta_L).\n\\] \nWe use the $\\tilde \\cdot$ to emphasize that $\\tilde v_L$ is different from $\\hat v_L$ (which is defined by a forward pass starting at $\\hat v_0=x$) and is defined in a fundamentally different way from the $v_\\ell$ terms (which do not necessarily satisfy $v_\\ell=f_\\ell(v_{\\ell-1};\\theta_\\ell)$). \nWe can then define the total summed magnitude of errors as\n\\[\nF=\\frac{1}{2}\\sum_{\\ell=1}^L \\|\\epsilon_\\ell\\|^2\n\\]\nUnder a formulation of the model in terms of approximate Bayesian inference, $F$ approximates the variational free energy~\\cite{friston2010free,bogacz2017tutorial,buckley2017free,millidge2020predictive}. \n\nUnder a more heuristic interpretation,  $v_\\ell$ represents the network's ``belief'' about $V_\\ell$, and $f_\\ell(v_{\\ell-1};\\theta_\\ell)$ is the ``prediction'' of $v_\\ell$ made by the previous layer. Under this interpretation, $\\epsilon_\\ell$ is the error made by the previous layer's prediction, so $\\epsilon_\\ell$ is called a ``prediction error.'' Then $F$ quantifies the total magnitude of  prediction errors given a set of beliefs, $v_\\ell$, parameters, $\\theta_\\ell$, and observations, $V_0=x$ and $V_L=y$. \n\n\n\n\nIn predictive coding, beliefs, $v_\\ell$, are updated to minimize the error, $F$. This can be achieved by gradient descent, \\emph{i.e.}, by making updates of the form \n\\[\nv_\\ell \\gets v_\\ell +\\eta dv_\\ell\n\\]\nwhere $\\eta$ is a step size and \n\\begin{equation}\\label{E:dvell}\n\\begin{aligned}\ndv_\\ell&=-\\frac{\\partial F}{\\partial v_\\ell}\\\\\n&=\\epsilon_\\ell-\\epsilon_{\\ell+1}\\frac{\\partial f_{\\ell+1}(v_{\\ell};\\theta_{\\ell+1})}{\\partial v_\\ell}\n\\end{aligned}\n\\end{equation}\nIn this expression, ${\\partial f_{\\ell+1}(v_{\\ell};\\theta_{\\ell+1})}/{\\partial v_\\ell}$ is a Jacobian matrix and $\\epsilon_{\\ell+1}$  is a row-vector to simplify notation, but a column-vector interpretation is similar. If $x$ is a mini-batch instead of one data point, then $v_\\ell$ is an $m\\times n_\\ell$ matrix and derivatives are tensors.  These conventions are used throughout the manuscript. \nThe updates in Eq.~\\eqref{E:dvell} can be iterated until convergence or approximate convergence. Note that the prediction errors, $\\epsilon_\\ell=v_\\ell-f_\\ell(v_{\\ell-1};\\theta_\\ell)$, should also be updated on each iteration. \n\nLearning can also be phrased as minimizing $F$ with gradient descent on parameters. Specifically, \n\\[\n\\theta_\\ell=\\theta_\\ell+\\eta_\\theta d\\theta_\\ell\n\\]\nwhere\n\\begin{equation}\\label{E:dtheta}\n\\begin{aligned}\nd\\theta_\\ell&=-\\frac{\\partial F}{\\partial \\theta_\\ell}\\\\\n&=-\\epsilon_\\ell\\frac{\\partial f_\\ell(v_{\\ell-1};\\theta_\\ell)}{\\partial \\theta_\\ell}.\n\\end{aligned}\n\\end{equation}\nNote that some previous work uses the negative of the prediction errors used here, {\\it i.e.}, they use $\\epsilon_\\ell=v_\\ell-f_\\ell(v_{\\ell-1};\\theta_\\ell)$. While this choice changes some of the expressions above, the value of $F$ and its dependence on $\\theta_\\ell$ is not changed because $F$ is defined by the norms of the $\\epsilon_\\ell$ terms. The complete algorithm is defined more precisely by the pseudocode below:\n\n\n\n\\begin{algorithm}[H]\n\\caption{A direct interpretation of predictive coding.}\\label{A:PC}\n \\vspace{.04in}\n\\begin{algorithmic\n\\State  {\\bf Given:} Input ($x$), label ($y$), and initial beliefs ($v_\\ell$)\n\n\n\\vspace{.06in} \n\\State {\\color{gray}\\# error and belief computation}\n\\For{$i=1,\\ldots,n$}\n\t\\State $\\tilde v_L=f_L(v_{L-1};\\theta_L)$\n\t\\State $\\epsilon_L=\\tfrac{\\partial {\\mathcal L}(\\tilde v_L,y)}{\\partial \\tilde v_L}$\n\t\\For{$\\ell=L-1,\\ldots,1$}\n\t\t\\State $\\epsilon_\\ell=v_\\ell-f_\\ell(v_{\\ell-1};\\theta_\\ell)$\t\t\n\t\n\t\t\\State $dv_\\ell=-\\epsilon_\\ell+\\epsilon_{\\ell+1}\\tfrac{\\partial f_{\\ell+1}(v_{\\ell};\\theta_{\\ell+1})}{\\partial v_\\ell}$\n\t\t\\State $v_\\ell=v_\\ell+\\eta dv_\\ell$ \n\t\\EndFor\n\\EndFor\n\n\\vspace{.06in} \n\\State {\\color{gray}\\# parameter update computation}\n\\For{$\\ell=1,\\ldots,L$}\n\\State $d\\theta_\\ell=-\\epsilon_\\ell\\tfrac{\\partial f_\\ell(v_{\\ell-1};\\theta_\\ell)}{\\partial \\theta_\\ell}$\\vspace{.05in}\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\nThe choice of initial beliefs is not specified in the algorithm above, but previous work~\\cite{whittington2017approximation,millidge2020predictive,salvatori2021predictive,song2020can} uses the results from a forward pass, $v_\\ell=\\hat v_\\ell$, as initial conditions and I do the same in all numerical examples. \n\nI tested Algorithm~\\ref{A:PC} on MNIST  using a 5-layer convolutional neural network. To be consistent with the definitions above, I used a mean-squared error (squared Euclidean) loss function, which required one-hot encoded labels~\\cite{goodfellow2016deep}. Algorithm~\\ref{A:PC} performed similarly to backpropagation  (Fig.~\\ref{F:PC}A,B) even though the parameter updates  did not match the true gradients  (Fig.~\\ref{F:PC}C,D). Algorithm~\\ref{A:PC} was slower than backpropagation (31s for Algorithm~\\ref{A:PC} versus 8s for backpropagation when training metrics were not  computed on every iteration) in part because Algorithm~\\ref{A:PC} requires several inner iterations to compute the prediction errors ($n=20$ iterations used in this example).  Algorithm~\\ref{A:PC} failed to converge on a larger model. Specifically, the loss grew consistently with iterations when trying to use Algorithm~\\ref{A:PC} to train the 6-layer CIFAR-10 model described in the next section.\n\n\n\n \\begin{figure}\n \\centering{\n \\includegraphics[width=6.2in]{PCMNIST.pdf}\n }\n \\caption{{\\bf Predictive coding compared to backpropagation in a convolutional neural network trained on MNIST.} \n{\\bf A,B)} The loss (A) and accuracy (B) on the training set (pastel) and test set (dark) when a 5-layer network was trained using a strict implementation of predictive coding (Algorithm~\\ref{A:PC} with $\\eta=0.1$ and $n=20$; red) and backpropagation (blue). {\\bf C,D)} The relative error (C) and angle (B) between the parameter update, $d\\theta$, computed by Algorithm~\\ref{A:PC} and the negative gradient of the loss at each layer. \n}\n \\label{F:PC}\n \\end{figure}\n\n\n\nFig.~\\ref{F:PC}C,D shows that predictive coding does not update parameters according to the true gradients, but it is not immediately clear whether this would be resolved by using more iterations (larger $n$) or different values of the step size, $\\eta$. I next compared the parameter updates, $d\\theta_\\ell$, to the true gradients, $\\partial {\\mathcal L}/\\partial \\theta_\\ell$ for different values of $n$ and $\\eta$ (Fig.~\\ref{F:PCErrs}). For small values of $\\eta$ and large values of $n$, parameter updates were similar to the true gradients in deeper layers, but they differed substantially in shallower layers. Larger values of $\\eta$ caused the iterations in Algorithm~\\ref{A:PC} to diverge. \n\n\n\n\\begin{figure}\n \\centering{\n \\includegraphics[width=6.2in]{PCMNISTErrsVsIts.pdf}\n }\n\\caption{{\\bf Comparing parameter updates from predictive coding to true gradients.} Relative error and angle between $d\\theta_\\ell$ produced by predictive coding (Algorithm~\\ref{A:PC}) as compared to the exact gradients, $\\partial {\\mathcal L}/\\partial \\theta_\\ell$ computed by backpropagation (relative error defined by $\\|d\\theta_{pc}-d\\theta_{bp}\\|/\\|d\\theta_{bp}\\|$). Updates were computed as a function of the number of iterations, $n$, used in Algorithm~\\ref{A:PC} for various values of the step size, $\\eta$, using the model from Fig.~\\ref{F:PC} applied to one mini-batch of data. Both models were initialized identically to the pre-trained parameter values from the trained model in Fig.~\\ref{F:PC}. }\n\\label{F:PCErrs}\n\\end{figure}\n\n\n\nSome choices in designing Algorithm~\\ref{A:PC} were made arbitrarily. For example, the three updates inside the inner for-loop over $\\ell$ could be performed in a different order or the outer for-loop over $i$  could be changed to a while-loop with a convergence criterion.   For any initial conditions and any of these design choices, if the iterations over $i$ are repeated until convergence or approximate convergence of each $v_\\ell$ to a fixed point, $v_\\ell^*$, then the increments must satisfy $dv_\\ell=0$ at the fixed point and therefore the fixed point values of the prediction errors, $\\epsilon^*_\\ell$, must satisfy\n\\begin{equation}\\label{E:EpsStarPC}\n\\epsilon^*_\\ell=\\frac{\\partial f_{\\ell+1}(v^*_{\\ell};\\theta_{\\ell+1})}{\\partial v^*_\\ell}\\epsilon^*_{\\ell+1}\n\\end{equation}\nfor $\\ell=1,\\ldots,L-1$. By the definition of $\\epsilon_L$, we have\n\\begin{equation}\\label{E:EpsLStarPC}\n\\epsilon_L^*=\\frac{\\partial {\\mathcal L}(\\tilde v^*_L,y)}{\\partial  \\tilde v^*_L}.\n\\end{equation}\nwhere\n\\[\n\\tilde v^*_L=f_L(v^*_{L-1};\\theta_L).\n\\]\nCombining Eqs.~\\eqref{E:EpsStarPC} and \\eqref{E:EpsLStarPC} gives the fixed point prediction errors of the penultimate layer\n\\[\n\\begin{aligned}\n\\epsilon_{L-1}^*&=\\frac{\\partial {\\mathcal L}(\\tilde v^*_L,y)}{\\partial  \\tilde v^*_L}\\frac{\\partial f_{L}(v^*_{L-1};\\theta_{L})}{\\partial v^*_{L-1}}\\\\\n&=\\frac{\\partial {\\mathcal L}(\\tilde v^*_L,y)}{\\partial v^*_{L-1}}\n\\end{aligned}\n\\]\nwhere I used the fact that $  \\tilde v^*_L=f_{L}(v^*_{L-1};\\theta_{L})$ and the chain rule. The error in layer $L-2$ is then given by \n\\[\n\\begin{aligned}\n\\epsilon_{L-2}^*&=\\frac{\\partial {\\mathcal L}(\\tilde v^*_L,y)}{\\partial v^*_{L-1}}\\frac{\\partial f_{L-1}(v^*_{L-2};\\theta_{L-1})}{\\partial v^*_{L-2}}.\n\\end{aligned}\n\\]\nNote that we cannot apply the chain rule to reduce this product further because it is not necessarily true that $ v^*_{L-1}= f_{L-1}(v^*_{L-2};\\theta_{L-1})$. We revisit this point below. Generalizing to earlier layers, we have \n\\begin{equation}\\label{E:epsilonellStar}\n\\begin{aligned}\n\\epsilon_{\\ell}^*&=\\frac{\\partial {\\mathcal L}(\\tilde v^*_L,y)}{\\partial  \\tilde v^*_{L-1}}\\prod_{\\ell'=\\ell}^{L-2} \\frac{\\partial f_{\\ell'+1}(v^*_{\\ell'};\\theta_{\\ell'+1})}{\\partial v^*_{\\ell'}}.\n\\end{aligned}\n\\end{equation}\nfor $\\ell=1,\\ldots,L-2$. \nTherefore, if the inference loop converges to a fixed point, then the subsequent parameter update obeys\n\\begin{equation}\\label{E:dthetaStar}\nd\\theta_\\ell=-\\frac{\\partial {\\mathcal L}(\\tilde v^*_L,y)}{\\partial  \\tilde v^*_{L-1}}\\left[\\prod_{\\ell'=\\ell}^{L-2} \\frac{\\partial f_{\\ell'+1}(v^*_{\\ell'};\\theta_{\\ell'+1})}{\\partial v^*_{\\ell'}}\\right]\\frac{\\partial f_\\ell(v^*_{\\ell-1};\\theta_\\ell)}{\\partial \\theta_\\ell}\n\\end{equation}\nby Eq.~\\eqref{E:dtheta}. \nIt is not clear whether  there is a simple mathematical relationship between these parameter updates and the negtive gradients, $d\\theta_\\ell=-\\partial {\\mathcal L}/\\partial \\theta_\\ell$, computed by backpropagation. \n\n\n\n\nIt is tempting to assume that $v^*_{\\ell}=f_{\\ell}(v^*_{\\ell-1};\\theta_{\\ell})$, in which case the product terms would be reduced by the chain rule. Indeed, this assumption would imply that $v^*_\\ell=\\hat v_\\ell$ and $\\tilde v_L^*=\\hat v_L$ and, finally, that $\\epsilon_\\ell=\\partial {\\mathcal L}/\\partial \\hat v_\\ell$  and $d\\theta_\\ell=-\\partial {\\mathcal L}/\\partial \\theta_\\ell$, identical to the values computed by backpropagation. However,  we cannot generally expect to have $v^*_{\\ell}=f_{\\ell}(v^*_{\\ell-1};\\theta_{\\ell})$ because this would imply that $\\epsilon_{\\ell}^*=0$ and therefore $\\partial {\\mathcal L}/\\partial v_\\ell^*=\\partial {\\mathcal L}/\\partial \\theta_\\ell=0$. In other words, Algorithm~\\ref{A:PC} is only equivalent to backpropagation in the case where parameters are at a critical point of the loss function, so all updates are zero. \nNevertheless, this thought experiment suggests a modification to Algorithm~\\ref{A:PC}  for which the fixed points {\\it do} represent the true gradients~\\cite{millidge2020predictive,whittington2017approximation}. We review that modification in the next section.\n\nNote also that the calculations above rely on the assumption of a Euclidean loss function, ${\\mathcal L}(\\hat y,y)=\\|\\hat y-y\\|^2/2$. If we want to generalize the algorithm to different loss functions, then Eqs.~\\eqref{E:epsLPC} and \\eqref{E:epsLPC2} could not both be true, and therefore Eqs.~\\eqref{E:EpsStarPC} and \\eqref{E:EpsLStarPC} could not both be true. This leaves open the question of how to define $\\epsilon_L$ when using loss functions that are not proportional to the squared Euclidean norm. \nIf we defined $\\epsilon_L$ by \\eqref{E:epsLPC}, at the expense of losing \\eqref{E:epsLPC2}, then the algorithm would not account for the loss function at all, so it would effectively assume a Euclidean loss, {\\it i.e.}, it would compute the same values that are computed by Algorithm~\\ref{A:PC} with a Euclidean loss. \nIf we instead defined $\\epsilon_L$ by Eq.~\\eqref{E:epsLPC2} at the expense of \\eqref{E:epsLPC}, then Eqs.~\\eqref{E:dvell} and \\eqref{E:EpsStarPC} would no longer be true for $\\ell=L-1$ and Eq.~\\eqref{E:dtheta} would no longer be true for $\\ell=L$. Instead, all three of these equations would involve second-order derivatives of the loss function, and therefore the fixed point equations \\eqref{E:epsilonellStar} and \\eqref{E:dthetaStar} would also involve second order derivatives. The interpretation of the parameter updates is not clear in this case.\nOne might instead try to define $\\epsilon_L$ by the result of a forward pass,\n\\[\n\\begin{aligned}\n\\epsilon_L&=f_L(\\hat v_{L-1};\\theta_L)-y\\\\\n&=\\hat v_L-y\n\\end{aligned}\n\\]\nbut then $\\epsilon_L$ would be a constant with respect to $v_{L-1}$, so we would have $\\partial \\epsilon_L/\\partial v_{L-1}=0$, and therefore  Eq.~\\eqref{E:dvell} at $\\ell=L-1$ would become\n\\[\n\\begin{aligned}\ndv_{L-1}&=-\\frac{\\partial F}{\\partial v_{L-1}}\\\\\n&=\\epsilon_{L-1}\n\\end{aligned}\n\\]\nwhich has a fixed point at $\\epsilon^*_{L-1}=0$. This would finally imply that all the errors converge to $\\epsilon_\\ell^*=0$ and therefore $d\\theta_\\ell=0$ at the fixed point.\n\nWe next discuss a modification of Algorithm~\\ref{A:PC} that converges to the same gradients computed by backpropagation, {\\it and} is applicable to general loss functions~\\cite{millidge2020predictive,whittington2017approximation}.  \n\n\n\n\n\n\n\n\n\n\n\\subsection{Predictive coding modified by the fixed prediction assumption converges to the gradients computed by backpropagation.}\n\nPrevious work~\\cite{whittington2017approximation,millidge2020predictive} proposes a modification of the predictive coding algorithm described above called the ``fixed prediction assumption.'' Motivated by the considerations in the last few paragraphs of the previous section, we can selectively substitute some terms of the form $v_\\ell$ and $f_\\ell (v_{\\ell-1};\\theta_\\ell)$ in Algorithm~\\ref{A:PC} with $\\hat v_\\ell$ (or, equivalently, $f_\\ell (\\hat v_{\\ell-1};\\theta_\\ell)$) where $\\hat v_\\ell$ are the results of the original forward pass starting from $\\hat v_0=x$. Specifically,  the following modifications are made to the quantities computed by Algorithm~\\ref{A:PC}\n\\[\n\\begin{aligned}\n\\epsilon_\\ell&=\\hat v_\\ell-v_\\ell\\\\\n\\epsilon_L&=\\frac{\\partial {\\mathcal L}(\\hat v_L,y)}{\\partial \\hat v_L}\\\\\ndv_\\ell&=\\epsilon_\\ell-\\epsilon_{\\ell+1}\\frac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_\\ell}\\\\\nd\\theta_\\ell&=-\\epsilon_\\ell\\frac{\\partial f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)}{\\partial \\theta_\\ell}\n\\end{aligned}\n\\]\nfor $\\ell=1,\\ldots,L-1$. This modification can be interpreted as ``fixing'' the predictions at the values computed by a forward pass and is therefore called the ``fixed prediction assumption''~\\cite{whittington2017approximation,millidge2020predictive}. \nAdditionally, the initial conditions of the beliefs are set to the results from a forward pass,\n$\nv_\\ell=\\hat v_\\ell\n$\nfor $\\ell=1,\\ldots,L-1$. \nThe complete modified algorithm is defined by the pseudocode below:\n\n\n\n\\begin{algorithm}[H]\n\\caption{Supervised learning with predictive coding modified by the fixed prediction assumption. Adapted from the algorithm in~\\cite{millidge2020predictive} and similar to the algorithm from~\\cite{whittington2017approximation}.}\\label{A:ModPC}\n\\begin{algorithmic\n \\vspace{.04in}\n\\State {\\bf Given:} Input ($x$) and label ($y$)\n\\vspace{.04in} \n\\State {\\color{gray}\\# forward pass}\n\\State $\\hat v_0=x$\n\\For{$\\ell=1,\\ldots,L$}\n            \\State $\\hat v_\\ell=f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)$\n            \\State $v_\\ell=\\hat v_\\ell$\n\\EndFor\n\n\\vspace{.04in} \n\\State {\\color{gray}\\# error and belief computation}\n\\State $\\epsilon_L=\\tfrac{\\partial {\\mathcal L}(\\hat v_L,y)}{\\partial  \\hat v_L}$\n\\For{$i=1,\\ldots,n$}\n\t\\For{$\\ell=L-1,\\ldots,1$}\n\t\t\\State $\\epsilon_\\ell=v_\\ell-\\hat v_\\ell$\n\t\t\\State $dv_\\ell=\\epsilon_\\ell-\\epsilon_{\\ell+1}\\tfrac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_\\ell}$\n\t\t\\State $v_\\ell=v_\\ell+\\eta dv_\\ell$ \n\t\\EndFor\n\\EndFor\n\n\\vspace{.06in} \n\\State {\\color{gray}\\# parameter update computation}\n\\For{$\\ell=1,\\ldots,L$}\n\\State $d\\theta_\\ell=-\\epsilon_\\ell\\tfrac{\\partial f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)}{\\partial \\theta_\\ell}$\\vspace{.05in}\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\nNote, again, that some choices in Algorithm~\\ref{A:ModPC} were made arbitrarily. The three updates inside the inner for-loop over $\\ell$ could be performed in a different order or the outer for loop over $i$  could be changed to a while-loop with a convergence criterion.\nRegardless of these choices, the fixed points, $\\epsilon^*_\\ell$, can again be computed by setting $dv_\\ell=0$ to obtain\n\\[\n\\epsilon^*_\\ell=\\epsilon^*_{\\ell+1}\\frac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_\\ell}.\n\\]\nNow note that $\\epsilon_L$ is fixed, so \n\\[\n\\epsilon^*_L=\\frac{\\partial {\\mathcal L}(\\hat v_L,y)}{\\partial  \\hat v_L}\n\\]\nand we can combine these two equations to compute \n\\[\n\\begin{aligned}\n\\epsilon^*_{L-1}&=\\frac{\\partial {\\mathcal L}(\\hat v_L,y)}{\\partial  \\hat v_L}\\frac{\\partial f_{L}(\\hat v_{L-1};\\theta_{L})}{\\partial \\hat v_{L-1}}\\\\\n&=\\frac{\\partial {\\mathcal L}(\\hat v_L,y)}{\\partial \\hat v_{L-1}}\n\\end{aligned}\n\\]\nwhere I used the chain rule and the fact that $\\hat v_\\ell=f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)$. Continuing this approach we have,\n\\[\n\\begin{aligned}\n\\epsilon^*_\\ell&=\\epsilon^*_{\\ell+1}\\frac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_\\ell}\\\\\n&=\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\hat v_{\\ell}}\n\\end{aligned}\n\\]\nfor all $\\ell=1,\\ldots,L$ (where recall that $\\hat y=\\hat v_L$ is the output from the feedfoward pass). \nCombining this with the modified definition of $d\\theta_\\ell$, we have\n\\[\n\\begin{aligned}\nd\\theta_\\ell&=-\\epsilon^*_\\ell\\frac{\\partial f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)}{\\partial \\theta_\\ell}\\\\\n&=-\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\hat v_{\\ell}}\\frac{\\partial \\hat v_\\ell}{\\partial \\theta_\\ell}\\\\\n&=-\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\theta_\\ell}\n\\end{aligned}\n\\]\nwhere I used the chain rule and the fact that $\\hat v_\\ell=f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)$. We may conclude that, if the inference step converges to a fixed point ($dv_\\ell=0$), then Algorithm~\\ref{A:ModPC}  computes the same values of $d\\theta_\\ell$ as backpropagation and also that the prediction errors, $\\epsilon_\\ell$, converge to the gradients, $\\delta_\\ell=\\partial {\\mathcal L}/\\partial \\hat v_\\ell$, computed by backpropagation. As long as the inference step  {\\it approximately} converges to a fixed point ($dv_\\ell\\approx 0$), then we should expect the parameter updates from Algorithm~\\ref{A:ModPC} to {\\it approximate} those computed by backpropagation. In the next section, I extend this result to show that a special case of the algorithm computes the true gradients in a fixed number of steps.\n\n \\begin{figure}\n \\centering{\n \\includegraphics[width=6.2in]{FixedPredPCMNIST.pdf}\n  \\includegraphics[width=6.2in]{FixedPredPCCIFAR.pdf}\n }\n \\caption{{\\bf Predictive coding modified by the fixed prediction assumption compared to backpropagation in convolutional neural networks trained on MNIST and CIFAR-10.} Same as Fig.~\\ref{F:PC} except Algorithm~\\ref{A:ModPC} was used (with $\\eta=0.1$ and $n=20$) in place of Algorithm~\\ref{A:PC}.   }\n \\label{F:ModPC}\n \\end{figure}\n\n\n\nI next tested Algorithm~\\ref{A:ModPC} on MNIST using the same 5-layer convolutional neural network considered above. I used a cross-entropy loss function, but otherwise used all of the same parameters used to test Algorithm~\\ref{A:PC} in Fig.~\\ref{F:PC}. The modified predictive coding algorithm (Algorithm~\\ref{A:ModPC}) performed similarly to backpropagation in terms of the loss and accuracy (Fig.~\\ref{F:ModPC}A,B). Parameter updates computed by Algorithm~\\ref{A:ModPC} did not match the true gradients, but pointed in a similar direction and provided a closer match than Algorithm~\\ref{A:PC} (compare Fig.~\\ref{F:ModPC}C,D to Fig.~\\ref{F:PC}C,D).  Algorithm~\\ref{A:ModPC} was similar to Algorithm~\\ref{A:PC} in terms of training time (29s for  Algorithm~\\ref{A:ModPC} versus 31s for Algorithm~\\ref{A:PC} and 8s for backpropagation).\n\n\nTo see how well these results extend to a larger model and more difficult benchmark, I next tested Algorithm~\\ref{A:ModPC} on CIFAR-10~\\cite{krizhevsky2009learning} using a six-layer convolutional network. While the network only has one more layer than the MNIST network used above, it has 141 times more parameters (32,695 trainable parameters in the MNIST model versus 4,633,738 in the CIFAR-10 model).\n\nAlgorithm~\\ref{A:ModPC} performed similarly to backpropagation in terms of loss and accuracy during learning (Fig.~\\ref{F:ModPC}E,F) and produced parameter updates that pointed in a similar direction, but still did not match the true gradients (Fig.~\\ref{F:ModPC}G,H). Algorithm~\\ref{A:ModPC} was substantially slower than backpropagation (848s for  Algorithm~\\ref{A:ModPC} versus 58s for backpropagation when training metrics were not  computed on every iteration).\n\nI next compared the parameter updates computed by Algorithm~\\ref{A:ModPC} to the true gradients for different values of $n$ and $\\eta$ (Fig.~\\ref{F:ModPCErrs}).    For $\\eta<1$, the parameter updates, $d\\theta_\\ell$, appear to converge, but do not converge exactly to the true gradients. This is likely due to numerical floating point errors accumulated over  iterations. When $\\eta=1$, the parameter updates at each layer remained constant for the first few iterations, then immediately jumped to become very near the updates from backpropagation. In the next section, I provide a mathematical analysis of this behavior and show that when $\\eta=1$, Algorithm~\\ref{A:ModPC} computes the true gradients in a fixed number of steps.\n\n\n\n\n\n\\begin{figure}\n \\centering{\n  \\includegraphics[width=6.2in]{FixedPredPCMNISTErrsVsIts.pdf}\n }\n \\caption{{\\bf Comparing parameter updates from predictive coding modified by the fixed prediction assumption to errors computed by backpropagation.} Relative error and angle between $d\\theta$ produced by predictive coding modified by the fixed prediction assumption (Algorithm~\\ref{A:ModPC}) as compared to the exact gradients computed by backpropagation (relative error defined by $\\|d\\theta_{pc}-d\\theta_{bp}\\|/\\|d\\theta_{bp}\\|$). Updates were computed as a function of the number of iterations, $n$, used in Algorithm~\\ref{A:ModPC} for various values of the step size, $\\eta$, using the model from Fig.~\\ref{F:ModPC} applied to one mini-batch of data. Both models were initialized identically to the pre-trained parameter values from the backpropagation-trained model in Fig.~\\ref{F:ModPC}. \n}\n \\label{F:ModPCErrs}\n \\end{figure}\n\n\n\n\n\\section{Predictive coding modified by the fixed prediction assumption using a step size of $\\eta=1$ computes exact gradients in a fixed number of steps.}\n\nA major disadvantage of the approach outlined above -- when compared to standard backpropagation --  is that it requires iterative updates to $v_\\ell$ and $\\epsilon_\\ell$. Indeed, previous work~\\cite{millidge2020predictive} used $n=100-200$ iterations, leading to substantially slower performance compared to standard backpropagation. Other work~\\cite{whittington2017approximation} used $n=20$ iterations like I did above. In general, there is a tradeoff between accuracy and performance when choosing $n$, as demonstrated in Fig.~\\ref{F:ModPCErrs}. \nHowever, more recent work~\\cite{song2020can,salvatori2021predictive} showed that, under the fixed prediction assumption, predictive coding can compute the exact same gradients computed by backpropagation in a fixed number of steps. That work used a more specific formulation of the neural network which can implement fully connected layers, convolutional layers, and recurrent layers. They also used an unconventional interpretation of neural networks in which weights are multiplied outside the activation function, {\\it i.e.}, $f_\\ell(x;\\theta_\\ell)=\\theta_\\ell g_\\ell(x)$, and inputs are fed into the last layer instead of the first. Next, I show that their result holds for arbitrary feedforward neural networks as formulated in Eq.~\\eqref{E:fwdpass} (with arbitrary functions, $f_\\ell$) and this result has a simple interpretation in terms of Algorithm~\\ref{A:ModPC}. Specifically, the following theorem shows that taking a step size of $\\eta=1$ yields an exact computation of gradients using just $n=L$ iterations (where $L$ is the depth of the network). \n\n\n\n\n\n\n\n\\begin{theorem}\\label{T:1}\nIf Algorithm~\\ref{A:ModPC} is run with step size $\\eta=1$ and at least $n=L$ iterations then the algorithm computes\n\\[\n\\epsilon_\\ell=\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\hat v_\\ell}\n\\]\nand\n\\[\nd\\theta_\\ell=-\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\theta_\\ell}\n\\]\nfor all $\\ell=1,\\ldots,L$ where $\\hat v_\\ell=f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)$ are the results from a forward pass with $\\hat v_0=x$ and  $\\hat y=\\hat v_L=f(x;\\theta)$ is the output.\n\\end{theorem}\n\\begin{proof}\nFor the sake of notational simplicity within this proof, define $\\delta_\\ell=\\partial {\\mathcal L}(\\hat v_L,y)/\\partial \\hat v_\\ell$. So we first need to prove that $\\epsilon_\\ell=\\delta_\\ell$. \nFirst, rewrite the inside of the error and belief loop from Algorithm~\\ref{A:ModPC}  while explicitly keeping track of the iteration number in which each variable was updated,\n\\[\n\\begin{aligned}\n\\epsilon^i_\\ell&=v^{i-1}_\\ell-\\hat v_\\ell\\\\\ndv^i_\\ell&=\\epsilon^i_\\ell-\\epsilon^i_{\\ell+1}\\frac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_\\ell}\\\\\nv^i_\\ell&=v^{i-1}_\\ell+ dv^i_\\ell.\n\\end{aligned}\n\\]\nHere, $v^i_\\ell$, $\\epsilon^i_\\ell$, and $dv^i_\\ell$ denote the values of $v^i_\\ell$, $\\epsilon^i_\\ell$, and $dv^i_\\ell$ respectively at the end of the $i$th iteration,  $v^0_\\ell=\\hat v_\\ell$ corresponds to the initial value, and all terms without superscripts are constant inside the inference loop. There are some subtleties here. For example, we have $v_\\ell^{i-1}$ in the first line because $v_\\ell$ is updated after $\\epsilon_\\ell$  in the loop. \nMore subtly, we have $\\epsilon^i_{\\ell+1}$ in the second equation instead of $\\epsilon^{i-1}_{\\ell+1}$ because the for loop goes backwards from $\\ell=L-1$ to $\\ell=1$, so $\\epsilon_{\\ell+1}$ is updated before $\\epsilon_{\\ell}$. \nFirst note that\n\\[\n\\epsilon^1_\\ell=0\n\\]\nfor $\\ell=1,\\ldots,L-1$ because $v_\\ell^0=\\hat v_\\ell$. Now compute the change in $\\epsilon_\\ell$ across one step,\n\\[\n\\begin{aligned}\n\\epsilon^{i+1}_\\ell-\\epsilon^{i}_\\ell&=v^i_\\ell-v^{i-1}_\\ell\\\\\n&=dv_\\ell^i\\\\\n&=\\epsilon^i_\\ell-\\epsilon^i_{\\ell+1}\\frac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_\\ell}.\n\\end{aligned}\n\\]\nNote that this equation is only valid for $i\\ge 1$ due to the $i-1$ term ($v^{-1}_\\ell$ is not defined). \nAdding $\\epsilon^i_\\ell$ to both sides of the resulting equation gives\n\\[\n\\epsilon^{i+1}_\\ell=\\epsilon^i_{\\ell+1}\\frac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_\\ell}.\n\\]\nWe now use induction to prove that $\\epsilon_\\ell=\\delta_\\ell$ after $n=L$ iterations. Indeed, we prove a stronger claim that $\\epsilon^i_\\ell=\\delta_\\ell$ at $i=L-\\ell+1$. First note that $\\epsilon^i_L=\\delta_L$ for all $i$ because $\\epsilon^i_L$ is initialized to $\\delta_L$ and then never changed. Therefore, our claim is true for the base case $\\ell=L$.  \n\nNow suppose that $\\epsilon^{i}_{\\ell+1}=\\delta_{\\ell+1}$ for $i=L-(\\ell+1)+1=L-\\ell$. We need to show that $\\epsilon^{i+1}_{\\ell}=\\delta_{\\ell}$. From above, we have\n\\[\n\\begin{aligned}\n\\epsilon^{i+1}_{\\ell}&=\\epsilon^i_{\\ell+1}\\frac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_{\\ell}}\\\\\n&=\\delta_{\\ell+1}\\frac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_{\\ell}}\\\\\n&=\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\hat v_{\\ell+1}}\\frac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_{\\ell}}\\\\\n&=\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\hat v_{\\ell+1}}\\frac{\\partial \\hat v_{\\ell+1}}{\\partial \\hat v_{\\ell}}\\\\\n&=\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\hat v_{\\ell}}\\\\\n&=\\delta_{\\ell}.\n\\end{aligned}\n\\]\nThis completes our induction argument. It follows that $\\epsilon^i_\\ell=\\delta_\\ell$ at iteration $i=L-\\ell+1$ at all layers $\\ell=1,\\ldots,L$. The last layer to be updated to the correct value is $\\ell=1$, which is updated on iteration number $i=L-1+1=L$. Hence, $\\epsilon_\\ell=\\delta_\\ell$ for all $\\ell=1,\\ldots,L$ after $n=L$ iterations. This proves the first statement in our theorem. The second statement then follows from the definition of $d\\theta_\\ell$, \n\\[\n\\begin{aligned}\nd\\theta_\\ell&=-\\epsilon_\\ell\\frac{\\partial f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)}{\\partial \\theta_\\ell}\\\\\n&=-\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\hat v_{\\ell}}\\frac{\\partial f_\\ell(\\hat v_{\\ell-1};\\theta_\\ell)}{\\partial \\theta_\\ell}\\\\\n&=-\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\hat v_{\\ell}}\\frac{\\partial \\hat v_{\\ell}}{\\partial \\theta_\\ell}\\\\\n&=-\\frac{\\partial {\\mathcal L}(\\hat y,y)}{\\partial \\theta_\\ell}.\n\\end{aligned}\n\\]\nThis completes the proof.\n\\end{proof}\n\n\n \\begin{figure}\n \\centering{\n \\includegraphics[width=6.2in]{FixedPredPCMNISTeta1.pdf}\n  \\includegraphics[width=6.2in]{FixedPredPCCIFAReta1.pdf}\n }\n \\caption{{\\bf Predictive coding modified by the fixed prediction assumption with $\\eta=1$ compared to backpropagation in convolutional neural networks trained on MNIST and CIFAR-10.} Same as Fig.~\\ref{F:ModPC} except I set $\\eta=1$ and $n=L$.  }\n \\label{F:ModPCeta1}\n \\end{figure}\n \n \nThis theorem ties together the implementation and formulation of predictive coding from~\\cite{millidge2020predictive} ({\\it i.e.}, Algorithm~\\ref{A:ModPC}) to the results in~\\cite{salvatori2021predictive,song2020can}. \nThe results of the theorem are illustrated  empirically by the convergence of the gradients shown in the rightmost panels of Fig.~\\ref{F:ModPCErrs}. To further test the result empirically, I repeated Fig.~\\ref{F:ModPC} with $\\eta=1$ and $n=L$ (in contrast to Fig.~\\ref{F:ModPC} which used $\\eta=0.1$ and $n=20$). The loss and accuracy closely match those computed by backpropagation (Fig.~\\ref{F:ModPCeta1}A,B,E,F). More importantly, the parameter updates closely matched the true gradients (Fig.~\\ref{F:ModPCeta1}C,D,G,H), as predicted by Theorem~\\ref{T:1}.\n\nThe differences between predictive coding and backpropagation in Fig.~\\ref{F:ModPCeta1} are due floating point errors and the non-determinism of computations performed GPUs. For example, similar differences to those seen in Fig.~\\ref{F:ModPCeta1}A,B are present when the same training algorithm is run twice with the same random seed.  \nThe smaller number of iterations ($n=L$ in Figure~\\ref{F:ModPCeta1} versus $n=20$ in Figure~\\ref{F:ModPC}) resulted in a shorter training time (13s for MNIST and 300s for CIFAR-10 for modified PC in Fig.~\\ref{F:ModPCeta1}, compare to 29s and 848s in Fig.~\\ref{F:ModPC}, and compare to 8s and 58s for backpropagation). \n\nIn summary, a review of the literature shows that a strict interpretation of predictive coding (Algorithm~\\ref{A:PC}) does not converge to the true gradients computed by backpropagation. To compute the true gradients, predictive coding must be modified by the fixed prediction assumption (Algorithm~\\ref{A:ModPC}). Further, I proved that Algorithm~\\ref{A:ModPC} computes the exact gradients if we set $\\eta=1$ and $n\\ge L$, which ties together results from previous work~\\cite{millidge2020predictive,salvatori2021predictive,song2020can}.\n\n\n\n\\section{Predictive coding with the fixed prediction assumption and $\\eta=1$ is functionally equivalent to a direct implementation of backpropagation.}\n\nThe proof of Theorem~\\ref{T:1} and the last panel of Fig.~\\ref{F:ModPCErrs} give some insight into a how Algorithm~\\ref{A:ModPC} works. First note that the values of $v_\\ell$ in Algorithm~\\ref{A:ModPC} are only used to compute the values of $\\epsilon_\\ell$ and are not otherwise used in the computation of $d\\theta_\\ell$ or any other quantities. Therefore, if we only care about understanding parameter updates, $d\\theta_\\ell$, we can ignore the values of $v_\\ell$ and only focus on how $\\epsilon_\\ell$ is updated on each iteration, $i$. Secondly, note that when $\\eta=1$, each $\\epsilon_\\ell$ is updated only once: $\\epsilon^i_\\ell=0$ for $i<L-\\ell+1$ and $\\epsilon^i_\\ell=\\epsilon^i_{\\ell+1}{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}/{\\partial \\hat v_{\\ell}}$ for $i\\ge L-\\ell+1$, so $\\epsilon_\\ell$ is only changed on iteration number $i=L-\\ell+1$. In other words, the error computation in Algorithm~\\ref{A:ModPC} when $\\eta=1$ is equivalent to\n\\begin{algorithmic\n \\vspace{.04in}\n\\State {\\color{gray}\\# error computation}\n\\State $\\epsilon_L=\\tfrac{\\partial {\\mathcal L}(\\hat v_L,y)}{\\partial  \\hat v_L}$\n\\For{$i=1,\\ldots,n$}\n\t\\For{$\\ell=L-1,\\ldots,1$}\n\t\t\\If{$\\ell==L-i+1$}\n\t\t\t\\State $\\epsilon_\\ell=\\epsilon_{\\ell+1}\\tfrac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_\\ell}$\n\t\t\\EndIf\n\t\\EndFor\n\\EndFor\n\\end{algorithmic}\nThe two computations are equivalent in the sense that they compute the same values of the errors, $\\epsilon_\\ell^i$, on every iteration. The formulation above makes it clear that the nested loops are unnecessary because for each value of $i$, $\\epsilon_\\ell$ is only updated at one value of $\\ell$. Therefore, the nested loops and if-statement can be replaced by a single for-loop. Specifically, the error computation  in Algorithm~\\ref{A:ModPC} when $\\eta=1$ is equivalent to\n\\begin{algorithmic\n \\vspace{.04in}\n\\State {\\color{gray}\\# error computation}\n\\State $\\epsilon_L=\\tfrac{\\partial {\\mathcal L}(\\hat v_L,y)}{\\partial  \\hat v_L}$\n\\For{$\\ell=L-1,\\ldots,1$}\n\t\\State $\\epsilon_\\ell=\\epsilon_{\\ell+1}\\tfrac{\\partial f_{\\ell+1}(\\hat v_{\\ell};\\theta_{\\ell+1})}{\\partial \\hat v_\\ell}$\n\\EndFor\n\\end{algorithmic}\nThis is {\\it exactly} the error computation from the standard backpropagation algorithm, {\\it i.e.}, Algorithm~\\ref{A:backprop}. Hence, if we use $\\eta=1$, then Algorithm~\\ref{A:ModPC} is just  backpropagation with extra steps and these extra steps do not compute any non-zero values. If we additionally want to compute the fixed point beliefs, then they can still be computed using the relationship\n\\[\nv_\\ell=\\epsilon_\\ell+\\hat v_\\ell.\n\\]\nWe conclude that, when $\\eta=1$, Algorithm~\\ref{A:ModPC} can be replaced by an exact implementation of backpropagation without any effect on the results or effective implementation of the algorithm. \n\n\n\n\n\n\\section{Prediction errors do not necessarily represent surprising or unexpected features of inputs.}\n\nDeep neural networks are often interpreted as abstract models of cortical neuronal networks. To this end, the activations of units in deep neural networks are compared to the activity (typically firing rates) of cortical neurons~\\cite{SchrimpfKubilius2018BrainScore,Schrimpf2020integrative,lillicrap2020backpropagation}. This approach ignores the representation of errors within the network. More generally, the activations in one particular layer of a feedforward deep neural network contain no information about the activations of deeper layers, the label, or the loss. On the other hand, the activity of cortical neurons can be modulated by downstream activity and information believed to be passed upstream by feedback projections. Predictive coding provides a precise model for the information that deeper layers send to shallower layers, specifically prediction errors.  \n\n\n\nUnder the fixed prediction assumption (Algorithm~\\ref{A:ModPC}), prediction errors in a particular layer are  approximated by the gradients of that layers' activations with respect to the loss function, $\\epsilon_\\ell= \\delta_\\ell=\\frac{\\partial {\\mathcal L}}{\\partial \\hat v_\\ell}$, but under a strict interpretation of predictive coding (Algorithm~\\ref{A:PC}), prediction errors do not necessarily reflect gradients. \nI next empirically explored how the representations of images differ between the activations from a feedforward pass, $\\hat v_\\ell$,  the prediction errors under the fixed prediction assumption, $\\epsilon_\\ell= \\delta_\\ell$, as well as the beliefs, $v_\\ell$, and prediction errors, $\\epsilon_\\ell$, under a strict interpretation of predictive coding (Algorithm~\\ref{A:PC}). To do so, I computed each quantity in VGG-19~\\cite{simonyan2014very}, which is a large, feedforward convolutional neural network (19 layers and 143,667,240 trainable parameters) pre-trained on ImageNet~\\cite{russakovsky2015imagenet}.  \n\n\n\n\\begin{figure}\n \\centering{\n \\includegraphics[width=6.2in]{VGG19Images.pdf}\n }\n \\caption{{\\bf Magnitude of activations, beliefs, and prediction errors in a convolutional neural network pre-trained on ImageNet.} The Euclidean norm of feedforward activations ($\\hat v$, interpreted as beliefs  under the fixed prediction assumption), gradients of the loss with respect to activations ($\\delta_\\ell=\\partial {\\mathcal L}/\\partial \\hat v$, interpreted as prediction errors  under the fixed prediction assumption), beliefs ($v$) under strict predictive coding, and prediction errors ($\\epsilon_\\ell)$) under strict predictive coding  computed from the VGG-19 network~\\cite{simonyan2014very} pre-trained on ImageNet~\\cite{russakovsky2015imagenet} with  two different photographs as inputs at two different layers. The vertical labels on the left (``triceratops'' and ``Irish wolfhound'') correspond to the guessed label which was also used as  the ``true'' label ($y$) used to compute the gradients.   }\n \\label{F:VGG19}\n \\end{figure}\n\n\nThe use of convolutional layers allows us to visualize the activations and prediction errors in each layer. Specifically, I took the Euclidean norm of each quantity across all channels and plotted them as two-dimensional images for layers $\\ell=1$ and $\\ell=10$ and for two different input images (Fig.~\\ref{F:VGG19}). Under predictive coding with the fixed prediction assumption, we can interpret the activations, $\\hat v_\\ell$, as ``beliefs'' and the gradients, $\\delta_\\ell$, as ``prediction errors.'' Strictly speaking, there is a distinction between the beliefs, $\\hat v_\\ell$, from a feedforward pass and the beliefs, $v_\\ell=\\hat v_\\ell+\\epsilon_\\ell$, when labels are provided. Either could be interpreted as a ``belief.'' However, I found that the difference between them was negligible  for the examples  considered here. \n\nOverall, the activations, $\\hat v_\\ell$, from a feedforward pass were qualitatively very similar to the beliefs, $v_\\ell$, computed under a strict interpretation of predictive coding (Algorithm~\\ref{A:PC}). To a slightly lesser degree, the gradients, $\\delta_\\ell$, from a feedforward pass were qualitatively similar to the prediction errors computed under a strict interpretation of predictive coding (Algorithm~\\ref{A:PC}). Since $\\hat v_\\ell$ and $\\delta_\\ell$ approximate beliefs and prediction errors under the fixed prediction assumption, these observations confirm that the fixed prediction assumption does not make large qualitative changes to the representation of beliefs and errors in these examples. Therefore, in the discussion below, I  use ``beliefs'' and ``prediction errors'' to refer to the quantities from both models. \n\n\nInterestingly, prediction errors are non-zero even when the image and the network's ``guess'' is consistent with the label (no ``mismatch''). Indeed, the prediction errors are largest in magnitude at pixels corresponding to the object predicted by the label, {\\it i.e.}, at the most predictable regions. While this observation is an obvious consequence of the fact that prediction errors are approximated by the gradients, $\\delta_\\ell=\\frac{\\partial {\\mathcal L}}{\\partial \\hat v_\\ell}$, it seems contradictory to the heuristic or intuitive interpretation of prediction errors as measurements of ``surprise'' in the colloquial sense of the word~\\cite{friston2010free}. \n\nAs an illustrative example from Fig.~\\ref{F:VGG19}, it is not surprising that an image labeled by ``triceratops'' contains a triceratops, but this does not imply a lack of prediction errors because the space of images containing a triceratops is large and any one image of a triceratops is not wholly representative of the label. Moreover, the pixels to which the loss is most sensitive are those pixels containing the triceratops. Therefore those pixels give rise to larger values of $\\epsilon_\\ell\\approx \\delta_\\ell=\\partial \\mathcal L/\\partial \\hat v_\\ell$. Hence, in high-dimensional sensory spaces, predictive coding models do not necessarily predict that prediction error units encode ``surprise'' in the colloquial sense of the word.\n\n\n\nIn both examples in Fig.~\\ref{F:VGG19}, I used an input, $y$, that matched the network's ``guessed'' label, {\\it i.e.}, the label to which the network assigned the highest probability ($\\operatornamewithlimits{argmax}(\\hat y)$). Prediction errors are often discussed in the context of mismatched stimuli in which top-down input is inconsistent with bottom-up predictions~\\cite{hertag2020learning,gillon2021learning,keller2012sensorimotor,zmarz2016mismatch,attinger2017visuomotor,homann2017predictive}. Mismatches can be modeled by  taking a label that is different from the network's guess. In Fig.~\\ref{F:VGG19MM}, I visualized the prediction errors in response to matched and mismatched labels. The network assigned a probability of $p=0.9991$ to the label ``carousel'' and a probability of $p=3.63\\times 10^{-8}$ to the label ``bald eagle''. When I applied the mismatched label ``bald eagle,'' prediction errors were larger in pixels that are salient for that label ({\\it e.g.}, the bird's white head, which is  a defining feature of a bald eagle). Moreover, the prediction errors as a whole are much larger in magnitude in response to the mismatched label (see the scales of the color bars in Fig.~\\ref{F:VGG19MM}). \n\n\n\\begin{figure}\n \\centering{\n \\includegraphics[width=5.2in]{VGG19ImagesMM.pdf}}\n \\caption{{\\bf Magnitude of activations, beliefs, and prediction errors in response to matched and mismatched inputs and labels.} Same as Fig.~\\ref{F:VGG19}, but for the bottom row the label did not match the network's guess.  }\n \\label{F:VGG19MM}\n \\end{figure}\n\n\n\n\n\n\nIn summary, the relationship between prediction errors and gradients helps demonstrate that  prediction errors sometimes, but do not always conform to their common interpretation as unexpected features of a bottom-up input in the context of a top-down input. Also, beliefs and prediction errors were qualitatively similar with and without the fixed prediction assumption for the examples considered here. \n\n\n\n\n\n\n\n\\section{Torch2PC software for predictive coding with PyTorch models}\n\nThe figures above were all produced using PyTorch~\\cite{paszke2019pytorch} models combined with custom written functions for predictive coding. The functions are collected in the GitHub repository Torch2PC (``torch to predictive coding'') which contains functions for predictive coding with PyTorch models. Currently, the only available functions are intended for models built using the Sequential class, but more general functions will be added to \\texttt{Torch2PC} in the future. The functions can be imported using the following commands\n\\begin{verbatim}\n!git clone https://github.com/RobertRosenbaum/Torch2PC.git\nfrom Torch2PC import TorchSeq2PC as T2PC\n\\end{verbatim}\nThe primary function in \\texttt{TorchSeq2PC} is \\texttt{PCInfer}, \nwhich performs one predictive coding step (computes one value of $d\\theta$) on a batch of inputs and labels. The function takes an input \\texttt{ErrType}, which is a string that determines whether to use a strict interpretation of predictive coding (Algorithm~\\ref{A:PC}; \\texttt{ErrType=\"Strict\"}), predictive coding with the fixed prediction assumption (Algorithm~\\ref{A:ModPC}; \\texttt{\"FixedPred\"}), or to compute the gradients exactly using backpropagation (Algorithm~\\ref{A:backprop}; \\texttt{\"Exact\"}). \nAlgorithm~\\ref{A:PC} can be called as follows,\n\\begin{verbatim}\nvhat,Loss,dLdy,v,epsilon=\n  T2PC.PCInfer(model,LossFun,X,Y,\"Strict\",eta,n,vinit)\n\\end{verbatim}\nwhere \\texttt{model} is a Sequential PyTorch model, \\texttt{LossFun} is a loss function, \\texttt{X} is a mini-batch of inputs, \\texttt{Y} is a mini-batch of labels, \\texttt{eta} is the step size, \\texttt{n} is the number of iterations to use, and \\texttt{vinit} is the initial value for the beliefs. If \\texttt{vinit} is not passed, it is set to the result from a forward pass, \\texttt{vinit=vhat}. The function returns a list of activations from a forward pass at each layer as \\texttt{vhat}, the loss as \\texttt{Loss}, the gradient of the output with respect to the loss as \\texttt{dLdy}, a list of beliefs, $v_\\ell$, at each layer as \\texttt{v}, and a list of prediction errors, $\\epsilon_\\ell$, at each layer as \\texttt{epsilon}. The values of the parameter updates, $d\\theta_\\ell$, are stored in the \\texttt{grad} attributes of each parameter, \\texttt{model.param.grad}. Hence, after a call to \\texttt{PCInfer}, gradient descent could be implemented by calling\n\\begin{verbatim}\nwith torch.no_grad():\n  for p in modelPC.parameters():\n    p-=eta*p.grad\n\\end{verbatim}\nAlternatively, an arbitrary optimizer could be used by calling\n\\begin{verbatim}\noptimizer.step() \n\\end{verbatim}\nwhere \\texttt{optimizer} is an optimizer created using the PyTorch \\texttt{optim} class, {\\it e.g.}, by calling \\\\ \\texttt{optimizer=optim.Adam(model.parameters())} before the call to \\texttt{T2PC.PCInfer}.\n\nThe input \\texttt{model} should be a PyTorch Sequential model. Each layer is treated as a single predictive coding layer. Multiple functions can be included within the same layer by wrapping them in a separate call to \\texttt{Sequential}. For example the following code:\n\\begin{verbatim}\nmodel=model=nn.Sequential(    \n    nn.Conv2d(1,10,3),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(10,10,3),\n    nn.ReLU()\n)\n\\end{verbatim}\nwill treat each item as its own layer (5 layers in all). To treat each \"convolutional block\" as a separate layer, instead do\n\\begin{verbatim}\nmodel=nn.Sequential(\n    \n    nn.Sequential(\n      nn.Conv2d(1,10,3),\n      nn.ReLU(),\n      nn.MaxPool2d(2)\n    ),\n    \n    nn.Sequential(\n      nn.Conv2d(10,10,3),\n      nn.ReLU()\n    )\n)\n\\end{verbatim}\nwhich has just 2 layers.\n\nAlgorithm~\\ref{A:ModPC} can be called as follows,\n\\begin{verbatim}\nvhat,Loss,dLdy,v,epsilon=\n  T2PC.PCInfer(model,LossFun,X,Y,\"FixedPred\",eta,n)\n\\end{verbatim}\nThe input \\texttt{vinit} is not used for Algorithm~\\ref{A:ModPC}, so it does not need to be passed in. \nThe exact values computed by backpropagation can be obtained by calling \n\\begin{verbatim}\nvhat,Loss,dLdy,v,epsilon=\n  T2PC.PCInfer(model,LossFun,X,Y,\"Exact\")\n\\end{verbatim}\nThe inputs \\texttt{vinit}, \\texttt{eta}, and \\texttt{n} are not used for computing exact gradients, so they do not need to be passed in. Theorem~\\ref{T:1} says that \n\\begin{verbatim}\nT2PC.PCInfer(model,LossFun,X,Y,\"FixedPred\",eta=1,n=len(model))\n\\end{verbatim}\ncomputes the same values as \n\\begin{verbatim}\nT2PC.PCInfer(model,LossFun,X,Y,\"Exact\")\n\\end{verbatim}\nup to numerical floating point errors. \nThe inputs \\texttt{eta}, \\texttt{n}, and \\texttt{vinit} are optional. If they are omitted by calling\n\\begin{verbatim}\nT2PC.PCInfer(model,LossFun,X,Y,ErrType)\n\\end{verbatim}\nthen they default to \\texttt{eta=.1,n=20,vinit=None} which produces \\texttt{vinit=vhat} when \\\\\n\\texttt{ErrType=\"Strict\"}. \nMore complete documentation and a complete example is provided as \\\\\n \\verb#SimpleExample.ipynb# in the GitHub repository and in the code accompanying this paper. More examples are provided by the code accompanying each figure above. \n\n\n\\section{Discussion}\n\nI reviewed and slightly extended previous work~\\cite{millidge2020predictive,whittington2017approximation,salvatori2021predictive,song2020can} on the relationship between predictive coding and backpropagation for learning in neural networks. I found that a strict interpretation of predictive coding does not accurately approximate backpropagation, but is still capable of learning (Figs.~\\ref{F:PC} and \\ref{F:PCErrs}). \nPrevious work  proposed a modification to predictive coding called the ``fixed prediction assumption'' which causes predictive coding to converge to the same parameter updates produced by backpropagation, under the assumption that the predictive coding iterations converge to fixed points. \n\nHence, the relationship between predictive coding and backpropagation identified in previous work relies critically on the fixed prediction assumption. Formal derivations of predictive coding in terms of approximate variational inference~\\cite{buckley2017free} does not produce the fixed prediction assumption. It is possible that an alternative probabilistic model or alternative approaches to the variational formulation could help formalize a model of predictive coding under the fixed prediction assumption. \n\nI proved analytically and verified empirically that taking a step size of $\\eta=1$ in the modified predictive coding algorithm computes the exact gradients computed by backpropagation in a fixed number of steps (modulo floating point numerical errors). This result is consistent with similar, but slightly less general, results in previous work~\\cite{salvatori2021predictive,song2020can}. \n\nA closer inspection of the algorithm that results from taking $\\eta=1$ under the fixed prediction assumption shows that it is functionally equivalent to a direct implementation of backpropagation. As such, the neural architecture and machinery needed to implement predictive coding with the fixed prediction assumption could also implement backpropagation directly. These results call into question whether predictive coding with the fixed prediction assumption is any more biologically plausible than a direct implementation of backpropagation. \n\n\n\nVisualizing the beliefs and prediction errors produced by predictive coding models applied to a large convolutional neural network pre-trained on ImageNet showed that beliefs and prediction errors were activated by distinct parts of input images, and the parts of the images that produced larger prediction errors were not always consistent with an intuitive interpretation of prediction errors as representing surprising or unexpected features of inputs. \n\nWhen interpreting artificial deep neural networks as models of biological neuronal networks, it is common to compare activations in the artificial network to biological neurons' firing rates~\\cite{SchrimpfKubilius2018BrainScore,Schrimpf2020integrative}. However, under predictive coding models and other models in which errors are propagated upstream by feedback connections, many biological interpretations posit the existence of ``error neurons'' that encode the errors sent upstream. In most such models (including predictive coding), error neurons reflect or approximate the gradient of the loss function with respect to artificial neurons' activations, $\\delta_\\ell$. Any model that hypothesizes the neural representation of backpropagated errors would predict that some recorded neural activity should reflect these errors. Therefore, if we want to draw analogues between artificial and biological neural networks, the activity of biological neurons should be compared  to both the activations {\\it and} the gradients of artificial neurons.  \n\n\n\n\n\nThe Brain-Score project provides an ideal environment for such a comparison. Brain-Score is a framework for directly comparing the activity of deep neural networks to cortical recordings from visual cortex~\\cite{SchrimpfKubilius2018BrainScore,Schrimpf2020integrative}. Several deep neural networks have been tested in Brain-Score, including VGG-19, but these tests only compare activations to the activity of biological neurons. In upcoming work, I will repeat the testing of VGG-19 in Brain-Score, but also include the gradients, $\\delta_\\ell$, and (where possible) prediction errors, $\\epsilon_\\ell$, in the comparisons.\n\nFollowing previous work~\\cite{millidge2020predictive,whittington2017approximation}, I took the covariance matrices underlying the probabilistic model to be identity matrices, $\\Sigma_\\ell=I$, when deriving the predictive coding model. Even with this assumption, predictive coding produces similar learning performance to backpropagation, which suggests that relaxing this assumption could lead to improvements over backpropagation. Future work will focus on these possibilities. Previous work derives local learning rules that account for non-diagonal covariance matrices~\\cite{bogacz2017tutorial}.\n\nPredictive coding and deep neural networks (trained by backpropagation) are often viewed as competing models of brain function. However, their relationships studied here suggest that they can be viewed as two different ways of looking at the same or similar learning algorithms. Understanding their relationship can help in the interpretation and implementation of each algorithm and their mutual relationships to biological neuronal networks. \n\n\n\n", "meta": {"timestamp": "2021-06-28T02:16:59", "yymm": "2106", "arxiv_id": "2106.13082", "language": "en", "url": "https://arxiv.org/abs/2106.13082"}}
{"text": "\\section{Introduction}\n\n\nOnline social media forums play a critical role in health-related information sharing~\\cite{record2018sought}. \nHealth experts have noted that they can help reduce healthcare inequalities and improve access to health care, for instance by empowering coalitions of people living with chronic illness or specific disabilities~\\cite{griffiths2012social}, or by providing an anonymous forum for people seeking emotional support~\\cite{de2014mental}.\nOn the other hand, these forums elevate concerns about spreading medically inaccurate, misleading, or unsound information \\cite{dominguez2015pediatric,gage2018cancer}, which has had harmful public health impacts \\cite{poland2011age,nobles2019requests}.\nOne study concluded that health information seekers in forums such as Reddit are likely to enact suggested behaviors regardless of perceived credibility \\cite{record2018sought}.\nHowever, the researchers also noted that this openness to information could be an opportunity for experts to encourage healthy behaviors through information sharing.\n\nIn this landscape, it is critical to understand the dynamics that cultivate safe communities that benefit the health and well-being of their participants and the broader implications for health communication \\cite{chou2009social}.\nHealth experts are thus considering social media's role in their interactions with patients and broader public health issues, and their role in engaging with the platforms~\\cite{dominguez2015pediatric,nobles2019requests,nobles2020examining}.\nThis motivates an important research direction: understanding how experts engage with users in online platforms.\nThis can inform platform design, moderation decisions, and health promotion efforts by experts.\n\nThis work focuses on understanding the engagement with professionals in the domain of mental health with two main research questions: (RQ1) Do experts have distinct influences as compared to non-experts in their interactions with support-seekers in online mental health?; and (RQ2) Do the experts' behaviors reflect established counseling principles and findings regarding behaviors associated with positive counseling outcomes?\nTo answer these questions, we analyze responses from self-identified mental health professionals (MHP) to support-seekers in mental health and support communities on Reddit, and compare them to responses from other users who we refer to as \\textit{peers}. This is an important comparison, as many peers share similar health experiences, which prior work has found is associated with higher empathic concern~\\cite{hodges2010giving}. \n\nFirst, we test whether a text classifier can distinguish between responses to support seekers from MHPs and peers.\nWe find that it can, with 70\\% accuracy (well above random chance of 50\\%).\nSecond, we analyze comments leading to further engagement with the support-seeking posters, as existing counseling principles emphasize the importance of eliciting client engagement in expert counseling sessions~\\cite{miller2012motivational,perez2018analyzing}. Third, we analyze the users' linguistic tendencies, drawing inspiration from analyses of counseling conversations, which have offered insight into counselor behaviors associated with high quality sessions grounded in existing theories from psychology and counseling research using computational methods~\\cite{althoff2016large,perez2018analyzing,zhang2019finding,miller2012motivational}.\n\nThe main contributions of this work are: (1) We construct a dataset of mental health conversations from Reddit users with self-identified counseling expertise, covering a set of mental health subreddits annotated with categories denoting the type of mental health concern; (2) We develop a classifier that can distinguish between the language of MHPs and that of peers; (3) We perform an analysis of the differences in language use between MHPs and peers; and (4) We provide insight into language that leads to further engagement with support-seekers, comparing responses to peers and MHPs.\n\n\n\n\\section{Related Work}\n\n\n\n\\begin{table*}[t]\n  \\parbox{.24\\linewidth}{\n        \\centering\n        \\small\n        \\begin{tabular}{lr}\n            \\toprule\n            Subreddits & 77 \\\\\n            Posts & 12,140 \\\\\n            Poster Replies & 24,357 \\\\\n            MHPs & 283 \\\\\n            Peers & 56,701 \\\\\n            \\midrule\n            \\multicolumn{2}{c}{Comments} \\\\\n            \\midrule\n            MHP & 9,685 \\\\\n            Peer & 92,698 \\\\\n            Total & 102,383 \\\\\n            \\midrule\n            \\multicolumn{2}{c}{Thread Length} \\\\\n            \\midrule\n            Mean & 8.4 \\\\\n            Median & 4 \\\\\n            Max & 64 \\\\\n            \\bottomrule\n        \\end{tabular}\n        \\caption{Dataset statistics.}\n        \\label{tab:data_stats}\n    }\n    \\hfill\n    \\parbox{.72\\linewidth}{\n        \\small\n        \\centering\n        \\begin{tabular}{ll}\n             \\toprule\n             \\textbf{Post:} & u/peer\\_user\\_X \\\\\n             \\midrule\n             \\multicolumn{2}{p{0.9\\linewidth}}{I've recently been struggling with paranoid thoughts, for which I was hospitalized for my own safety. I do not feel suicidal anymore, however everyday is a long struggle of thinking everyone is an undercover agent out to get me or keep tabs on what I'm doing. I was hoping to hear some tips and stories if anyone else has dealt with similar thoughts and overcome them? Or are they something I will have to deal with for the rest of my life? Thanks in advance}\n    \\\\\n            \\midrule\n             \\textbf{Comment:} & u/MHP\\_user\n             \\begin{tikzpicture}\n                \\tikz[baseline={([yshift=-0.4em]current bounding box.north)}] \\draw[rounded corners=5pt,xshift=1.0em] (-0.4,-0.2) rectangle (0.4,0.2) node[pos=.5] {LPC};\n            \\end{tikzpicture}\n             \\\\\n             \\midrule\n             \\multicolumn{2}{p{0.9\\linewidth}}{Paranoid thoughts are scared thoughts, justified or not. If you ignore the specific content of the thoughts and focus on the emotional valence (scared), is there something you can do in those moments to feel safer?} \\\\\n            \\midrule\n             \\textbf{Poster Reply:} & u/peer\\_user\\_X \\\\\n             \\midrule\n             \\multicolumn{2}{p{0.9\\linewidth}}{That's a good way of thinking about the situations as they arise. I will try to do that} \\\\\n             \\bottomrule\n        \\end{tabular}\n        \\caption{Example of an initial post, a reply from an MHP with the flair \\textit{LPC} (Licensed Professional Counselor), and a reply from the original user.}\n        \\label{tab:data_sample4}\n    }\n\\end{table*}\n\n\nStudies within the education and health domains have shown that advice and help-seeking interactions in online communities contribute positively to users' well-being, learning, and skills development~\\cite{Campbell16,wang2015eliciting}. This is particularly true\nfor applications such as computer programming, career development, mentoring, coping with chronic or life-threatening diseases, and mental health issues~\\cite{baltadzhieva-chrupala-2015,Tomprou19,wang2015eliciting,de2014mental}.\n\nIn the mental health domain, studies have explored online support communities and many have found positive outcomes associated with anonymity, perceived empathy, and active user engagement~\\cite{de2014mental, rheingold1993virtual, hodges2010giving, welbourne2009supportive, nambisan2011information}. Computational approaches have aided studies in mental health forums, helping reveal positive relationships between linguistic accommodation and social support across subreddits~\\cite{sharma2018support}.\nOne example of insights from this work is that topic-focused communities like subreddits may enable more peer-engagement than non-community based platforms~\\cite{sharma2020engagement}. Other studies have revealed certain trade-offs of online support platforms, such as disparities in the level of support offered toward support-seekers of various demographics~\\cite{wang2018s,nobles2020examining} and in condolences extended across different topics of distress~\\cite{zhou2020condolences}. Studying MHP behaviors in such scenarios might help develop approaches that balance these trade-offs.\n \nComputational approaches applied in these forums have also shed light on population-level health trends and health information needs, with examinations into how depression and post-traumatic stress disorder (PTSD) affect different demographic strata~\\cite{amir2019mental}.\nData mining has also been applied to understand adverse drug reactions~\\cite{wang2014sideeffectptm} and public reactions towards infectious diseases~\\cite{park2017tracking}. \\citet{nobles2018std} highlighted the potential for these forums to aid targeted health communication, for example by sharing information in r/STD, a subreddit about sexually transmitted diseases. Another case study of r/STD revealed the prevalence of diagnoses requests, and suggested that health professionals could partner with social media platforms to positively influence crowd-sourced diagnoses and help mitigate harmful misdiagnoses~\\cite{nobles2019requests}. \\citet{record2018sought} found that health information seeking Reddit users are likely to enact suggested behaviors regardless of perceived credibility, providing further reason for health expert engagement to intervene when harmful information sharing occurs and promote healthy behavior.\n \n Fewer studies have analyzed expert interactions in online forums. A study in a large Q\\&A community found that experts are more likely to provide help than peers and that their participation in discussions resulted in increased length and substance of discussions~\\cite{Procaci17}.\nRecent studies have compared interactions with experts to interactions with peers in broader scientific communities~\\cite{park2020trust} and r/AskDocs on Reddit~\\cite{nobles2020examining}.\nThe latter paper closely relates to our study, as they also consider posts from experts on Reddit, but solely within r/AskDocs about different health topics and with users of varying demographics.\n\n\nThe insights discussed above motivate investigations into how health experts and other users promote scientifically sound advice and offer supportive responses to health information seekers in online forums. In this work, we aim to contribute additional insights into expertise influence in online mental health communities by studying the dynamics of the communication process between support seekers and support providers.\n\n\n\n\\section{Data Collection}\\label{sec:dataset}\n\nWe seek to understand the tendencies of users with professional experience, and more specifically counseling expertise, when interacting with support-seekers in online mental health and support-related forums.\nIn uncovering which tendencies are associated with expertise, we enable further investigation into their role in the social dynamics of online support-seeking interactions, and potential applications of insight-driven recommendations for moderators and users of these forums.\n\n\n\n\\begin{figure*}\n    \\centering\n\\begin{tikzpicture}\n\\begin{axis}[\n    width=\\linewidth,\n    height=3.5cm,\n    grid=both,\n    bar width=.25cm,\n    legend style={font=\\small},\n    legend pos=north west,\n    symbolic x coords={health, tentat, work, anx, you, differ, discrep, they, foc.future, interrog, cogproc, quant, insight, risk, relig, assent, sad, motion, number, nonflu, we, ppron, time, leisure, ingest, family, body, home, anger, friend, filler, shehe, informal, foc.past, female, sexual, male, I, netspeak, swear}, \n    xtick=data,\n    enlarge x limits=0.01,\n    x tick label style={rotate=60, anchor=east, font=\\footnotesize},\n    ytick={0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2},\n    y tick label style={anchor=east, font=\\footnotesize}]\n    \\addplot[ybar,fill=lightgray,ybar legend] coordinates  {\n    \t(health, 0.7959860332257565)\n    \t(tentat, 0.8055825728949596)\n    \t(work, 0.8110132828134121)\n    \t(anx, 0.8111684471647727)\n    \t(you, 0.8210292413944864)\n    \t(differ, 0.8553742434797355)\n    \t(discrep, 0.8692382037337468)\n    \t(they, 0.8751238940358339)\n    \t(foc.future, 0.8915196663317453)\n    \t(interrog, 0.8989293471898235)\n    \t(cogproc, 0.9000317012729139)\n    \t(quant, 0.9003896144608834)\n    \t(insight, 0.9057980632031888)\n    \t(risk, 0.9078690597633751)\n    \t(relig, 1.1016018716659848)\n    \t(assent, 1.1058051745294315)\n    \t(sad, 1.111442609622524)\n    \t(motion, 1.1147313092899014)\n    \t(number, 1.123251240900814)\n    \t(nonflu, 1.1346822371433698)\n    \t(we, 1.1514960646135939)\n    \t(ppron, 1.1743292153901252)\n    \t(time, 1.1760614995095569)\n    \t(leisure, 1.1907022197071266)\n    \t(ingest, 1.268569982423503)\n    \t(family, 1.3287365916478084)\n    \t(body, 1.332310541479952)\n    \t(home, 1.3387387079172697)\n    \t(anger, 1.352991457094508)\n    \t(friend, 1.4228051728743212)\n    \t(filler, 1.4910379699677137)\n    \t(shehe, 1.506362829049222)\n    \t(informal, 1.5087378339468138)\n    \t(foc.past, 1.519990107618276)\n    \t(female, 1.553857654748661)\n    \t(sexual, 1.575562715003554)\n    \t(male, 1.6370479733076038)\n    \t(I, 1.7324946157824643)\n    \t(netspeak, 1.8811274256235888)\n    \t(swear, 2.1973744687818817)\n    }; \\addlegendentry{Peers}\n    \\addplot[ybar,fill=white,ybar legend] coordinates  {\n    \t(health, 0.7959860332257565)\n    \t(tentat, 0.8055825728949596)\n    \t(work, 0.8110132828134121)\n    \t(anx, 0.8111684471647727)\n    \t(you, 0.8210292413944864)\n    \t(differ, 0.8553742434797355)\n    \t(discrep, 0.8692382037337468)\n    \t(they, 0.8751238940358339)\n    \t(foc.future, 0.8915196663317453)\n    \t(interrog, 0.8989293471898235)\n    \t(cogproc, 0.9000317012729139)\n    \t(quant, 0.9003896144608834)\n    \t(insight, 0.9057980632031888)\n    \t(risk, 0.9078690597633751)\n    }; \\addlegendentry{MHPs}\n    \\addplot[draw=blue,ultra thick,smooth] \n    coordinates {(health, 1)(tentat, 1)(work, 1)(anx, 1)(you, 1)(differ, 1)(discrep, 1)(they, 1)(foc.future, 1)(interrog, 1)(cogproc, 1)(quant, 1)(insight, 1)(risk, 1)(relig, 1)(assent, 1)(sad, 1)(motion, 1)(number, 1)(nonflu, 1)(we, 1)(ppron, 1)(time, 1)(leisure, 1)(ingest, 1)(family, 1)(body, 1)(home, 1)(anger, 1)(friend, 1)(filler, 1)(shehe, 1)(informal, 1)(foc.past, 1)(female, 1)(sexual, 1)(male, 1)(I, 1)(netspeak, 1)(swear, 1)};\n    \\end{axis}\n    \\node[align=center,rotate=90,font=\\small] at (-1.15cm, 0.9cm) {Dominance};\n    \\node[align=center,rotate=90,font=\\small] at (-0.8cm, 0.9cm) {(peer/MHP)};\n    \\end{tikzpicture}\n    \\caption{LIWC category dominance scores, computed as the relative use by peers divided by the relative use by MHPs, so that equal use is at $y=1$ (blue line), higher dominance by peers at $y>1$ (grey bars) and higher dominance by MHPs at $y<1$ (white bars). Showing categories where frequency of use differs by at least 10\\%.}\n    \\label{fig:liwc}\n\n\\end{figure*}\n\n\n\\mysubsection{Source.}\nWe use Reddit for its quantity of publicly available interactions in communities called \\textit{subreddits} that discuss mental health issues.\nIn addition, Reddit has a system that allows users to indicate their professional expertise (Reddit Flairs), which we use to identify a set of users with mental health professional background, identified as \\textit{MHPs} during our study.\nWe obtained flairs from the r/psychotherapy subreddit,\\footnote{\\href{https://old.reddit.com/r/psychotherapy/wiki/acronyms\\#wiki_degrees_and_licenses}{Degree and license flair descriptions from r/psychotherapy wiki.}} a decision motivated by their reliability, as the moderators of this community allow comments and posts only by licensed therapy providers who may be asked to submit proof if concerns of falsely posing as a therapist arise.\\footnote{See rule 2 and 9 in \\url{https://www.reddit.com/r/psychotherapy/}, also listed in Appendix~\\ref{sec:appendix_flairs}.}\nSample flair tags in this set are: Psychiatrist (sometimes accompanied by MD or DO), LPC (or Licensed Professional Counselor), LMFT (or Licensed Marriage and Family Therapist), PsyD (or Doctorate of Psychology).\n\nWe use an existing list of mental health subreddits from r/ListOfSubreddits\\footnote{\\href{https://www.reddit.com/r/ListOfSubreddits/comments/dmic6o/advice_mental_health_subreddits/}{r/ListOfSubreddit's compilation of mental health and advice subreddits.}}\nwith additions from manual observations; all of the subreddits in our dataset with their number of comments are in Appendix~\\ref{sec:appendix_datacollection} in Table~\\ref{tab:appendix_subbreddits_and_comments}.\nFrom these, we retrieve threads where an MHP submitted a direct reply. During this step, we also kept posts made by peers i.e., individuals who did not use any of the mental health care professional flairs. Our collection spans threads created between November 29, 2009 and December 21, 2020. Table~\\ref{tab:data_stats} shows descriptive statistics for the final composition of the dataset, and  Table~\\ref{tab:data_sample4} shows a sample interaction demonstrating the structure we use for our analysis. This study focuses on direct replies to the poster, thus we attempt to eliminate \\textit{megathreads} which tend not to focus in individual support-seekers by removing those above the 95th percentile in their number of direct replies; we leave analysis of deeper nested replies for future work.\n\n\n\n\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{ll}\n\n\\toprule\nKey & Topic \\\\\n\\midrule\nTrauma & Trauma \\& Abuse \\\\\nAnx & Psychosis \\& Anxiety \\\\\nCompuls. & Compulsive Disorders \\\\\nCope & Coping \\& Therapy \\\\\nMood & Mood Disorders \\\\\nAddict. & Addiction \\& Impulse Control \\\\\nBody & Eating \\& Body  \\\\\nNeurodiv. & Neurodevelopmental Disorders  \\\\\nHealth & General \\\\\nSocial & Broad Social \\\\\n\\bottomrule\n    \\end{tabular}\n    \\caption{Health condition and other subreddit topics. Keys are shortened names we use to refer to the topics.}\n    \\label{tab:subreddit_health_categories}\n\\end{table}\n\n\\mysubsection{Health Topics.} To understand whether particular topics influence interactions with support-seekers, we group the subreddits into broader topics based on related health domains.\nWe begin by following the categorization of subreddits by \\citet{sharma2018support}, who used the $k$-means clustering algorithm to generate initial clusters on the $n$-grams $(n=3)$ of the posts and manually refined the categories based the community descriptions in their subreddit home pages.\nNext, we adjust the categories and their associated subreddits based on the World Health Organization's ICD-10 classification system of mental and behavioural disorders.\\footnote{\\url{https://www.who.int/substance_abuse/terminology/icd_10/en/}} The resulting topic categories are listed in Table~\\ref{tab:subreddit_health_categories} alongside shortened names which we use to refer to them. The full list of subreddits assigned to each topic are listed in Appendix~\\ref{sec:appendix_datacollection} in Table~\\ref{tab:appendix_subreddit_categories}.\n\n\n\n\\section{Distinguishing MHPs and Peers}\\label{sec:classification}\n\nTo begin our investigation into the linguistic behaviors of MHPs and peers, we test whether simple text classifiers are able to distinguish between comments authored by either MHPs or peers.\nWe build three classifiers with different feature sets; the first are unigram counts for unigrams occurring at least five times, the second includes counts for the 73 word classes in the LIWC (Linguistic Inquire and Word Count) lexicon~\\cite{pennebakerlinguistic}, and the third encodes a subset of LIWC word classes associated with perspective shifts (i.e., \\textit{focusfuture, focuspast, focuspresent, I, ipron, negemo, posemo, ppron, pronoun, shehe, they, we,} and \\textit{you})~\\cite{althoff2016large}; we elaborate on the psychological meaning behind these features in our analyses in the next section.\n\n\nDue to the class imbalance between the peer and MHPs classes, we first downsampled the peer class to get a balanced distribution with the MHP class. This resulted in a set of 9,685 instances per class. We conduct our evaluations using ten-fold cross validation. Across these folds, the number of features ranges from 8,668 to 8,703.\nWe use a Naive Bayes model, implemented with Sklearn\u2019s MultinomialNB module,~\\footnote{\\url{https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html}} which outperformed a logistic regression model and an SVM in preliminary experiments.\\footnote{Runs in $\\sim$40 seconds per fold on one AMD Ryzen 7 3700U CPU.}\n\n\n\nAll models outperform a random baseline\\footnote{$p<0.0001$ using a permutation test~\\cite{dror-etal-2018-hitchhikers2}} with all LIWC features bringing the accuracy to 59.12\\%, LIWC perspective features to 59.14\\%, and unigram features to 70.80\\%.\nOverall, the classification results indicate language differences exist between the MHPs and peers. Motivated by this result, we proceed to several analyses to gain insights.\n\n\\section{Linguistic and Dialogue Analysis}\\label{sec:ling_and_dialog}\n\nWe analyze the linguistic behaviors of MHPs and peers responding to support-seeking posts, and their potential influence in eliciting further engagement with the support-seeker.\nOur analyses are inspired by psychology and computational studies that have shown that conversational behavioral aspects such as word usage, client engagement, and language matching are positively related to successful counseling interactions~\\cite{gonzales2010language,althoff2016large,perez2018analyzing,zhang2019finding}.\n\n\n\n\n\\begin{figure}[t]\n    \\centering\n\\begin{tikzpicture}\n\\begin{axis}[\n    width=8.25cm,\n    height=3cm,\n    grid=both,\n    symbolic x coords={Disgust, Surprise, Anger, Fear, Sadness, Joy},\n    xtick=data,\n    x tick label style={rotate=0, font=\\footnotesize}, \n    ytick={0.9, 1.1, 1.3, 1.5, 1.7},\n    y tick label style={anchor=east, font=\\footnotesize}]\n    \\addplot[ybar,fill=lightgray] coordinates  {\n    \t(Disgust, 1.71028997750357)\n    \t(Surprise, 1.3983259978898)\n    \t(Anger, 1.14086998542706)\n    \t(Fear, 1.06922731579117)\n    \t(Sadness, 1.06749537047073)\n    \t(Joy, 1.00660420378226)\n    }; \n\\end{axis}\n\\node[align=center,rotate=90,font=\\small] at (-0.8cm, 0.7cm) {Dominance};\n\\end{tikzpicture}\n    \\caption{WordNet Affect usage (peers / MHPs)}\n    \\label{fig:wordnet_affect}\n\\end{figure}\n\n\\subsection{Linguistic Ethnography}\\label{sec:linguistic_ethno}\n\n\nNumerous studies have demonstrated relationships between the dominant usage of certain word categories with \nindividuals' psychological and physical health~\\cite{tausczik2010psychological,weintraub1981verbal,rude2004language}. \nIn alignment with these studies, we investigate the usage of such word categories using the LIWC and WordNet-Affect lexicons~\\cite{pennebakerlinguistic,wordnet-affect}.\n\nFor each group of users, we first compute the proportion of their words that fall in each category. Then, we compute the dominant use by dividing the proportion for peer users over the proportion for MHPs~\\cite{mihalcea2009linguistic}.\nFigure~\\ref{fig:liwc} shows \nLIWC categories where the rate of use differs by at least 10\\%, and results for WordNet Affect categories are shown in Figure~\\ref{fig:wordnet_affect}.\n\nSome observations such as the higher dominance of swear words (\\textit{swear}) and internet speak (\\textit{netspeak}) might be expected if professionals avoid such language. An interesting contrast in peers' language is the dominant use of first-person pronouns (\\textit{I, we}) and focus on the past (\\textit{focuspast}). In contrast, MHPs seem to use more non-first person pronouns (\\textit{you, they}) and focus on the future (\\textit{focusfuture}) instead.  Peers' use of first person pronouns might arise when they share similar experiences with support-seekers. MHPs' use of second-person pronouns might suggest they are focusing on the support-seekers' experiences as a counselor would with a client in a counseling encounter.\nWe also observe higher dominance of all WordNet Affect categories among peers, however the \\textit{joy} category (the most positive), is nearly equal with MHPs.\n\n\nThese observations of the peers' language are compelling because they align with existing theories linking depression to negative views of the future (i.e., \\textit{focuspast} and negative WordNet affects)~\\cite{pyszczynski1987depression} and self-focusing style (i.e., first-person pronouns)~\\cite{pyszczynski1987self,campbell2003secret}. Likewise, clients of SMS-based crisis counseling conversations were more likely to report feeling better after the encounter if they exhibited perspective shifts from these categories to their counterparts (i.e., toward \\textit{focusfuture}, non-first person pronouns, and positive sentiment)~\\cite{althoff2016large}.\n\nInterestingly, the same study found clients were more likely to shift perspective when their counselors exhibited use of the counterpart categories first, suggesting that the counselors may play a key role in helping drive the perspective shift. Given those positive outcomes, observing the same dominant linguistic aspects among MHPs is encouraging and potentially signals a connection between how counselors apply conversational behaviors in practice and in online forum interactions. Future work can investigate the progression of dialogue between MHPs and support-seekers to find if support-seekers similarly exhibit the perspective shifts associated with the positive outcomes of the prior study, and likewise whether users of the forums also experience positive outcomes where this occurs.\n\n\n\\begin{figure*}[t]\n   \\parbox{.53\\linewidth}{\n        \\centering\n        \\small\n        \\begin{tikzpicture}\n\\begin{axis}[\n    width=1.1\\linewidth,\n    height=3.7cm,\n    grid=both,\n    symbolic x coords={filler, health, body, bio, cause, tentat, you, affect, time, nonflu, certain, netspeak, hear, affiliation, foc.past, female, number, shehe, friend, leisure, male, ingest, anger, feel, sexual, informal, i, swear, assent, relig},\n    xtick=data,\n    enlarge x limits=0.01,\n    bar width=.15cm,\n    x tick label style={rotate=60, anchor=east, font=\\small},\n    ytick={0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3},\n    y tick label style={anchor=east, font=\\small}]\n    \\addplot[ybar,fill=white] coordinates  {\n        (filler, 1.3694959488399698)\n        (health, 1.2199518622464156)\n        (body, 1.1963945578342048)\n        (bio, 1.1361700435532724)\n        (cause, 1.0795619328576351)\n        (tentat, 1.0684715763913766)\n        (you, 1.0647948805231773)\n        (affect, 0.952284583106825)\n        (time, 0.9517636175949772)\n        (nonflu, 0.9511736159516474)\n        (certain, 0.9483112140262707)\n        (netspeak, 0.9463188721091061)\n        (hear, 0.9389897621251143)\n        (affiliation, 0.9356603838607868)\n        (foc.past, 0.932086788032833)\n        (female, 0.9316496264858395)\n        (number, 0.9205180739190895)\n        (shehe, 0.9183051123944717)\n        (friend, 0.8982679230950917)\n        (leisure, 0.8969320017601914)\n        (male, 0.890290971657498)\n        (ingest, 0.8876147841875732)\n        (anger, 0.8792607168321827)\n        (feel, 0.8702617564065401)\n        (sexual, 0.8668272078839886)\n        (informal, 0.8624389844393203)\n        (i, 0.8066956501832807)\n        (swear, 0.7870234652664857)\n        (assent, 0.7822245598953113)\n        (relig, 0.6755071110674885)\n    };\n    \\addplot[draw=blue,ultra thick,smooth]\n    coordinates {(filler, 1.0)(health, 1.0)(body, 1.0)(bio, 1.0)(cause, 1.0)(tentat, 1.0)(you, 1.0)(affect, 1.0)(time, 1.0)(nonflu, 1.0)(certain, 1.0)(netspeak, 1.0)(hear, 1.0)(affiliation, 1.0)(foc.past, 1.0)(female, 1.0)(number, 1.0)(shehe, 1.0)(friend, 1.0)(leisure, 1.0)(male, 1.0)(ingest, 1.0)(anger, 1.0)(feel, 1.0)(sexual, 1.0)(informal, 1.0)(i, 1.0)(swear, 1.0)(assent, 1.0)(relig, 1.0)};\n\\end{axis}\n\\node[align=center,rotate=90,font=\\small] at (-1.0cm, 1.0cm) {Dominance Score};\n\\end{tikzpicture}\n        \n    }\n    \\hskip1em\n    \\parbox{.39\\linewidth}{\n        \\small\n        \\centering\n        \\begin{tikzpicture}\n\\begin{axis}[\n    width=1.2\\linewidth,\n    height=3.7cm,\n    grid=both,\n    symbolic x coords={you, foc.future, interrog, shehe, filler, male, health, nonflu, affiliation, motion, anger, home, number, informal, certain, leisure, assent, netspeak, swear, relig, i, friend, we, death},\n    xtick=data,\n    enlarge x limits=0.01,\n    bar width=.15cm,\n    x tick label style={rotate=60, anchor=east, font=\\small},\n    ytick={0.7, 0.8, 0.9, 1.0, 1.1},\n    y tick label style={anchor=east, font=\\small}]\n    \\addplot[ybar,fill=lightgray] coordinates  {\n        (you, 1.1056504063587804)\n        (foc.future, 1.0832136349699277)\n        (interrog, 1.0690181983665104)\n        (shehe, 1.0677545462122848)\n        (filler, 1.0562119255014193)\n        (male, 1.052349793324354)\n        (health, 1.0510277322615278)\n        (nonflu, 0.9500031679667199)\n        (affiliation, 0.9457110526134682)\n        (motion, 0.9432732829160265)\n        (anger, 0.9416776171900192)\n        (home, 0.9377753113453201)\n        (number, 0.9338331405276905)\n        (informal, 0.9324061632862549)\n        (certain, 0.9316400785214803)\n        (leisure, 0.927558556791122)\n        (assent, 0.9206957666641701)\n        (netspeak, 0.9202391071079117)\n        (swear, 0.9141919663144525)\n        (relig, 0.9118338999145017)\n        (i, 0.9117467370956133)\n        (friend, 0.8866945520809406)\n        (we, 0.8279335154078911)\n        (death, 0.8171034372572407)\n    };\n    \\addplot[draw=blue,ultra thick,smooth]\n    coordinates {(you, 1.0)(foc.future, 1.0)(interrog, 1.0)(shehe, 1.0)(filler, 1.0)(male, 1.0)(health, 1.0)(nonflu, 1.0)(affiliation, 1.0)(motion, 1.0)(anger, 1.0)(home, 1.0)(number, 1.0)(informal, 1.0)(certain, 1.0)(leisure, 1.0)(assent, 1.0)(netspeak, 1.0)(swear, 1.0)(relig, 1.0)(i, 1.0)(friend, 1.0)(we, 1.0)(death, 1.0)};\n\\end{axis}\n\\end{tikzpicture}\n    }\n    \\caption{Dominance of LIWC categories, computed as the category relative frequencies among comments that \\textbf{prompt support-seeker responses} divided by the relative frequencies among comments that do not, computed separately for MHPs (left) and peers (right).}\n    \\label{fig:engagement_dominance}\n\\end{figure*}\n\\subsection{Engaging Support-Seekers}\\label{sec:engagement}\n\nTo understand if linguistic behaviors are associated with prompting further engagement with the support-seeker, we compare the dominance of LIWC categories in comments receiving replies compared to comments that do not by dividing the usage rates of the former by the latter. Figure~\\ref{fig:engagement_dominance} shows these ratios for categories that differ by at least 5\\%. A compelling observation is the dominance of the categories \\textit{health}, \\textit{tentat}, and \\textit{you} in the MHP comments prompting poster-replies, and \\textit{you}, \\textit{focusfuture}, \\textit{interrog}, and \\textit{health} in the peer comments prompting poster-replies, as was exhibited among MHPs (see Figure~\\ref{fig:liwc} in Section~\\ref{sec:linguistic_ethno}); on the other end, the categories are more dominant in comments that do not engage a reply such as \\textit{I}, \\textit{we}, \\textit{death}, \\textit{friend}, \\textit{relig}, \\textit{swear}, were similarly represented as dominant categories in the peer group.\n\nTo gain further insight into these observations, we perform the following analysis: for each user group (peers and MHPs) we use a foreground corpus of their comments that were replied to by the support-seekers, and a background corpus of their comments that were not, and compute the dominance of LIWC categories of the foreground over the background as a ratio of their relative frequencies. \nWe then rank the categories by highest to lowest dominance scores, and refer to this ranking by \\texttt{DRR} (for \\textbf{D}ominance \\textbf{R}ank for \\textbf{R}eplied comments). We compare the \\texttt{DRR}s of each user group to the ranking of LIWC category usage among MHP users and among peer users separately (from Section~\\ref{sec:linguistic_ethno}) by computing the Kendall Tau's coefficient between them.\nA positive correlation would thus indicate that the more (or less) dominant categories among a group's replied comments are also more (or less) dominant among the other group overall.\nThe correlation coefficients are shown in Table~\\ref{tab:ranked_comparison}.\n\n\\begin{table}\n    \\centering\n    \\small\n    \\begin{tabular}{llrr}\n        \\toprule\n        \\texttt{DRR} Group & OR Group &  $\\tau$ & p-value \\\\\n        \\midrule\n        Peer & MHP & .191 & .017 \\\\\n        MHP & MHP & .158 & .048 \\\\\n        Peer & Peer & -.031 & .689 \\\\\n        MHP & Peer & .008 & .916 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Kendall $\\tau$'s coefficient between the LIWC category dominance ranking in the replied comments (\\texttt{DRR}) of the user group on the left and the overall ranking of LIWC category usage (\\texttt{OR}) by the user group to the right.\n    }\n    \\label{tab:ranked_comparison}\n\\end{table}\n\nInterestingly, we observe a slight positive correlation between the \\texttt{DRR}s for both MHPs and peers with the overall LIWC category usage ranking for MHPs. On the other hand, we see no correlations with the LIWC usage ranks for peers. Intuitively, it appears that for both MHPs and peers, the comments prompting further engagement with the poster appear to reflect the overall dominant linguistic aspects captured by LIWC of MHPs, but not peers. As counseling principles have emphasized the importance of mutual engagement between counselors and clients~\\cite{miller2012motivational} and other work has shown that higher quality counseling sessions are associated with higher client engagement~\\cite{perez2018analyzing}, it is compelling to observe associations between linguistic aspects of MHPs with the aspects associated with poster-engagement. \n\n\n\\subsection{Linguistic Style Matching}\\label{sec:lsm}\n\nLinguistic Style Matching (LSM) measures the extent to which one speaker matches another~\\cite{gonzales2010language}. It compares two parties' relative use of function words as these words are more indicative of style rather than content~\\cite{ireland2010language}.\n\nPrevious studies in counseling conversations have measured LSM to understand the extent that counselors and clients match their language. \\citet{perez2019makes} showed higher LSM for high quality counseling sessions whereas \\citet{althoff2016large} showed lower LSM for higher quality sessions. \\citet{perez2019makes} attributed this to the differences between the conversations they analyzed, theirs being synchronous face-to-face interactions while \\citet{althoff2016large}'s was of asynchronous text messages, as well as differences in counseling styles.\n\nWe follow \\citet{nobles2020examining}'s approach leveraging \\citet{ireland2010language}'s procedure to measure LSM between support seekers and support providers.\n\n\\begin{figure*}\n    \\centering\n      \\begin{tikzpicture}\n      \\begin{axis}[\n      width  = \\linewidth-2cm,\n      height = 3cm,\n      ybar=2*\\pgflinewidth,\n      ylabel={LSM Score},\n      ylabel style={font=\\small},\n      ymajorgrids = true,\n      symbolic x coords={Trauma,Anx,Compuls.,Cope,Mood,Addict.,Body,Neurodiv.,Health,Social,All},\n      x tick label style={rotate=0, font=\\footnotesize},\n      xtick = data,\n      scaled y ticks = false,\n      ymin=0.4,\n      legend cell align=left,\n    legend style={at={(1.01,0.4)},anchor=west}\n      ]\n      \\addplot[style={fill=white},error bars/.cd, y dir=both, y explicit]\n          coordinates {\n(Trauma,0.6310119156541756) += (0,0.029962442897110475) -= (0,0.03081514525447937)\n(Anx,0.5997199013986588) += (0,0.01519174511662591) -= (0,0.015544352706185927)\n(Compuls.,0.6281754095486042) += (0,0.0357519342555046) -= (0,0.03738872492676926)\n(Cope,0.6473266495067116) += (0,0.010066984978184701) -= (0,0.010552935538052588)\n(Mood,0.6437280427655686) += (0,0.009180262091117664) -= (0,0.009328442085683064)\n(Addict.,0.5709948285212645) += (0,0.015199256087097268) -= (0,0.015134390184438407)\n(Body,0.5887944659204036) += (0,0.04432956318398418) -= (0,0.04696878984370534)\n(Neurodiv.,0.6309063259324916) += (0,0.01391890784736749) -= (0,0.014303173962970095)\n(Health,0.6144863767231595) += (0,0.007425543475062746) -= (0,0.007746525814329885)\n(Social,0.5060244756989379) += (0,0.00862992954522357) -= (0,0.00859438033560228)\n(All,0.5887940258624854) += (0,0.004103442535799973) -= (0,0.0041157209528478456)\n          };\n      \\addplot[style={fill=lightgray},error bars/.cd, y dir=both, y explicit,error bar style=black]\n           coordinates {\n(Trauma,0.6378865787475075) += (0,0.0119576476105826) -= (0,0.012096314353275406)\n(Anx,0.5922057266084959) += (0,0.004617719809004162) -= (0,0.004731922540469169)\n(Compuls.,0.5856424758280658) += (0,0.017818249302948175) -= (0,0.017936496148185377)\n(Cope,0.5887105654136473) += (0,0.0045978888128952455) -= (0,0.004486067568766683)\n(Mood,0.6105143832329811) += (0,0.004169661426287274) -= (0,0.004238386512265935)\n(Addict.,0.5408042001923349) += (0,0.0038984385211480177) -= (0,0.00395860140679527)\n(Body,0.6155639543382515) += (0,0.01529421389797847) -= (0,0.01587817947763348)\n(Neurodiv.,0.5992797132344616) += (0,0.0036899641469281264) -= (0,0.0036385952314014203)\n(Health,0.6089407444992813) += (0,0.0027053359645056174) -= (0,0.002665433221403779)\n(Social,0.6184805035707305) += (0,0.002843520445438563) -= (0,0.0028728095634809048)\n(All,0.5969832369722273) += (0,0.0013587580377160124) -= (0,0.0013578362432092872)\n          };\n      \\legend{MHPs, peers}\n  \\end{axis}\n  \\end{tikzpicture}\n \\caption{LSM scores with 95\\% confidence intervals calculated with non-parametric bootstrap resampling.}\n \\label{fig:lsm}\n\\end{figure*}\nFor a text sequence, we compute the percentage of words that belong to each of nine function-word categories $c$ from the LIWC lexicon, which include \\textit{auxiliary verbs, articles, common adverbs, personal/impersonal pronouns, prepositions, negations, conjunctions,} and \\textit{quantifiers}.\nThen, we compute the LSM of each word category $c$ as shown in Equation \\ref{eq:lsm} where $p$ represents \\textit{post} and $r$ represents the response. The composite LSM score for $p$ and $r$ is the mean of all category LSM scores. For each thread, we separate the MHP and peer replies, and take the mean of all composite LSM scores. \n\n\\begin{equation}\\label{eq:lsm}\n    LSM_c = 1 - \\frac{abs(cat\\%_p - cat\\%_r)}{cat\\%_p + cat\\%_r + .0001}\n\\end{equation}\n\nWe compute these LSM scores over all data together as well as separately for each subreddit topic (named in Table~\\ref{tab:subreddit_health_categories}). The resulting scores are shown in Figure~\\ref{fig:lsm}. \n\nWe observe LSM scores vary by topic, and most are similar for peers and MHPs or have overlapping confidence intervals. Compared to their LSMs in other topics, MHPs score lower in {\\sc Social}, which covers broad social issues that are less specialized to health conditions than the others. However, peers have high LSMs in {\\sc Social} relative to most other topics, and notably higher LSMs than the MHPs. Additionally, MHPs have higher LSMs than those of peers and relative to their own in communities that cover topics of specific compulsive, mood, and neurodevelopmental disorders ({\\sc Compuls., Mood,} and {\\sc Neurodiv.}), communities that orient toward counseling purposes ({\\sc Cope}), or toward advice-seeking communities for health and social concerns ({\\sc Health}). The influences in these results require further investigation, but a possible explanation could be that expert knowledge and experience may offer more benefit to specialized condition-related issues than to broader social issues.\n\n\n\n\n\\section{Language Modeling}\n\nWe further examine differences in word usage by building separate language models for MHPs and peers. We seek to identify language use that is indicative of one group or another by running the language model of one on the data of the other and analyzing words with high perplexity.\nTo run these experiments, we use the language model of \\citet{merity2018analysis,merity2018regularizing}, which is a recent LSTM-based language model that achieved state-of-the-art performance by combining several regularization techniques.\\footnote{\\url{htts://github.com/salesforce/awd-lstm-lm}}\n\nOur implementation uses a fixed vocabulary of 20,907 tokens for both the peer and MHP language models. This is determined by a minimum count of five across the set of posts from both groups. Each language model is trained for 50 epochs.\\footnote{Validation set perplexities for expert and score groups: peer on peer: 44, peer on MHP: 52, MHP on peer: 91, MHP on MHP: 74, low on high: 39, low on low: 43, high on high: 50, high on low: 57. The difference in perplexity is due to the difference in volume of posts between groups. Runs in $\\sim$2 min per epoch on a GeForce RTX 2080 Ti GPU.}\n\n\n\\begin{figure*}[t]\n    \\centering\n    \\small\n    \\begin{tikzpicture}\n    \\begin{axis}[\n        yshift=0.0cm, xshift=0.0cm,\n        width=0.29\\linewidth,\n        height=3cm,\n        grid=both,\n        symbolic x coords={leisure,sexual,number,article,relig,money,we,negate,friend,netspeak},\n        xtick=data,\n        legend style={font=\\small},\n        legend pos=south west,\n        enlarge x limits=0.1,\n        ymin=0.0, ymax=0.70,\n        bar width=.2cm,\n        x tick label style={rotate=60, anchor=east, font=\\small},\n        y tick label style={anchor=east, font=\\small}]\n        \\addplot[ybar,fill=white,ybar legend] coordinates  {\n            (leisure, 0.70)(sexual, 0.67)(number, 0.61)(article, 0.60)(relig, 0.57)(money, 0.56)(we, 0.56)(negate, 0.55)(friend, 0.55)(netspeak, 0.54)\n        }; \\addlegendentry{MHP on Peer}\n    \\end{axis}\n    \n    \\begin{axis}[\n        yshift=0.0cm, xshift=3.35cm,\n        width=0.29\\linewidth,\n        height=3cm,\n        grid=both,\n        symbolic x coords={filler,swear,anger,article,ingest,body,home,leisure,sexual,money},\n        xtick=data,\n        enlarge x limits=0.1,\n        ymin=0.0, ymax=0.70,\n        legend style={font=\\small},\n        bar width=.2cm,\n        x tick label style={rotate=60, anchor=east, font=\\small},\n        yticklabels={,,}\n        ]\n        \\addplot[ybar,fill=white,ybar legend] coordinates  {\n        \t(filler, 0.30)(swear, 0.26)(anger, 0.26)(article, 0.25)(ingest, 0.23)(body, 0.23)(home, 0.22)(leisure, 0.22)(sexual, 0.22)(money, 0.21)\n        }; \\addlegendentry{High on Low}\n    \\end{axis}\n    \n    \\begin{axis}[\n        yshift=0cm, xshift=7.35cm,\n        width=0.29\\linewidth,\n        height=3cm,\n        grid=both,\n        symbolic x coords={discrep,interrog,auxverb,cause,I,foc.future,foc.present,negate,verb,ipron},\n        xtick=data,\n        enlarge x limits=0.1,\n        ymin=-0.15, ymax=0.0,\n        legend style={font=\\small},\n        bar width=.2cm,\n        x tick label style={rotate=60, anchor=east, font=\\small},\n        yticklabel style={\n                /pgf/number format/fixed,\n                /pgf/number format/precision=5,\n                font=\\small\n        },\n        scaled y ticks=false\n        ]\n        \\addplot[ybar,fill=lightgray,ybar legend] coordinates  {\n            (discrep, -0.08)(interrog, -0.11)(auxverb, -0.11)(cause, -0.12)(I, -0.12)(foc.future, -0.12)(foc.present, -0.12)(negate, -0.12)(verb, -0.13)(ipron, -0.14)\n        }; \\addlegendentry{Peer on MHP}\n    \\end{axis}\n    \n    \\begin{axis}[\n        yshift=0cm, xshift=10.7cm,\n        width=0.29\\linewidth,\n        height=3cm,\n        grid=both,\n        symbolic x coords={discrep,assent,foc.future,insight,differ,negate,friend,tentat,foc.present,cogproc},\n        xtick=data,\n        legend style={font=\\small},\n        legend pos=south west,\n        enlarge x limits=0.1,\n        ymin=-0.15, ymax=0.0,\n        bar width=.2cm,\n        x tick label style={rotate=60, anchor=east, font=\\small},\n        yticklabels={,,}\n        ]\n        \\addplot[ybar,fill=lightgray,ybar legend] coordinates  {\n        \t(discrep, -0.05)(assent, -0.05)(foc.future, -0.07)(insight, -0.07)(differ, -0.08)(negate, -0.08)(friend, -0.08)(tentat, -0.08)(foc.present, -0.08)(cogproc, -0.08)\n        }; \\addlegendentry{Low on High}\n    \\end{axis}\n    \n    \\node[align=center,rotate=90,font=\\small] at (-1.0cm, 0.2cm) {Entropy Difference};\n    \\end{tikzpicture}\n    \\caption{Entropy differences for LIWC word categories when running both language models on one group's data. High entropy scores on one dataset indicate word types that are harder for the opposite group's model to predict.}\n    \\label{fig:ent_diff_charts}\n\\end{figure*}\n\nWe use the language model trained on MHP data to find words with high entropy in peer data and vice versa. Since we are concerned with the \\textit{difference} in predictability of words between the MHP and peer language models, we subtract the entropy given by the model trained on that data from the entropy assigned by the model that was not trained on that data. In other words, to find words difficult to predict in B's data, we subtract each word's entropy calculated by the model trained on B from the entropy calculated by the model trained on A as follows, for a set of words, X:\n\n\n\\begin{equation}\n    E_{A,B} = -\\frac{1}{\\vert X \\vert} \\sum_{x \\in X} log(p_{A}(x)) - log(p_{B}(x))\n\\end{equation}\n\nIf we calculate the entropy difference for each LIWC category and for each assignment of the MHP and peer groups to A and B, we find the highest differences for each category shown in the first and third plots of Figure~\\ref{fig:ent_diff_charts}. We find highest entropy scores for words relating to \\textit{leisure}, \\textit{sex}, and \\textit{numbers} when running the MHP language model on peer data.\nLikewise, when running the peer model on MHP data, the category of \\textit{discrepancy} contains words whose accuracy is improved the least by the peer model, again showing that these words are more indicative of the MHP group.\n\nWe perform a similar analysis, creating a language model for posts which have the highest score (or tied for highest) and another model for all other posts. We measure entropy differences and show the highest scoring categories for each group in the second and fourth plots. Some of the categories indicative of MHP language are also indicative of higher scoring posts; \\textit{discrepancy}, \\textit{present} and \\textit{future} words, and \\textit{negation} words, while other categories like \\textit{assent} and \\textit{insight} words are more dominant in higher scoring posts. The lower scoring posts have the highest entropy differences for some types of words in the peer data, however, we also see that \\textit{filler}, \\textit{anger}, and \\textit{swear} words had the highest entropy differences for the low scoring group. Qualitative example sentences with word-level entropy and LIWC annotations are shown in the appendix in Table~\\ref{tab:examples_mhp_data}.\n\n\n\\section{Discussion and Future Work}\n\n\nIn comparing linguistic aspects of MHP and peer comments, we find MHP tendencies align with established counseling principles and findings in counselor behaviors from recent literature. In particular, they align in the use of words that increase the likelihood of desired perspective shifts associated with clients feeling better after text counseling sessions (RQ2)~\\cite{althoff2016large}. We also found unique differences in the behavior of MHPs as compared to peers in how they respond to information seekers (RQ1).\nAlthough, comments by peers that prompt support-seeker replies also make use of similar word categories to MHPs, which shows that comparing MHPs to peers can offer insight into peer interactions as well.\n\nIt is important to note that our analyses rely heavily on the LIWC lexicon. While LIWC and other lexicons can help uncover variational language across groups at an exploratory stage, their use alone does not explain why variations are present. Certain limitations of LIWC are clear, such as when certain words that occur in multiple categories misleadingly boost the prominence of the categories equally. \\citet{kross2019does}'s and \\citet{jaidka2020estimating}'s studies have also demonstrated limitations of the use of LIWC when working with word counts to correlate with well-being metrics and an individual\u2019s emotional state. We utilize LIWC to understand linguistic behavior differences in conversations with peers and MHPs rather than to evaluate the emotional or mental health state of individuals; however, it is important to consider how these limitations could pertain to our interpretations of their differences, especially as we explore them more deeply in future work. In our study, we explore the patterns we find in the context of previous findings from related literature such as \\cite{althoff2016large} and \\cite{nobles2020examining}, however it warrants another study into nuanced aspects of the patterns to infer their social functions in support seeking forums in particular.\n\nAlthough our findings align MHP behaviors with certain counselor behaviors associated with positive outcomes, our analyses do not support claims that MHP behaviors are more beneficial to individuals seeking support; rather, we have shown that the general tendencies of MHPs are in accordance with principles and behaviors demonstrated by counselors in other settings. Understanding the outcomes of these interactions for individual support seekers remains as an area for future work, which could employ surveying methods from prior work to  measure perceived empathy in online communities~\\cite{nambisan2011information}. Our dataset also enables investigations\ninto whether support-seekers exhibit perspective shifts in interacting with MHPs or peers, and what MHP and peer tendencies are associated with these perspective shifts.\n\nAnother direction for future work could focus on modeling social media-specific engagement patterns of MHP and peer interactions.\nPrior work developed a model that accounts for variables indicating the level of attention threads receive (i.e., thread lengths and number of unique commenters), and variables indicating the degree of interaction between posters and commenters (i.e., time between responses and whether the poster replies to commenters), and used this model to study peer-to-peer interactions in online mental health platforms~\\cite{sharma2020engagement}; this approach may enable studying supportive interactions in megathreads and threads involving back-and-forth dialog between two or more parties.\n\nMore questions arise if we consider MHP tenure and specific domain of expertise (e.g., specializations, licenses, academic degrees). Prior work that studied longitudinal changes in counselor linguistic behaviors indicated that systematic changes occur over time as counselors develop personal styles that are more distinct from other counselors and exhibit more diversity across interactions~\\cite{zhang2019finding}. Future work could model the language longitudinally for MHPs and peers that have longer-term histories of participating in mental health forums to investigate whether systematic changes occur online as well, and if so, whether they reflect similar changes found in prior work.\n\n\n\n\n\n\n\\section{Limitations and Ethical Considerations}\n\n\nA number of unknowns exist in what we are able to extract from Reddit. For instance, we do not know if users that do not use flairs are mental health professionals. We assume that those who have used the MHP flairs are MHPs and those that have not used them are peers. Additionally, we have grouped all MHP flairs into one group for our analysis, though a more nuanced analysis based on particular professional roles (e.g., psychologists, psychiatrists, social workers) and specializations (e.g., motivational interviewing, cognitive behavioral therapy, family \\& marriage counseling) may reveal additional trends. Prior work found that disclosing credentials has impacts on engagements that vary by subreddit and linguistic patterns associated with different experience levels and expert domains~\\cite{park2020trust}, thus the effects of disclosing MHP credentials when responding to support-seekers should be investigated.\n\nA classifier or language model used to distinguish between MHPs and peers or to generate the language of either could have negative implications. A generative model that provides feedback to users could generate language that is harmful for those seeking help. Our work could be used to devise a tool to train counselors, however we do not have a direct measure of what type of responses are helpful or meaningful. In such an application, there is potential to reinforce harmful behaviors due to the inaccuracy of our models. Future studies are needed to determine how to best design a tool to train counselors and how models derived from corpora such as ours correspond to advice that patients find useful.\n\n\n\\section{Conclusion}\n\nAs the role of social networks is becoming more critical in how people seek health-information, it is important to understand their broader implications to health communication and how health experts can engage to promote the soundest information and offer support to their vulnerable users. By elucidating techniques employed by mental health professionals in their interactions with support-seekers in mental health forums, we have contributed insights toward the broader research direction of understanding how health experts currently engage with these platforms. With evidence that MHP linguistic behaviors associate with further engagement with support-seekers and that these same behaviors are associated with positive counseling conversation outcomes, we have shown that analyzing MHP behavior is a promising direction for better understanding online interaction outcomes, which can further inform forum design and moderation, and expert health promotion efforts. \n\nThe code used for our experiments and analyses, and the post ids in our dataset can be accessed at \\url{https://github.com/MichiganNLP/MHP-and-Peers-Reddit}.\n\n\n\\section*{Acknowledgments} \nThis material is based in part upon work supported by the Precision Health initiative at the University of Michigan, by the National Science Foundation (grant \\#1815291), and by the John Templeton Foundation (grant \\#61156). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the Precision Health initiative, the National Science Foundation, or John Templeton Foundation. \n\n\n\n\\bibliographystyle{acl_natbib}\n", "meta": {"timestamp": "2021-06-25T02:17:42", "yymm": "2106", "arxiv_id": "2106.12976", "language": "en", "url": "https://arxiv.org/abs/2106.12976"}}
{"text": "\\section{Introduction}\\label{sec:introduction}\nWe consider the classic problem of online prediction with expert\nadvice~\\cite{littlestone1994weighted} in a non-stationary environment.\nIn this model \\nature\\ sequentially generates outcomes which\n\\learner\\ attempts to predict. Before making each prediction,\n\\learner\\ listens to a set of $\\nexperts$ experts who each make their\nown predictions. \\Learner\\ bases its prediction on the advice of the\nexperts. After the prediction is made and the true outcome is revealed\nby \\nature, the accuracies of \\learner's prediction and the expert\npredictions are measured by a loss function. \\Learner\\ receives \ninformation on all expert losses on each trial. We make no\ndistributional assumptions about the outcomes generated, indeed\n\\nature\\ may be assumed to be adversarial. The goal of \\learner\\ is to\npredict well relative to a predetermined comparison class of\npredictors, in this case the set of experts themselves. Unlike the\nstandard regret model, where \\learner's performance is compared to the\nsingle best predictor in hindsight, our aim is for \\learner\\ to\npredict well relative to a sequence of comparison predictors. That is,\n``switches'' occur in the data sequence and different experts are\nassumed to  predict well at different times.\n\nIn this work our focus is on the case when this sequence consists of a\nfew unique predictors relative to the number of switches.\nThus most switches return to a previously ``good'' expert, and a\n\\learner\\ that can exploit this fact by ``remembering'' the past can adapt\nmore quickly than a \\learner\\ who has no memory and must re-learn the\nexperts after every switch.\nThe problem of switching with memory in online learning is part of a\nmuch broader and fundamental problem in machine learning: how a system\ncan adapt to new information yet retain knowledge of the past.\nThis is an area of research in many fields, including for example,\ncatastrophic forgetting in artificial neural\nnetworks~\\cite{french1999catastrophic,mccloskey1989catastrophic}. \n\n\\paragraph{Contributions.}\nIn this paper we present an $\\cO(\\nexperts)$-time per trial projection-based algorithm \nfor which we prove the best known regret bound for tracking experts\nwith memory. Our projection-based algorithm is intimately related to\na more traditional ``weight-sharing'' algorithm, which we show is a new method for {\\em Mixing Past\n  Posteriors} (MPP)~\\cite{bousquet2002tracking}. We show that\nsurprisingly this method corresponds to the algorithm with the\nprevious best known regret bound for this problem~\\cite{koolen2012putting}.\nWe also give an efficient $\\cO(\\nexperts)$-time algorithm for\ncomputing exact relative entropy projection onto a simplex with\nnon-uniform (lower) box constraints. \nFinally, we provide a guarantee which favors projection-based updates\nover weight-sharing updates when updating weights may incur costs.\n\n\nThe paper is organized as follows. We first  introduce\nthe model and discuss related work, giving a detailed\noverview of the previous results on which we improve. In\nSection~\\ref{sec:pods} we give our main results, a regret bound which holds for two algorithms, and an algorithm to\ncompute relative entropy projection with non-uniform lower box\nconstraints in linear time. In Section~\\ref{sec:mpp-mixing-scheme} we\nderive a new ``geometric-decay'' method for MPP, and show the correspondence\nto the current best known algorithm~\\cite{koolen2012putting}. We\ngive a few concluding remarks in Section~\\ref{sec:discussion}. All\nproofs are contained in the appendices.\n\n\\subsection{Preliminaries}\\label{sec:preliminaries}\nWe first introduce notation. Let $\\simplex:=\\{\\bu\\in[0,1]^{\\nexperts}:\\|\\bu\\|_{1}=1\\}$ be the\n$(\\nexperts-1)$-dimensional probability simplex. Let $\\alphasimplex :=\n\\{\\bu\\in[0,\\alpha]^{\\nexperts} : \\|\\bu\\|_{1} = \\alpha\\}$ be a scaled\nsimplex.\nLet $\\bm{1}$ denote the vector $(1,\\ldots,1)$ and $\\bm{0}$ denote the\nvector $(0,\\ldots,0)$. Let $\\bei$ denote the $i^{th}$ standard basis\nvector.\nWe define $\\RE{\\bu}{\\bw}\n:=\\sumin\\ui\\log{\\frac{\\ui}{\\wi}}$ to be the relative entropy between\n$\\bu$ and $\\bw$. We denote component-wise multiplication as\n$\\bu\\odot\\bw := (\\ui[1]\\wi[1],\\ldots,\\ui[n]\\wi[n])$.\nFor $p\\in[0,1]$ we define ${\\cH(p) := -p\\ln{p} - (1-p)\\ln{(1-p)}}$ to be\nthe binary entropy of $p$, using the convention that ${0\\ln{0}=0}$.\nWe define $\\relint{S}$ to be the relative interior of the set $S$.\nFor any positive integer $n$ we define $[n] := \\{1,\\ldots,n\\}$.\nWe overload notation such that $[\\texttt{pred}]$ is equal to $1$ if the predicate\n\\texttt{pred} is true and $0$ otherwise.\nFor two vectors $\\balpha$ and $\\bbeta$ we say $\\balpha\\preceq\\bbeta$\niff $\\alphai\\leq\\betai$ for all $i=1,\\ldots,\\nexperts$.\n\n\n\\section{Background}\\label{sec:background}\nIn sequential prediction with expert advice \\nature\\ generates\nelements from an outcome space, $\\cY$ while the predictions of \\learner\\\nand the experts are elements from a prediction space, $\\cD$ (e.g., we\nmay have $\\cY=\\{0,1\\}$ and $\\cD=[0,1]$). Given a\nnon-negative loss function  $\\ell:\\cD\\times\\cY\\to [0,\\infty)$,\nlearning proceeds in trials. On each trial $t=1,\\ldots,T$: $1)$\n\\learner\\ receives the expert predictions $\\bxt\\in\\cD^{\\nexperts}$,\n$2)$ \\learner\\ makes a prediction $\\ythat\\in\\cD$, $3)$ \\nature\\\nreveals the true label $\\yt\\in\\cY$, and $4)$ \\learner\\ suffers loss\n$\\lt:=\\ell(\\ythat,\\yt)$ and expert $i$ suffers loss\n$\\lti:=\\ell(\\xti,\\yt)$ for $i=1,\\ldots,\\nexperts$.\nCommon to the algorithms we consider in this paper is a weight vector,\n$\\bwt\\in\\simplex$, where $\\wti$ can be interpreted as the algorithm's\nconfidence in expert $i$ on trial $t$.\n\\Learner\\ uses a prediction function\n$\\pred:\\simplex\\times\\cD^{\\nexperts}\\to\\cD$ to generate its prediction\n${\\ythat = \\predt}$ on trial $t$. A classic example is to predict with\nthe weighted average of the expert predictions, that is, $\\predt\n=\\bwt\\cdot\\bxt$, although for some loss functions improved bounds are\nobtained with different prediction functions (see\ne.g.,~\\cite{vovk1990aggregating}). In this paper we assume\n$(c,\\eta)$-realizability of $\\ell$ and\n\\pred~\\cite{bousquet2002tracking,haussler1998sequential,vovk1998game}. That\nis, there exists constants $c,\\eta>0$ such that for all\n$\\bw\\in\\simplex$, $\\bx\\in\\cD^{\\nexperts}$, and $y\\in\\cY$, \n${\\ell(\\textup{\\pred}(\\bw,\\bx),\n  y) \\leq -c\\ln{\\sumin \\vi e^{-\\eta\\ell(\\xii,y)}}}$.\nThis includes $\\eta$-exp-concave losses when $\\predt =\\bwt\\cdot\\bxt$\nand $c=\\frac{1}{\\eta}$. For simplicity we present regret bound\nguarantees that assume $(c,\\frac{1}{c})$-realizability, that is\n$c\\eta=1$. This includes the log loss with $c=1$, and the square loss\nwith $c=\\frac{1}{2}$. The absolute loss is {\\em not}\n$(c,\\eta)$-realizable.  Generalizing our bounds for general bounded,\nconvex losses in the sense of online convex\noptimization~\\cite{zinkevich2003online} and the Hedge setting~\\cite{freund1997decision} is straightforward. For any comparison sequence of experts\n$\\iit[1],\\ldots,\\iit[T]\\in [\\nexperts]$ the regret of \\learner\\ with\nrespect to this sequence is defined as \n\\begin{equation*}\n  \\RT=\\regret\\,.\n\\end{equation*}\nWe consider and derive algorithms which belong to the family of\n``exponential weights'' (EW) algorithms (see\ne.g.,~\\cite{vovk1990aggregating,hoeven2018many,littlestone1994weighted}).\nAfter receiving the expert losses the EW algorithm applies the following\nincremental loss update to the expert weights,  \n\\begin{equation}\n  \\label{eq:aa-update}\n  \\wtdoti =\n  \\frac{\n    \\wti e^{-\\eta \\lti}\n  }{\n    \\sum_{j=1}^{\\nexperts}\\wtj e^{-\\eta \\lti[j]}\n  }\\,.\n\\end{equation}\n\\paragraph{Static setting.} In the static setting \\learner\\ competes against a\nsingle expert (i.e., $\\iit[1]=\\ldots=\\iit[T]$).\nFor the static setting the EW algorithm sets $\\bwt[t+1]=\\bwtdot$ for\nthe next trial, and for $(c,\\frac{1}{c})$-realizable losses and\nprediction functions achieves a static regret bound of $\\RT\\leq c\\ln{\\nexperts}$.\n\n\\paragraph{Switching.} In the switching (without memory) setting\n\\learner\\ competes against a sequence of experts\n${\\iit[1],\\ldots,\\iit[T]}$ with\n${k:=\\sum_{t=1}^{T-1}[\\iit\\neq\\iit[t+1]}]$ switches.\nThe well-known Fixed-Share algorithm~\\cite{herbster1998tracking}\nsolves the switching problem with the update\n\\begin{equation}\n  \\label{eq:fs-update}\n  \\bwt[t+1] = (1-\\alpha)\\bwtdot + \\alpha\\frac{\\bm{1}}{\\nexperts}\\,,\n\\end{equation}\nby forcing each expert to ``share'' a fraction of its weight\n\\emph{uniformly} with all experts.\\footnote{Technically in the original\n  Fixed-Share update each expert shares weight to all \\emph{other}\n  experts, i.e., ${\\wti[t+1]=(1-\\alpha)\\wtdoti+\\frac{\\alpha}{\\nexperts-1}\\sum_{j\\neq\n      i}\\wtdotj}$. The two updates achieve essentially the same regret bound\n  and are equivalent up to a scaling of $\\alpha$.}\nThe update is parameterized by a ``switching'' parameter, \n$\\alpha\\in [0,1]$, and the regret with respect to the best sequence\nof experts with $\\nswitches$ switches is\n\\begin{equation}\n  \\label{eq:fs-regret-2}\n  \\RT\\leq\n  c\\!\\left(\\!(\\nsegments)\\ln{\\nexperts} +\n    (T-1)\\entropyBig{\\frac{\\nswitches}{T-1}}\\!\\right)\\!\n  \\leq\n  c\\!\\left(\\!(\\nsegments)\\ln{\\nexperts} +\n    \\nswitches\\ln{\\frac{T-1}{\\nswitches}} +\n    \\nswitches\\right).\n\\end{equation}\n\\paragraph{Switching with memory.}\nFreund~\\cite{Freund2000problem} gave an open problem to improve on\nthe regret bound~\\eqref{eq:fs-regret-2} when the comparison sequence\nof experts is comprised of a small pool of size\n$\\pool:=\\vert\\cup_{t=1}^{T}\\{\\iit\\}\\vert\\ll\\nswitches$.\nUsing counting arguments Freund gave an exponential-time algorithm\nwith the information-theoretic ideal regret bound of\n${\\RT\\leq\n  c\\ln{(\\binom{\\nexperts}{\\pool}\\binom{T-1}{\\nswitches}\\pool(\\pool-1)^{\\nswitches})}}$,\nwhich is upper-bounded by\n\\begin{equation}\\label{eq:memory-target-bound}\n  c\\left(\\pool\\ln{\\nexperts} +\n    \\nswitches\\ln{\\frac{T-1}{\\nswitches}} +\n    (\\nswitches-\\pool+1)\\ln{\\pool} +\n    \\nswitches+\\pool\\right)\\,.\n\\end{equation}\nThe first efficient algorithm solving Freund's problem was presented\nin the seminal paper~\\cite{bousquet2002tracking}. This work\nintroduced the notion of a \\emph{mixing scheme}, which is a\ndistribution $\\bgammat$ with support $\\{0,\\ldots,t\\}$. Given\n$\\bgammat$, the algorithm's update on each trial is  the\n\\emph{mixture} over all past weight vectors,\n\\begin{equation}\n  \\label{eq:mpp-mixture}\n  \\bwt[t+1]=\\sum_{q=0}^{t}\\gammatq\\bwtdot[q]\\,,\n\\end{equation}\nwhere $\\bwtdot[0]:=\\frac{1}{\\nexperts}\\bm{1}$, and $\\gamma^{1}_{0} := 1$. \nIntuitively, by mixing all ``past posteriors'' (MPP) the weights \nof previously well-performing experts can be prevented from\nvanishing and recover quickly. An efficient mixing scheme requiring\n$\\cO(\\nexperts)$-time per trial is the ``\\emph{uniform}'' mixing\nscheme given by $\\gammatq[t]=1-\\alpha$ and $\\gammatq=\\frac{\\alpha}{t}$\nfor $0\\leq q<t$. A better regret bound was proved with a ``\\emph{decaying}''\nmixing scheme, given by\n\\begin{equation}\n  \\label{eq:mpp-decay-scheme}\n  \\gammatq =\n  \\begin{cases}\n    1-\\alpha & q=t\\\\\n    \\alpha\\frac{1}{(t-q)^{\\gamma}}\\frac{1}{Z_t} & 0\\leq q<t\\,,\n  \\end{cases}\n\\end{equation}\nwhere $Z_t=\\sum_{q=0}^{t-1}\\frac{1}{(t-q)^{\\gamma}}$ is a normalizing\nfactor, and $\\gamma\\geq 0$.\nWith a tuning of $\\alpha=\\frac{\\nswitches}{T-1}$ and $\\gamma=1$ this\nmixing scheme achieves a regret bound of\\footnote{\\eqref{eq:mpp-decay-regret} is a simplified upper bound of the\n  bound given in~\\cite[Corollary 9]{bousquet2002tracking}, using\n  $\\ln{(1+x)}\\leq x$.}\n\\begin{equation}\n  \\label{eq:mpp-decay-regret}\n  \\RT\\leq c\\left(\n    \\pool\\ln{\\nexperts} +\n    2\\nswitches\\ln{\\frac{T-1}{\\nswitches}} +\n    \\nswitches\\ln{(m-1)} +\n    \\nswitches +\n    \\nswitches\\ln{\\ln{(eT)}}\n  \\right)\\,.\n\\end{equation}\nIt appeared that to achieve the best regret bounds, the mixing scheme \nneeded to decay towards the past. \nUnfortunately, computing~\\eqref{eq:mpp-decay-scheme} exactly requires\nthe storage of all past weights, at a cost of $\\cO(\\nexperts t)$-time\nand space per trial. \nObserve that these schemes set $\\gammatq[t]\\!=\\!1\\!-\\!\\alpha$,\nwhere typically $\\alpha$ is small, since intuitively switches are\nassumed to happen infrequently. All updates using such schemes are of\nthe form\n\\begin{equation}\n  \\label{eq:generalised-share-update}\n  \\bwt[t+1] = (1-\\alpha)\\bwtdot + \\alpha\\bvtdt\\,,\n\\end{equation}\nwhich we will call the {\\em generalized share update} (see~\\cite{cesa2012mirror}). Fixed-Share is a\nspecial case when $\\bvtdt=\\frac{\\bm{1}}{\\nexperts}$ for all $t$. This\ngeneralized share update features heavily in this paper.\n\n\nFor a decade it remained an open problem to give the MPP update a\nBayesian interpretation. This  was finally solved\nin~\\cite{koolen2012putting} with the use of \\textit{partition\n  specialists}. Here on each trial $t$, a specialist (first introduced\nin~\\cite{freund1997using}) is either \\textit{awake} and predicts in\naccordance with a prescribed base expert, or is \\textit{asleep} and\nabstains from predicting. For $\\nexperts$ base experts and finite time\nhorizon $T$ there are $\\nexperts 2^{T}$ partition specialists. For Freund's problem an assembly of $\\pool$ partition specialists can\npredict exactly as the comparison sequence of experts. The Bayesian\ninterpretation of the MPP update given in~\\cite[Theorem\n2]{koolen2012putting} was simple: to define a mixing scheme $\\bgammat$\nwas to induce a prior over this set of partition specialists. \nThe authors of~\\cite{koolen2012putting} proposed a\nsimple Markov chain prior over the set of partition specialists,\ngiving an efficient $\\cO(\\nexperts)$-time per trial algorithm with the regret bound\n\\begin{align}\n  \\RT\\!&\\leq \\!c\\!\\left[\n         \\pool\\ln{\\!\\frac{\\nexperts}{\\pool}} \\!+\\!\n         \\pool\\entropyBig{\\frac{1}{\\pool}} \\!\\!+\\!\n         (T-1)\\entropyBig{\\frac{\\nswitches}{T-1}} \\!\\!+\\!\n         (\\pool-1)(T-1)\\entropyBig{\\frac{\\nswitches}{(\\pool-1)(T-1)}}\n         \\!\\!\\right]\\label{eq:pbts-regret}\\\\\n       &\\leq \\!c\\!\\left(\\!\n         \\pool\\ln{\\nexperts} \\!+\\!    \n         2\\nswitches\\ln{\\frac{T-1}{\\nswitches}} \\!+\\!\n         (\\nswitches-\\pool+1)\\ln{\\pool} \\!+\\!\n         2(\\nswitches+1)\n         \\!\\right)\\label{eq:pbts-regret-2}\\,,\n\\end{align}\nwhich is currently the best known regret bound for Freund's\nproblem. It is not known which MPP mixing scheme corresponds to this\nMarkov prior. In this work we improve on the\nbound~\\eqref{eq:pbts-regret} for tracking experts with memory\n(Theorem~\\ref{thm:pods-bound}), and also show that this Markov prior on\npartition specialists corresponds to a geometrically-decaying mixing\nscheme for MPP (Proposition~\\ref{prop:pbts-as-mpp}).\n\nAdaptive online learning algorithms with memory have been shown to\nhave better empirical performance than those without memory~\\cite{gramacy2002adaptive},\nand to be effective in real-world applications such as  intrusion detection\nsystems~\\cite{nguyen2012adaptive}. While considerable research has\nbeen done on switching with memory in online learning (see\ne.g.,~\\cite{bousquet2002tracking,cesa2012mirror,herbster2015predicting,\n  herbster2020longterm,koolen2012putting,zheng2019equippingbandits}), \nthere remain several open problems.\nFirstly, there remains a gap between the\nbest known regret bound for an efficient algorithm and the\ninformation-theoretic ideal bound~\\eqref{eq:memory-target-bound}. \nPresent in both bounds~\\eqref{eq:mpp-decay-regret}\nand~\\eqref{eq:pbts-regret-2} is the factor of $2$ in the second term,\nwhich does not appear\nin~\\eqref{eq:memory-target-bound}. In~\\cite{koolen2012putting} this\nwas interpreted as the cost of co-ordination between specialists, essentially one ``pays'' twice per switch as one specialist falls \nasleep and another awakens.\nIn this paper we  make progress in closing\nthis gap by avoiding such additional costs the first time each expert is learned\nby the  algorithm. That is, we pay to \\emph{remember} but not to\n\\emph{learn}.  \n\\begin{figure}[t]\n\t\\centering\n\t\\hspace*{-.6in}\n\t\\input{boundcomparison_final}\n\t\\caption{A comparison of the regret bounds discussed in this paper\n\t\tfor ${\\pool\\!\\in\\![2,\\nswitches\\!+\\!1]}$ with ${n\\!=\\!500000}$, ${\\nswitches\\!=\\!40}$,\n\t\tand ${T\\!=\\!4000}$. Previous ``memory'' bounds (blue \\& yellow)\n\t\tare much worse than Fixed-Share for larger values of $\\pool$ while\n\t\tour bound (red) improves on Fixed-Share for all $\\pool\\!\\in\\![2,\\nswitches]$.}\n\t\\label{fig:regret-bounds}\n\\end{figure}\n\nSecondly, unless $\\nexperts$ is very large\nthe current best known bound~\\eqref{eq:pbts-regret} beats Fixed-Share's\nbound~\\eqref{eq:fs-regret-2} only when $\\pool\\ll\\nswitches$,\nbut suffers when $m$ is even a moderate fraction of\n$k$.\nA natural question is can we improve on Fixed-Share when we relax the\nassumption that $\\pool\\ll\\nswitches$, and only a few members of a\nsequence of experts need remembering (consider for instance, $\\pool >\n\\nswitches/2$)? \nIn this paper we prove a regret bound that is not only tighter\nthan~\\eqref{eq:pbts-regret} for all $\\pool$, but under mild\nassumptions on $\\nexperts$ improves on Fixed-Share for all $m\\leq\nk$. See Figure~\\ref{fig:regret-bounds} where we show this behavior for\nseveral existing regret bounds and our regret bound.\n\nOur regret bound will hold for two algorithms; one utilizes a weight-sharing\nupdate in the sense of~\\eqref{eq:generalised-share-update}, and the other\nutilizes a projection update. Why should we consider projections? Consider for example a large\nmodel consisting of many weights, and to update these weights costs time and/or money. Alternatively consider the\napplication of regret-bounded adaptive algorithms in online portfolio\nselection (see e.g., \\cite{singer1997switching,li2014online}). Here\neach ``expert'' corresponds to a single asset, and the weight vector\n$\\bwt$ corresponds to a portfolio. If $\\lti$ is the negative log\nreturn of stock $i$ after trading period $t$, then the loss\nfunction ${\\lt:=-\\ln{\\sumin\\wti e^{-\\lti}}}$ is the negative log return\nof the portfolio. This loss is $(1,1)$-realizable by \ndefinition (although there is no prediction function~\\cite{adamskiy2016closer}). The algorithm's\nupdate corresponds to actively re-balancing the portfolio\nafter each trading period, but the investor may incur\ntransaction costs proportional to the amount bought or sold (see\ne.g.,~\\cite{blum1999universal,li2014online}). Online portfolio\nselection with transaction costs is an active area of\nresearch~\\cite{das2013online,kozat2008universal,li2014online,li2018transaction}. In\nSection~\\ref{sec:pods-update} we motivate the use of\nprojection updates over weight-sharing with a guarantee in terms\nof such costs.\n\n\\subsection{Related work}\\label{sec:related-work}\nSwitching (without memory) in online learning was first introduced\nin~\\cite{littlestone1994weighted}, and extended with the Fixed-Share\nalgorithm~\\cite{herbster1998tracking}. An extensive literature has\nbuilt on these works, including but not limited\nto~\\cite{adamskiy2016closer,bousquet2002tracking,cesa2012mirror,daniely2015strongly,gyorgy2005tracking,gyorgy2012efficient,herbster2020longterm,herbster2019online,herbster2001tracking,koolen2012putting,koolenl2010freezingsleeping,mourtada2017efficient,sharma2020learning,zheng2019equippingbandits}. \nRelevant to this work are the results for switching with\nmemory~\\cite{bousquet2002tracking,cesa2012mirror,herbster2020longterm,koolen2012putting,koolenl2010freezingsleeping,zheng2019equippingbandits}. The\nfirst was the seminal work of~\\cite{bousquet2002tracking}. The best known result\nis given in~\\cite{koolen2012putting}, which we improve on.\nIn~\\cite{zheng2019equippingbandits} a reduction of switching\nwith memory to switching without memory is given, although with a slightly\nworse regret bound than~\\cite{bousquet2002tracking}. Related to\nthe experts model is the {\\em bandits} setting, which was addressed in\nthe memory setting in~\\cite{zheng2019equippingbandits}.\nIn~\\cite{cesa2012mirror} a unified analysis of both Fixed-Share and\nMPP was given in the context of online convex optimization. They observed the\ngeneralized share update~\\eqref{eq:generalised-share-update} and\nslightly improved the bounds of~\\cite{bousquet2002tracking}.\nAdaptive\nregret~\\cite{adamskiy2016closer,daniely2015strongly,hazan2009efficient,littlestone1994weighted} has been used to prove regret bounds for\nswitching but unfortunately does not generalize to the memory setting.\nThis paper primarily builds on the work of~\\cite{bousquet2002tracking}\nwith a new geometrically-decaying mixing scheme, and\non~\\cite{herbster2001tracking} with a new relative entropy projection algorithm.\n\n\\section{Projection onto dynamic sets}\\label{sec:pods}\nIn this section we give a relative entropy projection-based algorithm\nfor tracking experts with memory.\nGiven a non-empty set $\\cC\\subseteq\\simplex$ and a point\n$\\bw\\in\\relint{\\simplex}$ we define\n\\begin{equation*}\n  \\projw[\\cC] := \\argmin\\limits_{\\bu\\in\\cC}\\RE{\\bu}{\\bw}\\, \n\\end{equation*}\nto be the projection with respect to the relative entropy of $\\bw$\nonto $\\cC$~\\cite{bregman1967relaxation}.\nSuch projections were first introduced for switching (without\nmemory) in online learning in~\\cite{herbster2001tracking}, in which\nafter every trial the weight vector $\\bwtdot$ is projected onto \n$\\cC=[\\frac{\\alpha}{\\nexperts},1]^{\\nexperts}\\cap\\simplex$, that is,\nthe simplex with uniform box constraints.\nFor prediction with expert advice this projection algorithm has the\nregret bound~\\eqref{eq:fs-regret-2} (see~\\cite{cesa2012mirror}).  \nIndeed, we will refer to \n$\\bwt[t+1]=\\proj[\\bwtdot]{[\\frac{\\alpha}{\\nexperts},1]^{\\nexperts}\\cap\\simplex}$\nas the ``projection analogue'' of~\\eqref{eq:fs-update}.  \n\nGiven $\\bbeta\\in (0,1)^{\\nexperts}$ such that $\\|\\bbeta\\|_{1}\\leq 1$, let  \n\\begin{equation*}\n  \\Cb:=\\{\\bx\\in\\simplex : \\xii\\geq\\betai, i=1,\\ldots,\\nexperts\\}\n\\end{equation*}\nbe a subset of the simplex which is convex and non-empty.\nGiven $\\bw\\in\\relint{\\simplex}$, intuitively $\\projw[\\Cb]$ is  the projection of $\\bw$ onto\nthe simplex with (non-uniform) lower box constraints $\\bbeta$. \nRelative entropy projection updates for tracking experts with memory\nwere first suggested in~\\cite[Section 5.2]{bousquet2002tracking}. The\nauthors observed that for any MPP mixing scheme $\\bgammat$, the update~\\eqref{eq:mpp-mixture} can be replaced with\n\\begin{equation}\n  \\label{eq:mpp-projection}\n  \\bwt[t+1]=\\proj[\\bwtdot]{\\{\\bw\\in\\simplex: \\bw\\succeq\\gammatq\\bwqdot,q=0,\\ldots,t\\}}\\,,\n\\end{equation}\nand achieve the same regret bound.\nWe build on this concept in this paper. \nObserve that for any choice of $\\bgammat$ the set $\\{\\bw\\in\\simplex:\n\\bw\\succeq\\gammatq\\bwqdot,q=0,\\ldots,t\\}$ corresponds to the set $\\Cb$\nwhere\n\\begin{equation}\n  \\label{eq:beta-def}\n  \\betai = \\max\\limits_{0\\leq q\\leq\n    t}\\gammatq\\wtdoti[q]\\qquad i=1,\\ldots,\\nexperts\\,.\n\\end{equation}\nIn this work we give an algorithm to compute $\\projw$ exactly\nfor any $\\Cb$ in $\\cO(\\nexperts)$ time. With this algorithm\nand the mapping~\\eqref{eq:beta-def}, one immediately obtains the\nprojection analogue of MPP for any mixing scheme $\\bgammat$ at\nessentially no additional computational cost.\nWe point out however that for arbitrary mixing schemes computing\n$\\bbeta$ from~\\eqref{eq:beta-def} takes $\\cO(\\nexperts t)$-time on\ntrial $t$, improving only when some structure of the scheme can be\nexploited.\nWe therefore propose the following method for tracking\nexperts with memory {\\em efficiently} using projection onto dynamic\nsets (``PoDS''). \n\nJust as~\\eqref{eq:generalised-share-update} generalizes the Fixed-Share update~\\eqref{eq:fs-update}, we\npropose PoDS as the analogous generalization of the update $\\bwt[t+1] \\!=\\!\n\\proj[\\bwtdot]{\\Cb[\\alpha\\frac{\\bm{1}}{\\nexperts}]}$ (the projection analogue of\nFixed-Share). \nPoDS maintains a vector $\\bbetat\\in\\alphasimplex$, and on each trial\nupdates the weights by setting ${\\bwt[t+1] \\!=\\! \\proj[\\bwtdot]{\\Cbt}}$.\nIntuitively PoDS is the projection analogue\nof~\\eqref{eq:generalised-share-update} with $\\bbetat$ corresponding\nsimply to $\\alpha\\bvtdt$. In some cases\n$\\bbetat\\!=\\!\\alpha\\bvtdt$ for all $t$ (e.g., for Fixed-Share), but\nin general equality may not hold since $\\bbetat$\nand $\\bvtdt$ can be functions of past weights, which may differ for\nweight-sharing and projection algorithms.\nRecall that~\\eqref{eq:generalised-share-update}\ndescribes all MPP mixing schemes that set $\\gammatq[t]\\!=\\!1\\!-\\!\\alpha$. PoDS\nimplicitly captures all such mixing schemes. This simple\nformulation of PoDS allows us to define new updates, which will\ncorrespond to new mixing schemes. In Section~\\ref{sec:pods-update} we\ngive a simple update and prove the best known regret bound.\n\n\\subsection{Computing\n  \\texorpdfstring{$\\projw$}{P(w;C(B))}}\\label{sec:computing-projection}\nBefore we consider PoDS further, we first discuss\nthe computation of $\\projw$.\nIn~\\cite{herbster2001tracking} the authors showed that computing\nrelative entropy projection onto the simplex with {\\em uniform} box\nconstraints is non-trivial, but gave an algorithm to compute it in\n$\\cO(\\nexperts)$ time. We give a generalization of their algorithm\nto compute $\\projw$ exactly for any non-empty set $\\Cb$ in $\\cO(\\nexperts)$ time.\nAs far as we are aware our method to compute exact relative entropy\nprojection onto the simplex with non-uniform (lower) box constraints\nin linear time is the first, and may be of independent interest (see e.g.,~\\cite{krichene2015efficient}).\n\nWe first give an intuition into the form of $\\projw[\\Cb]$, and then\ndescribe how Algorithm~\\ref{alg:linear-time-proj} computes this projection efficiently. \nFirstly consider the case that $\\bw\\in\\Cb$, then trivially $\\projw[\\Cb]=\\bw$,\ndue to the non-negativity of $\\RE{\\bu}{\\bw}$\nand the fact that $\\RE{\\bu}{\\bw}=0$ iff $\\bu=\\bw$ (see\ne.g.,~\\cite{bregman1967relaxation}). For the case that $\\bw\\notin\\Cb$,\nthis implies that the set $\\{i\\in [n] : \\wi<\\betai\\}$ is\nnon-empty. For each index $i$ in this set the projection of $\\bw$ onto\n$\\Cb$ must set the component $\\wi$ to its corresponding constraint\nvalue $\\betai$. The remaining components are then normalized, such that\n$\\sumin \\wi=1$. However, doing so may cause one (or more) of these\ncomponents $\\wi[j]$ to drop below its constraint $\\betai[j]$. The projection algorithm therefore finds the set of\ncomponents $\\Psi$ of least cardinality to set to their constraint\nvalues such that when the remaining components are normalized, no component lies below its constraint.\n\nConsider the following inefficient approach to finding $\\Psi$. Given\n$\\bw$ and $\\Cb$, let $\\br=\\bw\\odot\\frac{1}{\\bbeta}$ be a \n``ratio vector''.  Then sort $\\br$ \nin ascending order, and sort $\\bw$ and $\\bbeta$ according\nto the ordering of $\\br$. \nIf $\\ri[1] \\geq 1$ then $\\Psi=\\emptyset$ and we are done\n($\\Rightarrow\\bw\\in\\Cb$). \nOtherwise for each $k=1,\\ldots,\\nexperts$: $1)$ let the candidate set\n$\\Psi^{'} = [k]$, $2)$ let $\\bw'=\\bw$ except for each $i\\in\\Psi^{'}$\nset $\\wi'=\\betai$, $3)$ re-normalize the remaining components of\n$\\bw'$, and $4)$ let $\\br'=\\bw'\\odot\\frac{1}{\\bbeta}$. The set $\\Psi$\nis then the candidate set $\\Psi^{'}$ of least cardinality such that\n$\\br' \\succeq \\bm{1}$. This approach requires sorting $\\br$\nand therefore even an efficient implementation\ntakes $\\cO(\\nexperts \\log{\\nexperts})$\ntime. Algorithm~\\ref{alg:linear-time-proj} finds $\\Psi$ without having\nto sort $\\br$. It instead specifies $\\Psi$ uniquely with a threshold,\n$\\threshold$, such that $\\Psi=\\{i : \\ri < \\threshold\\}$.\nAlgorithm~\\ref{alg:linear-time-proj} finds $\\threshold$\nthrough repeatedly bisecting the set $\\mW=[\\nexperts]$ by finding the\nmedian of the set $\\{\\ri : i \\in \\mW\\}$ (which can be done in\n$\\cO(\\vert\\mW\\vert)$ time~\\cite{blum1973time}), and efficiently\ntesting this value as the candidate threshold on each iteration. The smallest\nvalid threshold then specifies the set $\\Psi$.\nThe following theorem states the time complexity of the algorithm and the form of the projection, which\nwe will use in proving our regret bound (the proof is in\nAppendix~\\ref{sec:proj-linear-time-proof}, where we give a more\ndetailed description of the algorithm).\n\\begin{theorem}\\label{thm:projection-form}\n  For any $\\bbeta\\in (0,1)^{\\nexperts}$ such that\n  $\\onenorm{\\bbeta}\\leq 1$, and for any $\\bw\\in\\relint{\\simplex}$, let\n  ${\\bp=\\projw[\\Cb]}$, where $\\Cb=\\{\\bx\\in\\simplex : \\xii\\geq\\betai, i=1,\\ldots,\\nexperts\\}$. Then\n  $\\bp$ is such that for all $i=1,\\ldots,n$, \n  \\begin{equation}\n    \\label{eq:proj-max-form}\n    \\pii = \\max\\left\\{\n      \\betai;\n      \\frac{\n        1-\\sum_{j\\in\\Psi}\\betai[j]\n      }{\n        1 - \\sum_{j\\in\\Psi}\\wi[j]\n      }\\wi\n    \\right\\}\\,,\n  \\end{equation}\n  where $\\Psi := \\{i \\in [n] : \\pii = \\betai\\}$. Furthermore,\n  Algorithm~\\ref{alg:linear-time-proj} computes $\\bp$ in\n  $\\cO(\\nexperts)$ time.\n\\end{theorem}\nThe following corollary will be used in the proof of our regret bound.\n\\begin{corollary}\\label{cor:no-switch}\n  Let $0<\\alpha<1$. Then for any $\\bu\\in\\simplex$,\n  $\\bw\\in\\relint{\\simplex}$, and $\\bbeta\\in\\relint{\\alphasimplex}$,\n  let $\\bp=\\proj[\\bw]{\\Cb}$. Then,\n  \\begin{equation}\\label{eq:pods-no-switch}\n    \\RE{\\bu}{\\bw} - \\RE{\\bu}{\\bp} \\geq\n    \\ln{(1-\\alpha)}\\,. \n  \\end{equation}  \n\\end{corollary}\n\n\\subsection{A simple update rule for PoDS}\\label{sec:pods-update}\nWe now suggest a simple update rule for $\\bbetat$ in PoDS for tracking\nexperts with memory. The bound for this algorithm is given in Theorem~\\ref{thm:pods-bound}. \nWe first set $\\bbetat[1]=\\alpha\\frac{\\bm{1}}{\\nexperts}$ to be uniform, and with a\nparameter $0\\leq\\theta\\leq 1$ update $\\bbetat$ on subsequent trials by setting\n\\begin{equation}\n  \\label{eq:update-beta}\n  \\bbetat[t+1] = (1-\\theta)\\bbetat + \\theta\\alpha\\bwtdot\\,.\n\\end{equation}\nWe refer to PoDS with this\nupdate as \\podsth. Intuitively the constraint\nvector $\\bbetat$ is updated in~\\eqref{eq:update-beta} by mixing in a\nsmall amount of the current  weight vector, $\\bwtdot$, scaled\nsuch that $\\onenorm{\\bbetat[t+1]}=\\alpha$. If expert $i$ predicted well\nin the past, then its constraint $\\betati$ will be relatively large,\npreventing the weight from vanishing even if that expert suffers large\nlosses locally. Using Algorithm~\\ref{alg:linear-time-proj} in its\nprojection step, \\podsth\\ has  $\\cO(\\nexperts)$ per-trial time complexity. \n\\input{algorithm_box}\n\nAs discussed, the vector $\\bbetat$ of PoDS is\nconceptually equivalent to the vector $\\alpha\\bvtdt$ of\nthe generalized share update~\\eqref{eq:generalised-share-update}.\nIf PoDS has a simple update rule such as~\\eqref{eq:update-beta} then it is\nstraightforward to recover the weight-sharing equivalent by simply \n``pretending'' equality holds on all trials. We now do this for \\podsth.\nClearly we have $\\bvtdt[1]=\\frac{\\bm{1}}{\\nexperts}$, and if $\\bbetat=\\alpha\\bvtdt$\nand $\\bbetat[t+1]=\\alpha\\bvtdt[t+1]$, then\n${\\bvtdt[t+1]\\!=\\!\\frac{1}{\\alpha}\\bbetat[t+1]\\!=\\!\\frac{1}{\\alpha}(1\\!-\\!\\theta)\\bbetat\n+ \\theta\\bwtdot\\!=\\!(1\\!-\\!\\theta)\\bvtdt+ \\theta\\bwtdot}$.\nThis then leads to an efficient sharing algorithm, which we call\n\\shareth. In Section~\\ref{sec:mpp-mixing-scheme} we show this\nalgorithm is in fact a new MPP mixing scheme, which surprisingly\ncorresponds to the previous best known algorithm for this problem.\nBoth \\podsth\\ and \\shareth\\ use the same parameters ($\\alpha$ \nand $\\theta$), differing only in the final update (see\nAlgorithms~\\ref{alg:pods}). We now give the regret bound which holds\nfor both algorithms.\n\\begin{theorem}\\label{thm:pods-bound}\n  For any comparison sequence $\\iit[1],\\ldots,\\iit[T]$ containing\n  $\\nswitches$ switches and consisting of $\\pool$ unique\n  experts from a set of size $\\nexperts$, if $\\alpha =\n  \\frac{\\nswitches}{T-1}$ and $\\theta=\\frac{\\nswitches-\\pool+1}{(\\pool\n    -1)(T-2)}$, the regret of both \\podsth\\ and \\shareth\\, with any prediction\n  function and loss function which are $(c,\\frac{1}{c})$-realizable is   \n  \\begin{equation}\n    \\label{eq:pods-bound}\n    \\RT\\leq\n    c\\!\\left(\\!\n      \\pool\\ln{\\nexperts} +\\!\n      (T-1)\\entropyBig{\\frac{\\nswitches}{T-1}}\\! +\n      (\\pool -1)(T-2)\\entropyBig{\n        \\frac{\n          \\nswitches-\\pool+1\n        }{\n          (\\pool -1)(T-2)\n        }\n      }\\!\n    \\right).\n  \\end{equation}\n\\end{theorem}\nThe regret bound~\\eqref{eq:pods-bound} is at least\n$c((\\pool\\!-\\!1)\\ln{\\frac{T-1}{\\nswitches}}\\!-\\!(\\nswitches\\!-\\!\\pool\\!+\\!1)\\ln{\\frac{\\nswitches}{\\nswitches-\\pool+1})})$\ntighter than the currently best known\nbound~\\eqref{eq:pbts-regret}. Thus if $\\pool\\!\\ll\\!\\nswitches$\nthen the improvement is $\\approx\\!\nc\\pool\\ln{\\frac{T}{\\nswitches}}$, and as $\\pool\\!\\rightarrow\\!\\nswitches\\!+\\!1$\nthen the improvement is $\\approx\\!\nc\\nswitches\\ln{\\frac{T}{\\nswitches}}$.  \nAdditionally note that if $\\pool\\!=\\nswitches\\!+\\!1$ (i.e., every switch we\ntrack a \\textit{new} expert) the optimal tuning of $\\theta$ is zero,\nand \\podsth\\ reduces to setting\n$\\bbetat=\\alpha\\frac{\\bm{1}}{\\nexperts}$ on\nevery trial. That is, we recover the projection analogue of\nFixed-Share. This is also reflected in the\nregret bound since~\\eqref{eq:pods-bound} reduces to~\\eqref{eq:fs-regret-2}.\nSince $x\\cH(\\frac{y}{x})\\leq y\\ln{(\\frac{x}{y})} + y$, the regret\nbound~\\eqref{eq:pods-bound} is upper-bounded by\n\\begin{equation*}\n  \\RT\\leq\\!\n  c\\!\\left[\\!\n    m\\ln{\\nexperts} \\!+\\!\n    \\nswitches\\ln{\\frac{T\\!-\\!1}{\\nswitches}} \\!+\\!\n    (\\nswitches\\!-\\!\\pool\\!+\\!1)\\ln{\\frac{T\\!-\\!2}{\\nswitches\\!-\\!\\pool\\!+\\!1}} \\!+\\!\n    (\\nswitches\\!-\\!\\pool\\!+\\!1)\\ln{(\\pool\\!-\\!1)} \\!+\\!\n    2\\nswitches-\\pool + 1\\right].\n\\end{equation*}\nComparing this to~\\eqref{eq:pbts-regret-2}, we see that instead\nof paying $c\\ln{\\frac{T-1}{\\nswitches}}$ {\\em twice} on every switch, we pay\n$c\\ln{\\frac{T-1}{\\nswitches}}$ once per switch and $c\\ln{\\frac{T-2}{\\nswitches-\\pool+1}}$\nfor every switch we {\\em remember} an old expert\n(${\\nswitches-\\pool+1}$ times).\nUnlike previous results for tracking experts with memory, \\podsth\\ and its regret\nbound~\\eqref{eq:pods-bound} smoothly interpolate between the two\nswitching settings. That is, it is capable of exploiting memory when necessary\nand on the other hand does not suffer when memory is not necessary\n(see Figure~\\ref{fig:regret-bounds}). \n\n\\paragraph{Projection vs. sharing in online learning.\nWe now briefly consider the two types of updates discussed in this\npaper (projection and weight-sharing) when updating weights may incur costs.\nRecall the motivating example introduced in\nSection~\\ref{sec:background} was in online portfolio selection with\ntransaction costs.\nIt is straightforward to show that in\nthis model transaction costs are proportional to the $1$-norm of the\ndifference in the weight vectors before and after re-balancing. \nIn Theorem~\\ref{thm:proj-lt-sharing-general} we give a\n result which in this context guarantees the ``cost'' of projecting is\n less than that of weight-sharing.\n\nTo compare the update of PoDS and the generalized share\nupdate~\\eqref{eq:generalised-share-update}, we must consider for a set\nof weights $\\bwtdot$, the point $\\proj[\\bwtdot]{\\Cbt}$ and the point\n${(1-\\alpha)\\bwtdot + \\alpha\\bvtdt}$. However these points depend on\n$\\bbetat$ and $\\bvtdt$ respectively, which may themselves be functions of\nprevious weight vectors $\\bwtdot[1],\\ldots,\\bwtdot[t-1]$, which as\ndiscussed are generally not the same for each of the two algorithms.\nTo compare the two updates equally we therefore assume that the\ncurrent weights are the same (i.e., they must both update the same\nweights $\\bwtdot$), and additionally that $\\bbetat=\\alpha\\bvtdt$.\nThe following theorem states that under mild conditions, PoDS is strictly less ``expensive'' than its weight-sharing\ncounterpart.\n\\begin{theorem}\\label{thm:proj-lt-sharing-general}\n  Let $0<\\alpha<1$. Then for any $\\bv\\in\\relint{\\simplex}$,\n  let $\\bbeta=\\alpha\\bv$, and for any $\\bw\\in\\relint{\\simplex}$, let\n  ${\\bw'=(1-\\alpha)\\bw + \\alpha\\bv}$. Then,\n  \\begin{equation*}\n    \\onenormBig{\\projw[\\Cb]-\\bw} < \\onenormBig{\\bw'-\\bw}\\,.\n  \\end{equation*}\n\\end{theorem}\nThus if one has to pay to update weights, projection is the economical choice. \n\n\\section{A geometrically-decaying mixing scheme for\n  MPP}\\label{sec:mpp-mixing-scheme}\nIn this section we look more closely at  \\shareth. We\nshow that it is in fact a new type of {\\em decaying} MPP \nmixing scheme which corresponds to the partition specialist algorithm with\nMarkov prior.\n\nRecall that the previous best known mixing scheme for MPP is the\ndecaying scheme~\\eqref{eq:mpp-decay-scheme}. Observe that\nin~\\eqref{eq:mpp-decay-scheme} the decay (with the ``distance'' to the\ncurrent trial $t$) follows a power-law, and that\ncomputing~\\eqref{eq:mpp-decay-scheme} exactly takes $\\cO(\\nexperts t)$\ntime per trial. We now derive an explicit MPP mixing scheme from the \nupdates~\\eqref{eq:update-wt} and~\\eqref{eq:update-vt} of \\shareth. Observe that if\nwe define $\\bwtdot[0]:=\\frac{\\bm{1}}{\\nexperts}$, then an iterative\nexpansion of~\\eqref{eq:update-vt} on any trial $t$ gives\n${\\bvt=\\sum_{q=0}^{t-1}\\theta^{[q\\neq\n    0]}(1-\\theta)^{t-q-1}\\bwtdot[q]}$,\nfrom which~\\eqref{eq:update-wt} implies\n${\\bwt[t+1] = (1-\\alpha)\\bwtdot + \\alpha\\bvt =\n  \\sum_{q=0}^{t}\\gammatq\\bwtdot[q]}$,\nwhere \n\\begin{equation}\\label{eq:new-mixing-scheme}\n  \\gammatq =\n  \\begin{cases}\n    1-\\alpha & q=t\\\\\n    \\theta(1-\\theta)^{t-q-1}\\alpha & 1\\leq q < t\\\\\n    (1-\\theta)^{t-1}\\alpha & q=0\\,.\n  \\end{cases}\\,.\n\\end{equation}\nNote that~\\eqref{eq:new-mixing-scheme} is a valid mixing scheme since\nfor all $t$, $\\sum_{q=0}^{t}\\gammatq = 1$. The \\shareth\\ update is \ntherefore a new kind of decaying mixing scheme. \nIn this new scheme the decay is {\\em geometric}, and can therefore be\ncomputed efficiently, requiring only $\\cO(\\nexperts)$ time and space\nper trial as we have shown. Furthermore MPP with this scheme has the improved regret\nbound~\\eqref{eq:pods-bound}.\n\nAnother interesting difference between the decaying\nschemes~\\eqref{eq:new-mixing-scheme} and~\\eqref{eq:mpp-decay-scheme} is\nthat when $\\theta$ is small then~\\eqref{eq:new-mixing-scheme} keeps\n$\\gammatq[0]$ relatively large initially and slowly decays this value as\n$t$ increases. Intuitively by heavily weighting the initial uniform\nvector $\\bwtdot[0]$ on each trial early on, the algorithm\ncan ``pick up'' the weights of new experts easily.\nFinally as in the case of \\podsth, if $\\pool=\\nsegments$, then with the optimal\ntuning of $\\theta=0$, this update reduces to the Fixed-Share update~\\eqref{eq:fs-update}.\n\n\\paragraph{Revisiting partition specialists.}\nWe now turn our attention to the previous best known result for\ntracking experts with memory (the partition specialists\nalgorithm with a Markov prior~\\cite{koolen2012putting}).\n\nFor sleep/wake patterns $(\\chit[1]\\ldots\\chit[T])$ the Markov prior\nis a Markov chain on states $\\{w,s\\}$, defined by the initial\ndistribution $\\bpi\\!=\\!(\\piw, \\pis)$ and transition probabilities\n$\\Pij\\!:=\\!P(\\chit[t+1]\\!=\\!j\\vert\\chit\\!=\\!i)$ for $i,j\\in \\{w,s\\}$.\nThe algorithm with these inputs efficiently collapses one weight per\nspecialist down to two weights per expert. These two weight vectors, which we denote\n$\\bat$ and $\\bst$, represent the total weight of all awake and sleeping specialists\nassociated with each expert, respectively. Note that the vectors\n$\\bat$ and $\\bst$ are not in the  simplex, but rather the vector\n$(\\bat,\\bst)\\in\\Delta_{2\\nexperts}$ and the ``awake vector'' $\\bat$ gets\nnormalized upon prediction. The weights are initialized by\nsetting $\\bat[1] = \\piw\\frac{\\bm{1}}{\\nexperts}$, and $\\bst[1]\n=\\pis\\frac{\\bm{1}}{\\nexperts}$. The update\\footnote{In~\\cite{koolen2012putting} the algorithm is presented in\n  terms of probabilities with the log loss. Here we give the update generalized to\n$(c,\\eta)$-realizable losses.}  of these weights after\nreceiving the true label $\\yt$ is given\nby\n${\\ati[t+1] =\n  \\Pww\\frac{\n    \\ati e^{-\\eta\\lti} (\\sumin[j]\\atj)\n  }{\n    \\sum_{j=1}^{\\nexperts}\\atj e^{-\\eta\\lti[j]}\n  }  + \\Psw\\sti\n}$,\nand\n${\\sti[t+1] =\n  \\Pws \\frac{\n    \\ati e^{-\\eta\\lti} (\\sumin[j]\\atj)\n  }{\n    \\sum_{j=1}^{\\nexperts}\\atj e^{-\\eta\\lti[j]}\n  }\n  + \\Pss\\sti \n}$ for $i=1,\\ldots,\\nexperts$.\nRecall that the authors of~\\cite{koolen2012putting} proved that an MPP\nmixing scheme implicitly induces a prior over partition\nspecialists. The following states that the Markov \nprior is induced by~\\eqref{eq:new-mixing-scheme}.\n\\begin{proposition}\n  \\label{prop:pbts-as-mpp}\n  Let $0< \\alpha < 1$, and $0< \\theta <1$.\n  Then the partition specialists algorithm with Markov prior\n  parameterized with $\\Psw=\\theta$, $\\Pws=\\alpha$, \n  $\\piw=\\frac{\\theta}{\\alpha+\\theta}$, and $\\pis=\\frac{\\alpha}{\\alpha+\\theta}$\n  is equivalent to \\shareth\\ parameterized with $\\alpha$ and $\\theta$.\n\\end{proposition}\nThe proof (given in Appendix~\\ref{sec:proof-pbts-as-mpp-thm}) amounts\nto showing for all $t$ that $\\frac{\\bat}{\\piw}=\\bwt$ and\n$\\frac{\\bst}{\\pis}=\\bvt$.\nThe Markov prior on partition specialists therefore corresponds to a\ngeometrically-decaying MPP mixing scheme!\nNote however that  we have proved a better regret bound for this algorithm in\nTheorem~\\ref{thm:pods-bound}.\n\n\\section{Discussion}\\label{sec:discussion}\nWe gave an efficient projection-based algorithm for tracking experts\nwith memory for which we proved the best known regret bound. We also\ngave an algorithm to compute relative entropy projection onto the\nsimplex with non-uniform (lower) box constraints exactly in\n$\\cO(\\nexperts)$ time, which may be of independent interest.\nWe showed that the weight-sharing equivalent of our projection-based\nalgorithm is in fact a geometrically-decaying mixing scheme for\n{\\em Mixing Past Posteriors}~\\cite{bousquet2002tracking}. Furthermore\nwe showed that this mixing scheme corresponds exactly\nto the previous best known result (the partition specialists algorithm with\nMarkov prior~\\cite{koolen2012putting}), and we therefore improved their bound.\nWe proved a guarantee favoring projection updates over\nweight-sharing when updating weights may incur costs, such as in\nportfolio optimization with proportional transaction costs. We are\ncurrently applying \\podsth\\ to this problem, primarily extending the\nwork of~\\cite{singer1997switching} in the sense of incorporating both\nthe assumption of ``memory'' and transaction costs.\n\nIn this work we focused on proving good regret bounds, which naturally\nrequired optimally-tuned parameters. A limitation of our work is that\nin practice the optimal parameters are unknown. \nThis is a common issue in online learning, and one may\nemploy standard techniques to address this such as the ``doubling\ntrick'', or by using a Bayesian mixture over parameters~\\cite{vovk1999derandomizing}.\nFor a prominent recent result in this area see~\\cite{jun2017improved}.\n\nFinally, the work of~\\cite{koolen2012putting} gave a Bayesian interpretation to\nMPP, however this is lost when one uses the projection update of\nPoDS. We ask: Is there also a Bayesian interpretation to these projection-based updates?\n\n\\paragraph{Ethical considerations.} While the scope of applicability of\nonline learning algorithms is wide, this research in regret-bounded\nonline learning is foundational in nature and we therefore cannot foresee the extent\nof any societal impacts (positive or negative) this research may have.  \n\n\n\\begin{ack}\nThis work was supported by the UK Engineering and Physical Sciences Research Council (EPSRC) grant EP/N509577/1.\n\\end{ack}\n\n\n\\bibliographystyle{abbrv}\n\n\n\n", "meta": {"timestamp": "2021-06-25T02:19:11", "yymm": "2106", "arxiv_id": "2106.13021", "language": "en", "url": "https://arxiv.org/abs/2106.13021"}}
